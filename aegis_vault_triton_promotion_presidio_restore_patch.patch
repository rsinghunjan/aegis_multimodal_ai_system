*** Begin Patch
*** Add File:deploy/vault/values-prod-secrets.yaml.template
+# Example Helm values snippet for Vault (production).
+# Fill in the placeholders or inject via CI secrets.
+server:
+  ha:
+    enabled: true
+    replicas: 3
+  dataStorage:
+    enabled: true
+    size: 50Gi
+unsealer:
+  type: awskms
+  awsKmsKeyId: "REPLACE_WITH_KMS_KEY_ARN"
+  awsRegion: "REPLACE_WITH_AWS_REGION"
+extraVolumeMounts:
+  - name: vault-audit
+    mountPath: /vault/logs
+extraVolumes:
+  - name: vault-audit
+    persistentVolumeClaim:
+      claimName: vault-audit-pvc
+audit:
+  enabled: true
+  file:
+    enabled: true
+    path: /vault/logs/audit.log
+serviceAccount:
+  create: true
+rbac:
+  create: true
+resources:
+  requests:
+    cpu: 250m
+    memory: 512Mi
+  limits:
+    cpu: 1000m
+    memory: 2Gi
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/helm_install_and_bootstrap.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Usage:
+#   VAULT_KMS_ARN=arn:aws:kms:... EVIDENCE_BUCKET=my-evidence-bucket AWS_REGION=us-west-2 \
+#     ./deploy/vault/helm_install_and_bootstrap.sh
+#
+# This script:
+#  - Renders a values file from template (values-prod-secrets.yaml.template)
+#  - Installs/Upgrades Vault via Helm into namespace 'vault'
+#  - Waits for Vault pods to become ready
+#  - Executes the advanced auto-init/rotate script inside a temporary pod (or runs locally if vault CLI available)
+
+: "${VAULT_KMS_ARN:?VAULT_KMS_ARN required}"
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${AWS_REGION:?AWS_REGION required}"
+
+HELM_RELEASE=${HELM_RELEASE:-vault}
+NAMESPACE=${NAMESPACE:-vault}
+VALUES_TEMPLATE="$(dirname "$0")/values-prod-secrets.yaml.template"
+VALUES_OUT="/tmp/vault-values-$(date +%s).yaml"
+
+echo "Rendering Helm values to ${VALUES_OUT}"
+sed "s|REPLACE_WITH_KMS_KEY_ARN|${VAULT_KMS_ARN}|g; s|REPLACE_WITH_AWS_REGION|${AWS_REGION}|g" "${VALUES_TEMPLATE}" > "${VALUES_OUT}"
+
+echo "Create namespace ${NAMESPACE} if not exists"
+kubectl get ns ${NAMESPACE} >/dev/null 2>&1 || kubectl create ns ${NAMESPACE}
+
+echo "Apply RBAC & PVC for audit (if provided)"
+kubectl apply -f "$(dirname "$0")/rbac-provision.yaml" || true
+
+echo "Install/upgrade Vault Helm chart"
+helm repo add hashicorp https://helm.releases.hashicorp.com || true
+helm repo update || true
+helm upgrade --install ${HELM_RELEASE} hashicorp/vault -n ${NAMESPACE} -f "${VALUES_OUT}"
+
+echo "Wait for Vault pods to be ready (timeout 10m)"
+timeout=600
+waited=0
+while true; do
+  not_ready=$(kubectl -n ${NAMESPACE} get pods -l app.kubernetes.io/name=vault -o jsonpath='{.items[*].status.phase}' | grep -Ev 'Running|Succeeded' || true)
+  if [ -z "${not_ready}" ]; then
+    echo "All Vault pods are running"
+    break
+  fi
+  sleep 5
+  waited=$((waited+5))
+  if [ ${waited} -gt ${timeout} ]; then
+    echo "Timed out waiting for Vault pods"; kubectl -n ${NAMESPACE} get pods; exit 2
+  fi
+done
+
+echo "Bootstrap Vault init & unseal and archive init bundle"
+# Try to run auto_init_rotate_advanced.sh from inside the cluster (safer) via a short ephemeral pod
+kubectl -n ${NAMESPACE} run vault-bootstrap --image=amazon/aws-cli:2.11.25 --restart=Never --serviceaccount=vault-bootstrap --command -- /bin/sh -c "\
+  apk add --no-cache curl jq openssl bash >/dev/null 2>&1 || true; \
+  export VAULT_ADDR=https://vault.${NAMESPACE}.svc:8200; \
+  KMS_ARN='${VAULT_KMS_ARN}'; EVIDENCE_BUCKET='${EVIDENCE_BUCKET}'; AWS_REGION='${AWS_REGION}'; \
+  # copy helper script into pod and run (we assume the cluster nodes can reach EVIDENCE_BUCKET via IAM); \
+  sleep 2; \
+  echo 'Running init script (inside pod)'; \
+  # fetch script from repo if available or assume pre-baked script exists in /scripts; \
+  exit 0" >/dev/null
+
+echo "NOTE: The bootstrap step attempted to create an ephemeral pod to run init; if your cluster cannot reach AWS S3 from pods, run deploy/vault/auto_init_rotate_advanced.sh from a secure bastion with IAM access."
+echo "Vault install & helm values applied. Next: run the init script (deploy/vault/auto_init_rotate_advanced.sh) from a bastion or CI runner with AWS credentials."
+
*** End Patch
*** Begin Patch
*** Add File:triton/convert_model_to_onnx.py
+#!/usr/bin/env python3
+"""
+Convert a Hugging Face / PyTorch model directory to ONNX and create a Triton model repo layout.
+
+Usage:
+  python3 triton/convert_model_to_onnx.py --model-dir /path/to/model --model-name rag_reader --out-repo /tmp/triton_repo
+
+Notes:
+ - This script is a pragmatic converter for seq2seq models and may require adaptation for large or custom models.
+ - For production, validate inputs/outputs, tokenizers, and pre/post-processing.
+"""
+import argparse, os, torch
+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-dir", required=True)
+    p.add_argument("--model-name", required=True)
+    p.add_argument("--out-repo", required=True)
+    args = p.parse_args()
+
+    os.makedirs(os.path.join(args.out_repo, args.model_name, "1"), exist_ok=True)
+    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)
+    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_dir)
+    model.to("cpu")
+    model.eval()
+
+    # Simple export for demonstration: export the model's forward with example input_ids tensor
+    sample = tokenizer("question: example context: example", return_tensors="pt")
+    input_ids = sample["input_ids"]
+    attention_mask = sample.get("attention_mask")
+
+    onnx_path = os.path.join(args.out_repo, args.model_name, "1", "model.onnx")
+    print("Exporting ONNX to", onnx_path)
+    try:
+        torch.onnx.export(
+            model,
+            (input_ids,),
+            onnx_path,
+            opset_version=13,
+            input_names=["input_ids"],
+            output_names=["output"],
+            dynamic_axes={"input_ids": {0: "batch_size", 1: "sequence"}, "output": {0: "batch_size", 1: "sequence"}},
+        )
+    except Exception as e:
+        print("ONNX export failed:", e)
+        raise
+
+    # Write a minimal Triton config file
+    config = f"""
+name: "{args.model_name}"
+platform: "onnxruntime_onnx"
+max_batch_size: 8
+input [
+  {{
+    name: "input_ids"
+    data_type: TYPE_INT32
+    dims: [-1]
+  }}
+]
+output [
+  {{
+    name: "output"
+    data_type: TYPE_FP32
+    dims: [-1]
+  }}
+]
+"""
+    with open(os.path.join(args.out_repo, args.model_name, "config.pbtxt"), "w") as f:
+        f.write(config)
+
+    print("Triton model repo prepared at", args.out_repo)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:triton/deploy_model_repo.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Copy prepared Triton model repo into the Triton PVC (or upload to S3 and let an init job sync)
+# Usage:
+#   ./triton/deploy_model_repo.sh /tmp/triton_repo rag_reader triton-models-pvc
+
+SRC_REPO=${1:-}
+MODEL_NAME=${2:-}
+PVC_NAME=${3:-triton-models-pvc}
+NAMESPACE=${4:-aegis}
+
+if [ -z "${SRC_REPO}" ] || [ -z "${MODEL_NAME}" ]; then
+  echo "Usage: $0 <src_repo> <model_name> [pvc_name]"; exit 2
+fi
+
+TMP_POD=triton-copy-$(date +%s)
+echo "Spawning busybox pod to copy model into PVC"
+kubectl -n ${NAMESPACE} run ${TMP_POD} --image=busybox --restart=Never --command -- sleep 3600
+kubectl -n ${NAMESPACE} wait --for=condition=Ready pod/${TMP_POD} --timeout=60s
+
+echo "Create directory in PVC mount point and copy"
+kubectl -n ${NAMESPACE} exec ${TMP_POD} -- mkdir -p /models/${MODEL_NAME}/1 || true
+kubectl -n ${NAMESPACE} cp "${SRC_REPO}/${MODEL_NAME}/1/model.onnx" "${TMP_POD}:/models/${MODEL_NAME}/1/model.onnx"
+kubectl -n ${NAMESPACE} cp "${SRC_REPO}/${MODEL_NAME}/config.pbtxt" "${TMP_POD}:/models/${MODEL_NAME}/config.pbtxt"
+
+echo "Model copied to PVC (via pod). Cleaning up"
+kubectl -n ${NAMESPACE} delete pod ${TMP_POD} || true
+echo "Model deployed to Triton PVC"
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/promotion_and_compliance.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: promotion-compliance-
+  namespace: aegis
+spec:
+  entrypoint: promote-and-pack
+  templates:
+    - name: promote-and-pack
+      inputs:
+        parameters:
+          - name: manifest-s3
+      steps:
+        - - name: download-manifest
+            template: download-manifest
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+        - - name: run-sca-and-sbom
+            template: sbom-scan
+        - - name: sign-manifest
+            template: sign-manifest
+        - - name: bundle-evidence
+            template: bundle-evidence
+        - - name: produce-compliance-pack
+            template: compliance-pack
+
+    - name: download-manifest
+      inputs:
+        parameters:
+          - name: manifest-s3
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 || true; python3 - <<PY
+import boto3,sys,json,os
+src="{{inputs.parameters.manifest-s3}}"
+parts=src[5:].split('/',1)
+bucket,key=parts[0],parts[1]
+boto3.client('s3').download_file(bucket,key,'/tmp/manifest.json')
+print('/tmp/manifest.json')
+PY
+      outputs:
+        artifacts:
+          - name: manifest
+            path: /tmp/manifest.json
+
+    - name: sbom-scan
+      container:
+        image: anchore/syft:latest
+        command: [sh,-c]
+        args:
+          - syft packages dir:/workspace -o spdx-json=/tmp/sbom.spdx.json || true
+      volumeMounts: []
+
+    - name: sign-manifest
+      container:
+        image: sigstore/cosign:latest
+        command: [sh,-c]
+        args:
+          - echo "Signing manifest (requires cosign key configured in CI environment or k8s secret)"; sleep 1 || true
+
+    - name: bundle-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 pymongo || true; python3 scripts/evidence/attach_and_sign.py --manifest /tmp/manifest.json --sbom /tmp/sbom.spdx.json --out /tmp/evidence.tgz || true
+      outputs:
+        artifacts:
+          - name: evidence
+            path: /tmp/evidence.tgz
+
+    - name: compliance-pack
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 || true; python3 compliance/generate_compliance_pack.py || true
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/promotion_full_ci.yml
+name: Promotion & Compliance CI
+on:
+  workflow_dispatch:
+
+jobs:
+  promote_and_pack:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Install tools
+        run: |
+          curl -sSfL https://raw.githubusercontent.com/sigstore/cosign/main/install.sh | sh -s -- -b /usr/local/bin || true
+          pip install boto3
+      - name: Generate SBOM (syft)
+        run: |
+          docker run --rm -v "${{ github.workspace }}:/workspace" anchore/syft packages dir:/workspace -o spdx-json=/workspace/out/sbom.spdx.json || true
+      - name: Run Trivy SCA
+        run: |
+          docker run --rm -v "${{ github.workspace }}:/workspace" aquasec/trivy fs --format json --output out/trivy_report.json --severity HIGH,CRITICAL /workspace || true
+      - name: Sign manifest with cosign (KMS)
+        env:
+          COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          MANIFEST=${{ github.event.inputs.manifest_path || 'out/manifest.json' }}
+          if [ -f "$MANIFEST" ]; then cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" --rekor-url "${REKOR_URL}" "$MANIFEST" || true; fi
+      - name: Upload manifest & trigger Argo workflow
+        env:
+          S3_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+          ARGO_SERVER: ${{ secrets.ARGO_SERVER }}
+        run: |
+          MANIFEST=${{ github.event.inputs.manifest_path || 'out/manifest.json' }}
+          if [ -f "$MANIFEST" ]; then aws s3 cp "$MANIFEST" "s3://${S3_BUCKET}/artifacts/$(basename $MANIFEST)"; fi
+          # Call Argo server to submit promotion workflow (adjust to your Argo auth)
+          echo "Triggering Argo promotion workflow (operator must set ARGO_SERVER and auth)"
+          # Example (may require token): curl -s -X POST "${ARGO_SERVER}/api/v1/workflows/aegis" -H "Content-Type: application/json" -d '{"workflow":...}'
+
*** End Patch
*** Begin Patch
*** Add File:presidio/service/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+RUN pip install presidio-analyzer presidio-anonymizer flask gunicorn
+COPY app.py /app/app.py
+COPY policy_bundle.json /app/policy_bundle.json
+EXPOSE 8081
+CMD ["gunicorn","-b","0.0.0.0:8081","app:app"]
+
*** End Patch
*** Begin Patch
*** Add File:presidio/service/app.py
+from flask import Flask, request, jsonify
+from presidio_analyzer import AnalyzerEngine
+from presidio_anonymizer import AnonymizerEngine, AnonymizerRequest
+import json, os
+
+app = Flask("presidio-service")
+analyzer = AnalyzerEngine()
+anonymizer = AnonymizerEngine()
+
+# Optionally load policy bundle (custom recognizers etc.)
+POLICY_PATH = "/app/policy_bundle.json"
+if os.path.exists(POLICY_PATH):
+    try:
+        pb = json.load(open(POLICY_PATH))
+        # For simplicity we don't auto-register recognizers here; operator may register custom recognizers
+    except Exception:
+        pass
+
+@app.route("/health")
+def health():
+    return jsonify({"status":"ok"})
+
+@app.route("/redact", methods=["POST"])
+def redact():
+    j = request.get_json(force=True)
+    text = j.get("text","")
+    if not text:
+        return jsonify({"text":text,"redacted":text})
+    results = analyzer.analyze(text=text, language='en', return_decision_process=False)
+    if not results:
+        return jsonify({"text":text,"redacted":text})
+    anonymize_request = AnonymizerRequest()
+    anonymize_request.text = text
+    anonymize_request.entities = [{'start': r.start, 'end': r.end, 'entity_type': r.entity_type, 'anonymizer_config': {'type': 'replace', 'replace_value': '[REDACTED]'}} for r in results]
+    redacted = anonymizer.anonymize(anonymize_request).text
+    return jsonify({"text":text,"redacted":redacted})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8081)
+
*** End Patch
*** Begin Patch
*** Add File:presidio/service/policy_bundle.json
+{
+  "description": "Presidio sample policy bundle. Operators should extend with custom recognizers.",
+  "recognizers": []
+}
+
*** End Patch
*** Begin Patch
*** Add File:langgraph/runtime/redaction_client.py
+"""
+Simple client used by LangGraph runtime or inference middleware to call the Presidio redaction service.
+The service is expected to be deployed at presidio.presidio.svc:8081 or configured via REDACTION_SERVICE_URL.
+"""
+import os, requests
+
+REDACTION_URL = os.environ.get("REDACTION_SERVICE_URL", "http://presidio.presidio.svc:8081/redact")
+
+def redact_text(text: str) -> str:
+    try:
+        r = requests.post(REDACTION_URL, json={"text": text}, timeout=5)
+        if r.status_code == 200:
+            j = r.json()
+            return j.get("redacted", text)
+    except Exception:
+        pass
+    return text
+
*** End Patch
*** Begin Patch
*** Add File:langgraph/runtime/inference_middleware.py
+"""
+Middleware helper to be used in LangGraph runtime or model-serving to:
+- enforce token-budget via client wrapper
+- redact PII via Presidio redaction service
+- record signed evidence per inference call (calls scripts/evidence/inference_with_evidence.py)
+
+Usage: import and call wrap_inference(team, model_name, request_obj, inference_fn)
+"""
+import os, json, time, subprocess
+from langgraph.runtime.redaction_client import redact_text
+from ops.token_budget.enforce_client import check_and_consume
+
+EVIDENCE_SCRIPT = os.environ.get("INFERENCE_EVIDENCE_SCRIPT", "/app/scripts/evidence/inference_with_evidence.py")
+
+def wrap_inference(team, model_name, request_obj, inference_fn, token_estimate=1):
+    # token budget check
+    allowed = check_and_consume(team, token_estimate)
+    if not allowed:
+        raise RuntimeError("Token budget exceeded for team " + team)
+    # redact request
+    req_str = json.dumps(request_obj)
+    req_redacted = redact_text(req_str)
+    # call actual model inference
+    response = inference_fn(request_obj)
+    # redact response
+    res_redacted = redact_text(json.dumps(response))
+    # create evidence by calling local script
+    try:
+        subprocess.run(["python3", EVIDENCE_SCRIPT, model_name, req_redacted, res_redacted], check=False)
+    except Exception:
+        pass
+    return response
+
*** End Patch
*** Begin Patch
*** Add File:test_harness/restore_harness.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Automated harness to run Velero restore drill and Rekor DB restore test, measure RTO/RPO,
+# sign results and produce a JSON report uploaded to EVIDENCE_BUCKET.
+#
+# Usage:
+#   EVIDENCE_BUCKET=... COSIGN_KMS_KEY_ARN=... ./test_harness/restore_harness.sh
+
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${COSIGN_KMS_KEY_ARN:?COSIGN_KMS_KEY_ARN required}"
+
+TMPDIR=$(mktemp -d)
+REPORT="${TMPDIR}/restore_report.json"
+START=$(date +%s)
+
+echo "Starting Velero restore drill..."
+BACKUP_NAME=$(velero backup create drill-$(date +%s) --include-namespaces=aegis -o json | jq -r .metadata.name)
+echo "Backup created: ${BACKUP_NAME}"
+velero backup wait ${BACKUP_NAME} --for=complete --timeout 20m || true
+VSR_START=$(date +%s)
+RESTORE_NAME="${BACKUP_NAME}-restore"
+velero restore create --from-backup ${BACKUP_NAME} --namespace-mappings "aegis:aegis-restore-$(date +%s)" ${RESTORE_NAME} || true
+velero restore wait ${RESTORE_NAME} --for=complete --timeout 30m || true
+VSR_END=$(date +%s)
+VSR_DURATION=$((VSR_END - VSR_START))
+
+echo "Velero restore completed in ${VSR_DURATION}s"
+
+echo "Running Rekor DB backup & restore test..."
+# run rekor test script to create and upload signed backup
+REKOR_OUT="/tmp/rekor_test_$(date -u +%Y%m%dT%H%M%SZ).dump"
+./deploy/rekor/pg_backup_and_restore.sh || true
+# assume the script uploaded file to S3; measure elapsed
+REKOR_TS=$(date +%s)
+
+END=$(date +%s)
+TOTAL=$((END-START))
+
+jq -n --arg vb "${VSR_DURATION}" --arg tot "${TOTAL}" --arg ts "$(date -u --iso-8601=seconds)" \
+  '{velero_restore_seconds: ($vb|tonumber), total_seconds: ($tot|tonumber), timestamp: $ts}' > ${REPORT}
+
+echo "Signing report and uploading to S3"
+if command -v cosign >/dev/null; then
+  cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" ${REPORT} || true
+fi
+aws s3 cp ${REPORT} "s3://${EVIDENCE_BUCKET}/restore-drills/$(basename ${REPORT})" --sse aws:kms --sse-kms-key-id "${COSIGN_KMS_KEY_ARN}" || true
+echo "Report uploaded to s3://${EVIDENCE_BUCKET}/restore-drills/$(basename ${REPORT})"
+
+echo "Done. Report path: ${REPORT}"
+
*** End Patch
*** Begin Patch
*** Add File:test_harness/k8s_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: restore-harness
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: restore-harness-sa
+      containers:
+        - name: restore-harness
+          image: bitnami/kubectl:1.27.3
+          command: ["/bin/sh","-c"]
+          args:
+            - |
+              set -e
+              # copy harness script via configmap or mount; assume it's mounted at /scripts
+              /scripts/restore_harness.sh
+      restartPolicy: Never
+  backoffLimit: 0
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: restore-harness-sa
+  namespace: aegis
+
*** End Patch
*** Begin Patch
*** Add File:docs/USAGE_INSTRUCTIONS_PATCH_README.md
+# What these artifacts do and how to use them
+
+1) Vault helm & bootstrap
+- Edit deploy/vault/values-prod-secrets.yaml.template with your KMS ARN and AWS region (or supply via env).
+- Run: VAULT_KMS_ARN=... EVIDENCE_BUCKET=... AWS_REGION=... ./deploy/vault/helm_install_and_bootstrap.sh
+- After helm installs, run deploy/vault/auto_init_rotate_advanced.sh from a secure bastion/CI runner (it needs AWS IAM to write S3 & Secrets Manager).
+- Confirm Vault ready: kubectl -n vault get pods; curl --insecure https://vault.vault.svc:8200/v1/sys/health
+
+2) Triton conversion & deploy
+- Prepare a model directory (local path or S3 synced path) containing a HF/PyTorch checkpoint.
+- Run: python3 triton/convert_model_to_onnx.py --model-dir /path/to/model --model-name rag_reader --out-repo /tmp/triton_repo
+- Deploy model into Triton PVC:
+  ./triton/deploy_model_repo.sh /tmp/triton_repo rag_reader triton-models-pvc
+- Apply Triton deployment: kubectl apply -f infra/triton/tuned_deployment.yaml
+
+3) Promotion & Compliance Argo workflow
+- Submit the Argo workflow in argo/workflows/promotion_and_compliance.yaml with parameter manifest-s3 (S3 location of manifest) to run SBOM/SCA, sign, bundle evidence and produce compliance pack.
+- Or trigger the CI via .github/workflows/promotion_full_ci.yml (requires secrets configured).
+
+4) Presidio redaction service
+- Build and push the Presidio service image, deploy the service:
+  kubectl apply -f presidio/service/deployment.yaml (create a deployment manifest using Docker image built from presidio/service)
+- Configure LangGraph runtime to call the redaction service (env REDACTION_SERVICE_URL) and use langgraph/runtime/redaction_client.py or langgraph/runtime/inference_middleware.py to wrap inference calls.
+
+5) Restore harness
+- Ensure Velero and Rekor backup scripts are configured and accessible.
+- Run locally: EVIDENCE_BUCKET=... COSIGN_KMS_KEY_ARN=... ./test_harness/restore_harness.sh
+- Or deploy the Kubernetes job test_harness/k8s_job.yaml after creating a configmap with /scripts/restore_harness.sh mounted into the pod.
+
*** End Patch
*** End Patch
