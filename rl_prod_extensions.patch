*** Begin Patch
*** Add File:onnx/hardened_export_and_validate.py
+#!/usr/bin/env python3
+"""
+Hardened ONNX export + validation for NLP (seq2seq / decoder) and CV (Detectron2) models.
+Usage examples:
+  python3 onnx/hardened_export_and_validate.py --model-type nlp --model-dir /path/to/hf-model --out /tmp/model.onnx
+  python3 onnx/hardened_export_and_validate.py --model-type cv --cfg /path/to/config.yaml --weights /path/to/model.pth --out /tmp/cv.onnx
+
+The script:
+ - Exports ONNX using model-specific exporter
+ - Validates the ONNX using onnxruntime (runs a small inference)
+ - Writes a small validation report (JSON) alongside the ONNX file
+"""
+import argparse
+import os
+import json
+import subprocess
+import sys
+
+def export_nlp(model_dir, out_path):
+    from triton.hardened_export_decoder import export_decoder
+    export_decoder(model_dir, out_path)
+
+def export_cv(cfg, weights, out_path):
+    from triton.hardened_convert_detectron2 import export_detectron
+    export_detectron(cfg, weights, out_path)
+
+def validate_onnx(onnx_path, model_type):
+    try:
+        import onnxruntime as ort
+        import numpy as np
+    except Exception as e:
+        print("Install onnxruntime and numpy to validate:", e)
+        return {"validated": False, "error": "onnxruntime missing"}
+
+    sess = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
+    inputs = sess.get_inputs()
+    feed = {}
+    # Build simple synthetic input according to type
+    if model_type == "nlp":
+        # assume input_ids int32 shape [1,8]
+        import numpy as _np
+        feed_name = inputs[0].name
+        feed[feed_name] = _np.zeros((1,8), dtype=_np.int32)
+    else:
+        # cv: assume input shape [1,3,H,W]; choose H=W=800 or from input dims if available
+        import numpy as _np
+        shape = inputs[0].shape
+        # replace dynamic dims with small values
+        s = [1 if (d is None or isinstance(d,str) or d < 1) else int(d) for d in shape]
+        if len(s) < 4:
+            s = [1,3,800,800]
+        feed[inputs[0].name] = _np.zeros(tuple(s), dtype=_np.float32)
+    try:
+        out = sess.run(None, feed)
+        return {"validated": True, "outputs": [o.shape for o in out]}
+    except Exception as e:
+        return {"validated": False, "error": str(e)}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-type", choices=["nlp","cv"], required=True)
+    p.add_argument("--model-dir", help="HF model dir (for nlp)")
+    p.add_argument("--cfg", help="Detectron2 cfg (for cv)")
+    p.add_argument("--weights", help="Detectron2 weights (for cv)")
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+
+    out_path = args.out
+    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
+
+    if args.model_type == "nlp":
+        if not args.model_dir:
+            print("model-dir required for nlp")
+            sys.exit(2)
+        export_nlp(args.model_dir, out_path)
+    else:
+        if not (args.cfg and args.weights):
+            print("cfg and weights required for cv")
+            sys.exit(2)
+        export_cv(args.cfg, args.weights, out_path)
+
+    report = validate_onnx(out_path, args.model_type)
+    report_path = out_path + ".validation.json"
+    with open(report_path, "w") as f:
+        json.dump(report, f, indent=2)
+    print("Export complete. ONNX:", out_path, "validation:", report_path)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:triton/generate_triton_config.py
+#!/usr/bin/env python3
+"""
+Generate a tuned Triton config.pbtxt for a model depending on model family.
+Supports ONNX runtime and TensorRT (for NVIDIA GPU) targets.
+
+Usage:
+  python3 triton/generate_triton_config.py --model-name mymodel --out /tmp/mymodel_cfg --platform onnxruntime_onnx --gpu true --max-batch 8 --memory-mb 16384 --input-names input_ids --input-dtypes INT32 --input-dims [-1]
+"""
+import argparse, os, json
+
+def build_config(args):
+    # build a simple config dict and write minimal pbtxt content
+    inputs = []
+    for i_name, i_dtype, i_dims in zip(args.input_names, args.input_dtypes, args.input_dims):
+        inputs.append({"name": i_name, "data_type": i_dtype, "dims": i_dims})
+    cfg = {
+        "name": args.model_name,
+        "platform": args.platform,
+        "max_batch_size": args.max_batch,
+        "instance_group": [{"count": 1 if args.gpu else 0, "kind": "KIND_GPU" if args.gpu else "KIND_CPU"}],
+        "inputs": inputs,
+        "outputs": [{"name": "output", "data_type": "TYPE_FP32", "dims": [-1]}],
+        "dynamic_batching": {"preferred_batch_size": [4,8], "max_queue_delay_microseconds": 1000},
+        "instance_group_memory_mb": args.memory_mb,
+        "optimization": {"enable_tensorrt": args.tensorrt}
+    }
+    return cfg
+
+def write_pbtxt(cfg, out_dir):
+    os.makedirs(out_dir, exist_ok=True)
+    pb = []
+    pb.append(f'name: "{cfg["name"]}"')
+    pb.append(f'platform: "{cfg["platform"]}"')
+    pb.append(f"max_batch_size: {cfg['max_batch_size']}")
+    # instance_group
+    if cfg["instance_group"][0]["count"] > 0:
+        pb.append('instance_group [ { count: 1 kind: KIND_GPU } ]')
+    else:
+        pb.append('instance_group [ { count: 1 kind: KIND_CPU } ]')
+    # inputs
+    pb.append("input [")
+    for inp in cfg["inputs"]:
+        pb.append("  {")
+        pb.append(f'    name: "{inp["name"]}"')
+        pb.append(f'    data_type: {inp["data_type"]}')
+        dims = " ".join(str(d) for d in inp["dims"])
+        pb.append(f"    dims: [ {dims} ]")
+        pb.append("  }")
+    pb.append("]")
+    # outputs
+    pb.append("output [ { name: \"output\" data_type: TYPE_FP32 dims: [ -1 ] } ]")
+    pb.append("dynamic_batching { preferred_batch_size: [ 4, 8 ] max_queue_delay_microseconds: 1000 }")
+    pb.append(f"instance_group_memory_mb: {cfg['instance_group_memory_mb']}")
+    path = os.path.join(out_dir, "config.pbtxt")
+    with open(path, "w") as f:
+        f.write("\n".join(pb))
+    print("Wrote Triton config to", path)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-name", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--platform", default="onnxruntime_onnx")
+    p.add_argument("--gpu", action="store_true")
+    p.add_argument("--max-batch", type=int, default=8)
+    p.add_argument("--memory-mb", type=int, default=8192)
+    p.add_argument("--tensorrt", action="store_true")
+    p.add_argument("--input-names", nargs="+", default=["input_ids"])
+    p.add_argument("--input-dtypes", nargs="+", default=["TYPE_INT32"])
+    p.add_argument("--input-dims", nargs="+", type=lambda s: json.loads(s), default=["[-1]"])
+    args = p.parse_args()
+    cfg = build_config(args)
+    write_pbtxt(cfg, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:rl/replay/s3_parquet_replay.py
+#!/usr/bin/env python3
+"""
+Replay buffer builder (S3 -> Parquet)
+ - Scans a prefix of episode JSON files under s3://{EVIDENCE_BUCKET}/rl/collect/{run}/
+ - Loads them and writes a single Parquet file containing transitions (obs, act, reward, next_obs, done)
+ - Writes a manifest JSON pointing to the Parquet file(s)
+
+Usage:
+  EVIDENCE_BUCKET=my-bucket python3 rl/replay/s3_parquet_replay.py --prefix rl/collect/<run> --out-prefix rl/replay/
+"""
+import argparse, os, json, tempfile
+import boto3
+import pyarrow as pa
+import pyarrow.parquet as pq
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def list_objects(prefix):
+    s3 = boto3.client("s3")
+    paginator = s3.get_paginator("list_objects_v2")
+    objs = []
+    for page in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=prefix):
+        for o in page.get("Contents", []):
+            objs.append(o["Key"])
+    return objs
+
+def download_to_tmp(s3_key, tmpdir):
+    s3 = boto3.client("s3")
+    local = os.path.join(tmpdir, os.path.basename(s3_key))
+    s3.download_file(EVIDENCE_BUCKET, s3_key, local)
+    return local
+
+def build_parquet(prefix, out_prefix):
+    tmp = tempfile.mkdtemp()
+    keys = list_objects(prefix)
+    all_rows = []
+    for k in keys:
+        if not k.endswith(".json"):
+            continue
+        local = download_to_tmp(k, tmp)
+        with open(local) as f:
+            j = json.load(f)
+        obs = j.get("obs", [])
+        acts = j.get("acts", [])
+        rews = j.get("rews", [])
+        for i in range(len(acts)):
+            row = {
+                "obs": obs[i],
+                "act": acts[i],
+                "rew": rews[i],
+                "ts": j.get("ts")
+            }
+            all_rows.append(row)
+    if not all_rows:
+        print("No transitions found under", prefix)
+        return None
+    # build Arrow table
+    table = pa.Table.from_pylist(all_rows)
+    out_parquet = os.path.join(tmp, "replay.parquet")
+    pq.write_table(table, out_parquet)
+    # upload
+    s3 = boto3.client("s3")
+    dest_key = out_prefix.rstrip("/") + "/replay-" + str(int(tempfile.mkstemp()[1])) + ".parquet"
+    s3.upload_file(out_parquet, EVIDENCE_BUCKET, dest_key)
+    manifest = {"parquet": dest_key, "count": len(all_rows)}
+    manifest_key = out_prefix.rstrip("/") + "/manifest-" + str(int(tempfile.mkstemp()[1])) + ".json"
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=manifest_key, Body=json.dumps(manifest))
+    print("Wrote replay parquet:", dest_key, "manifest:", manifest_key)
+    return {"parquet": dest_key, "manifest": manifest_key}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--prefix", required=True, help="S3 prefix containing episode JSONs (without bucket)")
+    p.add_argument("--out-prefix", required=True, help="S3 prefix to write replay material")
+    args = p.parse_args()
+    if not EVIDENCE_BUCKET:
+        print("EVIDENCE_BUCKET not set")
+        return
+    build_parquet(args.prefix, args.out_prefix)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:rl/replay/redis_streams_replay.py
+#!/usr/bin/env python3
+"""
+Simple Redis Streams-based replay writer/reader (PoC).
+ - Producer: writes transitions into Redis stream rl:transitions
+ - Consumer: groups transitions into minibatches or windows
+
+Usage:
+  python3 rl/replay/redis_streams_replay.py --mode producer --episodes 10
+  python3 rl/replay/redis_streams_replay.py --mode consumer
+"""
+import argparse, time, json, os, uuid
+import redis
+import gym
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis.aegis.svc:6379/0")
+
+def producer(env_name, episodes):
+    r = redis.Redis.from_url(REDIS_URL)
+    env = gym.make(env_name)
+    for ep in range(episodes):
+        obs = env.reset()
+        done = False
+        while not done:
+            action = env.action_space.sample()
+            next_obs, reward, done, info = env.step(action)
+            rec = {"obs": obs.tolist() if hasattr(obs,"tolist") else obs, "act": int(action), "rew": float(reward), "done": bool(done)}
+            r.xadd("rl:transitions", rec)
+            obs = next_obs
+        print("Produced episode", ep)
+
+def consumer(block=1000):
+    r = redis.Redis.from_url(REDIS_URL)
+    # simple consumer: read last N
+    last_id = "0-0"
+    while True:
+        res = r.xread({"rl:transitions": last_id}, block=block, count=100)
+        if not res:
+            print("No new transitions")
+            time.sleep(1)
+            continue
+        for stream, messages in res:
+            for msg_id, fields in messages:
+                print("Transition", msg_id, fields)
+                last_id = msg_id
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--mode", choices=["producer","consumer"], required=True)
+    p.add_argument("--env", default="CartPole-v1")
+    p.add_argument("--episodes", type=int, default=5)
+    args = p.parse_args()
+    if args.mode == "producer":
+        producer(args.env, args.episodes)
+    else:
+        consumer()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/rl/collector_scaled_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rl-collector-scaled-
+  namespace: aegis
+spec:
+  entrypoint: collector-scale
+  arguments:
+    parameters:
+      - name: parallelism
+        value: "4"
+      - name: env
+        value: "CartPole-v1"
+      - name: episodes-per
+        value: "10"
+  templates:
+    - name: collector-scale
+      steps:
+        - - name: launch-collectors
+            template: collector
+            arguments:
+              parameters:
+                - name: index
+                  value: "{{item}}"
+      # generate a list [0,1,...,N-1]
+      withParam: '{{=sprig.splitList(",", (seq int(0) (int(workflow.parameters.parallelism) -1) ))}}'
+
+    - name: collector
+      inputs:
+        parameters:
+          - name: index
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [python3]
+        args: ["rl/collector/collector.py", "--env", "{{workflow.parameters.env}}", "--episodes", "{{workflow.parameters.episodes-per}}", "--run-id", "collector-{{inputs.parameters.index}}-{{workflow.name}}"]
+        resources:
+          limits:
+            cpu: "1"
+            memory: "1Gi"
+
*** End Patch
*** Begin Patch
*** Add File:rl/trainer/rllib_trainer.py
+#!/usr/bin/env python3
+"""
+RLlib-based distributed trainer (PoC)
+ - Uses Ray RLlib to run distributed training (local or cluster)
+ - Checkpoints to S3 and registers manifests to the registry API
+ - This is a scaffold: in production run Ray cluster or Ray on K8s
+
+Usage:
+  python3 rl/trainer/rllib_trainer.py --config /tmp/rllib_config.json --timesteps 100000 --s3-checkpoint-prefix rl/checkpoints/
+"""
+import argparse, json, os, time, subprocess
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+REGISTRY_URL = os.environ.get("REGISTRY_URL", "")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN", "")
+
+def upload_to_s3(local, s3_key):
+    if not EVIDENCE_BUCKET:
+        print("No EVIDENCE_BUCKET; skipping upload")
+        return
+    import boto3
+    s3 = boto3.client("s3")
+    s3.upload_file(local, EVIDENCE_BUCKET, s3_key)
+    print("Uploaded", local, "->", s3_key)
+
+def register_manifest(manifest):
+    if not REGISTRY_URL:
+        print("No REGISTRY_URL; skipping registration")
+        return
+    import requests
+    try:
+        r = requests.post(f"{REGISTRY_URL}/api/artifacts", json=manifest, timeout=10)
+        print("Registry result:", r.status_code)
+    except Exception as e:
+        print("Registry post failed:", e)
+
+def run_rllib_train(config_path, timesteps, s3_prefix):
+    # This is a minimal example using local Ray (not production). For real distributed training, run Ray cluster.
+    import ray
+    from ray import tune
+    from ray.rllib.agents.ppo import PPOTrainer
+    ray.init(ignore_reinit_error=True)
+    config = json.load(open(config_path))
+    tune.register_env("cartpole", lambda cfg: __import__("gym").make("CartPole-v1"))
+    analysis = tune.run(PPOTrainer, config=config, stop={"timesteps_total": timesteps}, checkpoint_at_end=True)
+    chk = analysis.get_best_checkpoint(metric="episode_reward_mean", mode="max")
+    print("Best checkpoint:", chk)
+    # upload checkpoint dir (tar)
+    tmp = "/tmp/rllib_checkpoint.tar.gz"
+    subprocess.run(["tar","czf", tmp, "-C", os.path.dirname(chk), os.path.basename(chk)], check=False)
+    key = s3_prefix.rstrip("/") + "/rllib_checkpoint_{}.tar.gz".format(int(time.time()))
+    upload_to_s3(tmp, key)
+    manifest = {"artifact_id": "rllib-"+str(int(time.time())), "checkpoint": key, "ts": int(time.time())}
+    register_manifest(manifest)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--config", required=True)
+    p.add_argument("--timesteps", type=int, default=100000)
+    p.add_argument("--s3-checkpoint-prefix", default="rl/checkpoints")
+    args = p.parse_args()
+    run_rllib_train(args.config, args.timesteps, args.s3_checkpoint_prefix)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/rl/rllib_distributed_trainer.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rllib-train-
+  namespace: aegis
+spec:
+  entrypoint: rllib-train
+  templates:
+    - name: rllib-train
+      container:
+        image: ghcr.io/yourorg/aegis-rl-rllib:latest
+        command: [python3]
+        args: ["rl/trainer/rllib_trainer.py","--config","/workspace/config.json","--timesteps","200000","--s3-checkpoint-prefix","rl/checkpoints/{{workflow.name}}"]
+      volumeMounts:
+        - name: workspace
+          mountPath: /workspace
+      volumes:
+        - name: workspace
+          emptyDir: {}
+      resources:
+        limits:
+          nvidia.com/gpu: 1
+          memory: "16Gi"
+          cpu: "4"
+
*** End Patch
*** Begin Patch
*** Add File:rl/safety/safety_checks.py
+#!/usr/bin/env python3
+"""
+Safety checks for RL policies (PoC)
+ - Runs deterministic evaluation and checks for safety patterns (e.g., out-of-bound actions, reward spikes)
+ - Produces a JSON report and exits non-zero on failure
+"""
+import argparse, json, os, sys
+from rl.eval.evaluator import evaluate
+
+def run_safety(model_path, env, episodes, min_reward=None, max_reward=None):
+    out_path = "/tmp/rl_safety_report.json"
+    res = evaluate(model_path, env, episodes, out_path)
+    avg = res.get("avg_reward", 0.0)
+    ok = True
+    reasons = []
+    if min_reward is not None and avg < min_reward:
+        ok = False
+        reasons.append(f"avg_reward {avg} < min_required {min_reward}")
+    if max_reward is not None and avg > max_reward:
+        # sometimes unusually high rewards may indicate exploit of environment; keep as warning
+        reasons.append(f"avg_reward {avg} > max_expected {max_reward}")
+    report = {"ok": ok, "avg_reward": avg, "reasons": reasons}
+    with open(out_path, "w") as f:
+        json.dump(report, f, indent=2)
+    print("Safety report written to", out_path)
+    if not ok:
+        print("Safety checks failed:", reasons)
+        sys.exit(2)
+    return out_path
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--model", required=True)
+    p.add_argument("--env", default="CartPole-v1")
+    p.add_argument("--episodes", type=int, default=10)
+    p.add_argument("--min-reward", type=float, default=None)
+    args = p.parse_args()
+    run_safety(args.model, args.env, args.episodes, args.min_reward)
+
*** End Patch
*** Begin Patch
*** Add File:argo/rl/rl_safety_gate.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rl-safety-gate-
+  namespace: aegis
+spec:
+  entrypoint: safety-gate
+  templates:
+    - name: safety-gate
+      inputs:
+        parameters:
+          - name: model-s3
+          - name: manifest-s3
+      dag:
+        tasks:
+          - name: fetch-model
+            template: aws-sync
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{inputs.parameters.model-s3}}"
+                - name: dst
+                  value: "/workspace/model"
+          - name: safety-checks
+            template: safety-checks
+            dependencies: [fetch-model]
+            arguments:
+              parameters:
+                - name: model-path
+                  value: "/workspace/model/model.zip"
+          - name: bundle-evidence
+            template: bundle-evidence
+            dependencies: [safety-checks]
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+                - name: model-s3
+                  value: "{{inputs.parameters.model-s3}}"
+          - name: promote
+            template: promote
+            dependencies: [bundle-evidence]
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+
+    - name: aws-sync
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: amazon/aws-cli:latest
+        command: [sh, -c]
+        args:
+          - aws s3 cp --recursive "{{inputs.parameters.src}}" "{{inputs.parameters.dst}}"
+
+    - name: safety-checks
+      inputs:
+        parameters:
+          - name: model-path
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [python3]
+        args: ["rl/safety/safety_checks.py","--model","{{inputs.parameters.model-path}}","--env","CartPole-v1","--episodes","10","--min-reward","20"]
+
+    - name: bundle-evidence
+      inputs:
+        parameters:
+          - name: manifest-s3
+          - name: model-s3
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [sh, -c]
+        args:
+          - |
+            # produce a simple evidence bundle linking model and manifest, then sign and upload (reuse cosign if present)
+            set -e
+            TMP=/tmp/rl_evidence_$$
+            mkdir -p $TMP
+            aws s3 cp "{{inputs.parameters.manifest-s3}}" $TMP/manifest.json || true
+            aws s3 cp --recursive "{{inputs.parameters.model-s3}}" $TMP/model || true
+            tar czf $TMP/evidence.tgz -C $TMP .
+            if [ -n "${COSIGN_KMS_KEY_ARN:-}" ]; then
+              cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" $TMP/evidence.tgz || true
+            fi
+            aws s3 cp $TMP/evidence.tgz s3://${EVIDENCE_BUCKET}/rl/evidence/ || true
+
+    - name: promote
+      inputs:
+        parameters:
+          - name: manifest-s3
+      container:
+        image: curlimages/curl:7.73.0
+        command: [sh, -c]
+        env:
+          - name: WEBHOOK
+            value: "{{workflow.parameters.webhook_url}}"
+        args:
+          - |
+            # Notify promotion webhook (it will run the standard promotion pipeline)
+            curl -X POST -H "Content-Type: application/json" -d '{"type":"promotion","manifest_s3":"{{inputs.parameters.manifest-s3}}"}' ${WEBHOOK} || true
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_rules_rl.yaml
+groups:
+- name: aegis-rl.rules
+  rules:
+  - alert: AegisRLEvidenceMissing
+    expr: increase(aegis_rl_eval_episodes[1h]) == 0
+    for: 30m
+    labels:
+      severity: warning
+    annotations:
+      summary: "No RL evaluation evidence in the last hour"
+      description: "No evaluation artifacts exported (aegis_rl_eval_episodes == 0)."
+
+  - alert: AegisRLAvgRewardBelowSLO
+    expr: aegis_rl_avg_reward < 20
+    for: 15m
+    labels:
+      severity: page
+    annotations:
+      summary: "RL average reward is below SLO"
+      description: "Average reward (aegis_rl_avg_reward) is below configured SLO threshold."
+
+  - alert: AegisRLReplayLagging
+    expr: absent(aegis_rl_replay_size) or aegis_rl_replay_size < 100
+    for: 30m
+    labels:
+      severity: warning
+    annotations:
+      summary: "RL replay buffer low"
+      description: "Replay buffer size is below expected threshold; collectors may be down."
+
*** End Patch
*** Begin Patch
*** Add File:docs/rl_slos_and_alerting_playbook.md
+# RL SLOs & Alerting Playbook
+
+Purpose
+- Document sensible SLOs and what to do when RL alerts fire.
+
+Key SLOs (examples)
+- aegis_rl_avg_reward >= 20 (warning if < 20 for 15m)
+- aegis_rl_eval_episodes > 0 per hour (warning if none for 1h)
+- aegis_rl_replay_size >= 100 transitions (warning if below threshold)
+
+Prometheus setup
+- Add `monitoring/prometheus_rules_rl.yaml` to PrometheusRule CRD or load into Prometheus alerting rules config.
+
+On-call actions
+- AegisRLEvidenceMissing:
+  - Check collector pods logs (kubectl -n aegis logs -l app=rl-collector)
+  - Verify S3 access: aws s3 ls s3://$EVIDENCE_BUCKET/rl/collect/
+- AegisRLAvgRewardBelowSLO:
+  - Run deterministic eval on the latest checkpoint (use rl/eval/evaluator.py)
+  - If failure confirmed, create an incident & rollback to prior policy
+- AegisRLReplayLagging:
+  - Check Redis / S3 replay ingestion, restart collectors if necessary
+  - Re-run replay builder: python3 rl/replay/s3_parquet_replay.py --prefix rl/collect/<run> --out-prefix rl/replay/
+
+Promotion gating
+- Safety gate (argo/rl/rl_safety_gate.yaml) will run safety tests and only call the promotion webhook if checks pass.
+
*** End Patch
*** End Patch
