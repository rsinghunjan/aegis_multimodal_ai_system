*** Begin Patch
*** Add File: scripts/llm_client.py
+#!/usr/bin/env python3
+"""
+LLM client adapter with pluggable providers.
+
+Supported providers:
+ - openai: uses OPENAI_API_KEY and OpenAI Chat Completions API.
+ - self_hosted: posts {"prompt": "..."} to LLM_ENDPOINT and expects JSON {"response": "..."}.
+ - mock: returns a canned response (useful for local testing).
+
+Environment variables:
+ - LLM_PROVIDER (openai | self_hosted | mock)  default: openai if OPENAI_API_KEY present, else self_hosted
+ - OPENAI_API_KEY
+ - OPENAI_MODEL (optional, default "gpt-4o")
+ - LLM_ENDPOINT (for self_hosted)
+ - LLM_API_KEY (optional header for self_hosted)
+
+This is a lightweight adapter; extend with retries, auth, or provider-specific features as needed.
+"""
+import os
+import requests
+import json
+from typing import Optional
+
+LLM_PROVIDER = os.getenv("LLM_PROVIDER", "")
+OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
+OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
+LLM_ENDPOINT = os.getenv("LLM_ENDPOINT", "")
+LLM_API_KEY = os.getenv("LLM_API_KEY", "")
+
+class LLMClient:
+    def __init__(self):
+        prov = LLM_PROVIDER.strip().lower()
+        if prov:
+            self.provider = prov
+        else:
+            if OPENAI_API_KEY:
+                self.provider = "openai"
+            elif LLM_ENDPOINT:
+                self.provider = "self_hosted"
+            else:
+                self.provider = "mock"
+
+    def generate(self, prompt: str, max_tokens: int = 800, temperature: float = 0.2) -> str:
+        if self.provider == "openai":
+
î€€
