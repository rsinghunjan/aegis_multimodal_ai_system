*** Begin Patch
*** Add File:milvus/helm/values-production.yaml
+# Production values for Milvus Helm chart (example)
+# Replace placeholders and tune for your environment (storage class, resource sizes, replicas).
+cluster:
+  enabled: true
+  mode: cluster
+
+meta:
+  pod:
+    resources:
+      limits:
+        cpu: "2"
+        memory: "4Gi"
+      requests:
+        cpu: "500m"
+        memory: "1Gi"
+
+proxy:
+  replicaCount: 2
+  resources:
+    requests:
+      cpu: "200m"
+      memory: "512Mi"
+    limits:
+      cpu: "1"
+      memory: "1Gi"
+
+data:
+  wal:
+    enabled: true
+  storage:
+    type: persistentVolumeClaim
+    persistentVolumeClaim:
+      accessModes: [ "ReadWriteOnce" ]
+      resources:
+        requests:
+          storage: 200Gi
+      storageClassName: "REPLACE_WITH_STORAGECLASS"
+
+indexer:
+  resources:
+    requests:
+      cpu: "2"
+      memory: "8Gi"
+    limits:
+      cpu: "4"
+      memory: "32Gi"
+
+persistence:
+  enabled: true
+
+service:
+  type: ClusterIP
+
+ingress:
+  enabled: false
+
+securityContext:
+  runAsUser: 1000
+  runAsGroup: 1000
+
+# Tune JVM / Milvus params via configMap if needed
+
*** End Patch
*** Begin Patch
*** Add File:milvus/helm/pvc-example.yaml
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: milvus-data-pvc
+  namespace: milvus
+spec:
+  accessModes:
+    - ReadWriteOnce
+  resources:
+    requests:
+      storage: 200Gi
+  storageClassName: "REPLACE_WITH_STORAGECLASS"
+
*** End Patch
*** Begin Patch
*** Add File:rag/k8s/ingress_tls_certmanager.yaml
+apiVersion: networking.istio.io/v1alpha3
+kind: Gateway
+metadata:
+  name: rag-gateway
+  namespace: aegis-rag
+spec:
+  selector:
+    istio: ingressgateway
+  servers:
+    - port:
+        number: 443
+        name: https
+        protocol: HTTPS
+      tls:
+        mode: SIMPLE
+        credentialName: rag-tls  # cert-manager will create this secret
+      hosts:
+        - "rag.example.com"
+
+---
+# Certificate (cert-manager) - use ClusterIssuer corporate CA or letsencrypt as appropriate
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: rag-cert
+  namespace: aegis-rag
+spec:
+  secretName: rag-tls
+  issuerRef:
+    name: corporate-ca-issuer
+    kind: ClusterIssuer
+  dnsNames:
+    - "rag.example.com"
+
*** End Patch
*** Begin Patch
*** Add File:rag/k8s/service_monitor.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: rag-flask-servicemonitor
+  namespace: monitoring
+spec:
+  selector:
+    matchLabels:
+      app: rag-flask
+  namespaceSelector:
+    matchNames:
+      - aegis-rag
+  endpoints:
+    - port: http
+      path: /metrics
+      interval: 15s
+
*** End Patch
*** Begin Patch
*** Add File:llm/training/deepspeed_k8s_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-llm-finetune-deepspeed
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-finetune-sa
+      containers:
+        - name: deepspeed-finetune
+          image: ghcr.io/yourorg/aegis-llm-finetune:latest
+          command: ["/bin/sh","-c"]
+          args:
+            - |
+              set -e
+              # Run accelerate+deepspeed (single-node GPU). For multi-node, run via MPI or a distributed launcher.
+              accelerate launch --config_file /opt/finetune/accelerate_config.yaml deepspeed_accelerate_finetune.py
+          env:
+            - name: MODEL_NAME_OR_PATH
+              value: "gpt2"
+            - name: TRAIN_FILE
+              value: "/data/train.jsonl"
+            - name: USE_PEFT
+              value: "1"
+            - name: DS_CONFIG
+              value: "/opt/finetune/ds_config.json"
+            - name: MLFLOW_TRACKING_URI
+              value: "http://mlflow.aegis.svc.cluster.local"
+            - name: MLFLOW_EXPERIMENT
+              value: "aegis-llm-finetune"
+            - name: COSIGN_KMS_KEY_ARN
+              valueFrom:
+                secretKeyRef:
+                  name: cosign-kms
+                  key: COSIGN_KMS_KEY_ARN
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "8"
+              memory: "32Gi"
+            requests:
+              cpu: "4"
+              memory: "16Gi"
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/llm_finetune_smoke.yml
+name: LLM Finetune Smoke Test (mock)
+
+on:
+  workflow_dispatch:
+  pull_request:
+    paths:
+      - 'llm/**'
+      - 'training/**'
+
+jobs:
+  smoke:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install transformers datasets accelerate peft deepspeed sentence-transformers mlflow
+      - name: Run small local finetune (mock)
+        env:
+          MODEL_NAME_OR_PATH: "sshleifer/tiny-gpt2"
+          TRAIN_FILE: "llm/training/mock_train.jsonl"
+          USE_PEFT: "1"
+          MLFLOW_TRACKING_URI: ""
+        run: |
+          # create a tiny mock dataset
+          python - <<PY
+from datasets import Dataset
+ds = Dataset.from_list([{"text":"hello world"}]*10)
+ds.to_json("llm/training/mock_train.jsonl", orient="records", lines=True)
+PY
+          python3 llm/training/deepspeed_accelerate_finetune.py
+      - name: Archive artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: llm-smoke-artifacts
+          path: outputs || true
+
*** End Patch
*** Begin Patch
*** Add File:llm/eval/hallucination_eval.py
+#!/usr/bin/env python3
+"""
+Hallucination & factuality evaluation harness:
+ - Takes a QA dataset (SQuAD-style or custom JSONL of {context,question,answer})
+ - Runs RAG pipeline (embedding retrieval + LLM answer)
+ - Computes simple QA F1 and exact match metrics
+ - Uploads evaluation report to s3://<EVIDENCE_BUCKET>/llm/eval/<run_ts>.json
+ - Emits basic metrics (prints) that can be picked up by CI or Prometheus pushgateway
+"""
+import os, json, time
+import boto3
+from collections import Counter
+from rag.flask_rag_service import embed_query, milvus_search, call_llm  # reuse functions when run in cluster image
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "REPLACE_WITH_AWS_REGION")
+
+def normalize_answer(s):
+    return " ".join(s.lower().split())
+
+def f1_score(pred, gold):
+    p_tokens = normalize_answer(pred).split()
+    g_tokens = normalize_answer(gold).split()
+    common = Counter(p_tokens) & Counter(g_tokens)
+    num_same = sum(common.values())
+    if num_same == 0:
+        return 0.0
+    precision = num_same / len(p_tokens)
+    recall = num_same / len(g_tokens)
+    return (2 * precision * recall) / (precision + recall)
+
+def load_dataset(path):
+    with open(path) as f:
+        for l in f:
+            yield json.loads(l)
+
+def run_eval(dataset_path, top_k=5):
+    results = []
+    for ex in load_dataset(dataset_path):
+        q = ex["question"]
+        vec = embed_query(q)
+        docs = milvus_search(vec, top_k=top_k)
+        context = "\n\n".join([d["meta"].get("source","") for d in docs])
+        prompt = f"Use only the context below to answer the question:\n\n{context}\n\nQuestion: {q}"
+        pred = call_llm(prompt)
+        f1 = f1_score(pred, ex.get("answer",""))
+        em = 1 if normalize_answer(pred) == normalize_answer(ex.get("answer","")) else 0
+        results.append({"id": ex.get("id"), "pred": pred, "f1": f1, "em": em})
+    return results
+
+def upload_report(results):
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    ts = int(time.time())
+    key = f"llm/eval/report_{ts}.json"
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(results).encode())
+    print("Uploaded report to s3://%s/%s" % (EVIDENCE_BUCKET, key))
+    return key
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset", required=True)
+    args = p.parse_args()
+    res = run_eval(args.dataset)
+    upload_report(res)
+    # print summary
+    avg_f1 = sum(r["f1"] for r in res)/len(res) if res else 0
+    avg_em = sum(r["em"] for r in res)/len(res) if res else 0
+    print("Avg F1:", avg_f1, "Avg EM:", avg_em)
+
*** End Patch
*** Begin Patch
*** Add File:llm/eval/hallucination_eval_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-llm-eval
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-llm-eval-sa
+      containers:
+        - name: eval
+          image: ghcr.io/yourorg/aegis-llm-eval:latest
+          command: ["/bin/sh","-c"]
+          args:
+            - |
+              python3 /opt/eval/hallucination_eval.py --dataset /data/eval/qa_small.jsonl
+          env:
+            - name: EVIDENCE_BUCKET
+              value: "REPLACE_WITH_EVIDENCE_BUCKET"
+            - name: AWS_REGION
+              value: "REPLACE_WITH_AWS_REGION"
+          volumeMounts:
+            - name: eval-data
+              mountPath: /data/eval
+      restartPolicy: Never
+  backoffLimit: 0
+  ttlSecondsAfterFinished: 86400
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-llm-eval-sa
+  namespace: aegis
+
+---
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: llm-eval-data-pvc
+  namespace: aegis
+spec:
+  accessModes: ["ReadWriteOnce"]
+  resources:
+    requests:
+      storage: 1Gi
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/llm_staging_to_prod_runbook.md
+# LLM Staging â†’ Production Runbook (operator)
+
+Purpose
+- Step-by-step checklist to promote an LLM / RAG flow from staging to production on Aegis. Includes infrastructure, validation, safety and acceptance criteria, and S3 evidence keys produced by each step.
+
+Prereqs
+- Kubernetes admin access and kubeconfig for staging & production.
+- EVIDENCE_BUCKET and AWS_REGION set for your environment (no secrets placed here).
+- ExternalSecrets configured to sync production secrets (llm-secrets, cosign-kms, mlflow tokens).
+- GPU nodepool / HPC nodes available and properly tainted for LLM workloads.
+- Milvus deployed in production (Helm + PVCs) and accessible.
+
+Operator steps
+
+1) Provision infra (Milvus, Redis, GPU nodes, Seldon/Triton nodes)
+- Deploy Milvus via Helm using milvus/helm/values-production.yaml
+  - helm repo add milvus https://milvus-helm-repo.example
+  - helm install milvus milvus/milvus -f milvus/helm/values-production.yaml --namespace milvus --create-namespace
+  - Expected evidence: PVCs bound; test connection: milvus_client.connect(host=<host>, port=19530)
+
+2) Configure ExternalSecrets for production secrets
+- Ensure secrets exist in Secrets Manager (paths per devsecops/external_secrets_full.yaml.template)
+- kubectl apply -f devsecops/external_secrets_full.yaml
+- Evidence: k8s secrets present: kubectl -n aegis get secret llm-secrets cosign-kms
+
+3) Deploy Milvus indexer CronJob & run a full indexing pass
+- kubectl apply -f embeddings/milvus/milvus-index-cronjob.yaml
+- Trigger a manual job for initial full index:
+  - kubectl create job --from=cronjob/milvus-index-cronjob aegis-milvus-index-manual -n aegis
+- Evidence produced:
+  - s3://<EVIDENCE_BUCKET>/milvus/indexer/run_<ts>.log
+  - s3://<EVIDENCE_BUCKET>/milvus/indexer/manifest_<ts>.json
+
+4) Deploy RAG service (Flask or productionized service behind Istio/Gateway)
+- kubectl apply -f rag/k8s/flask_rag_deployment.yaml
+- kubectl apply -f rag/k8s/ingress_tls_certmanager.yaml
+- Verify: curl -k https://rag.example.com/query (staging) returns 200 on test queries
+- Evidence:
+  - s3://<EVIDENCE_BUCKET>/rag/smoke/<ts>.json
+
+5) Run small distributed finetune & sign:
+- For staging, run single-node job:
+  - kubectl apply -f llm/training/deepspeed_k8s_job.yaml -n aegis
+  - Monitor job logs: kubectl logs -f job/aegis-llm-finetune-deepspeed -n aegis
+- On success, artifacts will be written to outputs/ and MLflow (if configured)
+- Sign checkpoint (if COSIGN_KMS present) will create s3://<EVIDENCE_BUCKET>/llm/checkpoints/<name>.tar.gz and .sig
+
+6) Run hallucination/factuality evaluation
+- Prepare small QA dataset under PVC mount (llm/eval/qa_small.jsonl)
+- kubectl apply -f llm/eval/hallucination_eval_job.yaml
+- On completion, report uploaded to s3://<EVIDENCE_BUCKET>/llm/eval/report_<ts>.json
+- Acceptance: Avg F1 and EM meet team threshold (define per model)
+
+7) Safety & policy checks (automated)
+- Ensure conftest checks pass for RAG & LLM manifests:
+  - gh workflow run conftest_llm_policy_check.yml
+- Resolve any failures (missing RATE_LIMIT env, missing resource limits, or ClusterRoleBinding issues).
+
+8) Load / scale testing
+- Run scale tests for RAG endpoints (vegeta or custom), Milvus query throughput, and GPU training load.
+- Example:
+  - ./serving/load_test_vegeta.sh http://rag-flask.aegis.svc.cluster.local/predict 60s 200
+- Evidence: upload reports to s3://<EVIDENCE_BUCKET>/llm/scale_tests/
+
+9) Final sign-off & promotion
+- Validate evidence keys:
+  - s3://<EVIDENCE_BUCKET>/milvus/indexer/manifest_<ts>.json
+  - s3://<EVIDENCE_BUCKET>/llm/checkpoints/<checkpoint>.tar.gz(.sig)
+  - s3://<EVIDENCE_BUCKET>/llm/eval/report_<ts>.json
+  - s3://<EVIDENCE_BUCKET>/rag/smoke/<ts>.json
+- Security sign-off:
+  - cosign verification: cosign verify --key awskms://<COSIGN_KMS_ARN> <checkpoint>.tar.gz
+  - RBAC audit: run devsecops/security/rbac_continuous_audit.py and resolve findings
+- Promote service to prod (apply same manifests to prod namespace/kubeconfig or update VirtualService to point to prod subset)
+
+Acceptance criteria (LLM-specific)
+- Distributed finetune completed successfully with signed checkpoint and MLflow entry.
+- Milvus index health: p95 query latency within SLO for target QPS.
+- RAG endpoint: 95th percentile latency below target and autoscaling triggers appropriately.
+- Safety metrics: hallucination eval meets threshold; no blocked outputs in safety filters for sampled production traffic.
+- Billing/usage: token & GPU-hour accounting in place and alerted on unusual spend.
+
+Rollback plan
+- If production RAG endpoint degrades (latency or harmful outputs), revert VirtualService routing to previous stable deployment and disable new model via feature flag. Collect audit evidence and investigate.
+
+Notes & tips
+- Use small initial batch sizes and conservative HPA targets; iterate tuning after load tests.
+- Automate index maintenance: incremental updates on new evidence + periodic compaction.
+- Keep all artifacts (prompts, checkpoints, snapshots) signed and recorded in Rekor if available.
+
*** End Patch
*** End Patch
