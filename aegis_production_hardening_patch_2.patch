*** Begin Patch
*** Add File:accuracy/telemetry_healthcheck.py
+#!/usr/bin/env python3
+"""
+Telemetry healthcheck
+ - Verifies sustained telemetry coverage per node_type over a given window (default 7 days)
+ - Emits a simple JSON report and uploads to S3 (for auditing) and optionally prints a human summary
+
+Usage:
+  EVIDENCE_BUCKET=aegis-evidence AWS_REGION=us-west-2 python3 accuracy/telemetry_healthcheck.py --prefix telemetry/ --days 7
+"""
+import os, sys, argparse, boto3, json, datetime
+from collections import defaultdict
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+def list_keys(s3, bucket, prefix, start_ts):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            if obj["LastModified"] >= start_ts:
+                yield obj["Key"]
+
+def analyze(s3, bucket, prefix, days):
+    start_ts = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=days)
+    counts_by_node = defaultdict(int)
+    total = 0
+    for key in list_keys(s3, bucket, prefix, start_ts):
+        # heuristic: keys contain node_type or node id; fallback to file content sampling if needed
+        total += 1
+        parts = key.split("/")
+        # attempt to parse node_type from path like telemetry/<node_type>/<file>.csv
+        if len(parts) > 1:
+            counts_by_node[parts[1]] += 1
+    return {"window_days": days, "total_files": total, "counts_by_node": counts_by_node}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--prefix", default="telemetry/")
+    p.add_argument("--days", type=int, default=7)
+    args = p.parse_args()
+    if not EVIDENCE_BUCKET:
+        print("EVIDENCE_BUCKET env required", file=sys.stderr)
+        sys.exit(2)
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    report = analyze(s3, EVIDENCE_BUCKET, args.prefix, args.days)
+    out = "/tmp/telemetry_health.json"
+    with open(out, "w") as f:
+        json.dump(report, f, indent=2, default=str)
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=f"telemetry/health/health_{int(datetime.datetime.now().timestamp())}.json", Body=json.dumps(report).encode())
+    print(json.dumps(report, indent=2, default=str))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:accuracy/median_prometheus_reporter.py
+#!/usr/bin/env python3
+"""
+Compute median relative error across reconciliations and push metrics to Pushgateway
+ - Produces:
+   - aegis_reconciliation_median_rel_error (gauge)
+   - aegis_reconciliation_sample_count (gauge)
+"""
+import os, time, statistics, boto3, json, requests
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+PUSHGATEWAY = os.environ.get("PUSHGATEWAY", "pushgateway.monitoring.svc:9091")
+PREFIX = os.environ.get("RECON_PREFIX","reconciliations/")
+
+def list_keys(s3):
+    paginator = s3.get_paginator("list_objects_v2")
+    for p in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=PREFIX, MaxKeys=1000):
+        for o in p.get("Contents", []):
+            if o["Key"].endswith(".json"):
+                yield o["Key"]
+
+def get_vals(s3):
+    vals = []
+    for k in list_keys(s3):
+        body = s3.get_object(Bucket=EVIDENCE_BUCKET, Key=k)["Body"].read().decode()
+        j = json.loads(body)
+        est = j.get("estimate",{}).get("estimated_emissions_kg")
+        meas = j.get("measured",{}).get("measured_emissions_kg")
+        if est is None or meas is None or meas == 0:
+            continue
+        rel = abs(meas - est) / float(meas)
+        vals.append(rel)
+    return vals
+
+def push(metric, value, job="aegis_reconciliation"):
+    payload = f"{metric} {value}\n"
+    try:
+        requests.post(f"http://{PUSHGATEWAY}/metrics/job/{job}", data=payload, timeout=5)
+    except Exception:
+        pass
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET required")
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    vals = get_vals(s3)
+    if not vals:
+        print("No reconciliations found")
+        push("aegis_reconciliation_median_rel_error", 0)
+        push("aegis_reconciliation_sample_count", 0)
+        return
+    med = statistics.median(vals)
+    push("aegis_reconciliation_median_rel_error", round(med,4))
+    push("aegis_reconciliation_sample_count", len(vals))
+    print("median:", med, "count:", len(vals))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/extender_improvements.py
+#!/usr/bin/env python3
+"""
+Improved scheduler-extender scaffold:
+ - Adds simple health/metrics endpoints and configurable selectors via environment
+ - Emits a basic Prometheus-compatible /metrics using prom-client if available (fallback to simple text)
+"""
+from flask import Flask, request, jsonify, Response
+import os, json
+
+app = Flask("aegis-scheduler-extender")
+PREFERRED_LABEL = os.environ.get("PREFERRED_LABEL", "carbon_preference")
+PREFERRED_VALUE = os.environ.get("PREFERRED_VALUE", "low")
+
+@app.route("/filter", methods=["POST"])
+def filter_nodes():
+    body = request.get_json(force=True)
+    pod = body.get("pod", {})
+    nodes = body.get("nodes", {}).get("items", [])
+    annotations = pod.get("metadata", {}).get("annotations", {}) or {}
+    hint_json = annotations.get("aegis.carbon.hint")
+    if not hint_json:
+        return jsonify({"nodes": {"items": nodes}})
+    hint = json.loads(hint_json)
+    selector = {}
+    if hint.get("prefer_spot"):
+        selector["lifecycle"] = "spot"
+    filtered = []
+    for n in nodes:
+        labels = n.get("labels", {})
+        ok = True
+        for k,v in selector.items():
+            if labels.get(k) != v:
+                ok = False
+                break
+        if ok:
+            filtered.append(n)
+    return jsonify({"nodes": {"items": filtered}})
+
+@app.route("/prioritize", methods=["POST"])
+def prioritize():
+    body = request.get_json(force=True)
+    nodes = body.get("nodes", {}).get("items", [])
+    hostPriorityList = []
+    for n in nodes:
+        score = 0
+        labels = n.get("labels", {})
+        if labels.get(PREFERRED_LABEL) == PREFERRED_VALUE:
+            score += 100
+        if labels.get("lifecycle") == "spot":
+            score += 50
+        hostPriorityList.append({"host": n.get("name"), "score": score})
+    return jsonify({"hostPriorityList": hostPriorityList})
+
+@app.route("/healthz")
+def healthz():
+    return "ok"
+
+@app.route("/metrics")
+def metrics():
+    # Minimal static metrics for extender; integrate Prometheus client for dynamic values in production
+    m = []
+    m.append("# HELP aegis_extender_up 1=up")
+    m.append("# TYPE aegis_extender_up gauge")
+    m.append("aegis_extender_up 1")
+    return Response("\n".join(m), mimetype="text/plain")
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8000")))
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/enforcer_rbac_minimal.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-argo-enforcer-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-argo-enforcer-role
+rules:
+  - apiGroups: ["argoproj.io"]
+    resources: ["workflows"]
+    verbs: ["get","list","watch","patch"]
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["get","list"]
+  - apiGroups: [""]
+    resources: ["namespaces"]
+    verbs: ["get","list"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: aegis-argo-enforcer-binding
+subjects:
+  - kind: ServiceAccount
+    name: aegis-argo-enforcer-sa
+    namespace: aegis
+roleRef:
+  kind: ClusterRole
+  name: aegis-argo-enforcer-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File:etl/s3_parquet_pipeline_v3.py
+#!/usr/bin/env python3
+"""
+Bulk parquet pipeline v3 (improvements):
+ - Better checkpointing (atomic), batching, partitioning by start_dt
+ - Produces a JSON manifest of uploaded parquet files for COPY/LOAD ingestion
+ - Writes a checkpoint atomically (write to tmp then mv)
+ - Idempotent: files already uploaded are not re-uploaded
+"""
+import os, json, boto3, tempfile, pyarrow as pa, pyarrow.parquet as pq
+import pandas as pd
+from datetime import datetime, timezone
+
+S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "aegis-evidence")
+PREFIX = os.environ.get("EVIDENCE_PREFIX", "evidence/")
+OUT_PREFIX = os.environ.get("PARQUET_OUT_PREFIX", "parquet/evidence")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+CHECKPOINT_KEY = os.environ.get("PARQUET_CHECKPOINT_KEY", f"{PREFIX}.parquet_checkpoint.json")
+BATCH_SIZE = int(os.environ.get("PARQUET_BATCH_SIZE", "2000"))
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def read_checkpoint():
+    try:
+        obj = s3.get_object(Bucket=S3_BUCKET, Key=CHECKPOINT_KEY)
+        return json.loads(obj["Body"].read().decode())
+    except Exception:
+        return {"last_processed": None, "uploaded": []}
+
+def write_checkpoint_atomic(tmp_key, final_key, payload_bytes):
+    # write to a temp key then rename (no native rename; use put then put final)
+    s3.put_object(Bucket=S3_BUCKET, Key=tmp_key, Body=payload_bytes)
+    s3.copy_object(CopySource={'Bucket': S3_BUCKET, 'Key': tmp_key}, Bucket=S3_BUCKET, Key=final_key)
+    s3.delete_object(Bucket=S3_BUCKET, Key=tmp_key)
+
+def list_new_keys(prefix, marker=None):
+    paginator = s3.get_paginator("list_objects_v2")
+    kwargs = {"Bucket": S3_BUCKET, "Prefix": prefix}
+    for page in paginator.paginate(**kwargs):
+        for obj in page.get("Contents", []):
+            key = obj["Key"]
+            if marker and key <= marker:
+                continue
+            if key.endswith(".json"):
+                yield key
+
+def convert_and_upload(keys):
+    rows = []
+    for k in keys:
+        try:
+            body = s3.get_object(Bucket=S3_BUCKET, Key=k)["Body"].read().decode()
+            j = json.loads(body)
+            rows.append({
+                "s3_key": k,
+                "job_id": j.get("job_id"),
+                "team": j.get("team"),
+                "start_ts": j.get("start_ts"),
+                "end_ts": j.get("end_ts"),
+                "estimated_kg": j.get("estimated_kg"),
+                "model_name": j.get("model_name")
+            })
+        except Exception:
+            continue
+    if not rows:
+        return []
+    df = pd.DataFrame(rows)
+    df["start_dt"] = pd.to_datetime(df["start_ts"]).dt.date.astype(str)
+    uploaded = []
+    tmpdir = tempfile.mkdtemp()
+    for part, subdf in df.groupby("start_dt"):
+        out_file = os.path.join(tmpdir, f"batch_{part}_{int(datetime.now(timezone.utc).timestamp())}.parquet")
+        table = pa.Table.from_pandas(subdf.drop(columns=["start_dt"]))
+        pq.write_table(table, out_file, compression="snappy")
+        s3_key = f"{OUT_PREFIX}/start_dt={part}/{os.path.basename(out_file)}"
+        # skip if exists
+        try:
+            s3.head_object(Bucket=S3_BUCKET, Key=s3_key)
+            uploaded.append(s3_key)
+            continue
+        except Exception:
+            pass
+        s3.upload_file(out_file, S3_BUCKET, s3_key)
+        uploaded.append(s3_key)
+    return uploaded
+
+def main():
+    ck = read_checkpoint()
+    last = ck.get("last_processed")
+    keys = list(list_new_keys(PREFIX, marker=last))
+    batch = []
+    manifest = {"entries": []}
+    for k in keys:
+        batch.append(k)
+        if len(batch) >= BATCH_SIZE:
+            uploaded = convert_and_upload(batch)
+            manifest["entries"].extend(uploaded)
+            if batch:
+                last = batch[-1]
+            batch = []
+    if batch:
+        uploaded = convert_and_upload(batch)
+        manifest["entries"].extend(uploaded)
+        if batch:
+            last = batch[-1]
+    # write manifest and checkpoint atomically
+    if manifest["entries"]:
+        tmp_key = f"{CHECKPOINT_KEY}.tmp"
+        final_key = CHECKPOINT_KEY
+        payload = json.dumps({"last_processed": last, "uploaded": manifest["entries"], "ts": datetime.now(timezone.utc).isoformat()}).encode()
+        write_checkpoint_atomic(tmp_key, final_key, payload)
+        # also write manifest file for loaders
+        manifest_key = f"{OUT_PREFIX}/manifest_{int(datetime.now().timestamp())}.json"
+        s3.put_object(Bucket=S3_BUCKET, Key=manifest_key, Body=json.dumps(manifest).encode())
+        print("Wrote manifest", manifest_key)
+    else:
+        print("No new uploads")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:etl/redshift_loader_wrapper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper for Redshift COPY using manifest; idempotent with retries and basic healthchecks.
+MANIFEST_S3_URI=${MANIFEST_S3_URI:-s3://aegis-evidence/parquet/evidence/manifest.json}
+REDSHIFT_DSN=${REDSHIFT_DSN:-postgresql://user:pass@redshift-host:5439/dev}
+REDSHIFT_IAM_ROLE=${REDSHIFT_IAM_ROLE:-arn:aws:iam::123456789012:role/aegis-redshift-copy}
+MAX_RETRIES=${MAX_RETRIES:-3}
+
+SQL="COPY staging.raw_job_runs_stg FROM '${MANIFEST_S3_URI}' IAM_ROLE '${REDSHIFT_IAM_ROLE}' FORMAT AS PARQUET;"
+
+attempt=1
+while [ $attempt -le $MAX_RETRIES ]; do
+  echo "Attempt $attempt: running COPY"
+  if psql "${REDSHIFT_DSN}" -c "${SQL}"; then
+    echo "COPY succeeded; running MERGE/upsert"
+    psql "${REDSHIFT_DSN}" -f /opt/etl/redshift_merge_from_staging.sql
+    exit 0
+  else
+    echo "COPY failed, sleeping and retrying"
+    attempt=$((attempt+1))
+    sleep $((attempt*30))
+  fi
+done
+echo "COPY failed after ${MAX_RETRIES} attempts" >&2
+exit 2
+
*** End Patch
*** Begin Patch
*** Add File:etl/dbt_incremental_patterns.md
+# dbt incremental patterns & idempotency notes
+
+Use `materialized='incremental'` with `unique_key='job_id'` for core_job_runs.
+Key points:
+- Avoid `depends_on_past` in Airflow for dbt tasks; use incremental logic to pick up deltas.
+- Use `snapshots` for slowly changing refs (e.g., model versions).
+- Add dbt tests (unique, not_null) to prevent bad promotions.
+- For heavy aggregate tables, use `materialized='table'` on a scheduled cadence (nightly) and incremental for streaming deltas.
+
*** End Patch
*** Begin Patch
*** Add File:security/certmanager_clusterissuer_corporate.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: corporate-ca-issuer
+spec:
+  ca:
+    secretName: corporate-ca-keypair
+
+# Operators should create a Kubernetes secret `corporate-ca-keypair` in cert-manager namespace with:
+#   tls.crt (CA cert) and tls.key (CA private key)
+# This lets cert-manager issue cluster certs signed by corporate CA.
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign_rotate_workflow.yml
+name: Cosign KMS Key Rotation (skeleton)
+
+on:
+  workflow_dispatch:
+
+jobs:
+  rotate:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Rotate KMS key (operator step)
+        run: |
+          echo "This workflow is a skeleton. Rotation must be executed by a security engineer."
+          echo "Follow security/cosign/README_SIGNING_POLICY.md for steps."
+      - name: Notify
+        run: |
+          echo "Notify key owners and update CI secrets referencing COSIGN_KMS_KEY_ARN"
+
*** End Patch
*** Begin Patch
*** Add File:security/rbac/aegis_least_privilege_check.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple script to list high-privilege service accounts in `aegis` namespace and their clusterroles
+NS=${1:-aegis}
+echo "Listing SA -> ClusterRoleBindings for namespace ${NS}"
+kubectl get sa -n "${NS}" -o json | jq -r '.items[].metadata.name' | while read sa; do
+  echo "ServiceAccount: ${sa}"
+  kubectl get clusterrolebinding -o json | jq -r --arg sa "${sa}" '.items[] | select(.subjects != null) | select(.subjects[].name == $sa) | .metadata.name' || true
+done
+echo "Run this script and inspect any SA with cluster-admin style bindings; tighten RBAC accordingly."
+
*** End Patch
*** Begin Patch
*** Add File:operational/aegis_etl_idempotent_dag.py
+from datetime import datetime, timedelta
+from airflow import DAG
+from airflow.operators.python import PythonOperator
+from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
+import os, subprocess, requests
+from operational.airflow_idempotent_helper import already_done_marker, mark_done
+
+DEFAULT_ARGS = {
+    'owner': 'aegis',
+    'depends_on_past': False,
+    'email_on_failure': False,
+    'retries': 1,
+    'retry_delay': timedelta(minutes=10),
+}
+
+WAREHOUSE_DSN = os.environ.get("WAREHOUSE_DSN", "postgresql://user:pass@host:5432/aegis_dw")
+
+def check_and_mark(job_key):
+    if already_done_marker(WAREHOUSE_DSN, job_key):
+        print("Job already done:", job_key)
+        return False
+    return True
+
+def run_parquet_task(**kwargs):
+    job_key = "parquet_pipeline_" + datetime.utcnow().strftime("%Y%m%d")
+    if not check_and_mark(job_key):
+        return
+    ret = subprocess.run("python3 /opt/ingest/s3_parquet_pipeline_v3.py", shell=True)
+    if ret.returncode != 0:
+        raise RuntimeError("parquet pipeline failed")
+    mark_done(WAREHOUSE_DSN, job_key)
+
+with DAG(
+    dag_id="aegis_etl_idempotent",
+    default_args=DEFAULT_ARGS,
+    schedule_interval="@daily",
+    start_date=datetime(2025,1,1),
+    catchup=False,
+    max_active_runs=1
+) as dag:
+
+    parquet = PythonOperator(task_id="s3_to_parquet_idempotent", python_callable=run_parquet_task)
+
+    gen_manifest = PythonOperator(task_id="generate_manifest", python_callable=lambda: subprocess.check_call("python3 /opt/ingest/copy_manifest_generator.py", shell=True))
+
+    bulk_loader = KubernetesPodOperator(
+        task_id="redshift_copy",
+        name="redshift-copy",
+        namespace="aegis",
+        image="ghcr.io/yourorg/aegis-redshift-loader:latest",
+        cmds=["/bin/sh","-c"],
+        arguments=["/opt/etl/redshift_loader_wrapper.sh"],
+        get_logs=True,
+        is_delete_operator_pod=True,
+        in_cluster=True
+    )
+
+    parquet >> gen_manifest >> bulk_loader
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_rules_additional.yaml
+groups:
+- name: aegis-critical.rules
+  rules:
+  - alert: HighMedianEstimatorError
+    expr: aegis_reconciliation_median_rel_error > 0.10
+    for: 30m
+    labels:
+      severity: page
+    annotations:
+      summary: "Median relative error above 10% - audit risk"
+
+  - alert: EnforcementCoverageDegraded
+    expr: aegis_enforcement_coverage < 0.95
+    for: 30m
+    labels:
+      severity: page
+    annotations:
+      summary: "Enforcement coverage below 95%"
+
+  - alert: ETLBacklog
+    expr: increase(aegis_etl_parquet_failed[1h]) > 0
+    for: 15m
+    labels:
+      severity: page
+    annotations:
+      summary: "ETL parquet pipeline failures detected"
+
*** End Patch
*** Begin Patch
*** Add File:docs/production_hardening_summary.md
+# Production Hardening Summary (one page)
+
+Focus areas and immediate actions:
+
+- Data accuracy
+  - Collect 7–14 days telemetry per node_type.
+  - Run iterative calibration CronJob (aegis-iterative-calibration) weekly.
+  - Use CI gating: do not promote profiles unless median_relative_error ≤ 0.10.
+  - Reporter scripts push reconciliation median to Pushgateway / Prometheus.
+
+- Enforcement coverage
+  - Deploy scheduler extender + Argo admission enforcer in staging.
+  - Run enforcement E2E tests and enforcement_coverage exporter; iterate until coverage ≥ 95%.
+  - Stage-by-stage rollout to teams in prod.
+
+- ETL scale & cost
+  - Use s3_parquet_pipeline_v3.py to create partitioned parquet + manifest.
+  - Bulk-load with redshift_loader_wrapper.sh using manifest and MERGE step.
+  - Provision IAM role scoped to prefix for DW copying.
+  - Tune parquet sizes and partitions for cost efficiencies.
+
+- Security hardening
+  - Use cert-manager corporate ClusterIssuer (Vault/CA) for webhook TLS.
+  - Provision KMS key with restricted principals and rotate keys according to policy.
+  - Use least-privilege RBAC for webhook/enforcer/service accounts.
+  - Configure Superset SSO (OIDC) and RLS for multi-team datasets.
+
+- Operational resilience
+  - Idempotent Airflow DAGs with bookkeeping table (etl_runs).
+  - Alerting rules for median error, enforcement coverage and ETL failures.
+  - Incident playbooks and DR/resume drill scripts.
+  - CI gating and manual promotion steps in GitHub Actions.
+
+Next immediate step (1 action): Run `accuracy/telemetry_healthcheck.py` and confirm telemetry coverage for the next 7 days.
+
*** End Patch
*** End Patch
