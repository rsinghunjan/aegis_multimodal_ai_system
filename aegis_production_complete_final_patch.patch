*** Begin Patch
*** Add File:tenant-finalization/README.md
+Tenant finalization helpers
+==========================
+
+This directory contains operator-driven scripts and helpers to finalize tenant IAM/identity/KMS wiring for multiple clouds.
+
+Important:
+- These scripts are templates and helpers. They will print commands and payloads; you MUST review and run them in your staging accounts.
+- They do NOT contain any credentials. Do not paste secrets into code or chat.
+
+Files:
+- aws/finalize_aws_tenant.sh
+- gcp/finalize_gcp_tenant.sh
+- azure/finalize_azure_tenant.sh
+- oci/finalize_oci_tenant.sh
+- alibaba/finalize_alibaba_tenant.sh
+- ibm/finalize_ibm_tenant.sh
+- ghcli/create_repo_secrets.sh (prints gh commands to create secrets)
+
+Runbook:
+- See runbooks/tenant_iam_finalization.md for the step-by-step operator procedures and verification checklists.
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/ghcli/create_repo_secrets.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Print gh CLI commands to set repository secrets for Aegis tenant wiring.
+#
+# Usage:
+#   ./create_repo_secrets.sh --repo owner/repo --secret NAME=VALUE ...
+#
+REPO=""
+declare -a SECRETS
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --repo) REPO="$2"; shift 2;;
+    *=*) SECRETS+=("$1"); shift;;
+    -h|--help) echo "Usage: $0 --repo owner/repo NAME=VALUE ..."; exit 0;;
+    *) echo "Unknown: $1"; exit 1;;
+  esac
+done
+
+if [ -z "$REPO" ] || [ ${#SECRETS[@]} -eq 0 ]; then
+  echo "Usage: $0 --repo owner/repo NAME=VALUE ..."
+  exit 1
+fi
+
+echo "Run the following commands to create repository secrets (requires gh CLI auth):"
+for s in "${SECRETS[@]}"; do
+  NAME="${s%%=*}"
+  VALUE="${s#*=}"
+  printf 'gh secret set %s --repo %s --body "%s"\n' "$NAME" "$REPO" "$VALUE"
+done
+
+echo
+echo "Note: For safety, prefer 'gh secret set <name> --repo <repo> --env-file <file>' when value is large or multi-line."
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/aws/finalize_aws_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_aws_tenant.sh
+# Operator helper to create an IAM role for GitHub OIDC, attach minimal policies for KMS & S3,
+# and optionally create a KMS key. Prints GitHub secret values to be saved.
+#
+usage() { cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --region <aws-region> --s3-bucket <bucket> [--create-kms]
+
+Outputs (print to save as GitHub secrets):
+  AWS_ROLE_ARN
+  AWS_REGION
+  KMS_KEY_ARN
+  S3_BUCKET
+
+EOF
+exit 1; }
+
+CREATE_KMS=false
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --create-kms) CREATE_KMS=true; shift;;
+    -h|--help) usage;;
+    *) echo "Unknown arg $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${REGION:-}" ] || usage
+[ -n "${S3_BUCKET:-}" ] || usage
+
+ACCOUNT_ID=$(aws sts get-caller-identity --query Account -o text)
+ROLE_NAME="aegis-github-oidc-${ORG}-${REPO}"
+
+echo "Creating IAM role $ROLE_NAME with OIDC trust for token.actions.githubusercontent.com (subject repo:${ORG}/${REPO}:ref:refs/heads/main)"
+
+OIDC_PROVIDER_ARN=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[?contains(Arn, `token.actions.githubusercontent.com`)].Arn' --output text || true)
+if [ -z "$OIDC_PROVIDER_ARN" ]; then
+  echo "No OIDC provider for GitHub found in this account. Create one manually in IAM using issuer https://token.actions.githubusercontent.com and thumbprint from GitHub docs."
+fi
+
+TRUST_POLICY=$(cat <<EOF
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Principal": { "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/token.actions.githubusercontent.com" },
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Condition": {
+        "StringLike": {
+          "token.actions.githubusercontent.com:sub": "repo:${ORG}/${REPO}:ref:refs/heads/main"
+        }
+      }
+    }
+  ]
+}
+EOF
+)
+
+echo "$TRUST_POLICY" > /tmp/aegis-trust.json
+if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
+  echo "Role exists; skipping create"
+else
+  aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file:///tmp/aegis-trust.json
+fi
+
+INLINE_POLICY=$(cat <<EOF
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {"Effect":"Allow","Action":["kms:Sign","kms:GetPublicKey"],"Resource":"*"},
+    {"Effect":"Allow","Action":["s3:GetObject","s3:PutObject","s3:ListBucket"],"Resource":["arn:aws:s3:::${S3_BUCKET}","arn:aws:s3:::${S3_BUCKET}/*"]}
+  ]
+}
+EOF
+)
+echo "$INLINE_POLICY" > /tmp/aegis-inline-policy.json
+aws iam put-role-policy --role-name "$ROLE_NAME" --policy-name "AegisOIDCPolicy" --policy-document file:///tmp/aegis-inline-policy.json
+
+KMS_KEY_ARN="<PROVIDE_KMS_KEY_ARN>"
+if $CREATE_KMS; then
+  echo "Creating KMS key (symmetric) for Aegis signing. You may prefer asymmetric with GetPublicKey permissions."
+  KMS_KEY_ARN=$(aws kms create-key --description "Aegis signing key" --query KeyMetadata.Arn -o text)
+  aws kms create-alias --alias-name "alias/aegis-signing-key" --target-key-id $(aws kms describe-key --key-id "$KMS_KEY_ARN" --query KeyMetadata.KeyId -o text)
+  echo "Created key: $KMS_KEY_ARN"
+fi
+
+cat <<EOF
+==== GitHub secrets to set (copy/paste) ====
+AWS_ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query Role.Arn -o text)
+AWS_REGION=$REGION
+KMS_KEY_ARN=$KMS_KEY_ARN
+S3_BUCKET=$S3_BUCKET
+==========================================
+
+Operator notes:
+- Replace wildcards in policies with specific resource ARNs for least privilege.
+- Consider using asymmetric KMS keys and restrict kms:Sign to specific principals.
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/gcp/finalize_gcp_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_gcp_tenant.sh
+# Helper to instruct and automate parts of GCP workload identity / service account setup for GitHub OIDC.
+#
+usage(){ cat <<EOF
+Usage: $0 --project <project> --pool-id <pool-id> --provider-id <provider-id> --bucket <gcs-bucket>
+
+Outputs guidance and commands to bind IAM roles and emit secrets to put into GitHub.
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --project) PROJECT="$2"; shift 2;;
+    --pool-id) POOL="$2"; shift 2;;
+    --provider-id) PROVIDER="$2"; shift 2;;
+    --bucket) BUCKET="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${PROJECT:-}" ] || usage
+[ -n "${POOL:-}" ] || usage
+[ -n "${PROVIDER:-}" ] || usage
+[ -n "${BUCKET:-}" ] || usage
+
+SA_NAME="aegis-github-oidc-sa"
+gcloud iam service-accounts create "$SA_NAME" --project="$PROJECT" || true
+SA_EMAIL="${SA_NAME}@${PROJECT}.iam.gserviceaccount.com"
+
+echo "Grant minimal roles to service account:"
+echo "gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:$SA_EMAIL --role=roles/cloudkms.signerVerifier"
+echo "gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:$SA_EMAIL --role=roles/storage.objectAdmin"
+
+echo
+echo "Create Workload Identity Pool & Provider (console recommended for initial setup) and configure 'Trust config' to accept tokens from token.actions.githubusercontent.com"
+echo "After creating provider, create IAM binding to allow the subject 'principal://iam.googleapis.com/${POOL}/subject/*' to impersonate the SA."
+
+echo
+cat <<EOF
+Set the following repository secrets:
+GCP_PROJECT=$PROJECT
+GCP_SA_EMAIL=$SA_EMAIL
+GCP_BUCKET=$BUCKET
+GCP_WORKLOAD_POOL=$POOL
+GCP_WORKLOAD_POOL_PROVIDER=$PROVIDER
+EOF
+
+echo
+echo "Operator: follow GCP Workload Identity docs to map GitHub repo claims to the service account via service-account-iam-policy-binding with 'principalSet://iam.googleapis.com/${POOL}/attribute.repository/${REPO}'"
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/azure/finalize_azure_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_azure_tenant.sh
+# Create or prepare Azure objects required for GitHub OIDC federation and Key Vault access.
+#
+usage(){ cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --resource-group <rg> --vault-name <vault> --location <location>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --resource-group) RG="$2"; shift 2;;
+    --vault-name) VAULT="$2"; shift 2;;
+    --location) LOC="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${RG:-}" ] || usage
+[ -n "${VAULT:-}" ] || usage
+[ -n "${LOC:-}" ] || usage
+
+APP_NAME="aegis-github-federation-${ORG}-${REPO}"
+echo "Creating app registration and federated credential (if possible)"
+if az ad app show --id "http://$APP_NAME" >/dev/null 2>&1; then
+  echo "App already exists"
+else
+  APP_JSON=$(az ad app create --display-name "$APP_NAME" --query "{appId:appId,id:id}" -o json)
+  APP_ID=$(echo "$APP_JSON" | jq -r .appId)
+  APP_OBJ_ID=$(echo "$APP_JSON" | jq -r .id)
+  echo "Created app: $APP_ID"
+fi
+
+echo "Create user-assigned managed identity for CI workloads"
+MI_NAME="aegis-ci-identity"
+az identity create -g "$RG" -n "$MI_NAME" --location "$LOC" -o json || true
+MI_JSON=$(az identity show -g "$RG" -n "$MI_NAME" -o json)
+MI_CLIENT_ID=$(echo "$MI_JSON" | jq -r .clientId)
+MI_PRINCIPAL_ID=$(echo "$MI_JSON" | jq -r .principalId)
+
+echo "Grant Key Vault permission to MI: wrapKey, unwrapKey, get"
+echo "az keyvault set-policy --name $VAULT --object-id $MI_PRINCIPAL_ID --key-permissions wrapKey unwrapKey get"
+
+KEY_NAME="aegis-signing-key"
+echo "Create Key in Key Vault (if missing) and print URI"
+az keyvault key create --vault-name "$VAULT" --name "$KEY_NAME" --kty EC || true
+KEY_URI="azurekms://${VAULT}.vault.azure.net/keys/${KEY_NAME}"
+
+cat <<EOF
+=== GitHub secrets to set ===
+AZURE_OIDC_CLIENT_ID=$(az ad app list --display-name "$APP_NAME" --query "[0].appId" -o tsv)
+AZURE_TENANT_ID=$(az account show --query tenantId -o tsv)
+AZURE_KEY_URI=$KEY_URI
+AZURE_MANAGED_IDENTITY_CLIENT_ID=$MI_CLIENT_ID
+EOF
+
+echo "Note: create a federated credential on the app registration with issuer https://token.actions.githubusercontent.com and subject repo:${ORG}/${REPO}:ref:refs/heads/main"
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/oci/finalize_oci_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_oci_tenant.sh - print steps and artifacts to create OCI OIDC provider, dynamic group and policies
+#
+usage(){ cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --compartment <compartment-ocid>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --compartment) COMP="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${COMP:-}" ] || usage
+
+ISSUER="https://token.actions.githubusercontent.com"
+SUB="repo:${ORG}/${REPO}:ref:refs/heads/main"
+
+cat <<EOF
+OCI Tenant finalization - steps to perform in OCI Console:
+
+1) Identity -> Identity Providers -> Add Identity Provider
+   - Type: OpenID Connect (OIDC)
+   - Issuer: $ISSUER
+
+2) Create Dynamic Group:
+   - Name: aegis-github-oidc-${ORG}-${REPO}
+   - Rule:
+       ALL {request.principal.claims["iss"] = "$ISSUER" && request.principal.claims["sub"] = "$SUB"}
+
+3) Create policy in appropriate policy compartment:
+   Allow dynamic-group aegis-github-oidc-${ORG}-${REPO} to use keys in compartment $COMP
+   Allow dynamic-group aegis-github-oidc-${ORG}-${REPO} to inspect keys in compartment $COMP
+
+4) Provision key in OCI Vault and note the KMS URI to be used by cosign/OCI KMS integrations
+
+After creating these, set the following GitHub secrets:
+  OCI_EXCHANGE_CMD (example placeholder: oci sts create-session-token --assume-role-with-web-identity --web-identity-token '<<TOKEN>>' --role-arn ocid1.role.oc1..example)
+  OCI_KEY_URI (provider-specific)
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/alibaba/finalize_alibaba_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_alibaba_tenant.sh - prints steps and artifacts to create RAM OIDC role and policy
+#
+usage(){ cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --role-name <role-name> --key-id <kms-key-id>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --role-name) ROLE="$2"; shift 2;;
+    --key-id) KEYID="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${ROLE:-}" ] || usage
+[ -n "${KEYID:-}" ] || usage
+
+SUB="repo:${ORG}/${REPO}:ref:refs/heads/main"
+
+cat <<EOF
+Alibaba Cloud tenant finalization (manual steps)
+
+1) Create a RAM role with web identity federation to token.actions.githubusercontent.com
+   Use trust policy:
+   {
+     "Statement": [
+       {
+         "Action": "sts:AssumeRoleWithWebIdentity",
+         "Effect": "Allow",
+         "Principal": { "Federated": ["https://token.actions.githubusercontent.com"] },
+         "Condition": { "StringEquals": { "token.actions.githubusercontent.com:sub": "$SUB" } }
+       }
+     ],
+     "Version": "1"
+   }
+
+2) Attach RAM policy granting kms:Sign and kms:GetPublicKey on resource acs:kms:*:*:key/$KEYID
+
+3) Example ALICLOUD_EXCHANGE_CMD to store in GitHub secret:
+   ALICLOUD_EXCHANGE_CMD="aliyun sts AssumeRoleWithWebIdentity --WebIdentityToken '<<TOKEN>>' --RoleArn 'acs:ram::123456:role/$ROLE' --RoleSessionName gha-oidc --Output json"
+
+4) Set ALICLOUD_KEY_URI to your KMS key identifier.
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/ibm/finalize_ibm_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_ibm_tenant.sh - create a Service ID and minimal policies for Key Protect & COS
+#
+usage(){ cat <<EOF
+Usage: $0 --resource-group <rg> --cos-bucket <bucket> --region <region>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --resource-group) RG="$2"; shift 2;;
+    --cos-bucket) BUCKET="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${RG:-}" ] || usage
+[ -n "${BUCKET:-}" ] || usage
+[ -n "${REGION:-}" ] || usage
+
+NAME="aegis-ci-serviceid"
+ibmcloud iam service-id-create "$NAME" -d "Aegis CI" || true
+SID=$(ibmcloud iam service-id "$NAME" --output json | jq -r .guid)
+echo "Service ID: $SID"
+
+APIKEY_JSON=$(ibmcloud iam service-id-api-key-create "$NAME" aegis-ci-key -d "CI key" -f || true)
+APIKEY=$(echo "$APIKEY_JSON" | jq -r .apikey || true)
+
+cat <<EOF
+Save these as GitHub secrets:
+IBM_CLOUD_API_KEY=<APIKEY printed above>
+IBM_COS_BUCKET=$BUCKET
+IBM_REGION=$REGION
+
+Operator: attach minimal policies to Service ID for Key Protect key operations (sign/get-public-key) and COS object access.
+EOF
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/README.md
+Managed services Terraform modules
+================================
+
+This directory includes modules to provision managed services (ElastiCache, Memorystore, Bigtable, S3 Bucket for lakeFS, Vault host infrastructure).
+They are templates; you must fill provider credentials, networking (VPC / subnets) and run terraform init/apply in your operator environment.
+
+Modules:
+- terraform/managed/aws/elasticache
+- terraform/managed/gcp/memorystore
+- terraform/managed/gcp/bigtable
+- terraform/managed/s3 (lakeFS bucket)
+- terraform/managed/vault (infrastructure for HA Vault)
+
+Runbook:
+- See prod/runbooks/ha_and_managed_services.md for guidance on applying and validating.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/aws/elasticache/main.tf
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws" }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_elasticache_subnet_group" "this" {
+  name       = "${var.stage}-subnet-group"
+  subnet_ids = var.subnet_ids
+}
+
+resource "aws_elasticache_replication_group" "redis" {
+  replication_group_id          = "${var.stage}-aegis-redis"
+  replication_group_description = "Aegis Redis for Feast online store"
+  node_type                     = var.node_type
+  number_cache_clusters         = var.num_nodes
+  automatic_failover_enabled    = true
+  subnet_group_name             = aws_elasticache_subnet_group.this.name
+  security_group_ids            = var.security_group_ids
+  engine                        = "redis"
+  engine_version                = var.engine_version
+  apply_immediately             = true
+}
+
+output "redis_primary_endpoint" { value = aws_elasticache_replication_group.redis.primary_endpoint_address }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/aws/elasticache/variables.tf
+variable "aws_region" { type = string default = "us-east-1" }
+variable "subnet_ids" { type = list(string) }
+variable "security_group_ids" { type = list(string) }
+variable "stage" { type = string default = "staging" }
+variable "node_type" { type = string default = "cache.m6g.large" }
+variable "num_nodes" { type = number default = 3 }
+variable "engine_version" { type = string default = "6.x" }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/gcp/memorystore/main.tf
+terraform {
+  required_providers {
+    google = { source = "hashicorp/google" }
+  }
+}
+provider "google" {}
+
+resource "google_redis_instance" "feast_redis" {
+  name           = "${var.stage}-aegis-redis"
+  tier           = "STANDARD_HA"
+  memory_size_gb = var.memory_size_gb
+  region         = var.region
+  redis_version  = "6.x"
+  authorized_network = var.network
+}
+
+output "redis_host" { value = google_redis_instance.feast_redis.host }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/gcp/bigtable/main.tf
+resource "google_bigtable_instance" "feast" {
+  project      = var.project
+  name         = "${var.stage}-feast-instance"
+  cluster {
+    cluster_id   = "${var.stage}-feast-cluster"
+    zone         = var.zone
+    num_nodes    = var.num_nodes
+    storage_type = "SSD"
+  }
+  instance_type = "PRODUCTION"
+}
+
+output "bigtable_instance_id" { value = google_bigtable_instance.feast.name }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/s3/lakefs_bucket.tf
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws" }
+  }
+}
+provider "aws" { region = var.region }
+
+resource "aws_s3_bucket" "lakefs" {
+  bucket = var.bucket_name
+  acl    = "private"
+  versioning { enabled = true }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm = "aws:kms"
+        kms_master_key_id = var.kms_key_id
+      }
+    }
+  }
+  lifecycle_rule {
+    enabled = true
+    noncurrent_version_expiration { days = 30 }
+  }
+}
+
+output "bucket_name" { value = aws_s3_bucket.lakefs.bucket }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/vault/main.tf
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws" }
+  }
+}
+provider "aws" { region = var.region }
+
+resource "aws_ebs_volume" "vault_data" {
+  availability_zone = var.availability_zone
+  size              = var.volume_size_gb
+  type              = "gp3"
+  encrypted         = true
+}
+
+resource "aws_instance" "vault" {
+  ami           = var.ami
+  instance_type = var.instance_type
+  subnet_id     = var.subnet_id
+  tags = { Name = "${var.stage}-vault-instance" }
+}
+
+output "vault_instance_ip" { value = aws_instance.vault.public_ip }
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/pentest/orchestrate_pentest.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate pre-pentest scans, gather artifacts and prepare vendor intake bundle.
+#
+OUTDIR=${OUTDIR:-/tmp/aegis-pentest-$(date -u +%Y%m%dT%H%M%SZ)}
+mkdir -p "$OUTDIR"
+
+echo "Running trivy scan on top images"
+if command -v trivy >/dev/null 2>&1; then
+  kubectl get pods --all-namespaces -o jsonpath='{..image}' | tr -s '[[:space:]]' '\n' | sort -u > "$OUTDIR/images.txt"
+  while read -r img; do
+    echo "Scanning $img"
+    trivy image --severity HIGH,CRITICAL -o "$OUTDIR/$(echo $img | tr '/:' '_' ).json" "$img" || true
+  done < "$OUTDIR/images.txt"
+else
+  echo "trivy not installed; skipping image scans"
+fi
+
+echo "Running kube-bench (CIS checks) if available"
+if command -v kube-bench >/dev/null 2>&1; then
+  kube-bench run --json > "$OUTDIR/kube-bench.json" || true
+fi
+
+echo "Collect cluster manifests and helm releases"
+kubectl get all --all-namespaces -o yaml > "$OUTDIR/k8s_all.yaml"
+helm list -A -o yaml > "$OUTDIR/helm_list.yaml"
+
+echo "Collect Vault policies & audit config"
+if command -v vault >/dev/null 2>&1; then
+  vault audit list -format=json > "$OUTDIR/vault_audit.json" || true
+  vault policy list -format=json > "$OUTDIR/vault_policies.json" || true
+fi
+
+echo "Create vendor intake CSV"
+cat > "$OUTDIR/vendor_intake.csv" <<CSV
+component,contact,notes
+kubernetes,ops@example.com,Kubeconfig and API access
+vault,security@example.com,VAULT_ADDR and admin token
+object-storage,storage@example.com,S3/GCS/COS access to sample artifacts
+CSV
+
+echo "Pentest bundle prepared at $OUTDIR"
+
*** End Patch
*** Begin Patch
*** Add File:prod/vault/audit_forwarding/fluentd_forwarder.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: vault-fluentd-config
+  namespace: ops
+data:
+  fluent.conf: |
+    <source>
+      @type tail
+      path /var/log/vault_audit.log
+      pos_file /var/log/fluentd-vault.pos
+      tag vault.audit
+      <parse>
+        @type none
+      </parse>
+    </source>
+    <match vault.audit>
+      @type forward
+      <server>
+        host syslog.col.example.com
+        port 24224
+      </server>
+    </match>
+
+---
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: fluentd-vault
+  namespace: ops
+spec:
+  selector:
+    matchLabels: { app: fluentd-vault }
+  template:
+    metadata:
+      labels: { app: fluentd-vault }
+    spec:
+      containers:
+      - name: fluentd
+        image: fluent/fluentd:v1.14
+        volumeMounts:
+        - name: config
+          mountPath: /fluentd/etc
+        - name: varlog
+          mountPath: /var/log
+      volumes:
+      - name: config
+        configMap:
+          name: vault-fluentd-config
+      - name: varlog
+        hostPath:
+          path: /var/log
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/mtls/generate_and_apply_mtls.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Generate a small CA & server cert and apply as k8s secrets for mTLS usage with webhooks.
+#
+NS=${NS:-ops}
+SECRET_NAME=${SECRET_NAME:-aegis-tls}
+TEMP_DIR=$(mktemp -d)
+
+openssl genrsa -out $TEMP_DIR/ca.key 4096
+openssl req -x509 -new -nodes -key $TEMP_DIR/ca.key -subj "/CN=aegis-root-ca" -days 3650 -out $TEMP_DIR/ca.crt
+openssl genrsa -out $TEMP_DIR/server.key 2048
+openssl req -new -key $TEMP_DIR/server.key -subj "/CN=aegis-internal.svc.cluster.local" -out $TEMP_DIR/server.csr
+openssl x509 -req -in $TEMP_DIR/server.csr -CA $TEMP_DIR/ca.crt -CAkey $TEMP_DIR/ca.key -CAcreateserial -out $TEMP_DIR/server.crt -days 365
+
+kubectl create ns "$NS" || true
+kubectl create secret tls "$SECRET_NAME" --cert="$TEMP_DIR/server.crt" --key="$TEMP_DIR/server.key" -n "$NS" --dry-run=client -o yaml | kubectl apply -f -
+kubectl create secret generic aegis-ca --from-file=ca.crt="$TEMP_DIR/ca.crt" -n "$NS" --dry-run=client -o yaml | kubectl apply -f -
+
+echo "mTLS secrets applied: secret/$SECRET_NAME and secret/aegis-ca in namespace $NS"
+
*** End Patch
*** Begin Patch
*** Add File:prod/reliability/snapshot_and_dr/run_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run a DR drill: snapshot Vault, backup lakeFS to backup bucket, and optionally restore (dry-run by default).
+#
+DR_PREFIX=${DR_PREFIX:-aegis-drill-$(date -u +%Y%m%dT%H%M%SZ)}
+RECOVER=${RECOVER:-0}
+BACKUP_BUCKET=${BACKUP_BUCKET:-aegis-lakefs-backups}
+
+echo "Vault snapshot"
+SNAP="/tmp/vault-snapshot-$(date -u +%s).snap"
+vault operator raft snapshot save "$SNAP"
+echo "Snapshot saved to $SNAP"
+
+echo "Sync lakeFS to backup bucket s3://$BACKUP_BUCKET/$DR_PREFIX"
+aws s3 sync s3://aegis-lakefs s3://$BACKUP_BUCKET/$DR_PREFIX
+
+if [ "$RECOVER" -eq 1 ]; then
+  echo "Restoring Vault snapshot (destructive) - operator must ensure safety"
+  vault operator raft snapshot restore "$SNAP"
+  echo "Restore done (follow Vault cluster rejoin steps)"
+fi
+
+echo "DR drill completed (RECOVER=$RECOVER). Artifacts uploaded to s3://$BACKUP_BUCKET/$DR_PREFIX"
+
*** End Patch
*** Begin Patch
*** Add File:prod/serving/perf/run_triton_perf.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# run_triton_perf.sh --host <host> --model <model> --requests <N> --batch <size> --concurrency <c>
+#
+HOST=""
+MODEL="aegis-triton-model"
+REQS=200
+BATCH=8
+CONCURRENCY=4
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --host) HOST="$2"; shift 2;;
+    --model) MODEL="$2"; shift 2;;
+    --requests) REQS="$2"; shift 2;;
+    --batch) BATCH="$2"; shift 2;;
+    --concurrency) CONCURRENCY="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+if [ -z "$HOST" ]; then
+  echo "Usage: $0 --host <host> ..."
+  exit 1
+fi
+
+if command -v perf_analyzer >/dev/null 2>&1; then
+  echo "Running perf_analyzer with batch=$BATCH concurrency=$CONCURRENCY"
+  perf_analyzer -m "$MODEL" -u "$HOST" -b "$BATCH" -n "$REQS" --concurrency-range 1:$CONCURRENCY || true
+else
+  echo "perf_analyzer not available; falling back to k6 HTTP tests"
+  if ! command -v k6 >/dev/null 2>&1; then
+    echo "Install perf_analyzer or k6 to run perf tests."
+    exit 2
+  fi
+  cat > /tmp/k6_script.js <<'K6'
+import http from 'k6/http';
+import { sleep } from 'k6';
+export let options = { vus: 10, duration: '30s' };
+export default function () {
+  http.post(__ENV.URL, JSON.stringify({inputs:[{name:'input__0',shape:[1,3,224,224],datatype:'FP32',data:[0]}]}), { headers: { 'Content-Type': 'application/json' }});
+  sleep(0.1);
+}
+K6
+  URL="https://${HOST}/v1/models/${MODEL}:predict"
+  URL="${URL}" k6 run /tmp/k6_script.js || true
+fi
+
*** End Patch
*** Begin Patch
*** Add File:prod/serving/autoscale/keda/tuner/suggest_thresholds.py
+#!/usr/bin/env python3
+"""
+Basic tuner that reads a perf sweep results directory and suggests KEDA Prometheus threshold values.
+Assumes text files produced by perf_analyzer with measured GPU utilization or latency lines.
+This is a heuristic starter that you should adapt to your metrics.
+"""
+import sys, os, re, statistics
+
+def parse_perf_file(path):
+    # naive: find lines with "Throughput" or "Latency"
+    with open(path) as f:
+        data=f.read()
+    lat = re.findall(r'Latency.*?([\d\.]+)ms', data)
+    thr = re.findall(r'Throughput.*?([\d\.]+)', data)
+    return {'lat_ms': [float(x) for x in lat] if lat else [], 'thr': [float(x) for x in thr] if thr else []}
+
+def main(dirpath):
+    lat_all=[]
+    for fn in os.listdir(dirpath):
+        vals=parse_perf_file(os.path.join(dirpath,fn))
+        lat_all.extend(vals['lat_ms'])
+    if lat_all:
+        p95 = sorted(lat_all)[int(0.95*len(lat_all))-1]
+        suggested = max(10, int(60 * (0.5 / (p95/1000.0))))  # heuristic
+        print("Suggested KEDA threshold (metric: gpu_util %) =", suggested)
+    else:
+        print("No latency data found; output default threshold 60")
+
+if __name__=="__main__":
+    if len(sys.argv)<2:
+        print("Usage: suggest_thresholds.py <perf_results_dir>")
+    else:
+        main(sys.argv[1])
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/volcano/checkpoint_hooks/README.md
+# Checkpointing hooks for preemptible/spot workloads
+
+This folder documents approaches to implement checkpointing for training jobs:
+- Upload model checkpoints periodically to S3/COS.
+- On job start, resume from latest checkpoint if present.
+- Provide init containers or wrapper scripts that handle checkpoint upload/download.
+
+Files:
+- example wrapper scripts provided in prod/autoscale/volcano/checkpoint_hooks/*.sh
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/volcano/checkpoint_hooks/upload_checkpoint.sh
+#!/usr/bin/env bash
+set -euo pipefail
+CHECKPOINT_DIR=${1:-/workspace/checkpoints}
+DEST=${2:-s3://aegis-checkpoints}
+echo "Uploading checkpoints from $CHECKPOINT_DIR to $DEST"
+aws s3 sync "$CHECKPOINT_DIR" "$DEST" --storage-class STANDARD_IA
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/attestation/TPM_README.md
+# Production Edge/IoT Attestation notes
+
+This folder contains reference code for TPM-backed attestation and enrollment. Operator responsibilities:
+- Integrate with device TPM (tpm2-tools) on devices.
+- Maintain a secure CA for signing device certificates (HSM-backed).
+- Use attestation server to validate quotes, enroll device, and mint device certificates.
+- Use device-side cosign/TSS-based signing when available; bundle offline Rekor artifacts for devices without connectivity.
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/attestation/device_agent_demo/README.md
+# Device agent demo
+
+Contains a demo agent that:
+- Generates key material (software/TSS if available),
+- Creates CSR and sends to enrollment server,
+- Receives signed certificate and performs signed artifact creation.
+
+This is a demo only; replace with TPM-backed flows for production.
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/attestation/enroll_server/README.md
+# Enrollment server for devices (reference)
+
+The enroll server validates attestation evidence and issues device certs. For production:
+- Use TEE or TPM-based verification.
+- Ensure rate limiting, replay protection and audit logs.
+- Store device metadata in a hardened DB.
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/connectors/README.md
+# Quantum connector scaffolds
+
+This folder includes starter connectors and Argo orchestrations for hybrid quantum/classical jobs:
+- AWS Braket connector and example
+- IBM Qiskit connector and example
+- Local simulator wrapper
+
+These are templates: attach provider credentials and test in a small pilot project before productionizing.
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/connectors/aws_braket_template.py
+#!/usr/bin/env python3
+"""
+AWS Braket connector template. Requires AWS credentials and Braket account provisioning.
+Use this template to submit small circuits or hybrid tasks to Braket.
+"""
+from braket.circuits import Circuit
+from braket.aws import AwsDevice
+
+def run_braket_job(shots=100):
+    device = AwsDevice("arn:aws:braket:::device/quantum-simulator/amazon/sv1")
+    circuit = Circuit().h(0).cnot(0,1).measure([0,1])
+    task = device.run(circuit, shots=shots)
+    print("Braket task id:", task.id)
+
+if __name__=="__main__":
+    run_braket_job()
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/auth_flow.py
+"""
+Implement aegis SDK auth flow: browser/OIDC device flow + token refresh skeleton.
+This module stores short-lived tokens in ~/.aegis/credentials.json
+"""
+import os, json, time, webbrowser, requests
+CRED_PATH=os.path.expanduser("~/.aegis/credentials.json")
+
+def save_token(data):
+    os.makedirs(os.path.dirname(CRED_PATH), exist_ok=True)
+    data['saved_at']=int(time.time())
+    with open(CRED_PATH,'w') as f: json.dump(data,f)
+
+def load_token():
+    if not os.path.exists(CRED_PATH): return {}
+    return json.load(open(CRED_PATH))
+
+def refresh_token_if_needed():
+    tok=load_token()
+    if not tok: return False
+    if tok.get('expires_at',0)-time.time() < 300 and tok.get('refresh_token'):
+        resp=requests.post(tok['token_url'], data={'grant_type':'refresh_token','refresh_token':tok['refresh_token'],'client_id':tok['client_id']})
+        if resp.status_code==200:
+            new=resp.json()
+            new['expires_at']=time.time()+new.get('expires_in',3600)
+            save_token(new)
+            return True
+    return False
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_cli/aegisctl_auth.md
+# aegisctl authentication usage
+
+Provide 'aegisctl login' that opens an OIDC browser flow or uses device flow. Tokens are stored in ~/.aegis/credentials.json.
+
+CLI commands example:
+- aegisctl login --provider github
+- aegisctl token refresh
+- aegisctl deploy --model-uri s3://...
+
+CI: use GitHub OIDC to exchange for Vault token via Vault JWT auth (see prod/runbooks/tenant_iam_finalization.md).
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/tenant_iam_finalization.md
+# Tenant & IAM finalization runbook
+
+This runbook walks the operator through finalizing identities, KMS bindings and GitHub secrets per cloud.
+
+Steps (per cloud)
+- AWS: run tenant-finalization/aws/finalize_aws_tenant.sh and save printed secrets to GitHub repo secrets.
+- GCP: run tenant-finalization/gcp/finalize_gcp_tenant.sh and complete provider steps in console for workload identity.
+- Azure: run tenant-finalization/azure/finalize_azure_tenant.sh, create federated credential and save AZURE_* secrets.
+- OCI/Alibaba/IBM: follow the respective scripts' printed guidance and set secrets.
+
+Verification
+- Trigger the provider CI signing workflow (e.g., .github/workflows/oci_oidc_cosign_sign.yml) and confirm cosign sign succeeds and Rekor entries appear.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/ha_and_managed_services.md
+# HA & Managed services runbook
+
+Provisioning steps
+1) Use terraform/managed/* modules to create managed Redis (ElastiCache/MemoryStore), Bigtable (optional), and a hardened S3 bucket for lakeFS.
+2) Deploy lakeFS with Postgres backing using prod/feature-store/helm/lakefs-values-production.yaml and verify durability.
+3) Migrate Feast registry/offline-store to use new managed stores; run end-to-end ingestion & training reproduction.
+
+Failover validation
+- Simulate node failure in managed Redis (or take a replica offline) and ensure Feast can still serve online features.
+- Test lakeFS snapshot restoration to a recovery bucket and replay a training pipeline.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/security_and_pentest.md
+# Security hardening & pen-test runbook
+
+Steps:
+1) Run prod/security/pentest/orchestrate_pentest.sh to gather artifacts and run initial scans.
+2) Engage external pentest vendor with vendor_intake.csv and artifacts.
+3) Track issues in ticketing system, remediate and rerun scans until green.
+4) Enable Vault audit forwarding (prod/vault/audit_forwarding/fluentd_forwarder.yaml) and verify SIEM ingestion.
+5) Apply mTLS via prod/security/mtls/generate_and_apply_mtls.sh and enforce network policies (prod/security/networkpolicies).
+6) Enforce least privilege IAM by reviewing policies created by tenant-finalization scripts and narrowing resource ARNs.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/reliability_dr.md
+# Reliability & DR runbook
+
+Weekly
+- Cron snapshot for Vault (prod/reliability/cron_vault_snapshot.yaml)
+- lakeFS backup to backup bucket
+
+Quarterly
+- Run DR drill (prod/reliability/snapshot_and_dr/run_drill.sh) in dry-run mode and produce DR report.
+ - Run full recovery test on a recovery cluster (operator supervised).
+
+Post-DR
+- Document gaps, fix runbooks and re-run until acceptable RTO/RPO reached.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/perf_and_autoscale.md
+# Performance tuning & autoscale runbook
+
+1) Run perf sweeps using prod/serving/perf/sweep_batch.sh and run_triton_perf.sh to gather throughput/latency metrics across batch/concurrency.
+2) Use prod/serving/autoscale/keda/tuner/suggest_thresholds.py on results and adjust KEDA thresholds (prod/serving/autoscale/keda/prometheus-scaledobject.yaml).
+3) For training, use Volcano queues and apply checkpoint hooks (prod/autoscale/volcano/checkpoint_hooks/upload_checkpoint.sh).
+4) Integrate kube-cost and schedule periodic cost/carb on reports (prod/cost/kubecost/*).
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/edge_prod.md
+# Edge/IoT productionization runbook
+
+1) Replace software-only attestation with TPM/TEE-based flows:
+ - Devices must generate EK/AK and provide attestation quotes.
+ - Server validates quote, issues certs, records device metadata.
+2) OTA pipeline:
+ - Sign firmware/images with cosign and record to Rekor.
+ - Use phased rollout and canary groups.
+ - For offline devices, provide offline Rekor bundle and secure channel for later upload.
+3) Fleet testing:
+ - Use device agent demo to test at scale and simulate intermittent connectivity and rollback scenarios.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/quantum_prod.md
+# Quantum production readiness runbook
+
+1) Pick a pilot provider (AWS Braket or IBM) and create small validated use-case.
+2) Integrate provider connector (prod/quantum/connectors/*) and run pilot jobs.
+3) Add cost gating and quotas to avoid runaway experiments.
+4) Assess whether quantum hybrid workflow improves business metrics; if so, integrate into pipelines with controlled promotion and provenance.
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/ci.yml
+name: Aegis SDK CI
+on:
+  push:
+    paths:
+      - "prod/devtools/aegis_sdk/**"
+  workflow_dispatch:
+jobs:
+  test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install package
+        run: |
+          pip install -e prod/devtools/aegis_sdk
+      - name: Smoke test
+        run: |
+          python - <<'PY'
+from aegis.client import AegisClient
+print("ok", hasattr(AegisClient,'list_models'))
+PY
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/__init__.py
+from .client import AegisClient
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/client.py
+import os, requests
+class AegisClient:
+    def __init__(self, base_url):
+        self.base = base_url.rstrip('/')
+    @classmethod
+    def from_env(cls):
+        return cls(os.environ.get("AEGIS_API","http://aegis-api.local"))
+    def list_models(self):
+        try:
+            return requests.get(f"{self.base}/models").json()
+        except Exception:
+            return []
+    def deploy_model(self, model_uri, infra="kserve"):
+        try:
+            r = requests.post(f"{self.base}/deploy", json={"model_uri": model_uri, "infra": infra})
+            return r.json()
+        except Exception as e:
+            return {"error": str(e)}
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_cli/README.md
+# aegisctl
+
+aegisctl is a small CLI that wraps aegis-sdk for common operator tasks:
+- login, deploy, list models, run validations.
+
+Install:
+  pip install -e prod/devtools/aegis_sdk
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_cli/aegisctl.py
+#!/usr/bin/env python3
+import click
+from aegis.client import AegisClient
+@click.group()
+def cli():
+    pass
+@cli.command()
+def list_models():
+    c = AegisClient.from_env()
+    print(c.list_models())
+@cli.command()
+@click.argument("model_uri")
+def deploy(model_uri):
+    c = AegisClient.from_env()
+    print(c.deploy_model(model_uri))
+if __name__ == "__main__":
+    cli()
+
*** End Patch
*** Begin Patch
*** Add File:prod/validation/validate_end_to_end.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# high-level end-to-end validation script for production readiness
+#
+echo "1) check vault"
+if command -v vault >/dev/null 2>&1; then vault status || true; else echo "vault cli missing"; fi
+
+echo "2) check inferenceservice"
+kubectl get inferenceservice aegis-triton-model -n ml -o yaml || true
+
+echo "3) run perf quick test (operator sets INFERENCE_HOST)"
+if [ -n "${INFERENCE_HOST:-}" ]; then
+  bash prod/serving/perf/run_triton_perf.sh --host "$INFERENCE_HOST" || true
+else
+  echo "INFERENCE_HOST not set; skip perf"
+fi
+
+echo "4) trigger MLflow promote & sign in CI manually and verify Rekor entries"
+
+echo "5) verify drift alert triggers Argo retrain (simulate metric)"
+
+echo "6) run DR drill dry-run"
+bash prod/reliability/snapshot_and_dr/run_drill.sh || true
+
+echo "Validation complete (operator must inspect logs, rekor, and SLO dashboards)"
+
*** End Patch
*** Begin Patch
*** Add File:prod/finish/production_go_live_checklist.md
+# Production Go-Live Checklist
+
+1) Tenant/IAM finalization for each cloud (tenant-finalization/*)
+2) Provision managed services (terraform/managed/*) and point Feast/lakeFS to them
+3) Deploy Vault HA with auto-unseal and configure audit forwarding (prod/vault)
+4) Deploy KServe/Triton with production values and run perf sweeps (prod/serving/perf)
+5) Tune KEDA/Volcano and test spot/preempt flows (prod/autoscale)
+6) Run pentest orchestration and remediate (prod/security/pentest)
+7) Run DR drill and validate restore (prod/reliability)
+8) Replace demo edge attestation with TPM/TEE flow and test OTA pipeline (prod/edge)
+9) Pilot quantum connectors (prod/quantum) if required
+10) Harden aegisctl and aegis-sdk, run CI and smoke tests
+11) Execute full acceptance: prod/validation/validate_end_to_end.sh and validate SLOs and Rekor provenance
+
*** End Patch
*** End Patch
