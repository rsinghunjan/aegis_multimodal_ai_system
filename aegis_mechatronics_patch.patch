*** Begin Patch
*** Add File:hil/can_eth_hil_replay_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: hil-can-eth-replay-
+  namespace: aegis
+spec:
+  entrypoint: hil-can-eth
+  templates:
+    - name: hil-can-eth
+      steps:
+        - - name: fetch-trace
+            template: fetch-trace
+        - - name: replay-can
+            template: replay-can
+            when: "{{steps.fetch-trace.outputs.parameters.protocol}} == can"
+            arguments:
+              parameters:
+                - name: trace
+                  value: "{{steps.fetch-trace.outputs.parameters.trace}}"
+        - - name: replay-ethercat
+            template: replay-ethercat
+            when: "{{steps.fetch-trace.outputs.parameters.protocol}} == ethercat"
+            arguments:
+              parameters:
+                - name: trace
+                  value: "{{steps.fetch-trace.outputs.parameters.trace}}"
+        - - name: run-safety-oracles
+            template: safety-oracles
+            arguments:
+              parameters:
+                - name: report
+                  value: /tmp/hil_safety_report.json
+
+    - name: fetch-trace
+      outputs:
+        parameters:
+          - name: trace
+            valueFrom:
+              path: /tmp/trace_path.txt
+          - name: protocol
+            valueFrom:
+              path: /tmp/trace_protocol.txt
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            # Try S3 first, fallback to bundled sample
+            if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+              # operator: adjust prefix as needed
+              aws s3 ls s3://${MODEL_ARTIFACT_BUCKET}/hil/ --recursive --human-readable | tail -n1 || true
+              aws s3 cp s3://${MODEL_ARTIFACT_BUCKET}/hil/sample_can_trace.json /tmp/trace.json || true
+              echo "/tmp/trace.json" > /tmp/trace_path.txt
+              echo "can" > /tmp/trace_protocol.txt
+            else
+              cp hil/replay/sample_trace_can.json /tmp/trace.json || true
+              echo "/tmp/trace.json" > /tmp/trace_path.txt
+              echo "can" > /tmp/trace_protocol.txt
+            fi
+            ls -l /tmp/trace.json
+
+    - name: replay-can
+      inputs:
+        parameters:
+          - name: trace
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            pip install python-can || true
+            python3 hil/replay/can_replayer.py --trace "{{inputs.parameters.trace}}" --interface ${CAN_INTERFACE:-can0} --dry-run ${HIL_DRY_RUN:-true}
+
+    - name: replay-ethercat
+      inputs:
+        parameters:
+          - name: trace
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            python3 hil/replay/ethercat_replayer.py --trace "{{inputs.parameters.trace}}" --interface ${ETHERCAT_IFACE:-eth0} --dry-run ${HIL_DRY_RUN:-true}
+
+    - name: safety-oracles
+      inputs:
+        parameters:
+          - name: report
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            pip install pandas || true
+            python3 hil/replay/safety_oracles.py --trace /tmp/trace.json --out /tmp/hil_safety_report.json || true
+            cat /tmp/hil_safety_report.json
+
*** End Patch
*** Begin Patch
*** Add File:hil/replay/can_replayer.py
+#!/usr/bin/env python3
+"""
+CAN bus replayer for HIL rigs using SocketCAN (python-can).
+Replays CAN frames from a sequence-first trace file (JSON) into a CAN interface.
+Supports dry-run (no hardware) and safe backoff/limits.
+"""
+import argparse, json, time, os
+
+def replay_can(trace, interface='can0', dry_run=True, inter_delay_ms=1):
+    print(f"Loading trace {trace}")
+    obj = json.load(open(trace))
+    steps = obj.get("steps", [])
+    print(f"Trace contains {len(steps)} steps (first 5 shown):")
+    for s in steps[:5]:
+        print(s)
+    if dry_run:
+        print("DRY RUN enabled - not sending frames to hardware. Simulating timings.")
+        for s in steps:
+            time.sleep(inter_delay_ms/1000.0)
+        print("Dry-run replay complete")
+        return
+    try:
+        import can
+    except Exception as e:
+        raise RuntimeError("python-can not installed or SocketCAN not available: " + str(e))
+    bus = can.interface.Bus(interface, bustype='socketcan')
+    sent = 0
+    for s in steps:
+        # expected step structure: {"timestamp_ns":..., "can_id": 512, "data": "deadbeef"}
+        can_id = s.get("can_id")
+        data_hex = s.get("data","")
+        if data_hex.startswith("0x"):
+            data_hex = data_hex[2:]
+        data = bytes.fromhex(data_hex)
+        msg = can.Message(arbitration_id=int(can_id), data=data, is_extended_id=False)
+        try:
+            bus.send(msg, timeout=0.01)
+            sent += 1
+        except Exception as e:
+            print("Send failed", e)
+        # maintain timing if provided (not strict real-time)
+        time.sleep(inter_delay_ms/1000.0)
+    print(f"Sent {sent} CAN frames to {interface}")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--interface", default="can0")
+    p.add_argument("--dry-run", type=lambda x: x.lower() in ("true","1","yes"), default=True)
+    p.add_argument("--inter-delay-ms", type=int, default=1)
+    args = p.parse_args()
+    replay_can(args.trace, args.interface, args.dry_run, args.inter_delay_ms)
+
*** End Patch
*** Begin Patch
*** Add File:hil/replay/ethercat_replayer.py
+#!/usr/bin/env python3
+"""
+EtherCAT replayer stub.
+Real EtherCAT replayer requires an EtherCAT master (e.g., SOEM/IgH EtherCAT) and proper privileges.
+This scaffold parses sequence traces and can run in dry-run mode.
+Operator must adapt to their EtherCAT stack and mapping (PDI/SDO/SM).
+"""
+import argparse, json, time
+
+def replay_ethercat(trace, interface='eth0', dry_run=True, inter_delay_ms=1):
+    print("Loading EtherCAT trace", trace)
+    obj = json.load(open(trace))
+    steps = obj.get("steps",[])
+    print(f"{len(steps)} steps loaded; dry_run={dry_run}")
+    if dry_run:
+        for s in steps[:20]:
+            print("DRY:", s)
+        print("Dry-run EtherCAT replay complete")
+        return
+    # In production: instantiate SOEM/IgH EtherCAT master, map slaves, and write process data
+    # This is environment-specific and requires root privileges.
+    raise RuntimeError("EtherCAT hardware replay not implemented in scaffold. Adapt with SOEM or vendor SDK.")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--interface", default="eth0")
+    p.add_argument("--dry-run", type=lambda x: x.lower() in ("true","1","yes"), default=True)
+    p.add_argument("--inter-delay-ms", type=int, default=1)
+    args = p.parse_args()
+    replay_ethercat(args.trace, args.interface, args.dry_run, args.inter_delay_ms)
+
*** End Patch
*** Begin Patch
*** Add File:hil/replay/safety_oracles.py
+#!/usr/bin/env python3
+"""
+Run safety checks against a replay trace and produce a JSON report.
+Example checks:
+ - actuator command saturation
+ - commanded joint limits exceeded
+ - watchdog/resets present
+ - missed_deadlines approximated by timestamp gaps
+"""
+import argparse, json, os, math
+
+def run_safety_checks(trace, out="/tmp/hil_safety_report.json", max_delta_ms=50):
+    obj = json.load(open(trace))
+    steps = obj.get("steps", [])
+    report = {"total_steps": len(steps), "violations": [], "summary": {}}
+    last_ts = None
+    for i,s in enumerate(steps):
+        ts = s.get("timestamp_ns")
+        if ts is not None:
+            if last_ts:
+                delta_ms = (ts - last_ts) / 1e6
+                if delta_ms > max_delta_ms:
+                    report["violations"].append({"type":"missed_deadline","index":i,"delta_ms":delta_ms})
+            last_ts = ts
+        # example actuator saturation
+        act = s.get("actuators",{})
+        for k,v in act.items():
+            if abs(v) > 1e3:  # example threshold
+                report["violations"].append({"type":"actuator_saturation","index":i,"actuator":k,"value":v})
+        # safety state
+        safety = s.get("safety_state","ok")
+        if safety != "ok":
+            report["violations"].append({"type":"safety_state_bad","index":i,"state":safety})
+    report["summary"]["violations_count"] = len(report["violations"])
+    json.dump(report, open(out,"w"), indent=2)
+    print("Wrote safety report to", out)
+    return out
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--out", default="/tmp/hil_safety_report.json")
+    p.add_argument("--max-delta-ms", type=int, default=50)
+    args = p.parse_args()
+    run_safety_checks(args.trace, args.out, args.max_delta_ms)
+
*** End Patch
*** Begin Patch
*** Add File:docs/HIL_CAN_ETH_README.md
+# HIL CAN & EtherCAT Replay - Quickstart (Aegis)
+
+Overview
+- Argo workflow: hil/can_eth_hil_replay_workflow.yaml
+- CAN replayer: hil/replay/can_replayer.py (uses SocketCAN / python-can)
+- EtherCAT replayer stub: hil/replay/ethercat_replayer.py (operator must integrate SOEM or vendor SDK)
+- Safety oracle checks: hil/replay/safety_oracles.py
+
+Run a dry-run replay (recommended) from a host with kubectl/argo:
+- argo submit hil/can_eth_hil_replay_workflow.yaml -n aegis --watch
+
+To run against hardware:
+- Mount host SocketCAN device into container (privileged, careful).
+- Set HIL_DRY_RUN=false in workflow/container env or pass via ConfigMap.
+- Ensure CAN interface (e.g., can0) exists and is configured (sudo ip link set can0 up type can bitrate 500000)
+
+Notes:
+- EtherCAT requires an EtherCAT master stack (SOEM/IgH) and is environment-specific. The example is a dry-run scaffold â€” do not run on critical hardware without safety interlocks.
+- Always ensure independent hardware safety is present (kill switch) when doing HIL tests.
+
*** End Patch
*** Begin Patch
*** Add File:edge/real_time/kernel_tuning.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Kernel tuning and runtime setup for deterministic real-time operation (PREEMPT_RT / real-time Linux)
+# This script makes best-effort recommendations and can apply sysctl & CPU isolation. Run as root.
+#
+echo "Applying basic real-time tuning (dry-run by default). Use --apply to change system settings."
+DRY_RUN=1
+if [ "${1:-}" = "--apply" ]; then DRY_RUN=0; fi
+
+# Example CPU isolation (operator must choose cores)
+ISOLATED_CPUS=${ISOLATED_CPUS:-"2-3"}
+echo "Isolated CPUs: $ISOLATED_CPUS"
+
+if [ "$DRY_RUN" -eq 1 ]; then
+  echo "DRY RUN: would set kernel.boot parameters: isolcpus=${ISOLATED_CPUS} nohz_full=${ISOLATED_CPUS} rcu_nocbs=${ISOLATED_CPUS}"
+  echo "DRY RUN: would set /proc/sys/kernel/sched_rt_runtime_us to -1 and set NIC IRQ affinities"
+  exit 0
+fi
+
+echo "Applying sysctl tuning..."
+sysctl -w kernel.sched_rt_runtime_us=-1
+sysctl -w vm.swappiness=10
+sysctl -w net.core.netdev_max_backlog=5000
+sysctl -w net.core.rmem_max=16777216
+sysctl -w net.core.wmem_max=16777216
+
+echo "Pinning IRQs for NICs to non-isolated CPUs (operator may adapt NIC name)"
+for irq in $(grep -n eth /proc/interrupts | cut -d: -f1); do
+  echo "Setting IRQ $irq to affinity CPU 0 (example)"
+  echo 1 > /proc/irq/$irq/smp_affinity
+done
+
+echo "Setting CPU governor to performance"
+for cpu in /sys/devices/system/cpu/cpu[0-9]*; do
+  if [ -f "$cpu/cpufreq/scaling_governor" ]; then
+    echo performance > "$cpu/cpufreq/scaling_governor"
+  fi
+done
+
+echo "Kernel tuning applied. Reboot recommended for isolcpus/nohz_full changes to take effect."
+
*** End Patch
*** Begin Patch
*** Add File:edge/real_time/systemd/real_time_controller.service
+[Unit]
+Description=Aegis Real-Time Controller Runner
+After=network.target
+
+[Service]
+Type=simple
+User=root
+ExecStart=/opt/aegis/real_time/run_controller.sh
+Restart=on-failure
+LimitCORE=infinity
+LimitNOFILE=65536
+CPUAffinity=2-3
+Nice=-10
+
+[Install]
+WantedBy=multi-user.target
+
*** End Patch
*** Begin Patch
*** Add File:edge/real_time/README_RT_SETUP.md
+# Edge Real-Time Host Setup (Aegis)
+
+This folder contains a kernel tuning script and a systemd unit template to run deterministic controllers on real-time Linux.
+
+Steps (recommended):
+1. Review and run kernel tuning in dry-run mode:
+   sudo bash edge/real_time/kernel_tuning.sh
+2. If comfortable, apply:
+   sudo bash edge/real_time/kernel_tuning.sh --apply
+3. Place your controller launch script at /opt/aegis/real_time/run_controller.sh and make executable.
+4. Copy systemd unit:
+   sudo cp edge/real_time/systemd/real_time_controller.service /etc/systemd/system/
+   sudo systemctl daemon-reload
+   sudo systemctl enable --now real_time_controller.service
+
+Notes:
+- The script sets performance governor, sysctl tuning, and IRQ affinity. Operators must adapt NIC/IRQ selection and isolated CPU ranges to the target hardware.
+- Changing kernel boot parameters (isolcpus,nohz_full,rcu_nocbs) requires editing the bootloader (GRUB) and rebooting.
+
*** End Patch
*** Begin Patch
*** Add File:ci/cross_compile/Dockerfile.toolchain
+FROM ubuntu:22.04
+RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y build-essential cmake git python3 python3-pip qemu-user-static gcc-arm-none-eabi clang
+RUN pip3 install setuptools wheel
+WORKDIR /work
+ENTRYPOINT ["/bin/bash"]
+
*** End Patch
*** Begin Patch
*** Add File:ci/cross_compile/build_and_pack.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Cross-compile controller binary and package with model artifacts.
+# Expects:
+# - CONTROLLER_SRC_DIR mounted into /work/src
+# - TARGET_TRIPLE (e.g., arm-linux-gnueabihf) or cross toolchain available
+# - MODEL_ARTIFACT (onnx/tvm) in /work/model/model.onnx
+OUTDIR=${OUTDIR:-/work/out}
+mkdir -p "$OUTDIR"
+echo "Building controller (scaffold)"
+# Example: use clang to build a small C program - operator should adapt to real build system
+cat > /work/src/main.c <<'C'
+#include <stdio.h>
+int main(){ printf("Controller v1.0\\n"); return 0; }
+C
+gcc -O2 /work/src/main.c -o "$OUTDIR/controller"
+echo "Controller built at $OUTDIR/controller"
+
+# Package artifacts
+cp /work/model/* "$OUTDIR/" || true
+tar -czf "$OUTDIR/controller_package.tar.gz" -C "$OUTDIR" .
+echo "Packaged artifacts: $OUTDIR/controller_package.tar.gz"
+
*** End Patch
*** Begin Patch
*** Add File:ci/cross_compile/tvm_cross_compile_pipeline.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Convert ONNX model to TVM module for target (toy example - uses CPU llvm target)
+ONNX=${1:-/work/model/model.onnx}
+OUTDIR=${2:-/work/out/tvm}
+mkdir -p "$OUTDIR"
+python3 - <<PY
+import onnx, tvm, os
+from tvm import relay
+model = onnx.load("${ONNX}")
+shape_dict = {"input": (1,3,224,224)}
+mod, params = relay.frontend.from_onnx(model, shape_dict)
+target = "llvm"
+with tvm.transform.PassContext(opt_level=3):
+    lib = relay.build(mod, target=target, params=params)
+lib.export_library(os.path.join("${OUTDIR}","lib.so"))
+open(os.path.join("${OUTDIR}","params.txt"),"w").write(str(list(params.keys())))
+print("TVM build written to ${OUTDIR}")
+PY
+
*** End Patch
*** Begin Patch
*** Add File:ci/ota/ota_sign_and_package.sh
+#!/usr/bin/env bash
+set -euo pipefail
+PACKAGE=${1:-/work/out/controller_package.tar.gz}
+OUTDIR=${2:-/work/out/signed}
+mkdir -p "$OUTDIR"
+if [ ! -f "$PACKAGE" ]; then
+  echo "Package not found: $PACKAGE"
+  exit 2
+fi
+cp "$PACKAGE" "$OUTDIR/"
+BASENAME=$(basename "$PACKAGE")
+SIGFILE="$OUTDIR/${BASENAME}.sig"
+echo "Signing package (dry-run if signing helper missing)"
+python3 - <<PY
+import os,sys
+pkg="$PACKAGE"
+sigfile="$SIGFILE"
+try:
+    from production.policy.signing.sign_with_retry import sign_payload
+    data=open(pkg,"rb").read()
+    sig,meta = sign_payload(data, None)
+    open(sigfile,"wb").write(sig)
+    print("Signed package and wrote", sigfile)
+except Exception as e:
+    print("Signing helper not available, writing placeholder signature")
+    open(sigfile,"w").write("placeholder-signature")
+PY
+echo "Package and signature in $OUTDIR"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cross_compile_ota.yml
+name: Cross-Compile & OTA packaging
+on:
+  workflow_dispatch:
+
+jobs:
+  build-package:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Build toolchain image
+        run: docker build -t aegis/toolchain -f ci/cross_compile/Dockerfile.toolchain .
+      - name: Run cross-compile inside container
+        run: |
+          docker run --rm -v "${{ github.workspace }}:/work" aegis/toolchain bash -lc "bash /work/ci/cross_compile/build_and_pack.sh"
+      - name: Run TVM cross-compile (optional)
+        run: docker run --rm -v "${{ github.workspace }}:/work" aegis/toolchain bash -lc "bash /work/ci/cross_compile/tvm_cross_compile_pipeline.sh /work/model/model.onnx /work/out/tvm || true"
+      - name: Sign and package OTA artifact
+        run: docker run --rm -v "${{ github.workspace }}:/work" aegis/toolchain bash -lc "bash /work/ci/ota/ota_sign_and_package.sh /work/out/controller_package.tar.gz /work/out/signed"
+      - name: Upload artifact to S3 (if configured)
+        env:
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+        run: |
+          if [ -n "${{ secrets.S3_BUCKET }}" ]; then
+            aws s3 cp /work/out/signed/controller_package.tar.gz s3://${{ secrets.S3_BUCKET }}/ota/ || true
+            aws s3 cp /work/out/signed/controller_package.tar.gz.sig s3://${{ secrets.S3_BUCKET }}/ota/ || true
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:docs/EDGE_CONTROLLER_DEPLOY.md
+# Controller Cross-Compile, TVM Build, OTA & Signing - Quickstart
+
+This document explains the CI and packaging flow created by Aegis scaffolds:
+
+1) Cross-compile controller binary and package with model artifacts:
+   - Uses ci/cross_compile/Dockerfile.toolchain and ci/cross_compile/build_and_pack.sh
+   - Artifacts: controller, model.onnx (or TVM lib), packaged into controller_package.tar.gz
+
+2) TVM cross-compile pipeline:
+   - ci/cross_compile/tvm_cross_compile_pipeline.sh converts ONNX -> TVM lib (cpu/llvm by default)
+   - Adjust target for embedded toolchain (ARM/FPGA) as needed
+
+3) OTA signing & packaging:
+   - ci/ota/ota_sign_and_package.sh will attempt to sign package using production.policy.signing.sign_with_retry if available
+   - If signing helper not available, a placeholder signature is produced (DO NOT use for production)
+
+4) CI integration:
+   - .github/workflows/cross_compile_ota.yml runs the full flow in CI and uploads signed artifacts to S3 if configured
+
+Operator actions to finalize:
+- Replace scaffolded build steps with your real cross-toolchain commands and test on target hardware.
+- Ensure production HSM signing helper is available in CI runners for real signatures.
+- Configure S3_BUCKET / MODEL_ARTIFACT_BUCKET / EVIDENCE_BUCKET secrets in CI.
+
*** End Patch
*** End Patch
