*** Begin Patch
*** Add File: docs/cloudhsm_runbook.md
+CloudHSM Runbook — Initialization, PKCS11 Key Import, cosign PKCS11 Validation & Rotation Drill
+
+Purpose
+- Step-by-step guide operators use to initialize AWS CloudHSM, create/import keys for cosign PKCS11 signing, validate signing, enable CloudTrail auditing, and run a rotation drill.
+
+Scope & Assumptions
+- You have AWS account privileges to create CloudHSM clusters and manage keys.
+- You have an admin host (bastion/admin VM) where CloudHSM client tools and pkcs11 tools are installed.
+- cosign with PKCS11 support is installed on admin host (or use cosign built with pkcs11).
+- Rekor CLI is available for attestation checks.
+
+High-level steps
+1) Provision CloudHSM cluster (Terraform or Console)
+2) Initialize & claim HSMs (AWS Console / CLI guided)
+3) Configure CloudHSM client on admin host
+4) Create Crypto User (CU) and HSM key; label key for cosign (PKCS11 label)
+5) Validate cosign PKCS11 signing end-to-end against a sample manifest
+6) Enable CloudTrail and verify KMS/HSM events
+7) Perform rotation drill: create new key (or KMS key), update alias/label, update CI secret(s), validate and audit
+8) Acceptance checklist & teardown notes
+
+Detailed operator commands (run from admin host)
+
+A) Provision cluster (example using Terraform)
+ - Use infra/hsm/cloudhsm_full.tf or your infra tooling.
+   terraform init
+   terraform apply -var 'aws_region=us-west-2' -var 'vpc_id=<vpc>' -var 'subnet_ids=["subnet-..."]'
+
+B) Initialize & claim HSMs (one-time manual steps)
+ - In AWS Console > CloudHSM > Clusters: select cluster and click "Initialize".
+ - Follow the console to claim HSMs; wait until status is INITIALIZED.
+ - Note the cluster_id and HSM IPs.
+
+C) Configure CloudHSM client on admin host
+ - Install CloudHSM client and pkcs11 tools (Debian/Ubuntu example):
+   sudo apt-get update
+   sudo apt-get install -y cloudhsm-client libcloudhsm pkcs11-tool
+ - Configure client (replace cluster id / IP):
+   sudo /opt/cloudhsm/bin/configure -a <cluster-ip> -c <cluster-id>
+ - Verify connectivity:
+   sudo /opt/cloudhsm/bin/cloudhsm_client_util status
+
+D) Create Crypto User (CU) and label keys
+ - Using key_mgmt_util on admin host (interactive):
+   sudo /opt/cloudhsm/bin/key_mgmt_util
+   # inside utility:
+   # createUser CUadmin <password> - creates Crypto User (CU)
+ - Use pkcs11-tool to generate an RSA signing key and label it:
+   pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so --keypairgen --key-type rsa:2048 --label cosign-key --id 01
+ - Verify key:
+   pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so -O
+
+E) Validate cosign PKCS11 signing
+ - Prepare a small manifest:
+   echo '{"test":"aegis-cosign-pkcs11"}' > /tmp/test_manifest.json
+ - Run cosign sign using PKCS11:
+   export PKCS11_MODULE=/usr/lib/libcloudhsm_pkcs11.so
+   cosign sign --key "pkcs11:token=cosign-key?module-path=${PKCS11_MODULE}" /tmp/test_manifest.json
+ - Verify:
+   cosign verify /tmp/test_manifest.json
+ - If sign/verify succeeds, create Rekor entry:
+   rekor-cli upload --artifact /tmp/test_manifest.json
+
+F) Enable CloudTrail & audit
+ - Ensure CloudTrail recording is enabled for management events.
+ - Example query:
+   aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign
+ - Use hsm/cloudtrail/generate_audit_report.py with COSIGN_KEY_ID env to produce audit JSON.
+
+G) Rotation drill (KMS or HSM)
+ - KMS flow (automated):
+   aws kms create-key --description "cosign rotation test" --query KeyMetadata.KeyId --output text
+   aws kms create-alias --alias-name alias/aegis-cosign --target-key-id <new-key-id>
+   # Update CI secret COSIGN_KMS_KEY -> awskms:///<new-key-id>
+   # Validate sign with new key:
+   cosign sign --key "awskms:///<new-key-id>" /tmp/test_manifest.json
+ - CloudHSM flow (manual):
+   - Create new key via pkcs11-tool in HSM: pkcs11-tool --keypairgen --label cosign-key-rot1
+   - Update your CI secret to set COSIGN_PKCS11_LABEL=cosign-key-rot1
+   - Run cosign PKCS11 validation script
+ - After rotation: query CloudTrail for key usage, ensure only expected principals used.
+
+Acceptance checklist
+ - [ ] CloudHSM cluster shows status INITIALIZED and all HSMs healthy
+ - [ ] CloudHSM client installed & can list tokens/slots on admin host
+ - [ ] CU created and HSM signing key exists with label 'cosign-key'
+ - [ ] cosign sign/verify via PKCS11 succeeds on sample manifest
+ - [ ] Rekor entry created for signed sample manifest
+ - [ ] CloudTrail shows kms/sign or HSM sign events for the operation
+ - [ ] Rotation drill executed and CI secret updated; new key used successfully
+
+Rollback & remediation
+ - If signing fails: check pkcs11 module path, CU permissions, HSM network connectivity, and that cosign binary supports PKCS11.
+ - If CloudTrail has unexpected principals: revoke keys immediately and open incident.
+
+References
+ - AWS CloudHSM documentation
+ - cosign PKCS11 docs
+ - Rekor and sigstore docs
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/cloudhsm_admin_playbook.yml
+- name: CloudHSM Admin Playbook — post-provision operations
+  hosts: cloudhsm_admin
+  become: true
+  vars:
+    pkcs11_module_path: /usr/lib/libcloudhsm_pkcs11.so
+    cosign_pkcs11_label: cosign-key
+  tasks:
+    - name: Ensure client utilities installed (Debian-based)
+      apt:
+        name:
+          - cloudhsm-client
+          - libcloudhsm
+          - pkcs11-tool
+        state: present
+      ignore_errors: yes
+
+    - name: Deploy PKCS11 conf for cosign
+      copy:
+        dest: /etc/cosign/pkcs11.conf
+        content: |
+          module_path = {{ pkcs11_module_path }}
+          token_label = {{ cosign_pkcs11_label }}
+      mode: '0600'
+
+    - name: Place cosign PKCS11 validation script
+      copy:
+        src: hsm/pkcs11/cosign_pkcs11_validation.sh
+        dest: /usr/local/bin/cosign_pkcs11_validation.sh
+        mode: '0755'
+
+    - name: Run a smoke pkcs11-tool list (non-fatal)
+      command: pkcs11-tool --module "{{ pkcs11_module_path }}" --list-slots
+      register: pkout
+      ignore_errors: yes
+
+    - name: Show pkcs11 output
+      debug:
+        var: pkout.stdout_lines
+
*** End Patch
*** Begin Patch
*** Add File: docs/cloudhsm_acceptance_checklist.md
+CloudHSM Acceptance Checklist
+
+Operator: __________________   Date: __________
+
+Checklist:
+- Cluster: CloudHSM cluster created (cluster id: __________) and status INITIALIZED.
+- HSMs: All HSMs show healthy and in service.
+- Client: CloudHSM client installed on admin host and can reach cluster.
+- CU: Crypto User (CU) created with admin credential stored securely.
+- Key: PKCS11 key created and labeled (label used by cosign): __cosign-key__ (or specified label)
+- cosign: cosign sign/verify works using PKCS11 key (test artifact signed).
+- Rekor: Rekor entry exists for the signed artifact.
+- Audit: CloudTrail shows sign events for expected principal(s).
+- Rotation Drill: Key rotation executed and CI updated (or documented for HSM flow); new key validated.
+- Documentation: All steps and outputs captured in runbook and uploaded to compliance bucket.
+
+If any item fails, stop and escalate to cloud security + HSM vendor support.
+
*** End Patch
*** Begin Patch
*** Add File: docs/jetson_procurement_checklist.md
+Jetson/ARM Runner Procurement & Site Preparation Checklist
+
+Procurement
+- Device model: NVIDIA Jetson (TX2/NX/TX1) or equivalent ARM board — confirm CPU/ram/flash meets your workload.
+- Storage: NVMe or high-endurance eMMC/SD (recommend >=64GB).
+- RAM: recommend >=8GB for heavy validation workloads.
+- TPM / Secure Boot: preferred for device identity.
+- Power: PoE or DC supplies; plan UPS for site.
+- Network: at least 1Gbps LAN, private subnet connection, redundant uplinks if possible.
+
+Accessories & Requirements
+- Cables, racks, labeling, KVM/bastion access, serial console access for low-level debugging.
+- Asset management tags (serial numbers).
+- Physical security: locked racks or cabinets.
+- Environmental controls & UPS.
+
+Inventory & pre-provisioning
+- Create inventory CSV (ansible/provision/hosts.ini template).
+- Prepare cloud-init image & provisioning tokens.
+- Ensure network team configures private subnets and bastion host.
+
+Acceptance criteria before onsite run
+- Each device boots to a configured image and appears in inventory.
+- Node exporter reachable from Prometheus in private network.
+- Runner service registers to CI (self-hosted runner) with correct labels.
+- MDM agent fetches certs and registers device identity.
+- Device passes initial TensorRT hardware validation test.
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/jetson_site_playbook.yml
+- name: Jetson Site Playbook — bootstrap, harden and register runners
+  hosts: runners
+  become: true
+  vars:
+    runner_user: aegis-runner
+    repo_url: "https://github.com/owner/repo"
+  tasks:
+    - name: Ensure OS updates and essentials
+      apt:
+        update_cache: yes
+        name:
+          - apt-transport-https
+          - ca-certificates
+          - curl
+          - jq
+          - python3-pip
+        state: present
+
+    - name: Create runner user
+      user:
+        name: "{{ runner_user }}"
+        shell: /bin/bash
+        create_home: yes
+
+    - name: Deploy cloud-init produced runner software (if needed)
+      file:
+        path: /home/{{ runner_user }}/actions-runner
+        state: directory
+        owner: "{{ runner_user }}"
+
+    - name: Configure /etc/hosts for local resolver (optional)
+      lineinfile:
+        path: /etc/hosts
+        line: "10.0.0.1 controller.internal aegis-controller"
+        state: present
+
+    - name: Deploy MDM agent and start
+      copy:
+        src: runner/mdm/mdm_agent.py
+        dest: /usr/local/bin/mdm_agent.py
+        mode: '0755'
+    - copy:
+        src: runner/mdm/mdm_agent.service
+        dest: /etc/systemd/system/mdm_agent.service
+        mode: '0644'
+    - name: Start mdm agent
+      systemd:
+        name: mdm_agent
+        enabled: yes
+        state: started
+
+    - name: Install node_exporter (ARM)
+      include_tasks: monitoring/node_exporter_install.yml
+
+    - name: Configure UFW to restrict ingress to bastion
+      ufw:
+        state: enabled
+      ignore_errors: yes
+
+    - name: Ensure runner service unit (systemd) is present (placeholder)
+      copy:
+        dest: /etc/systemd/system/github-runner.service
+        content: |
+          [Unit]
+          Description=GitHub Actions Runner
+          After=network.target
+
+          [Service]
+          Type=simple
+          User={{ runner_user }}
+          WorkingDirectory=/home/{{ runner_user }}/actions-runner
+          ExecStart=/home/{{ runner_user }}/actions-runner/run.sh
+          Restart=always
+          RestartSec=5
+
+          [Install]
+          WantedBy=multi-user.target
+        mode: '0644'
+
+    - name: Start runner service
+      systemd:
+        name: github-runner
+        state: started
+        enabled: yes
+
+    - name: Register runner with CI (token stored in Ansible Vault as runner_token)
+      become_user: "{{ runner_user }}"
+      shell: |
+        cd /home/{{ runner_user }}/actions-runner || exit 0
+        if [ ! -f .credentials ]; then
+          ./config.sh --url "{{ repo_url }}" --token "{{ runner_token }}" --labels "self-hosted,jetson" --unattended
+        fi
+      args:
+        creates: "/home/{{ runner_user }}/actions-runner/.credentials"
+
+    - name: Run initial TensorRT validation smoke test (script must be present)
+      shell: |
+        /usr/local/bin/validate_tensorrt_local.sh || true
+      register: tensorrt_out
+    - debug:
+        var: tensorrt_out.stdout_lines
+
*** End Patch
*** Begin Patch
*** Add File: docs/jetson_ops_checklist.md
+Jetson Site Ops Checklist — Pilot Run
+
+Pre-deployment
+- Confirm private subnets and bastion host are ready.
+- Prepare inventory CSV and Ansible Vault with runner_token values.
+- Ensure Prometheus scrape config allows node_exporter metrics from the site.
+
+Onsite pilot steps
+1. Rack devices; connect network and power; assign device hostnames/IPs to inventory.
+2. Boot devices with cloud-init image configured for your environment.
+3. Run Ansible: ansible-playbook -i inventory ansible/provision/jetson_site_playbook.yml --ask-vault-pass
+4. Verify node_exporter metrics appear in Prometheus.
+5. Verify runner registered in CI: check self-hosted runners list for expected labels.
+6. Run hardware TensorRT CI job and ensure pass.
+7. Test automated reprovision: stop runner service and validate reprovision playbook runs.
+
+Operational checks
+- Confirm UFW/SSH access limited to bastion.
+- Confirm MDM agent has retrieved certs and device authentication to controller succeeds.
+- Ensure backup power and network redundancy tested.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/jetson/validate_tensorrt_local.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Minimal TensorRT validation smoke test (placeholder)
+echo "Running TensorRT smoke test..."
+if command -v trtexec >/dev/null 2>&1; then
+  trtexec --onnx=model.onnx --explicitBatch --minShapes=input:1x3x224x224 --shapes=input:1x3x224x224 --maxShapes=input:1x3x224x224 --fp16 || true
+else
+  echo "trtexec not found; skipping heavy test"
+fi
+echo "TensorRT smoke test complete."
+
*** End Patch
*** Begin Patch
*** Add File: docs/provider_live_test_plan.md
+Provider Live Test Orchestration — Plan & Runbook
+
+Goal
+- Execute distributed SLA harness against chosen provider(s), aggregate results, tune adapters, run mitigation experiments and reconcile billing for the test window.
+
+Prereqs
+- BAA signed (if regulated=true) and uploaded to compliance store.
+- Provider credentials in Vault/CI and Rekor + cosign accessible.
+- Kubernetes cluster to run distributed harness jobs and adapter configmap apply.
+- MLflow configured for mitigation artifact storage.
+- Billing tags enforced (aegis:tenant).
+
+High-level steps
+1. Launch distributed SLA harness jobs (k8s jobs or CI runners) to exercise providers under representative traffic.
+2. Aggregate harness reports into /tmp/qpu_reports and run aggregation script.
+3. Generate adapter tuning recommendations and apply to adapters ConfigMap.
+4. Run mitigation experiments (mitigation/run_mitigation_batch.py) to collect calibration snapshots and ZNE/readout runs — store artifacts in MLflow.
+5. Re-run harness to validate tuning improvements.
+6. Fetch provider billing for test window and run billing/reconcile_automated.py to reconcile.
+7. Produce final report and archive to compliance bucket.
+
+Acceptance criteria
+- p95 latency and error rate meet pre-agreed thresholds OR documented exceptions with mitigation plan.
+- Adapter recommendations applied and validated in a follow-up run.
+- Mitigation artifacts stored with provenance in MLflow.
+- Billing matched to qpu_charges or exceptions recorded for manual follow-up.
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/sla_harness_k8s_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: qpu-sla-harness
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: harness
+          image: aegis/quantum-harness:latest
+          env:
+            - name: BRAKET_DEVICE_ARN
+              valueFrom:
+                secretKeyRef:
+                  name: provider-secrets
+                  key: BRAKET_DEVICE_ARN
+            - name: IBMQ_API_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: provider-secrets
+                  key: IBMQ_API_TOKEN
+          command: ["python", "quantum/sla/provider_sla_test_harness.py"]
+          volumeMounts:
+            - mountPath: /tmp/reports
+              name: reports
+      restartPolicy: OnFailure
+      volumes:
+        - name: reports
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/provider_live_test_orchestrator.py
+#!/usr/bin/env python3
+"""
+Orchestrator for Provider Live Test
+ - Deploys N SLA harness jobs
+ - Waits for completion and aggregates JSON reports
+ - Runs adapter tuning and applies ConfigMap
+ - Runs mitigation experiments and stores artifacts in MLflow
+ - Runs billing reconciliation for test window
+ - Produces final report and archives it to S3
+
+Usage:
+  python quantum/hardening/provider_live_test_orchestrator.py --jobs 10 --test-window 1h --s3-bucket my-compliance
+"""
+import argparse, subprocess, time, json, os
+from datetime import datetime, timedelta
+
+def kubectl_apply_job(count):
+    for i in range(count):
+        name=f"qpu-sla-harness-{int(time.time())}-{i}"
+        cmd=["kubectl","create","job",name,"--image=aegis/quantum-harness:latest","--","python","quantum/sla/provider_sla_test_harness.py"]
+        subprocess.run(cmd, check=False)
+
+def wait_for_jobs(timeout_minutes=60):
+    deadline=time.time()+timeout_minutes*60
+    while time.time()<deadline:
+        out=subprocess.check_output(["kubectl","get","jobs","-n","aegis","-o","json"])
+        js=json.loads(out)
+        not_done=[j for j in js["items"] if j.get("status",{}).get("succeeded",0)==0 and j.get("status",{}).get("failed",0)==0]
+        if not not_done:
+            return True
+        print("waiting for jobs to finish, remaining:", len(not_done))
+        time.sleep(15)
+    return False
+
+def aggregate_reports(report_dir="/tmp/qpu_reports"):
+    # assume harness writes JSON reports to directory via shared volume or artifact fetch
+    reports=[]
+    for fn in sorted(os.listdir(report_dir)):
+        if fn.endswith(".json"):
+            reports.append(json.load(open(os.path.join(report_dir,fn))))
+    agg_path="/tmp/qpu_sla_summary.json"
+    json.dump({"generated_at":datetime.utcnow().isoformat(),"reports":reports}, open(agg_path,"w"), indent=2)
+    return agg_path
+
+def run_adapter_tuning():
+    subprocess.run(["python","quantum/hardening/measure_and_tune.py"], check=False)
+
+def run_mitigation_batch():
+    subprocess.run(["python","mitigation/run_mitigation_batch.py"], check=False)
+
+def run_billing_reconcile():
+    subprocess.run(["python","billing/reconcile_automated.py"], check=False)
+
+def archive_report(s3_bucket, report_path):
+    key=f"compliance/provider_live_test/{os.path.basename(report_path)}"
+    subprocess.run(["aws","s3","cp",report_path,f"s3://{s3_bucket}/{key}"], check=False)
+    print("Archived report to s3://{}/{}".format(s3_bucket,key))
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--jobs", type=int, default=10)
+    p.add_argument("--timeout-min", type=int, default=60)
+    p.add_argument("--s3-bucket", required=True)
+    args=p.parse_args()
+    print("Launching jobs:", args.jobs)
+    kubectl_apply_job(args.jobs)
+    ok=wait_for_jobs(timeout_minutes=args.timeout_min)
+    if not ok:
+        print("Jobs timeout or failed; collect partial logs and continue")
+    # copy reports from a known shared location or fetch via kubectl logs; here assume /tmp/qpu_reports
+    summary=aggregate_reports("/tmp/qpu_reports")
+    run_adapter_tuning()
+    run_mitigation_batch()
+    run_billing_reconcile()
+    archive_report(args.s3_bucket, summary)
+    print("Provider live test orchestration complete")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: ci/.github/workflows/provider_live_test.yml
+name: Provider Live Test Orchestration
+on:
+  workflow_dispatch:
+    inputs:
+      jobs:
+        description: Number of distributed harness jobs
+        required: false
+        default: '10'
+      s3-bucket:
+        description: S3 bucket to archive results
+        required: true
+
+jobs:
+  run-live-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup kubectl & AWS CLI
+        run: |
+          sudo snap install kubectl --classic || true
+          pip install awscli
+      - name: Launch provider live test orchestrator
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          DATABASE_URL: ${{ secrets.DATABASE_URL }}
+        run: |
+          python quantum/hardening/provider_live_test_orchestrator.py --jobs ${{ github.event.inputs.jobs }} --s3-bucket "${{ github.event.inputs.s3-bucket }}"
+
*** End Patch
*** Begin Patch
*** Add File: docs/provider_live_test_acceptance_checklist.md
+Provider Live Test — Acceptance Checklist
+
+Test name: ____________________    Provider: ____________________
+Date: __________   Operator: __________
+
+Checklist:
+- [ ] BAA signed and uploaded if regulated=true (verify via provider/legal/check_baa.py)
+- [ ] Provider credentials present in Vault/Secrets and accessible to harness jobs
+- [ ] Distributed SLA harness jobs completed and JSON reports present
+- [ ] Aggregated report /tmp/qpu_sla_summary.json created
+- [ ] Adapter tuning recommendations produced and applied to ConfigMap
+- [ ] Mitigation experiments ran; artifacts logged to MLflow with run ids recorded
+- [ ] Billing reconciliation ran for test window and unmatched items < threshold (or exceptions recorded)
+- [ ] Final report archived to s3://<compliance-bucket>/provider_live_test/
+
+If any item fails, capture logs, artifacts and open an incident ticket.
+
*** End Patch
*** End Patch
