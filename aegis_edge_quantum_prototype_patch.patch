*** Begin Patch
*** Add File: device/verifier/build_device_verifier.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Cross-compile the Go device verifier for common architectures.
+# Produces: device_verifier-linux-amd64, device_verifier-linux-arm64, device_verifier-linux-arm
+#
+OUT_DIR=${1:-./device/verifier/bin}
+mkdir -p "$OUT_DIR"
+echo "Building binaries into $OUT_DIR"
+for GOOS in linux; do
+  for GOARCH in amd64 arm64 arm; do
+    case "$GOARCH" in
+      arm) GOARM=7 ;;
+      *) GOARM= ;;
+    esac
+    export GOOS GOARCH
+    if [ -n "$GOARM" ]; then export GOARM; fi
+    BIN="$OUT_DIR/device_verifier-${GOOS}-${GOARCH}"
+    echo "Building $BIN"
+    env GOOS=$GOOS GOARCH=$GOARCH GOARM=${GOARM:-} go build -o "$BIN" ./device/verifier || {
+      echo "go build failed for $GOOS/$GOARCH"
+    }
+  done
+done
+echo "Build complete. Binaries: $(ls -1 $OUT_DIR)"
+
*** End Patch
*** Begin Patch
*** Add File: device/verifier/README_BUILD.md
+Cross-compile device verifier
+
+Requirements:
+- Go toolchain (1.20+ recommended)
+
+Build:
+  ./build_device_verifier.sh ./bin
+
+The script builds the Go verifier for linux/amd64, linux/arm64 and linux/arm (GOARM=7).
+Copy the resulting binaries to your device provisioning pipeline or embed into firmware images.
+
*** End Patch
*** Begin Patch
*** Add File: edge/cdn/s3_cloudfront_signed_url.py
+#!/usr/bin/env python3
+"""
+Small helper to generate S3 presigned URLs and CloudFront signed URLs for OTA distribution.
+
+S3 presigned (recommended for authenticated short-lived downloads):
+  generate_presigned_s3('bucket', 'key', expires=600)
+
+CloudFront signed URL (for private CloudFront distributions) requires:
+  - key_pair_id (CloudFront key pair id)
+  - private_key_pem (PEM with RSA private key)
+
+For production use, generate signed URLs in a short-lived service or Lambda and never embed private keys in code repositories.
+"""
+import boto3
+import time
+import base64
+import rsa
+import hashlib
+from urllib.parse import quote_plus
+
+def generate_presigned_s3(bucket, key, expires=600, region=None):
+    s3 = boto3.client("s3", region_name=region)
+    return s3.generate_presigned_url("get_object", Params={"Bucket": bucket, "Key": key}, ExpiresIn=expires)
+
+def generate_cloudfront_signed_url(distribution_domain, resource_path, key_pair_id, private_key_pem, expires=600):
+    # CloudFront RSA signed URL (policy-canned)
+    expire_time = int(time.time()) + expires
+    policy = f'{{"Statement":[{{"Resource":"https://{distribution_domain}{resource_path}","Condition":{{"DateLessThan":{{"AWS:EpochTime":{expire_time}}}}}}}]}}'
+    # Sign policy with RSA-SHA1/SHA256 per CloudFront requirements (this is an illustrative example)
+    key = rsa.PrivateKey.load_pkcs1(private_key_pem.encode())
+    signature = rsa.sign(policy.encode(), key, 'SHA-1')
+    sig_b64 = base64.b64encode(signature).decode().replace('+', '-').replace('=', '_').replace('/', '~')
+    # Build URL
+    return f"https://{distribution_domain}{resource_path}?Policy={quote_plus(policy)}&Signature={sig_b64}&Key-Pair-Id={key_pair_id}"
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--mode", choices=["s3","cf"], default="s3")
+    p.add_argument("--bucket")
+    p.add_argument("--key")
+    p.add_argument("--expires", type=int, default=600)
+    p.add_argument("--domain")
+    p.add_argument("--resource")
+    p.add_argument("--key-pair-id")
+    p.add_argument("--private-key-file")
+    args = p.parse_args()
+    if args.mode == "s3":
+        url = generate_presigned_s3(args.bucket, args.key, expires=args.expires)
+        print(url)
+    else:
+        pk = open(args.private_key_file).read()
+        url = generate_cloudfront_signed_url(args.domain, args.resource, args.key_pair_id, pk, expires=args.expires)
+        print(url)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/gateway/rekor_cache_gateway.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: rekor-cache-gateway
+  namespace: staging
+spec:
+  selector:
+    matchLabels:
+      app: rekor-cache-gateway
+  template:
+    metadata:
+      labels:
+        app: rekor-cache-gateway
+    spec:
+      hostNetwork: false
+      containers:
+      - name: gateway
+        image: ghcr.io/yourorg/rekor-cache-gateway:latest
+        env:
+        - name: REKOR_URL
+          value: "https://rekor.sigstore.dev"
+        - name: CACHE_TTL
+          value: "300"
+        ports:
+        - containerPort: 8080
+        volumeMounts:
+        - name: cache
+          mountPath: /data/cache
+      volumes:
+      - name: cache
+        emptyDir: {}
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor-cache-gateway
+  namespace: staging
+spec:
+  selector:
+    app: rekor-cache-gateway
+  ports:
+  - port: 8080
+    targetPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File: edge/canary/canary_controller_telemetry.py
+#!/usr/bin/env python3
+"""
+Canary controller with Prometheus telemetry integration.
+ - Select a cohort from the registry
+ - Trigger update via registry/OTA API
+ - Monitor device health via Prometheus queries (success rate / error rate)
+ - Promote or rollback based on thresholds
+"""
+import requests
+import time
+import os
+import sys
+
+REGISTRY_URL = os.environ.get("REGISTRY_URL", "http://registry.staging.svc.cluster.local:5000")
+OTA_SERVER = os.environ.get("OTA_SERVER", "http://ota.staging.svc.cluster.local:8090")
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.monitoring.svc.cluster.local:9090")
+COHORT_SIZE = int(os.environ.get("COHORT_SIZE", "10"))
+HEALTH_THRESHOLD_PCT = float(os.environ.get("HEALTH_THRESHOLD_PCT", "90"))
+
+def pick_cohort(group, n):
+    r = requests.get(f"{REGISTRY_URL}/groups/{group}/devices")
+    r.raise_for_status()
+    devices = r.json()
+    return devices[:n]
+
+def trigger_update(device_id, target_version):
+    # Instruct device registries or management agent to trigger immediate check
+    return requests.post(f"{REGISTRY_URL}/devices/{device_id}/trigger_check", json={"target": f"{OTA_SERVER}/firmware/download/{target_version}"})
+
+def query_prometheus_success_rate(group, time_window="5m"):
+    # Example: device_success_total / device_checks_total by group label
+    q = f'sum(rate(device_success_total{{group="{group}"}}[{time_window}])) / sum(rate(device_checks_total{{group="{group}"}}[{time_window}]))'
+    resp = requests.get(f"{PROM_URL}/api/v1/query", params={"query": q}, timeout=10)
+    resp.raise_for_status()
+    data = resp.json()
+    try:
+        val = float(data["data"]["result"][0]["value"][1])
+    except Exception:
+        val = 0.0
+    return val * 100.0
+
+def run_cohort_update(group, target_version):
+    cohort = pick_cohort(group, COHORT_SIZE)
+    print("Cohort:", [d["device_id"] for d in cohort])
+    for d in cohort:
+        trigger_update(d["device_id"], target_version)
+    # wait a bit then query Prometheus
+    time.sleep(15)
+    healthy_pct = query_prometheus_success_rate(group)
+    print(f"Observed success rate for group {group}: {healthy_pct:.1f}%")
+    if healthy_pct < HEALTH_THRESHOLD_PCT:
+        print("Health below threshold => rollback")
+        for d in cohort:
+            requests.post(f"{REGISTRY_URL}/devices/{d['device_id']}/rollback")
+        return False
+    print("Promote to full rollout")
+    requests.post(f"{REGISTRY_URL}/groups/{group}/rollout", json={"version": target_version})
+    return True
+
+if __name__ == "__main__":
+    group = sys.argv[1] if len(sys.argv) > 1 else "default"
+    version = int(sys.argv[2]) if len(sys.argv) > 2 else 1
+    run_cohort_update(group, version)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/braket/pennylane_submit.py
+#!/usr/bin/env python3
+"""
+Example script: run a PennyLane quantum circuit on AWS Braket (via PennyLane-Braket plugin).
+ - Requires AWS credentials or IAM role with Braket permissions and S3 access.
+ - Writes results to MLflow if MLFLOW_TRACKING_URI is set.
+"""
+import os
+import pennylane as qml
+from pennylane import qnodes
+import boto3
+import json
+import mlflow
+
+S3_BUCKET = os.environ.get("BRK_S3_BUCKET", "")
+REGION = os.environ.get("AWS_REGION", "us-west-2")
+MLFLOW = os.environ.get("MLFLOW_TRACKING_URI")
+
+def sample_pennylane_run(shots=1024):
+    dev = qml.device("braket.aws.qubit", device_arn=os.environ.get("BRAKET_DEVICE_ARN","arn:aws:braket:..."), s3_destination_folder=(S3_BUCKET, "braket-results"), shots=shots, region=REGION)
+
+    @qml.qnode(dev)
+    def circuit(params):
+        qml.RX(params[0], wires=0)
+        qml.RY(params[1], wires=1)
+        qml.CNOT(wires=[0,1])
+        return qml.expval(qml.PauliZ(0))
+
+    params = [0.1, 0.2]
+    res = circuit(params)
+    return {"expval": res}
+
+if __name__ == "__main__":
+    if MLFLOW:
+        mlflow.set_tracking_uri(MLFLOW)
+        mlflow.start_run(run_name="braket-pennylane-sample")
+        mlflow.log_param("shots", 1024)
+    result = sample_pennylane_run()
+    print("Result:", result)
+    if MLFLOW:
+        mlflow.log_metrics(result)
+        mlflow.end_run()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/argo/braket_pennylane_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: braket-pennylane-job
+  namespace: staging
+spec:
+  entrypoint: run-braket
+  templates:
+  - name: run-braket
+    container:
+      image: ghcr.io/yourorg/pennylane-braket:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -euxo pipefail
+          export AWS_REGION=${AWS_REGION:-us-west-2}
+          python3 /workspace/pennylane_submit.py
+    volumes:
+    - name: workspace
+      emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/job_controller_flask/app.py
+#!/usr/bin/env python3
+"""
+Simple Flask Quantum Job Controller prototype that logs job metadata to MLflow.
+ - POST /submit -> enqueues job to Redis (same queue name as existing controller)
+ - GET /status/<id> -> query SQLite (shared schema)
+This is a minimal prototype; for production prefer the FastAPI controller scaffold already present.
+"""
+import os
+import uuid
+import json
+import time
+import sqlite3
+import redis
+from flask import Flask, request, jsonify
+import mlflow
+
+DB_PATH = os.environ.get("DB_PATH", "./quantum_jobs.db")
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+r = redis.from_url(REDIS_URL, decode_responses=True)
+QUEUE = "quantum:queue"
+
+app = Flask("quantum-flask")
+
+def init_db():
+    conn = sqlite3.connect(DB_PATH)
+    cur = conn.cursor()
+    cur.execute("""CREATE TABLE IF NOT EXISTS jobs (
+        id TEXT PRIMARY KEY, name TEXT, backend TEXT, circuit TEXT, params TEXT, status TEXT, created_at INTEGER, updated_at INTEGER, result TEXT
+    )""")
+    conn.commit(); conn.close()
+
+@app.route("/submit", methods=["POST"])
+def submit():
+    body = request.get_json() or {}
+    name = body.get("name","job")
+    backend = body.get("backend","simulator")
+    circuit = body.get("circuit","")
+    params = body.get("params", {})
+    job_id = str(uuid.uuid4())
+    conn = sqlite3.connect(DB_PATH)
+    cur = conn.cursor()
+    cur.execute("INSERT INTO jobs (id,name,backend,circuit,params,status,created_at,updated_at) VALUES (?,?,?,?,?,?,?,?)",
+                (job_id, name, backend, circuit, json.dumps(params), "queued", int(time.time()), int(time.time())))
+    conn.commit(); conn.close()
+    payload = {"id": job_id, "name": name, "backend": backend, "circuit": circuit, "params": params}
+    r.lpush(QUEUE, json.dumps(payload))
+    # MLflow lightweight log
+    if os.environ.get("MLFLOW_TRACKING_URI"):
+        mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI"))
+        with mlflow.start_run(run_name=f"qjob-{job_id}"):
+            mlflow.log_param("backend", backend)
+            mlflow.log_text(circuit, "circuit.txt")
+    return jsonify({"id": job_id, "status": "queued"})
+
+@app.route("/status/<job_id>", methods=["GET"])
+def status(job_id):
+    conn = sqlite3.connect(DB_PATH)
+    cur = conn.cursor()
+    cur.execute("SELECT id,name,backend,status,created_at,updated_at,result FROM jobs WHERE id=?", (job_id,))
+    r = cur.fetchone()
+    conn.close()
+    if not r:
+        return jsonify({"error":"not found"}), 404
+    return jsonify({"id": r[0], "name": r[1], "backend": r[2], "status": r[3], "created_at": r[4], "updated_at": r[5], "result": json.loads(r[6]) if r[6] else None})
+
+if __name__ == "__main__":
+    init_db()
+    app.run(host="0.0.0.0", port=8081)
+
*** End Patch
*** Begin Patch
*** Add File: iam/aws/braket_iam_snippet.tf
+/*
+Terraform snippet: IAM policy for a service account that can submit Braket tasks and write/read S3 results.
+Fill in the S3 bucket and principal (role) as needed.
+*/
+data "aws_iam_policy_document" "braket_policy" {
+  statement {
+    actions = [
+      "braket:CreateQuantumTask",
+      "braket:GetQuantumTask",
+      "braket:SearchQuantumTasks",
+      "s3:PutObject",
+      "s3:GetObject",
+      "s3:ListBucket"
+    ]
+    resources = ["*"]
+  }
+}
+
+resource "aws_iam_policy" "braket_policy" {
+  name   = "AegisBraketPolicy"
+  policy = data.aws_iam_policy_document.braket_policy.json
+}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci_braket_submit.yml
+name: CI: Braket job submit (uses OIDC)
+on:
+  workflow_dispatch:
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  submit-braket:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Configure AWS credentials via OIDC (assume role)
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          role-to-assume: ${{ secrets.BRAKET_OIDC_ROLE_ARN }}
+          aws-region: ${{ secrets.AWS_REGION }}
+      - name: Submit Braket job via PennyLane script
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          BRAKET_DEVICE_ARN: ${{ secrets.BRAKET_DEVICE_ARN }}
+          BRK_S3_BUCKET: ${{ secrets.BRK_S3_BUCKET }}
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          docker run --rm -v $PWD:/workspace -w /workspace python:3.10-slim bash -lc "pip install pennylane pennylane-braket boto3 mlflow && python3 quantum/braket/pennylane_submit.py"
+
*** End Patch
*** End Patch
