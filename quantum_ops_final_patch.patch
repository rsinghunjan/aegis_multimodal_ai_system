*** Begin Patch
*** Add File:terraform/irsa/final_irsa.tf
+/*
+IRSA Terraform template for Aegis quantum provider service accounts.
+Replace REPLACE_* placeholders before running. This template creates IAM roles
+and attaches provided policy ARNs. It uses the cluster OIDC provider.
+
+Operator note: Do NOT commit real credentials to git. Use secure local tfvars
+or CI secrets to supply sensitive values.
+*/
+variable "cluster_name" { type = string }
+variable "account_id" { type = string }
+variable "oidc_provider_arn" { type = string }
+variable "policy_arn_braket" { type = string }
+variable "policy_arn_ibm" { type = string }
+variable "policy_arn_azure" { type = string }
+variable "policy_arn_rigetti" { type = string }
+
+data "aws_iam_policy_document" "assume_role_with_web_identity_braket" {
+  statement {
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(split("/", var.oidc_provider_arn)[-1], "oidc-provider/","")}:sub"
+      values   = ["system:serviceaccount:aegis:braket-sa"]
+    }
+  }
+}
+
+resource "aws_iam_role" "irsa_braket" {
+  name               = "${var.cluster_name}-braket-irsa-role"
+  assume_role_policy = data.aws_iam_policy_document.assume_role_with_web_identity_braket.json
+}
+
+resource "aws_iam_role_policy_attachment" "attach_braket" {
+  role       = aws_iam_role.irsa_braket.name
+  policy_arn = var.policy_arn_braket
+}
+
+# IBM role
+data "aws_iam_policy_document" "assume_role_with_web_identity_ibm" {
+  statement {
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(split("/", var.oidc_provider_arn)[-1], "oidc-provider/","")}:sub"
+      values   = ["system:serviceaccount:aegis:ibm-quantum-sa"]
+    }
+  }
+}
+
+resource "aws_iam_role" "irsa_ibm" {
+  name               = "${var.cluster_name}-ibm-irsa-role"
+  assume_role_policy = data.aws_iam_policy_document.assume_role_with_web_identity_ibm.json
+}
+
+resource "aws_iam_role_policy_attachment" "attach_ibm" {
+  role       = aws_iam_role.irsa_ibm.name
+  policy_arn = var.policy_arn_ibm
+}
+
+# Azure role
+data "aws_iam_policy_document" "assume_role_with_web_identity_azure" {
+  statement {
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(split("/", var.oidc_provider_arn)[-1], "oidc-provider/","")}:sub"
+      values   = ["system:serviceaccount:aegis:azure-quantum-sa"]
+    }
+  }
+}
+
+resource "aws_iam_role" "irsa_azure" {
+  name               = "${var.cluster_name}-azure-irsa-role"
+  assume_role_policy = data.aws_iam_policy_document.assume_role_with_web_identity_azure.json
+}
+
+resource "aws_iam_role_policy_attachment" "attach_azure" {
+  role       = aws_iam_role.irsa_azure.name
+  policy_arn = var.policy_arn_azure
+}
+
+# Rigetti role
+data "aws_iam_policy_document" "assume_role_with_web_identity_rigetti" {
+  statement {
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(split("/", var.oidc_provider_arn)[-1], "oidc-provider/","")}:sub"
+      values   = ["system:serviceaccount:aegis:rigetti-quantum-sa"]
+    }
+  }
+}
+
+resource "aws_iam_role" "irsa_rigetti" {
+  name               = "${var.cluster_name}-rigetti-irsa-role"
+  assume_role_policy = data.aws_iam_policy_document.assume_role_with_web_identity_rigetti.json
+}
+
+resource "aws_iam_role_policy_attachment" "attach_rigetti" {
+  role       = aws_iam_role.irsa_rigetti.name
+  policy_arn = var.policy_arn_rigetti
+}
+
+output "irsa_braket_role_arn" { value = aws_iam_role.irsa_braket.arn }
+output "irsa_ibm_role_arn"   { value = aws_iam_role.irsa_ibm.arn }
+output "irsa_azure_role_arn" { value = aws_iam_role.irsa_azure.arn }
+output "irsa_rigetti_role_arn"{ value = aws_iam_role.irsa_rigetti.arn }
+
*** End Patch
*** Begin Patch
*** Add File:ops/create_github_secrets_example.sh
+#!/usr/bin/env bash
+#
+# Example script to create GitHub secrets for the repo using gh CLI.
+# Replace REPLACE_* tokens or export environment variables before running.
+#
+REPO=${GITHUB_REPOSITORY:-"REPLACE_OWNER/REPLACE_REPO"}
+
+: ${COSIGN_KMS_ARN:="REPLACE_COSIGN_KMS_ARN"}
+: ${REKOR_URL:="REPLACE_REKOR_URL"}
+: ${EVIDENCE_BUCKET:="REPLACE_EVIDENCE_BUCKET"}
+: ${MLFLOW_TRACKING_URI:="REPLACE_MLFLOW_URI"}
+: ${SANDBOX_QPU_BUDGET_USD:="REPLACE_SANDBOX_QPU_BUDGET_USD"}
+
+echo "Creating GitHub secrets in $REPO (using gh CLI)."
+gh secret set COSIGN_KMS_ARN --repo "$REPO" --body "$COSIGN_KMS_ARN"
+gh secret set REKOR_URL --repo "$REPO" --body "$REKOR_URL"
+gh secret set EVIDENCE_BUCKET --repo "$REPO" --body "$EVIDENCE_BUCKET"
+gh secret set MLFLOW_TRACKING_URI --repo "$REPO" --body "$MLFLOW_TRACKING_URI"
+gh secret set SANDBOX_QPU_BUDGET_USD --repo "$REPO" --body "$SANDBOX_QPU_BUDGET_USD"
+
+echo "Secrets created (verify in GitHub repo Settings -> Secrets)."
+echo "Note: Do NOT store long-lived provider credentials here; use ExternalSecrets or vault integration for runtime access."
+
*** End Patch
*** Begin Patch
*** Add File:ops/build_and_cosign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Build, push and cosign quantum image. Requires docker, cosign and credentials to push.
+# Usage:
+#   ./ops/build_and_cosign.sh ghcr.io/yourorg/aegis-quantum v1.0.0
+IMAGE=${1:-"REPLACE_GHCR_ORG/aegis-quantum"}
+TAG=${2:-"REPLACE_IMAGE_TAG"}
+REPO_IMAGE="${IMAGE}:${TAG}"
+
+echo "Building image ${REPO_IMAGE}"
+docker build -t "${REPO_IMAGE}" -f docker/quantum/Dockerfile .
+
+echo "Pushing ${REPO_IMAGE}"
+docker push "${REPO_IMAGE}"
+
+echo "Signing image with cosign (requires COSIGN_KMS_ARN set in env)"
+: ${COSIGN_KMS_ARN:?"Set COSIGN_KMS_ARN env var (KMS ARN for cosign signing)."}
+cosign sign --key "${COSIGN_KMS_ARN}" "${REPO_IMAGE}"
+
+echo "Verifying signature against Rekor (requires REKOR_URL in env)"
+: ${REKOR_URL:?"Set REKOR_URL env var for Rekor server."}
+cosign verify --rekor-server "${REKOR_URL}" "${REPO_IMAGE}"
+
+echo "Image built, pushed and signed: ${REPO_IMAGE}"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/rbac/argo_quantum_submitter_role.yaml
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-quantum-submitter
+rules:
+  - apiGroups: ["argoproj.io"]
+    resources: ["workflows", "workflows/status"]
+    verbs: ["get", "list", "watch", "create", "patch", "delete"]
+  - apiGroups: [""]
+    resources: ["pods", "pods/log"]
+    verbs: ["get", "list", "watch"]
+  - apiGroups: ["batch"]
+    resources: ["jobs"]
+    verbs: ["create", "get", "list", "watch", "delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: aegis-quantum-submitter-binding
+  namespace: aegis
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: aegis-quantum-submitter
+subjects:
+  - kind: ServiceAccount
+    name: braket-sa
+    namespace: aegis
+
+---
+# Role that allows operator admins to manage Gatekeeper constraints (limited)
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: gatekeeper-admin
+  namespace: gatekeeper-system
+rules:
+  - apiGroups: ["constraints.gatekeeper.sh"]
+    resources: ["*"]
+    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_require_qpu_approved_binding.md
+This file documents the operator action to apply Gatekeeper constraint templates and RBAC bindings.
+
+Operator steps:
+1. Ensure Gatekeeper controller is installed in cluster (https://open-policy-agent.github.io/gatekeeper/).
+2. Apply templates and constraints in k8s/gatekeeper/ (some are already present in the repo).
+3. Ensure Role 'gatekeeper-admin' is bound to the operator/admin group and only explicit approvers have ability to change constraints.
+
+kubectl apply -f k8s/gatekeeper/constraint_template_qpuapproved.yaml
+kubectl apply -f k8s/gatekeeper/constraint_qpu_approved.yaml
+
*** End Patch
*** Begin Patch
*** Add File:k8s/cert-manager/argo_tls_mtls_placeholder.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: aegis-issuer
+spec:
+  # Placeholder: choose an issuer type for your environment. Examples:
+  # For Let’s Encrypt:
+  # acme:
+  #   server: https://acme-v02.api.letsencrypt.org/directory
+  #   email: "ops@example.com"
+  #   privateKeySecretRef:
+  #     name: aegis-acme-private-key
+  #   solvers:
+  #   - http01:
+  #       ingress:
+  #         class: nginx
+  # For AWS ACM (via cert-manager issuer integration), replace with your spec.
+  selfSigned: {}
+
+---
+# Argo Server TLS ingress (example using cert-manager)
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: argo-server-tls
+  namespace: argo
+  annotations:
+    kubernetes.io/ingress.class: "nginx"
+    cert-manager.io/cluster-issuer: "aegis-issuer"
+spec:
+  tls:
+    - hosts:
+        - "REPLACE_ARGO_HOSTNAME"
+      secretName: argo-server-tls
+  rules:
+    - host: "REPLACE_ARGO_HOSTNAME"
+      http:
+        paths:
+          - path: /
+            pathType: Prefix
+            backend:
+              service:
+                name: argo-server
+                port:
+                  number: 2746
+
+---
+# mTLS placeholder: create a Secret with CA and client certs; Argo server must be configured for mTLS
+# The actual Argo server helm values must enable mTLS and point to these secrets.
+apiVersion: v1
+kind: Secret
+metadata:
+  name: argo-mtls-ca
+  namespace: argo
+type: Opaque
+data:
+  # base64-encoded CA cert; replace locally or via cert-manager issuing a CA
+  ca.crt: "REPLACE_BASE64_CA_CERT"
+
*** End Patch
*** Begin Patch
*** Add File:ops/legal_signoff_template.md
+# Legal sign-off template for Quantum experiments
+
+Save a signed copy of this file to the evidence bucket (s3://REPLACE_EVIDENCE_BUCKET/legal_signoff/<file>.md)
+
+Document title: Aegis Quantum Experiment Data Policy Signoff
+Date: REPLACE_DATE
+Signed by: REPLACE_LEGAL_CONTACT_NAME (Title)
+
+Summary:
+- Approved data types (allowed): e.g., synthetic data, anonymized datasets, public datasets, non-PII aggregates.
+- Prohibited data types (forbidden): raw PII, HIPAA/PHI, unredacted financial identifiers, classified data.
+- Retention policy: signed artifacts and experiment receipts will be retained for REPLACE_YEARS years in the evidence bucket.
+- Audit requirements: each QPU job must produce a signed archive uploaded to evidence bucket with Rekor signature; MLflow metadata must reference signed_receipt_s3_uri.
+- Responsible owner: REPLACE_TEAM (Ops/Security/Data)
+
+Signatures:
+- Legal reviewer: ___________________  Date: ___________
+- Security reviewer: ________________  Date: ___________
+- Platform lead: ____________________  Date: ___________
+
*** End Patch
*** Begin Patch
*** Add File:ops/qpu_cost_estimator_braket_integration.py
+#!/usr/bin/env python3
+"""
+Braket pricing integration for accurate cost estimates.
+
+This script attempts to fetch relevant pricing info using AWS Pricing API and compute a per-shot estimate.
+Operator note: The Pricing API is in us-east-1 only and may require additional IAM permissions:
+  - pricing:GetProducts
+  - (optional) service-specific read permissions
+If Pricing API doesn't provide sufficient detail for the account's Braket SKUs, the script falls back to heuristics.
+"""
+import boto3, json, argparse, logging, math
+from decimal import Decimal
+
+logging.basicConfig(level=logging.INFO)
+log = logging.getLogger("braket_integration")
+
+def query_pricing(service_code='AmazonBraket'):
+    pricing = boto3.client("pricing", region_name="us-east-1")
+    paginator = pricing.get_paginator("get_products")
+    results = []
+    try:
+        for page in paginator.paginate(ServiceCode=service_code, MaxResults=100):
+            results.extend(page.get("PriceList", []))
+    except Exception as e:
+        log.warning("Pricing API unavailable or permission denied: %s", e)
+        return []
+    return results
+
+def parse_price_list_for_device(price_list, device_name):
+    # Best-effort parsing: look for mentions of device_name and usage-type/per unit price
+    for p in price_list:
+        try:
+            data = json.loads(p)
+            product = data.get("product", {})
+            sku = product.get("sku", "")
+            title = product.get("attributes", {}).get("usagetype", "") or json.dumps(product.get("attributes", {}))
+            if device_name.lower() in json.dumps(product).lower() or device_name.lower() in title.lower():
+                # Look for priceDimensions in terms
+                terms = data.get("terms", {})
+                for term_type in ("OnDemand","Reserved"):
+                    term_map = terms.get(term_type, {})
+                    for sku, term_desc in term_map.items():
+                        for td_key, td in term_desc.get("priceDimensions", {}).items():
+                            price_per_unit = td.get("pricePerUnit", {}).get("USD")
+                            if price_per_unit:
+                                return float(price_per_unit)
+        except Exception:
+            continue
+    return None
+
+def heuristic_estimate(shots, device, complexity):
+    # Conservative fallback heuristic: simulator cheap, QPU more expensive
+    if "simulator" in device:
+        per_shot = Decimal("0.0005")
+    else:
+        per_shot = Decimal("0.005") * (Decimal(1) + Decimal(complexity) * Decimal(0.1))
+    est = float((per_shot * Decimal(shots)).quantize(Decimal("0.0001")))
+    return {"provider":"braket","estimated_usd":est,"shots":shots,"device":device}
+
+def estimate(shots, device, complexity):
+    price_list = query_pricing()
+    if not price_list:
+        return heuristic_estimate(shots, device, complexity)
+    per_unit = parse_price_list_for_device(price_list, device)
+    if per_unit:
+        # if per_unit looks like per-hour or per-shot, we attempt to treat it as per-shot
+        est = float(Decimal(per_unit) * Decimal(shots) * (Decimal(1) + Decimal(complexity)*Decimal(0.1)))
+        return {"provider":"braket","estimated_usd":round(est,4),"shots":shots,"device":device, "price_per_unit": per_unit}
+    else:
+        return heuristic_estimate(shots, device, complexity)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="simulator")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(estimate(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/qpu_sanitizer_ci_step.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run the legal-safe sanitizer on any .qpu inputs in the repo.
+# Intended to be used as a GitHub Action step before merges.
+#
+FOUND=0
+for f in $(git ls-files | grep "^.qpu/" || true); do
+  echo "Sanitizing $f"
+  python3 ops/legal_safe_sanitizer.py --input "$f" --mode fail || { echo "PII detected in $f"; FOUND=1; }
+done
+if [ "$FOUND" -ne 0 ]; then
+  echo "Sanitizer found PII in .qpu inputs. Abort merge and remediate."
+  exit 2
+fi
+echo "No PII detected in .qpu inputs."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/secret_and_deploy_checks.yml
+name: Secret & Deploy Preflight (example)
+
+on:
+  workflow_dispatch:
+
+jobs:
+  check-secrets:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Check required repo secrets exist
+        uses: crazy-max/ghaction-github-secret@v2
+        with:
+          github_token: ${{ secrets.GITHUB_TOKEN }}
+          required: COSIGN_KMS_ARN,REKOR_URL,EVIDENCE_BUCKET,MLFLOW_TRACKING_URI
+
+  sanitizer:
+    runs-on: ubuntu-latest
+    needs: check-secrets
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run sanitizer on .qpu inputs
+        run: ./ops/qpu_sanitizer_ci_step.sh
+
+  placeholder-guard:
+    runs-on: ubuntu-latest
+    needs: sanitizer
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure no REPLACE_ tokens remain in tracked files
+        run: ./ops/validate_no_placeholders.sh
+
*** End Patch
*** Begin Patch
*** Add File:ops/submitter_logging_to_cloudwatch_example.conf
+# Example Fluent Bit configuration to forward structured JSON logs from the submitter container to CloudWatch Logs
+# Deploy as a ConfigMap to the fluent-bit DaemonSet or sidecar collector.
+[SERVICE]
+    Flush        1
+    Daemon       Off
+    Log_Level    info
+    Parsers_File parsers.conf
+
+[INPUT]
+    Name              tail
+    Path              /var/log/containers/*qpu-argo-submitter*.log
+    Parser            json
+    Tag               qpu.submitter
+    DB                /var/log/flb_kube.db
+    Mem_Buf_Limit     5MB
+    Skip_Long_Lines   On
+
+[OUTPUT]
+    Name                cloudwatch_logs
+    Match               qpu.submitter
+    region              REPLACE_AWS_REGION
+    log_group_name      /aegis/qpu-audit
+    log_stream_prefix   submitter-
+
*** End Patch
*** Begin Patch
*** Add File:docs/operator_final_instructions.md
+# Final Operator Instructions (summary)
+
+This patch added:
+- Terraform IRSA template (terraform/irsa/final_irsa.tf) — replace REPLACE_* in your local my.tfvars and apply.
+- GitHub secrets helper (ops/create_github_secrets_example.sh) — use gh CLI to create repo secrets.
+- Image build & cosign helper (ops/build_and_cosign.sh) — run locally or via CI; requires COSIGN_KMS_ARN and REKOR_URL be set as secrets.
+- RBAC role for Argo quantum submitter (k8s/rbac/argo_quantum_submitter_role.yaml) — bind to braket-sa/serviceaccounts only.
+- Gatekeeper application instructions (k8s/gatekeeper/*).
+- cert-manager/Ingress + mTLS placeholders for Argo server (k8s/cert-manager/argo_tls_mtls_placeholder.yaml). Replace REPLACE_ARGO_HOSTNAME and CA data.
+- Legal signoff template (ops/legal_signoff_template.md) — store signed artifact in S3 evidence bucket.
+- Braket Pricing API enhanced connector (ops/qpu_cost_estimator_braket_integration.py) — attempts Pricing API then falls back to heuristics.
+- CI sanitizer and preflight checks (.github/workflows/secret_and_deploy_checks.yml, ops/qpu_sanitizer_ci_step.sh).
+- Structured logging example for CloudWatch (ops/submitter_logging_to_cloudwatch_example.conf).
+
+Operator checklist (minimum to run):
+1. Replace REPLACE_* tokens in terraform/irsa/my.tfvars locally (do not commit).
+2. Apply Terraform: terraform init && terraform apply -var-file=terraform/irsa/my.tfvars
+3. Create GitHub secrets for COSIGN_KMS_ARN, REKOR_URL, EVIDENCE_BUCKET, MLFLOW_TRACKING_URI, SANDBOX_QPU_BUDGET_USD.
+4. Build & push images and run ops/build_and_cosign.sh (requires COSIGN_KMS_ARN and REKOR_URL env).
+5. Deploy RBAC and Gatekeeper constraints to cluster.
+6. Install cert-manager (if using) and configure ClusterIssuer; then apply Argo TLS ingress manifest with actual hostname and CA.
+7. Deploy hardened submitter and create qpu-github-token secret in operator namespace.
+8. Run a simulate Argo workflow and validate MLflow metadata and signed artifacts.
+
+Security reminders:
+- Do not commit secrets to git.
+- Use IRSA and ExternalSecrets to fetch credentials at runtime.
+- Rotate keys and restrict who can apply PR labels (qpu-approved).
+
*** End Patch
*** End Patch
