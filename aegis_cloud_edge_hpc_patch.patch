*** Begin Patch
*** Add File:infra/k8s/cluster_autoscaler/values-cluster-autoscaler.yaml
+# Helm values example for Cluster Autoscaler (generic)
+autoDiscovery:
+  clusterName: "aegis-cluster"
+  enabled: true
+  # Use provider-specific tags/labels to auto-discover node groups (EKS/GKE/AKS/ClusterAPI)
+  # Example for AWS: use nodegroup tags "k8s.io/cluster-autoscaler/enabled"
+
+awsRegion: "us-west-2"
+rbac:
+  create: true
+extraArgs:
+  skip-nodes-with-local-storage: false
+  expander: least-waste
+  balance-similar-node-groups: true
+  skip-nodes-with-system-pods: false
+resources:
+  limits:
+    cpu: 500m
+    memory: 512Mi
+  requests:
+    cpu: 100m
+    memory: 128Mi
+nodeSelector: {}
+tolerations: []
+affinity: {}
+
*** End Patch
*** Begin Patch
*** Add File:infra/k8s/nodepools/gpu_nodepool_example.yaml
+# Example NodePool/NodeGroup manifest (provider-specific bodies are placeholders).
+# Operators should translate this to their cloud provider (EKS NodeGroup / GKE NodePool / AKS VMSS / Cluster API).
+apiVersion: aegis.io/v1alpha1
+kind: NodePool
+metadata:
+  name: gpu-nodepool
+  namespace: aegis
+spec:
+  provider: aws # change to gcp/azure/other
+  instanceType: p4d.24xlarge
+  minSize: 1
+  maxSize: 20
+  labels:
+    node.kubernetes.io/instance-type: p4d.24xlarge
+    aegis.pool: gpu
+  taints:
+    - key: "aegis.pool"
+      value: "gpu"
+      effect: "NoSchedule"
+  kubelet:
+    kubeReserved:
+      cpu: "1000m"
+      memory: "1Gi"
+    evictionHard:
+      memory.available: "200Mi"
+
*** End Patch
*** Begin Patch
*** Add File:argo/hpc/argo_to_slurm_bridge_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: argo-to-slurm-
+  namespace: aegis
+spec:
+  entrypoint: argo-to-slurm
+  templates:
+    - name: argo-to-slurm
+      inputs:
+        parameters:
+          - name: slurm_host
+          - name: slurm_user
+          - name: ssh_key_secret
+          - name: job_script_s3
+      steps:
+        - - name: submit-slurm
+            template: submit-slurm
+            arguments:
+              parameters:
+                - name: slurm_host
+                  value: "{{inputs.parameters.slurm_host}}"
+                - name: slurm_user
+                  value: "{{inputs.parameters.slurm_user}}"
+                - name: ssh_key_secret
+                  value: "{{inputs.parameters.ssh_key_secret}}"
+                - name: job_script_s3
+                  value: "{{inputs.parameters.job_script_s3}}"
+        - - name: poll-status
+            template: poll-status
+
+    - name: submit-slurm
+      inputs:
+        parameters:
+          - name: slurm_host
+          - name: slurm_user
+          - name: ssh_key_secret
+          - name: job_script_s3
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            pip install paramiko boto3 || true
+            python3 scripts/hpc/argo_to_slurm_bridge.py --host "{{inputs.parameters.slurm_host}}" --user "{{inputs.parameters.slurm_user}}" --ssh-key-secret "{{inputs.parameters.ssh_key_secret}}" --job-script-s3 "{{inputs.parameters.job_script_s3}}" --out /tmp/slurm_submission.json
+            cat /tmp/slurm_submission.json
+
+    - name: poll-status
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install paramiko || true; python3 scripts/hpc/argo_to_slurm_bridge.py --poll /tmp/slurm_submission.json --out /tmp/slurm_poll.json || true; cat /tmp/slurm_poll.json
+
*** End Patch
*** Begin Patch
*** Add File:scripts/hpc/argo_to_slurm_bridge.py
+#!/usr/bin/env python3
+"""
+Argo -> Slurm bridge.
+- Uploads a job script from S3 (or repo) to the Slurm head node via SSH and sbatch it.
+- Polls job status and retrieves logs when finished.
+
+Usage:
+  python3 scripts/hpc/argo_to_slurm_bridge.py --host slurm.example --user ubuntu --ssh-key-secret secret-name --job-script-s3 s3://bucket/path/job.sh --out /tmp/sub.json
+  python3 scripts/hpc/argo_to_slurm_bridge.py --poll /tmp/sub.json --out /tmp/poll.json
+"""
+import argparse, json, os, time, paramiko, boto3, tempfile
+
+def download_s3(uri, dst):
+    s3 = boto3.client("s3")
+    # simple s3://bucket/key
+    if not uri.startswith("s3://"):
+        raise ValueError("Only s3:// URIs supported in this scaffold")
+    parts = uri[5:].split("/",1)
+    bucket = parts[0]; key = parts[1]
+    s3.download_file(bucket, key, dst)
+
+def ssh_connect(host, user, key_path, timeout=10):
+    k = paramiko.RSAKey.from_private_key_file(key_path)
+    client = paramiko.SSHClient()
+    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
+    client.connect(hostname=host, username=user, pkey=k, timeout=timeout)
+    return client
+
+def submit_job(host, user, key_path, job_script_local):
+    client = ssh_connect(host, user, key_path)
+    sftp = client.open_sftp()
+    remote_path = "/tmp/argo_submitted_job.sh"
+    sftp.put(job_script_local, remote_path)
+    sftp.close()
+    stdin, stdout, stderr = client.exec_command(f"sbatch {remote_path}")
+    out = stdout.read().decode()
+    client.close()
+    # parse job id from sbatch output "Submitted batch job 12345"
+    job_id = None
+    for tok in out.split():
+        if tok.isdigit():
+            job_id = tok
+            break
+    return job_id, out
+
+def poll_job(host, user, key_path, job_id):
+    client = ssh_connect(host, user, key_path)
+    stdin, stdout, stderr = client.exec_command(f"squeue -j {job_id} -o '%i %t %M %R' | tail -n +2")
+    out = stdout.read().decode()
+    # if empty, job finished; fetch sacct
+    if not out.strip():
+        stdin, stdout, stderr = client.exec_command(f"sacct -j {job_id} --format=JobID,State,ExitCode -P -n")
+        sacct = stdout.read().decode()
+        # try to fetch logs (assumes default Slurm output file in /tmp/)
+        logs = {}
+        for path in [f"/tmp/slurm-{job_id}.out", f"slurm-{job_id}.out"]:
+            try:
+                sftp = client.open_sftp()
+                with sftp.file(path, 'r') as f:
+                    logs[path] = f.read().decode()
+                sftp.close()
+            except Exception:
+                pass
+        client.close()
+        return {"state":"COMPLETED","sacct":sacct,"logs":logs}
+    else:
+        client.close()
+        return {"state":"PENDING","squeue":out}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--host")
+    p.add_argument("--user", default="ubuntu")
+    p.add_argument("--ssh-key-secret", default=None)
+    p.add_argument("--job-script-s3", default=None)
+    p.add_argument("--out", default="/tmp/argo_slurm_out.json")
+    p.add_argument("--poll", default=None)
+    args = p.parse_args()
+    if args.poll:
+        # poll flow
+        sub = json.load(open(args.poll))
+        host = sub["host"]; user = sub["user"]; key_path = sub["key_path"]; job_id = sub["job_id"]
+        res = poll_job(host, user, key_path, job_id)
+        open(args.out,"w").write(json.dumps(res))
+        print(json.dumps(res, indent=2))
+        return
+    # submit flow
+    tmp = tempfile.mkdtemp()
+    local_script = os.path.join(tmp, "job.sh")
+    if args.job_script_s3:
+        download_s3(args.job_script_s3, local_script)
+    else:
+        raise ValueError("job-script-s3 required")
+    # write SSH key to temp file from mounted secret (operator: mount secret to /tmp/ssh_key)
+    key_path = "/tmp/ssh_key"  # Argo step must mount secret here
+    if not os.path.exists(key_path):
+        raise RuntimeError("SSH key not found at /tmp/ssh_key; mount secret into container")
+    job_id, out = submit_job(args.host, args.user, key_path, local_script)
+    result = {"host": args.host, "user": args.user, "job_id": job_id, "submit_output": out, "key_path": key_path}
+    open(args.out,"w").write(json.dumps(result))
+    print(json.dumps(result, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:hpc/slurm/job_template.sh
+#!/bin/bash
+#SBATCH --job-name=aegis-deepspeed
+#SBATCH --nodes=4
+#SBATCH --ntasks-per-node=8
+#SBATCH --gres=gpu:4
+#SBATCH --time=24:00:00
+#SBATCH --output=/tmp/slurm-%j.out
+
+module load cuda/12.0
+module load python/3.10
+source /opt/venv/bin/activate
+export NCCL_DEBUG=INFO
+deepspeed --num_nodes=4 --num_gpus=4 training/world_model/deepspeed_entrypoint.py --deepspeed_config deepspeed/deepspeed_config_zero3.json
+
*** End Patch
*** Begin Patch
*** Add File:scripts/checkpoint/offload_multipart_upload.py
+#!/usr/bin/env python3
+"""
+Multipart upload checkpoint to S3 + produce manifest.
+Usage: python3 scripts/checkpoint/offload_multipart_upload.py --file /path/to/checkpoint.tar.gz --bucket my-bucket --key path/checkpoint.tar.gz
+"""
+import argparse, boto3, os, json, hashlib
+
+def sha256sum(path):
+    h = hashlib.sha256()
+    with open(path, "rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def multipart_upload(file, bucket, key):
+    s3 = boto3.client("s3")
+    mp = s3.create_multipart_upload(Bucket=bucket, Key=key)
+    upload_id = mp["UploadId"]
+    parts = []
+    part_num = 1
+    part_size = 50 * 1024 * 1024
+    with open(file, "rb") as f:
+        while True:
+            data = f.read(part_size)
+            if not data:
+                break
+            res = s3.upload_part(Bucket=bucket, Key=key, PartNumber=part_num, UploadId=upload_id, Body=data)
+            parts.append({"ETag": res["ETag"], "PartNumber": part_num})
+            part_num += 1
+    s3.complete_multipart_upload(Bucket=bucket, Key=key, UploadId=upload_id, MultipartUpload={"Parts": parts})
+    return {"Bucket": bucket, "Key": key}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--file", required=True)
+    p.add_argument("--bucket", required=True)
+    p.add_argument("--key", required=True)
+    args = p.parse_args()
+    manifest = {"file": args.file, "bucket": args.bucket, "key": args.key, "sha256": sha256sum(args.file)}
+    print("Uploading", args.file, "-> s3://%s/%s" % (args.bucket, args.key))
+    multipart_upload(args.file, args.bucket, args.key)
+    # write manifest locally
+    manpath = args.file + ".manifest.json"
+    with open(manpath, "w") as f:
+        json.dump(manifest, f, indent=2)
+    print("Wrote manifest", manpath)
+    print(json.dumps(manifest, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/checkpoint/sign_and_manifest.py
+#!/usr/bin/env python3
+"""
+Sign a manifest with HSM helper (if available) and produce signed manifest JSON.
+Usage: python3 scripts/checkpoint/sign_and_manifest.py --manifest /path/to/checkpoint.tar.gz.manifest.json --out /tmp/signed_manifest.json
+"""
+import argparse, json, os
+
+def sign_manifest(manifest_path, out):
+    m = json.load(open(manifest_path))
+    try:
+        from production.policy.signing.sign_with_retry import sign_payload
+        sig, meta = sign_payload(json.dumps(m).encode(), None)
+        m["signature"] = sig
+        m["signed_by"] = meta
+    except Exception as e:
+        m["signature_error"] = str(e)
+    with open(out, "w") as f:
+        json.dump(m, f, indent=2)
+    print("Wrote signed manifest to", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--out", default="/tmp/signed_manifest.json")
+    args = p.parse_args()
+    sign_manifest(args.manifest, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/checkpoint/offload_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: checkpoint-offload-
+  namespace: aegis
+spec:
+  entrypoint: offload
+  templates:
+    - name: offload
+      inputs:
+        parameters:
+          - name: local_path
+          - name: s3_bucket
+          - name: s3_key
+      steps:
+        - - name: multipart-upload
+            template: multipart-upload
+            arguments:
+              parameters:
+                - name: file
+                  value: "{{inputs.parameters.local_path}}"
+                - name: bucket
+                  value: "{{inputs.parameters.s3_bucket}}"
+                - name: key
+                  value: "{{inputs.parameters.s3_key}}"
+        - - name: sign-manifest
+            template: sign-manifest
+
+    - name: multipart-upload
+      inputs:
+        parameters:
+          - name: file
+          - name: bucket
+          - name: key
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 || true
+            python3 scripts/checkpoint/offload_multipart_upload.py --file "{{inputs.parameters.file}}" --bucket "{{inputs.parameters.bucket}}" --key "{{inputs.parameters.key}}" || true
+
+    - name: sign-manifest
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 || true
+            python3 scripts/checkpoint/sign_and_manifest.py --manifest "{{inputs.parameters.file}}.manifest.json" --out /tmp/signed_manifest.json || true
+            cat /tmp/signed_manifest.json
+
*** End Patch
*** Begin Patch
*** Add File:triton/autoscale/hpa_triton.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: triton-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: triton-onrack
+  minReplicas: 1
+  maxReplicas: 5
+  metrics:
+    - type: Pods
+      pods:
+        metric:
+          name: model_inference_latency_seconds_p95
+        target:
+          type: AverageValue
+          averageValue: "0.05" # 50 ms
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/service_monitor_triton.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: triton-servicemonitor
+  namespace: aegis
+spec:
+  selector:
+    matchLabels:
+      app: triton-onrack
+  endpoints:
+    - port: metrics
+      interval: 15s
+
*** End Patch
*** Begin Patch
*** Add File:scripts/cost/cost_controller.py
+#!/usr/bin/env python3
+"""
+Simple cost controller:
+- Watches Job objects in namespace and cancels (deletes) ones that have annotation aegis/cost-limit set (USD)
+- Stores per-project spend in a ConfigMap (operator-managed) or uses a stub costing model.
+
+This is a scaffold: replace costing with real cloud billing queries or internal chargeback metrics.
+"""
+import os, time, json
+from kubernetes import client, config, watch
+
+NAMESPACE = os.environ.get("NAMESPACE","aegis")
+CONFIGMAP = os.environ.get("COST_CONFIGMAP","aegis-costs")
+
+def load_config():
+    v1 = client.CoreV1Api()
+    try:
+        cm = v1.read_namespaced_config_map(CONFIGMAP, NAMESPACE)
+        return json.loads(cm.data.get("costs","{}"))
+    except Exception:
+        return {}
+
+def reconcile():
+    config.load_incluster_config()
+    batch = client.BatchV1Api()
+    v1 = client.CoreV1Api()
+    w = watch.Watch()
+    for event in w.stream(batch.list_namespaced_job, namespace=NAMESPACE, timeout_seconds=0):
+        job = event['object']
+        name = job.metadata.name
+        ann = job.metadata.annotations or {}
+        if 'aegis/cost-limit' in ann:
+            try:
+                cost_limit = float(ann['aegis/cost-limit'])
+            except:
+                continue
+            # stub: get estimated job cost from label or config
+            est_cost = float(job.metadata.labels.get('aegis/est-cost', '0'))
+            if est_cost > cost_limit:
+                print("Job",name,"exceeds cost limit. Deleting job.")
+                try:
+                    batch.delete_namespaced_job(name, NAMESPACE, propagation_policy="Background")
+                except Exception as e:
+                    print("Failed to delete job", e)
+
+if __name__ == "__main__":
+    reconcile()
+
*** End Patch
*** Begin Patch
*** Add File:argo/cost/enforce_budget_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: cost-enforce-
+  namespace: aegis
+spec:
+  entrypoint: cost-enforce
+  templates:
+    - name: cost-enforce
+      steps:
+        - - name: run-controller
+            template: run-controller
+
+    - name: run-controller
+      container:
+        image: registry.example.com/aegis/cost-controller:latest
+        command: [sh, -c]
+        args:
+          - python3 scripts/cost/cost_controller.py
+
*** End Patch
*** Begin Patch
*** Add File:dr/replicate_s3_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: s3-replicate-
+  namespace: aegis
+spec:
+  entrypoint: s3-replicate
+  templates:
+    - name: s3-replicate
+      inputs:
+        parameters:
+          - name: src_bucket
+          - name: dest_bucket
+      steps:
+        - - name: replicate
+            template: replicate
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{inputs.parameters.src_bucket}}"
+                - name: dst
+                  value: "{{inputs.parameters.dest_bucket}}"
+
+    - name: replicate
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 || true
+            python3 scripts/dr/s3_replica.py --src "{{inputs.parameters.src}}" --dst "{{inputs.parameters.dst}}" --out /tmp/replica_result.json || true
+            cat /tmp/replica_result.json
+
*** End Patch
*** Begin Patch
*** Add File:scripts/dr/s3_replica.py
+#!/usr/bin/env python3
+"""
+Replicate objects from one S3 bucket/prefix to another region/bucket.
+Attempts to preserve metadata and verify sha256 checksum when possible.
+"""
+import argparse, boto3, hashlib, json, os
+
+def sha256_s3_object(s3, bucket, key):
+    # download small chunks to compute hash (not efficient for huge objects, but scaffold)
+    obj = s3.get_object(Bucket=bucket, Key=key)
+    h = hashlib.sha256(obj['Body'].read()).hexdigest()
+    return h
+
+def replicate(src, dst):
+    s3 = boto3.client("s3")
+    src_bucket, src_prefix = src.split("/",1)
+    dst_bucket, dst_prefix = dst.split("/",1)
+    paginator = s3.get_paginator("list_objects_v2")
+    keys = []
+    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):
+        for o in page.get("Contents", []):
+            keys.append(o["Key"])
+    results = []
+    for k in keys:
+        dest_key = k.replace(src_prefix, dst_prefix, 1)
+        copy_source = {"Bucket": src_bucket, "Key": k}
+        s3.copy(copy_source, dst_bucket, dest_key)
+        results.append({"src": k, "dst": dest_key})
+    summary = {"replicated": len(results), "items": results[:100]}
+    return summary
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--src", required=True)
+    p.add_argument("--dst", required=True)
+    p.add_argument("--out", default="/tmp/replica_result.json")
+    args = p.parse_args()
+    res = replicate(args.src, args.dst)
+    open(args.out,"w").write(json.dumps(res, indent=2))
+    print("Wrote", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:docs/cloud_edge_hpc_runbook.md
+# Cloud / Edge / HPC Runbook (Aegis)
+
+This runbook summarizes how to use the newly added Cloud/Edge/HPC artifacts and example commands.
+
+1) Cluster Autoscaler & Node Pools
+- Use infra/k8s/cluster_autoscaler/values-cluster-autoscaler.yaml as Helm values for deploying Cluster Autoscaler.
+- Create provider node pools (see infra/k8s/nodepools/gpu_nodepool_example.yaml) via your cloud provider's API (EKS/GKE/AKS) or Cluster API; ensure autoscaler tags are present.
+
+2) Argo â†’ Slurm Bridge
+- Submit Argo workflow: argo submit argo/hpc/argo_to_slurm_bridge_workflow.yaml -n aegis --watch
+- Provide parameters: slurm_host, slurm_user, ssh_key_secret, job_script_s3
+- Ensure SSH key secret is mounted into Argo container at /tmp/ssh_key (workflow uses that path)
+
+3) Checkpoint Offload & Sign
+- Run offload Argo workflow:
+  argo submit argo/checkpoint/offload_workflow.yaml -n aegis -p local_path=/tmp/checkpoint.tar.gz -p s3_bucket=my-bucket -p s3_key=checkpoints/ckpt.tar.gz --watch
+- The workflow uploads checkpoint via multipart upload and signs the manifest.
+
+4) Triton Autoscaling
+- Apply HPA and ServiceMonitor for Triton:
+  kubectl apply -f triton/autoscale/hpa_triton.yaml -n aegis
+  kubectl apply -f monitoring/prometheus/service_monitor_triton.yaml -n aegis
+- Ensure Prometheus Adapter is configured to expose model_inference_latency_seconds_p95 metric to the HPA.
+
+5) Cost Controller & Budget Enforcement
+- Deploy cost controller container (image built from scripts/cost/cost_controller.py) and run:
+  argo submit argo/cost/enforce_budget_workflow.yaml -n aegis --watch
+- Annotate Jobs with aegis/cost-limit to enforce limits (stub costing uses label aegis/est-cost).
+
+6) DR / Cross-Region Replication
+- Use Argo replicate workflow:
+  argo submit dr/replicate_s3_workflow.yaml -n aegis -p src_bucket=my-bucket/src -p dest_bucket=other-bucket/dst --watch
+- The script will attempt to copy keys from src prefix to dest prefix; verify checksums manually.
+
+7) Example Quick Commands
+- Submit Argo->Slurm: argo submit argo/hpc/argo_to_slurm_bridge_workflow.yaml -n aegis -p slurm_host=slurm.example -p slurm_user=ubuntu -p ssh_key_secret=slurm-ssh -p job_script_s3=s3://my-bucket/job_template.sh --watch
+- Offload checkpoint: argo submit argo/checkpoint/offload_workflow.yaml -n aegis -p local_path=/tmp/checkpoint.tar.gz -p s3_bucket=my-bucket -p s3_key=checkpoints/exp1/ckpt.tar.gz --watch
+- Replicate s3: argo submit dr/replicate_s3_workflow.yaml -n aegis -p src_bucket=my-bucket/exp1 -p dest_bucket=backup-bucket/exp1 --watch
+
+Notes & Next Steps
+- Replace scaffolded tooling with hardened vendor-specific implementations (SSH key handling, Slurm role account, S3 cross-region replication with lifecycle and encryption).
+- Integrate cloud billing APIs or internal chargeback for cost_controller to make enforcement accurate.
+- Configure Prometheus Adapter for custom metrics used by HPA.
+
*** End Patch
*** End Patch
