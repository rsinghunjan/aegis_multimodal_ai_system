*** Begin Patch
*** Add File:quantum/crypto/rekor_cosign_integration.py
+#!/usr/bin/env python3
+"""
+Integrate cosign + Rekor in a robust way:
+- Sign an artifact with cosign and ensure Rekor entry is created.
+- Verify Rekor entry exists for a given artifact digest.
+
+This is a thin wrapper that uses the cosign and rekor-cli binaries (recommended production tools).
+It falls back to a minimal Rekor REST upload if rekor-cli is not available (placeholder).
+
+Requirements:
+ - cosign (https://github.com/sigstore/cosign)
+ - rekor-cli (preferred) or rekor server reachable from REST API
+
+Usage:
+  python3 rekor_cosign_integration.py sign --artifact file --cosign-key /path/to/key
+  python3 rekor_cosign_integration.py verify --artifact file
+"""
+import argparse
+import subprocess
+import json
+import os
+import hashlib
+import sys
+
+def sha256_hex(path):
+    import hashlib
+    h = hashlib.sha256()
+    with open(path,"rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def cosign_sign(artifact, key=None, output_sig=None):
+    cmd = ["cosign", "sign-blob"]
+    if key:
+        cmd += ["--key", key]
+    if output_sig:
+        cmd += ["--output-signature", output_sig]
+    cmd += [artifact]
+    subprocess.check_call(cmd)
+    return output_sig
+
+def rekor_upload_with_cli(artifact, signature, public_key=None):
+    cmd = ["rekor-cli", "upload", "--artifact", artifact]
+    if signature:
+        cmd += ["--signature", signature]
+    if public_key:
+        cmd += ["--public-key", public_key]
+    subprocess.check_call(cmd)
+    print("rekor-cli upload attempted for", artifact)
+
+def rekor_rest_submit(artifact, signature, rekor_server="https://rekor.sigstore.dev"):
+    # Minimal placeholder: in production use cosign/releasetools or rekor client library
+    print("Submitting to Rekor via REST is not implemented here. Please use rekor-cli or official Rekor client.")
+    return
+
+def sign_and_rekor(args):
+    artifact = args.artifact
+    outdir = args.outdir or os.path.dirname(os.path.abspath(artifact))
+    os.makedirs(outdir, exist_ok=True)
+    sigfile = os.path.join(outdir, "classical.sig")
+    print("Signing with cosign -> signature:", sigfile)
+    cosign_sign(artifact, key=args.cosign_key, output_sig=sigfile)
+    # attempt rekor-cli upload
+    try:
+        rekor_upload_with_cli(artifact, sigfile, public_key=args.public_key)
+    except FileNotFoundError:
+        print("rekor-cli not found; falling back to REST placeholder")
+        rekor_rest_submit(artifact, sigfile, rekor_server=args.rekor_server)
+    sha = sha256_hex(artifact)
+    print("Artifact SHA256:", sha)
+
+def verify_entry(args):
+    artifact = args.artifact
+    sha = sha256_hex(artifact)
+    print("Looking for Rekor entry for SHA256:", sha)
+    # Use rekor-cli search if available
+    try:
+        out = subprocess.check_output(["rekor-cli", "search", "--hash", sha])
+        print("rekor-cli result:", out.decode("utf-8"))
+    except FileNotFoundError:
+        print("rekor-cli not found: please install or query Rekor REST API.")
+
+def main():
+    p = argparse.ArgumentParser()
+    sub = p.add_subparsers(dest="cmd")
+    s = sub.add_parser("sign")
+    s.add_argument("--artifact", required=True)
+    s.add_argument("--cosign-key", default=None)
+    s.add_argument("--public-key", default=None)
+    s.add_argument("--outdir", default=None)
+    s.add_argument("--rekor-server", default="https://rekor.sigstore.dev")
+    v = sub.add_parser("verify")
+    v.add_argument("--artifact", required=True)
+    args = p.parse_args()
+    if args.cmd == "sign":
+        sign_and_rekor(args)
+    elif args.cmd == "verify":
+        verify_entry(args)
+    else:
+        p.print_help()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/crypto/pkcs11_example_hsm.tf
+/*
+Terraform example (SoftHSM/pass-through) to provision a placeholder for HSM integration.
+In production replace with HSM vendor Terraform provider or cloud KMS/HSM resource (AWS CloudHSM / Azure KeyVault HSM / Google Cloud HSM).
+
+This resource demonstrates storing PKCS#11 configuration secrets in Vault (example).
+*/
+provider "vault" {
+  address = var.vault_addr
+  token   = var.vault_token
+}
+
+resource "vault_kv_secret_v2" "pkcs11_config" {
+  path = "hsm/pkcs11-config"
+  data_json = jsonencode({
+    module_path = var.pkcs11_module_path
+    slot        = var.pkcs11_slot
+    token_label  = var.token_label
+  })
+}
+
+variable "vault_addr" { type = string }
+variable "vault_token" { type = string, sensitive = true }
+variable "pkcs11_module_path" { type = string }
+variable "pkcs11_slot" { type = string }
+variable "token_label" { type = string }
+
*** End Patch
*** Begin Patch
*** Add File:quantum/crypto/soft_hsm_setup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Quick local SoftHSM setup script for development/test
+#
+if ! command -v softhsm2-util >/dev/null 2>&1; then
+  echo "Install softhsm2 & pkcs11-tool first (your OS package manager)."
+  exit 1
+fi
+
+TOKENDIR="$(pwd)/tokens"
+mkdir -p "$TOKENDIR"
+export SOFTHSM2_CONF="$(pwd)/softhsm2.conf"
+cat > "$SOFTHSM2_CONF" <<EOF
+directories.tokendir = ${TOKENDIR}
+EOF
+
+echo "Initialising token..."
+softhsm2-util --init-token --free --label "aegis-pq-token" --so-pin 1234 --pin 1234
+echo "Creating RSA key (placeholder)..."
+pkcs11-tool --module /usr/lib/softhsm/libsofthsm2.so --login --pin 1234 --keypairgen --key-type rsa:2048 --label pqkey || true
+echo "SoftHSM init complete. Token dir: $TOKENDIR"
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/worker_postgres_enhanced.py
+#!/usr/bin/env python3
+"""
+Enhanced worker:
+ - Uses Postgres-backed job queue (SQLAlchemy)
+ - Provides cost/quota enforcement and fallback to simulator if QPU unavailable or quota exceeded
+ - Emits Prometheus metrics for queue depth, job_processed_total, job_errors_total
+"""
+import time, json, os
+from prometheus_client import start_http_server, Counter, Gauge
+from quantum.qbackend.qiskit_adapter import QiskitAdapter
+from quantum.job_broker import db as jb_db
+
+QUEUE_DEPTH = Gauge('aegis_quantum_queue_depth', 'Number of pending quantum jobs')
+JOB_PROCESSED = Counter('aegis_quantum_jobs_processed_total', 'Total processed jobs')
+JOB_ERRORS = Counter('aegis_quantum_jobs_errors_total', 'Total job errors')
+
+adapter = QiskitAdapter()
+
+def has_quota_for_user(user_id, estimated_cost):
+    # Placeholder: integrate with billing/Quota service. For demo, accept small jobs only.
+    # Example: block jobs with estimated_cost > 100 (units)
+    return estimated_cost <= 100
+
+def poll_loop(poll_interval=2):
+    start_http_server(9091)  # Prometheus metrics endpoint
+    print("Prometheus metrics available on :9091")
+    while True:
+        db = jb_db.SessionLocal()
+        pending = db.query(jb_db.Job).filter(jb_db.Job.status == "PENDING").all()
+        QUEUE_DEPTH.set(len(pending))
+        rows = jb_db.fetch_pending(db, limit=5)
+        if not rows:
+            db.close()
+            time.sleep(poll_interval)
+            continue
+        for r in rows:
+            job_id = r.id
+            spec = json.loads(r.spec)
+            # Basic cost estimation heuristic: shots / 100 => cost units
+            estimated_cost = int(spec.get("shots", 1024)/100)
+            user_id = "unknown"
+            try:
+                if not has_quota_for_user(user_id, estimated_cost):
+                    # fallback to simulator
+                    spec["backend"] = "aer_simulator"
+                jid = adapter.submit_job(spec)
+                res = adapter.get_result(jid)
+                jb_db.update_job(db, job_id, "DONE", json.dumps(res))
+                JOB_PROCESSED.inc()
+                print("Job", job_id, "completed")
+            except Exception as e:
+                jb_db.update_job(db, job_id, "ERROR", json.dumps({"error": str(e)}))
+                JOB_ERRORS.inc()
+                print("Job", job_id, "failed:", e)
+        db.close()
+        time.sleep(poll_interval)
+
+if __name__ == "__main__":
+    poll_loop()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/metrics_dashboard/grafana_quantum_broker_dashboard.json
+{
+  "title": "Aegis Quantum Broker",
+  "panels": [
+    {
+      "type": "stat",
+      "title": "Queue Depth",
+      "targets": [{"expr":"aegis_quantum_queue_depth","refId":"A"}]
+    },
+    {
+      "type": "graph",
+      "title": "Jobs Processed",
+      "targets": [{"expr":"increase(aegis_quantum_jobs_processed_total[5m])","refId":"B"}]
+    },
+    {
+      "type": "graph",
+      "title": "Job Errors",
+      "targets": [{"expr":"increase(aegis_quantum_jobs_errors_total[5m])","refId":"C"}]
+    }
+  ],
+  "schemaVersion": 18
+}
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/braket_adapter_complete.py
+"""
+AWS Braket adapter (example).
+This implementation uses boto3 braket 'search_devices' and 'create_quantum_task' APIs where available.
+Note: you must configure AWS credentials with permission to use Braket and S3 for input/output.
+"""
+import uuid, os, time, json
+import boto3
+from botocore.exceptions import ClientError
+
+from .adapter import QuantumBackendAdapter
+
+class BraketAdapter(QuantumBackendAdapter):
+    def __init__(self, region_name=None, s3_bucket=None, s3_prefix="braket"):
+        self.region = region_name or os.environ.get("AWS_REGION", "us-west-2")
+        self.client = boto3.client("braket", region_name=self.region)
+        self.s3_bucket = s3_bucket or os.environ.get("BRAKET_RESULTS_BUCKET")
+        self.s3_prefix = s3_prefix
+        self.jobs = {}
+
+    def _upload_program_to_s3(self, program_str, key_prefix):
+        if not self.s3_bucket:
+            raise RuntimeError("S3 bucket not configured for Braket results/programs.")
+        s3 = boto3.client("s3", region_name=self.region)
+        key = f"{key_prefix}/{uuid.uuid4()}.json"
+        s3.put_object(Bucket=self.s3_bucket, Key=key, Body=program_str.encode("utf-8"))
+        return f"s3://{self.s3_bucket}/{key}"
+
+    def submit_job(self, job_spec: dict) -> str:
+        # job_spec expected: { "program": <json program or openqasm>, "device": "<deviceArn>", "shots": int }
+        job_id = str(uuid.uuid4())
+        program = job_spec.get("program") or job_spec.get("circuit_qasm")
+        device_arn = job_spec.get("device")
+        if not device_arn:
+            # fallback to local Aer
+            self.jobs[job_id] = {"status":"ERROR","error":"No device specified"}
+            return job_id
+        try:
+            program_s3 = self._upload_program_to_s3(program, self.s3_prefix)
+            # create quantum task (placeholder payload â€” adapt to the precise API for your Braket SDK)
+            resp = self.client.create_quantum_task(
+                action={ "content": program }, # placeholder
+                deviceArn=device_arn,
+                shots=job_spec.get("shots", 1000),
+                s3DestinationFolder={ "bucket": self.s3_bucket, "key": f"{self.s3_prefix}/{job_id}" }
+            )
+            task_arn = resp.get("quantumTaskArn")
+            self.jobs[job_id] = {"status":"SUBMITTED","task_arn": task_arn}
+            return job_id
+        except ClientError as e:
+            self.jobs[job_id] = {"status":"ERROR","error": str(e)}
+            return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        info = self.jobs.get(job_id)
+        if not info:
+            return {"status":"UNKNOWN"}
+        if info.get("status") == "SUBMITTED":
+            # In production poll Braket to get final result and fetch S3 artifacts
+            return {"status":"SUBMITTED","task_arn": info.get("task_arn")}
+        return info
+
+    def cancel_job(self, job_id: str) -> bool:
+        # Braket supports cancellation via API; placeholder here
+        if job_id in self.jobs:
+            self.jobs[job_id]["status"] = "CANCELLED"
+            return True
+        return False
+
+    def list_backends(self) -> list:
+        try:
+            resp = self.client.search_devices()
+            devices = [{"id": d["deviceArn"], "type": d.get("deviceType")} for d in resp.get("devices", [])]
+            return devices
+        except Exception:
+            return []
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/ibm_adapter_complete.py
+"""
+IBM Quantum adapter using qiskit-ibm-runtime.
+Requires QISKIT_IBM_TOKEN or other auth configured.
+"""
+import uuid, os, json, time
+from qiskit_ibm_runtime import IBMQRuntimeService, Session, Sampler
+from .adapter import QuantumBackendAdapter
+
+class IBMAdapter(QuantumBackendAdapter):
+    def __init__(self, instance=None):
+        token = os.environ.get("QISKIT_IBM_TOKEN")
+        if not token:
+            raise RuntimeError("QISKIT_IBM_TOKEN not set")
+        self.service = IBMQRuntimeService(channel="ibm_cloud", token=token)
+        self.jobs = {}
+
+    def submit_job(self, job_spec: dict) -> str:
+        job_id = str(uuid.uuid4())
+        # job_spec: { "circuit_qasm": "...", "backend": "<backend>" }
+        backend_name = job_spec.get("backend")
+        qasm = job_spec.get("circuit_qasm")
+        # For demonstration we submit a Sampler run in a new session
+        try:
+            qc_program = qasm
+            with Session(service=self.service, backend=backend_name) as session:
+                sampler = Sampler(session=session)
+                result = sampler.run(qc_program, shots=job_spec.get("shots", 1024))
+                counts = result.shots_counts()
+                self.jobs[job_id] = {"status":"DONE","counts":counts}
+            return job_id
+        except Exception as e:
+            self.jobs[job_id] = {"status":"ERROR","error": str(e)}
+            return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        return self.jobs.get(job_id, {"status":"UNKNOWN"})
+
+    def cancel_job(self, job_id: str) -> bool:
+        # IBM runtime cancel not shown here; implement via session cancellation or job id
+        if job_id in self.jobs:
+            self.jobs[job_id]["status"] = "CANCELLED"
+            return True
+        return False
+
+    def list_backends(self):
+        # Return available backends via service
+        try:
+            backends = self.service.backends()
+            return [{"id": b.name, "capabilities": []} for b in backends]
+        except Exception:
+            return []
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/azure_adapter_complete.py
+"""
+Azure Quantum adapter skeleton using azure-quantum SDK (placeholder).
+Requires AZURE_SUBSCRIPTION_ID and workspace info in env.
+"""
+import uuid, os, time, json
+from .adapter import QuantumBackendAdapter
+
+class AzureAdapter(QuantumBackendAdapter):
+    def __init__(self, workspace=None):
+        self.workspace = workspace or os.environ.get("AZURE_QUANTUM_WORKSPACE")
+        self.jobs = {}
+
+    def submit_job(self, job_spec: dict) -> str:
+        job_id = str(uuid.uuid4())
+        # Placeholder: implement azure-quantum job submission using azure.quantum SDK
+        self.jobs[job_id] = {"status":"SUBMITTED","spec":job_spec}
+        return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        return self.jobs.get(job_id, {"status":"UNKNOWN"})
+
+    def cancel_job(self, job_id: str) -> bool:
+        if job_id in self.jobs:
+            self.jobs[job_id]["status"] = "CANCELLED"
+            return True
+        return False
+
+    def list_backends(self) -> list:
+        # TODO: query Azure workspace to list providers & devices
+        return []
+
*** End Patch
*** Begin Patch
*** Add File:quantum/observability/prometheus_exporter.py
+#!/usr/bin/env python3
+"""
+Prometheus exporter for quantum adapters and HSM status.
+Expose metrics such as:
+ - hsm_available (0/1)
+ - job_submission_count
+ - rekor_submission_count / rekor_last_status
+"""
+from prometheus_client import start_http_server, Gauge, Counter
+import time, os, subprocess
+
+HSM_AVAILABLE = Gauge('aegis_hsm_available', 'SoftHSM/HSM availability (1=up)')
+JOB_SUB = Counter('aegis_job_submissions_total', 'Number of job submissions')
+REKOR_SUB = Counter('aegis_rekor_submissions_total', 'Number of Rekor submissions')
+
+def check_hsm(module_path="/usr/lib/softhsm/libsofthsm2.so"):
+    # simple check: try to run pkcs11-tool --module <module> --list-objects
+    try:
+        subprocess.check_call(["pkcs11-tool", "--module", module_path, "--list-objects"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
+        HSM_AVAILABLE.set(1)
+    except Exception:
+        HSM_AVAILABLE.set(0)
+
+def run_exporter(port=9112, module_path="/usr/lib/softhsm/libsofthsm2.so"):
+    start_http_server(port)
+    print("Prometheus exporter running on port", port)
+    while True:
+        check_hsm(module_path)
+        time.sleep(10)
+
+if __name__ == "__main__":
+    run_exporter()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/observability/grafana_quantum_full.json
+{
+  "title": "Aegis Quantum Full Observability",
+  "panels": [
+    {
+      "type": "stat",
+      "title": "HSM Available",
+      "targets":[{"expr":"aegis_hsm_available","refId":"A"}]
+    },
+    {
+      "type": "graph",
+      "title": "Job Submissions",
+      "targets":[{"expr":"increase(aegis_job_submissions_total[5m])","refId":"B"}]
+    },
+    {
+      "type": "graph",
+      "title": "Rekor Submissions",
+      "targets":[{"expr":"increase(aegis_rekor_submissions_total[5m])","refId":"C"}]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:quantum/pipeline/e2e_staging_run.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# End-to-end staging demonstration script (operator-run).
+# Requirements:
+#  - Postgres-backed job broker running
+#  - Prometheus exporter optionally running
+#  - SoftHSM or vendor HSM available and configured
+#  - cosign & rekor-cli installed
+#
+ART=/tmp/aegis_quantum_demo.bin
+SIGDIR=/tmp/aegis_qsigs
+JOB_BROKER_URL=${JOB_BROKER_URL:-http://localhost:8080}
+JWT_TOKEN=${BROKER_JWT:-}
+
+echo "Create demo artifact"
+echo "hello quantum E2E" > $ART
+
+echo "Sign artifact with hybrid signer (PKCS#11 path if configured)"
+python3 quantum/crypto/hybrid_signer_hsm.py sign --artifact $ART --outdir $SIGDIR --use-pkcs11 --pkcs11-lib /usr/lib/softhsm/libsofthsm2.so --pkcs11-slot 0 --pkcs11-pin 1234 --pkcs11-keylabel pqkey --rekor
+
+echo "Verify Rekor entry (attempt)"
+python3 quantum/crypto/rekor_cosign_integration.py verify --artifact $ART || true
+
+if [ -z "$JWT_TOKEN" ]; then
+  echo "No JWT set; please set BROKER_JWT env var with a valid token to submit job to broker."
+  exit 0
+fi
+
+echo "Submitting quantum job to broker (simulator fallback)"
+JOB_SPEC='{"circuit_qasm":"OPENQASM 2.0; // placeholder","shots":1024,"backend":"aer_simulator"}'
+curl -s -X POST $JOB_BROKER_URL/submit -H "Authorization: Bearer $JWT_TOKEN" -H "Content-Type: application/json" -d "$JOB_SPEC" | jq .
+
+echo "Poll status manually or use broker status endpoint."
+
*** End Patch
*** Begin Patch
*** Add File:quantum/compliance/export_control_and_soc2_CHECKLIST.md
+# Quantum / PQ Compliance Checklist (export controls & SOC2)
+
+1) Export control scoping
+ - Inventory cryptographic algorithms used (PQ algorithms planned + classical)
+ - Determine if project crosses export control boundaries (consult legal)
+ - Maintain documentation of any data sent to foreign QPU providers
+
+2) Vendor contracts & SLAs
+ - Ensure vendor terms permit required audits and evidence collection
+ - Negotiate data handling and retention terms for QPU job artifacts
+
+3) SOC2 evidence updates
+ - Include HSM key lifecycle records and rotation metadata in evidence store
+ - Include Rekor entries for quantum experiments in evidence bundle
+ - Document who has access to PQ private keys and HSM operational roles
+
+4) Audit & logging
+ - Ensure HSM access logs are captured & archived
+ - Job broker access logs and Prometheus/Grafana dashboards are preserved for audit windows
+
+5) Incident response & egress
+ - Define playbooks for compromised signing keys or HSM incidents
+ - Implement secure key rotation & emergency revocation steps in runbooks
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/helm/README.md
+Deploying the hardened job broker via Helm (staging)
+--------------------------------------------------
+Steps:
+1) Build and push Docker image for broker:
+   docker build -t <registry>/aegis-quantum-broker:staging -f quantum/job_broker/Dockerfile .
+   docker push <registry>/aegis-quantum-broker:staging
+
+2) Configure values.yaml with production JOB_DATABASE_URL and set BROKER_API_JWT_SECRET as a Kubernetes secret (helm templating present in chart).
+
+3) Deploy:
+   helm install aegis-quantum-broker quantum/job_broker/helm -f quantum/job_broker/helm/values.yaml
+
+4) Deploy Postgres (managed or the included statefulset for staging). Ensure service DNS resolves.
+
+5) Start worker deployments (use the same image but run worker_postgres_enhanced.py as entrypoint).
+
*** End Patch
*** Begin Patch
*** Update File:quantum/job_broker/app_postgres.py
@@
-from fastapi import FastAPI, HTTPException, Depends, Header, Request
+from fastapi import FastAPI, HTTPException, Depends, Header, Request
 from pydantic import BaseModel
 import os, uuid, json, time
 from sqlalchemy.orm import Session
 from jose import jwt, JWTError
 from . import db as jb_db
+from prometheus_client import start_http_server, Counter
+
+# Start a local Prometheus metrics server for the broker itself
+start_http_server(9100)
+BRK_JOB_SUB = Counter('aegis_broker_job_submissions_total', 'Total job submissions to broker')
*** End Patch
*** Begin Patch
*** Update File:quantum/job_broker/app_postgres.py
@@
 @app.post("/submit")
 def submit(spec: JobSpec, auth=Depends(validate_token)):
     job_id = str(uuid.uuid4())
     db = jb_db.SessionLocal()
     job = jb_db.create_job(db, job_id, json.dumps(spec.dict()))
+    BRK_JOB_SUB.inc()
     return {"job_id": job_id}
*** End Patch
*** Begin Patch
*** Add File:quantum/hsm/soft_hsm_terraform_example.tf
+/*
+Example: create a Kubernetes secret with PKCS#11 configuration for SoftHSM path.
+In production, replace with vendor HSM integration (CloudHSM/KeyVault/HSM provider).
+*/
+provider "kubernetes" {
+  config_path = "~/.kube/config"
+}
+
+resource "kubernetes_secret" "pkcs11_config" {
+  metadata {
+    name = "aegis-pkcs11-config"
+    namespace = "aegis"
+  }
+  data = {
+    pkcs11_lib = base64encode(file("${path.module}/soft_hsm_lib_path.txt"))
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:quantum/README_PRODUCTION_NEXT_STEPS.md
+Production steps to finish HSM & Broker hardening (operator checklist)
+-----------------------------------------------------------------
+HSM integration:
+ - Replace SoftHSM test environment with vendor HSM (obtain PKCS#11 module path and slot info)
+ - Provision HSM credentials in Vault and restrict access via policies
+ - Update hybrid_signer_hsm invocation to use PKCS#11 module path and key label
+ - Validate key rotation procedures and record rotation metadata to compliance/evidence
+
+Broker & compute:
+ - Deploy Postgres managed DB (RDS/Cloud SQL) and update JOB_DATABASE_URL secret
+ - Configure broker Helm values and create Kubernetes secrets for JWT secret
+ - Deploy enhanced worker as a Deployment (use worker_postgres_enhanced.py)
+ - Configure Prometheus scrape for job broker metrics (9100) and worker exporter (9091)
+
+Provider adapters:
+ - Create credentials in Vault for AWS Braket, IBM Quantum, and Azure Quantum
+ - Test each adapter end-to-end with small pilot jobs; capture and store calibration metadata and raw shots
+
+Observability:
+ - Import Grafana dashboards (provided) and create alerts for queue depth, HSM availability, cost spikes
+ - Integrate cost accounting (tag jobs by team and attribute cost estimates)
+
+Compliance:
+ - Provide key rotation evidence and Rekor entries for pilot runs to compliance team
+ - Perform export-control review with legal before running cross-border QPU pilots
+
*** End Patch
*** End Patch
