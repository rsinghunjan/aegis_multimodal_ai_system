*** Begin Patch
*** Add File:ops/operator_full_hardening.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Operator orchestrator to finalize Aegis staging hardening.
+# Runs:
+#  - secrets & IRSA checks
+#  - triggers image build workflow and waits for completion
+#  - wires image names into kustomize overlays
+#  - deploys HA Helm values (Postgres/Milvus) as provided
+#  - runs DR drill and collects report
+#  - deploys cert-manager + KServe TLS ingress
+#  - deploys MCPx auth config + NetworkPolicy
+#  - generates monitoring baselines and applies tuned Prometheus rules
+#  - replaces demo fallbacks (interactive confirmation) or fails
+#  - runs parallel inference smoke test
+#
+# Usage:
+#   export GITHUB_REPOSITORY="owner/repo"
+#   export GITHUB_TOKEN=...
+#   export GHCR_ORG="ghcr.io/yourorg"
+#   export EVIDENCE_BUCKET="aegis-evidence-12345"
+#   ./ops/operator_full_hardening.sh /tmp/aegis_tf_output.json
+#
+TF_OUT=${1:-/tmp/aegis_tf_output.json}
+KUBECTX=${KUBECTX:-}
+GHCR_ORG=${GHCR_ORG:-ghcr.io/yourorg}
+IMAGE_TAG=${IMAGE_TAG:-latest}
+WORKFLOW_BUILD="Build and Push All Images"
+
+function step() { echo; echo "=== $* ==="; }
+
+step "1) Verify secrets & IRSA"
+./ops/populate_and_verify_secrets_irsa.sh "$TF_OUT" "$KUBECTX"
+
+step "2) Trigger GitHub build workflow and wait"
+./ops/trigger_github_workflow_and_wait.sh --repo "${GITHUB_REPOSITORY}" --workflow "build_and_push_all_images.yml" --ref main
+
+step "3) Wire image names into kustomize overlays"
+./ops/wire_images_into_kustomize.sh --registry "$GHCR_ORG" --tag "$IMAGE_TAG" --overlay k8s/overlays/staging
+
+step "4) Apply production HA Helm values (operator review required)"
+echo "Applying HA Helm values — operator must review k8s/ha/production_values.yaml and run helm upgrades"
+# Example commands (commented out): uncomment to run automatically
+# helm upgrade --install postgres bitnami/postgresql -f k8s/ha/production_values.yaml --namespace aegis
+# helm upgrade --install milvus milvus/milvus -f k8s/ha/production_values.yaml --namespace aegis
+
+step "5) Run full DR drill and collect report"
+./scripts/dr/full_dr_drill.sh "$TF_OUT" backups/postgres/latest.sql.gz /tmp/aegis_dr_report.json
+echo "DR report at /tmp/aegis_dr_report.json"
+
+step "6) Ensure cert-manager + KServe TLS"
+kubectl apply -f k8s/kserve/cert-manager-clusterissuer.yaml || true
+kubectl apply -f k8s/kserve/ingress_tls.yaml || true
+
+step "7) Deploy MCPx auth sidecar & network policy"
+kubectl apply -f k8s/mcpx/mcpx-hardened-auth-sidecar.yaml || true
+kubectl apply -f k8s/mcpx/mcpx-auth-networkpolicy.yaml || true
+
+step "8) Tighten IAM (operator action required)"
+echo "Please review iam/least_privilege_full.json and apply via IAM admin. Example:"
+echo "  aws iam create-policy --policy-name aegis-least-privilege --policy-document file://iam/least_privilege_full.json"
+
+step "9) Enable CloudTrail and produce initial audit (operator creds required)"
+./ops/enable_cloudtrail_and_configure.sh "$EVIDENCE_BUCKET"
+./ops/cloudtrail_audit_review.sh --out /tmp/cloudtrail_summary.json || true
+
+step "10) Generate monitoring baselines and apply tuned Prometheus rules"
+SAMPLE_S3="s3://${EVIDENCE_BUCKET}/monitoring/baseline_sample.parquet"
+./ops/apply_baseline_and_prometheus.sh "$SAMPLE_S3" monitoring/tuned_prometheus_rules.yaml
+
+step "11) Scan & optionally replace demo fallbacks"
+./ops/replace_demo_refs_confirm.sh
+
+step "12) Run parallel inference smoke test (adjust endpoint)"
+read -p "Run parallel inference smoke test now? (y/N): " RUN_SMOKE
+if [ "${RUN_SMOKE:-N}" = "y" ]; then
+  ./testing/parallel_inference_runner.py --endpoint "${SMOKE_ENDPOINT:-https://kserve.aegis.example.com/v1/models/aegis-transfer-sentiment:predict}" --concurrency 20 --requests 200
+fi
+
+step "13) Final manual reviews"
+echo "Manual actions remaining:"
+echo "- Review CloudTrail summary: /tmp/cloudtrail_summary.json"
+echo "- Confirm images in registry and Rekor entries"
+echo "- Tune Helm values for PVCs & replicas based on DR report RTO"
+echo "- Run production scale tests as next step (LLM, RL, Federated prototypes)"
+
+echo "Operator hardening run completed (partial automation). Review logs above and follow up on manual items."
+
*** End Patch
*** Begin Patch
*** Add File:ops/trigger_github_workflow_and_wait.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Trigger a GitHub Actions workflow via gh CLI and poll until completion.
+# Usage:
+#   ./ops/trigger_github_workflow_and_wait.sh --repo owner/repo --workflow build_and_push_all_images.yml --ref main
+#
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --repo) REPO="$2"; shift 2;;
+    --workflow) WORKFLOW_FILE="$2"; shift 2;;
+    --ref) REF="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+: "${REPO:?--repo required}"
+: "${WORKFLOW_FILE:?--workflow required}"
+: "${REF:=main}"
+
+echo "Triggering workflow $WORKFLOW_FILE on $REPO@$REF"
+run_id=$(gh workflow run "$WORKFLOW_FILE" --repo "$REPO" --ref "$REF" --json url -q .url)
+echo "Triggered: $run_id"
+echo "Polling workflow runs (this may take several minutes)..."
+sleep 5
+while true; do
+  status=$(gh run list --repo "$REPO" --workflow="$WORKFLOW_FILE" --limit 1 --json conclusion,status -q '.[0]')
+  # Example output: {"conclusion":"success","status":"completed"}
+  concl=$(echo "$status" | jq -r '.conclusion // empty')
+  stat=$(echo "$status" | jq -r '.status // empty')
+  echo "Workflow status: $stat, conclusion: $concl"
+  if [ "$stat" = "completed" ]; then
+    if [ "$concl" = "success" ]; then
+      echo "Workflow completed successfully."
+      exit 0
+    else
+      echo "Workflow completed with conclusion: $concl"
+      exit 3
+    fi
+  fi
+  sleep 10
+done
+
*** End Patch
*** Begin Patch
*** Add File:ops/wire_images_into_kustomize.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Replace placeholder image names in kustomize overlays with actual registry and tag.
+# Usage:
+#  ./ops/wire_images_into_kustomize.sh --registry ghcr.io/myorg --tag latest --overlay k8s/overlays/staging
+#
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --registry) REGISTRY="$2"; shift 2;;
+    --tag) TAG="$2"; shift 2;;
+    --overlay) OVERLAY="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+: "${REGISTRY:?--registry required}"
+: "${TAG:=latest}"
+: "${OVERLAY:?--overlay required}"
+
+echo "Patching kustomize overlay $OVERLAY with registry $REGISTRY and tag $TAG"
+find "$OVERLAY" -type f -name "*.yaml" -o -name "*.yml" | while read -r f; do
+  sed -i.bak -E "s|ghcr.io/yourorg/([a-zA-Z0-9_\-]+):?latest|${REGISTRY}/\\1:${TAG}|g" "$f" || true
+done
+echo "Wiring complete. Review diffs and run: kubectl -n aegis apply -k $OVERLAY"
+
*** End Patch
*** Begin Patch
*** Add File:ops/cloudtrail_audit_review.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Lightweight CloudTrail audit summary: pulls events for the last N hours and writes a JSON summary.
+#
+OUT=${1:-/tmp/cloudtrail_summary.json}
+HOURS=${2:-24}
+
+echo "Fetching CloudTrail events for the last $HOURS hours (this requires AWS creds with CloudTrail read perms)"
+START_TIME=$(date -u -d "-${HOURS} hours" +"%Y-%m-%dT%H:%M:%SZ")
+EVENTS=$(aws cloudtrail lookup-events --start-time "$START_TIME" --max-results 50 2>/dev/null || echo "[]")
+
+jq -n --arg start "$START_TIME" --argjson events "$EVENTS" '{start:$start, count: ($events.Events | length), events: $events.Events}' > "$OUT"
+echo "CloudTrail summary written to $OUT"
+
*** End Patch
*** Begin Patch
*** Add File:ops/apply_baseline_and_prometheus.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Runs baseline generation, uploads baseline to evidence bucket, writes tuned Prometheus rules and applies them.
+# Usage:
+#  ./ops/apply_baseline_and_prometheus.sh s3://bucket/path/features.parquet [out_rules.yaml]
+SAMPLE_S3=${1:-}
+OUT_RULES=${2:-monitoring/tuned_prometheus_rules.yaml}
+
+if [ -z "$SAMPLE_S3" ]; then
+  echo "Usage: $0 s3://bucket/path/features.parquet" >&2
+  exit 2
+fi
+
+echo "Generating baseline and tuned rules from $SAMPLE_S3"
+python3 ops/generate_baseline_and_tune_prometheus.py --sample "$SAMPLE_S3" --out "$OUT_RULES"
+
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET env var must be set}"
+aws s3 cp "$OUT_RULES" "s3://${EVIDENCE_BUCKET}/monitoring/$(basename $OUT_RULES)"
+
+echo "Applying rules to Prometheus operator"
+kubectl -n monitoring apply -f "$OUT_RULES" || echo "Make sure the Prometheus operator has been installed. You may need to convert to PrometheusRule CRD."
+
*** End Patch
*** Begin Patch
*** Add File:ops/replace_demo_refs_confirm.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Scan repo for common demo/synthetic markers and optionally replace them with provided production refs.
+# This script is intentionally conservative — it reports findings and asks for confirmation before changes.
+#
+REPLACE_MAP_FILE=${1:-}
+
+echo "Searching for demo/synthetic fallback patterns..."
+found=$(grep -RIn --exclude-dir=.git -E "demo|synthetic|SYNTHETIC|GENERATE_DEMO|sample_data|FAKE_DATA|UNLABELED_S3_PATH" || true)
+if [ -z "$found" ]; then
+  echo "No demo/synthetic markers found."
+  exit 0
+fi
+
+echo "Found the following potential demo references:"
+echo "$found"
+
+read -p "Would you like to attempt an automated replacement? (y/N) " choice
+if [ "${choice:-N}" != "y" ]; then
+  echo "Aborting automatic replacement. Please update code to point to production data sources."
+  exit 0
+fi
+
+if [ -z "$REPLACE_MAP_FILE" ]; then
+  echo "Please provide a replacement mapping file (YAML or CSV) mapping patterns to replacements." >&2
+  exit 2
+fi
+
+echo "Applying replacements from $REPLACE_MAP_FILE (this will modify files; ensure you have a branch/PR)"
+# Format of file: pattern<tab>replacement per line
+while IFS=$'\t' read -r pattern replacement; do
+  echo "Replacing pattern: $pattern -> $replacement"
+  grep -RIl --exclude-dir=.git -e "$pattern" | xargs -r sed -i.bak "s|$pattern|$replacement|g"
+done < "$REPLACE_MAP_FILE"
+
+echo "Replacement complete. Review changes and open a PR."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_template_require_signed_annotation.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequiredsignedannotation
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequiredSignedAnnotation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8srequiredsignedannotation
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Deployment"
+          not input.review.object.metadata.annotations["cosign.sigstore.dev/signed"]
+          msg := "Deployment must include cosign.sigstore.dev/signed annotation (set to 'true') indicating artifact was signed"
+        }
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_require_signed_images.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequiredSignedAnnotation
+metadata:
+  name: require-signed-images
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["aegis"]
+
*** End Patch
*** Begin Patch
*** Add File:k8s/llm/llm_serving_template.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: llm-inference
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: llm-inference
+  template:
+    metadata:
+      labels:
+        app: llm-inference
+    spec:
+      serviceAccountName: llm-sa
+      containers:
+        - name: llm-server
+          image: "{{IMAGE_REGISTRY}}/aegis-llm-serve:latest"
+          env:
+            - name: MODEL_S3_URI
+              value: "s3://REPLACE_EVIDENCE_BUCKET/llm_models/latest/"
+            - name: MAX_BATCH_SIZE
+              value: "8"
+          resources:
+            requests:
+              nvidia.com/gpu: "1"
+              cpu: "2000m"
+              memory: "16Gi"
+            limits:
+              nvidia.com/gpu: "1"
+              cpu: "8000m"
+              memory: "64Gi"
+---
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: llm-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: llm-inference
+  minReplicas: 1
+  maxReplicas: 4
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:federated/flower/flower_server.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: flower-server
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: flower
+  template:
+    metadata:
+      labels:
+        app: flower
+    spec:
+      containers:
+        - name: flwr
+          image: ghcr.io/yourorg/flower-server:latest
+          env:
+            - name: FLOWER_PORT
+              value: "8080"
+            - name: SECURE_AGG_ENABLED
+              value: "true"
+          ports:
+            - containerPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File:ray/raycluster_values.yaml
+# RayCluster Helm values (example)
+clusterName: aegis-ray
+head:
+  replicas: 1
+  resources:
+    requests:
+      cpu: "2000m"
+      memory: "4Gi"
+worker:
+  replicas: 3
+  resources:
+    requests:
+      cpu: "4000m"
+      memory: "16Gi"
+autoshutdown:
+  enabled: false
+
*** End Patch
*** Begin Patch
*** Add File:ops/tighten_iam_apply_hint.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Guidance script: outputs AWS CLI commands to tighten IAM policies. Operator must review before running.
+#
+POLICY_FILE="iam/least_privilege_full.json"
+echo "Reviewing policy file: $POLICY_FILE"
+echo "To apply policy, run (as IAM admin):"
+echo "  aws iam create-policy --policy-name aegis-least-privilege --policy-document file://$POLICY_FILE"
+echo ""
+echo "Then attach to roles used by IRSA (replace ROLE_ARN placeholders):"
+echo "  aws iam attach-role-policy --role-name <role-name> --policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/aegis-least-privilege"
+
*** End Patch
*** Begin Patch
*** Add File:ops/monitoring_review_checklist.md
+- Monitoring Review Checklist
+
+- Ensure tuned_prometheus_rules.yaml applied and PrometheusRule objects exist in monitoring namespace.
+- Validate Alertmanager routes & receiver mappings to PagerDuty/Slack.
+- Confirm baseline files present in s3://EVIDENCE_BUCKET/monitoring/.
+- Run smoke alerts: simulate drift input and confirm alert triggers and PagerDuty notification.
+- Review Prometheus alert firing frequency and adjust 'for' thresholds to avoid flapping.
+
*** End Patch
*** End Patch
