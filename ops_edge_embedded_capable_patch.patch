*** Begin Patch
*** Add File:ops/edge/convert_model.py
+#!/usr/bin/env python3
+"""
+Convert a PyTorch/Transformers model into edge-ready artifacts (ONNX, TorchScript, optionally TFLite).
+
+Usage:
+  python ops/edge/convert_model.py --model-name-or-path /path/to/pytorch_model --out-dir ./artifacts --onnx-opset 13
+
+Notes:
+ - This script is a pragmatic converter. For production you should test accuracy and latency after conversion,
+   apply quantization, and validate outputs across representative inputs.
+ - TFLite conversion is optional and requires TensorFlow installed. Quantization (dynamic/static) is recommended.
+ - Do NOT commit large model artifacts to git. Store artifacts in an artifact registry (S3/ECR/GCS).
+"""
+import argparse
+import os
+import sys
+import json
+import tempfile
+
+try:
+    import torch
+except Exception:
+    torch = None
+
+def export_onnx(model_path, sample_input_path, out_path, opset=13):
+    if torch is None:
+        raise RuntimeError("PyTorch not installed in this environment")
+    # Load model (expects a torch.load-able module or scripted model)
+    model = torch.load(model_path, map_location="cpu")
+    model.eval()
+    # Load sample input for shape inference
+    sample = torch.load(sample_input_path) if os.path.exists(sample_input_path) else torch.zeros((1,3,224,224))
+    torch.onnx.export(model, sample, out_path, opset_version=opset, do_constant_folding=True)
+    print(f"Exported ONNX to {out_path}")
+
+def export_torchscript(model_path, out_path):
+    if torch is None:
+        raise RuntimeError("PyTorch not installed in this environment")
+    model = torch.load(model_path, map_location="cpu")
+    model.eval()
+    scripted = torch.jit.script(model)
+    scripted.save(out_path)
+    print(f"Saved TorchScript to {out_path}")
+
+def convert_to_tflite(tflite_out, saved_model_dir):
+    # Requires tensorflow to be installed
+    import tensorflow as tf
+    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
+    converter.optimizations = [tf.lite.Optimize.DEFAULT]
+    tflite_model = converter.convert()
+    with open(tflite_out, "wb") as f:
+        f.write(tflite_model)
+    print(f"Wrote TFLite model to {tflite_out}")
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-path", required=True, help="Path to saved PyTorch model")
+    p.add_argument("--out-dir", required=True)
+    p.add_argument("--sample-input", default="")
+    p.add_argument("--onnx-opset", type=int, default=13)
+    p.add_argument("--tflite", action="store_true")
+    args = p.parse_args()
+
+    os.makedirs(args.out_dir, exist_ok=True)
+    onnx_out = os.path.join(args.out_dir, "model.onnx")
+    ts_out = os.path.join(args.out_dir, "model.pt")
+    export_onnx(args.model_path, args.sample_input, onnx_out, opset=args.onnx_opset)
+    export_torchscript(args.model_path, ts_out)
+    if args.tflite:
+        # Convert TorchScript -> SavedModel -> TFLite (requires extra steps; placeholder)
+        sm_dir = os.path.join(args.out_dir, "saved_model")
+        print("Converting TorchScript -> SavedModel (placeholder). Implement export flow for your model.")
+        # After exporting SavedModel, call convert_to_tflite(tflite_out, sm_dir)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/build_and_publish_edge_image.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Build an edge runtime Docker image, bundle model artifacts, and push to registry.
+# Use CI secrets for registry auth; do NOT store credentials in repo.
+#
+# Usage:
+#   export IMAGE_REGISTRY=123456.dkr.ecr.us-east-1.amazonaws.com
+#   export IMAGE_NAME=aegis-edge-inference
+#   ./ops/edge/build_and_publish_edge_image.sh ./artifacts model-tag
+
+ARTIFACT_DIR=${1:-./artifacts}
+TAG=${2:-latest}
+IMAGE_REGISTRY=${IMAGE_REGISTRY:-REPLACE_IMAGE_REGISTRY}
+IMAGE_NAME=${IMAGE_NAME:-aegis-edge-inference}
+
+if [ ! -d "${ARTIFACT_DIR}" ]; then
+  echo "artifact dir ${ARTIFACT_DIR} not found"
+  exit 2
+fi
+
+IMAGE="${IMAGE_REGISTRY}/${IMAGE_NAME}:${TAG}"
+
+echo "[edge-build] Copying artifacts into image context..."
+TMPDIR=$(mktemp -d)
+cp -r "${ARTIFACT_DIR}" "${TMPDIR}/artifacts"
+cp ops/edge/Dockerfile.edge "${TMPDIR}/Dockerfile"
+pushd "${TMPDIR}" >/dev/null
+echo "[edge-build] Building ${IMAGE}"
+docker build -t "${IMAGE}" .
+echo "[edge-build] Pushing ${IMAGE}"
+docker push "${IMAGE}"
+popd >/dev/null
+rm -rf "${TMPDIR}"
+echo "[edge-build] Image pushed: ${IMAGE}"
+echo "Use ops/edge/ota_release.sh to update fleet/gateway to this tag"
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/Dockerfile.edge
+FROM python:3.11-slim
+WORKDIR /app
+COPY . /app
+RUN pip install --no-cache-dir fastapi uvicorn onnxruntime numpy
+# Copy model artifacts (artifacts/ will be added into build context)
+ENV MODEL_PATH=/app/artifacts
+EXPOSE 8080
+CMD ["uvicorn", "ops.edge.edge_runtime:app", "--host", "0.0.0.0", "--port", "8080", "--workers", "1"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/edge_runtime.py
+#!/usr/bin/env python3
+"""
+Edge inference runtime:
+- Serves a minimal ONNX inference endpoint compatible with cloud inference-adapter API.
+- Designed to run on gateways or powerful edge devices (SBCs with GPU).
+"""
+import os
+import numpy as np
+from fastapi import FastAPI, File, UploadFile, HTTPException
+import onnxruntime as ort
+import json
+
+MODEL_DIR = os.environ.get("MODEL_PATH", "/app/artifacts")
+ONNX_MODEL = os.path.join(MODEL_DIR, "model.onnx")
+
+app = FastAPI()
+
+session = None
+if os.path.exists(ONNX_MODEL):
+    session = ort.InferenceSession(ONNX_MODEL)
+
+@app.get("/health")
+def health():
+    return {"ok": True, "model": bool(session)}
+
+@app.post("/v1/embeddings/image")
+async def image_embed(file: UploadFile = File(...)):
+    if session is None:
+        raise HTTPException(status_code=503, detail="model not loaded")
+    data = await file.read()
+    # Placeholder image preprocessing - operator must implement proper transform
+    # For demonstration we assume input is preprocessed numpy float32 array serialized as bytes (not realistic)
+    arr = np.frombuffer(data, dtype=np.float32)
+    arr = arr.reshape(1, *arr.shape)
+    outputs = session.run(None, {"input": arr})
+    return {"data": [{"embedding": outputs[0].tolist()}]}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/edge/gateway-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-edge-gateway
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-edge-gateway
+  template:
+    metadata:
+      labels:
+        app: aegis-edge-gateway
+    spec:
+      serviceAccountName: edge-gateway-sa
+      containers:
+        - name: edge-inference
+          image: REPLACE_IMAGE_REGISTRY/aegis-edge-inference:REPLACE_TAG
+          ports:
+            - containerPort: 8080
+          resources:
+            requests:
+              cpu: "1000m"
+              memory: "2Gi"
+            limits:
+              cpu: "2000m"
+              memory: "8Gi"
+        - name: edge-collector
+          image: REPLACE_IMAGE_REGISTRY/edge-collector:latest
+          env:
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-runtime-secrets
+                  key: EVIDENCE_BUCKET
+      tolerations:
+        - key: "edge"
+          operator: "Exists"
+      nodeSelector:
+        node-role.kubernetes.io/edge: "true"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-edge-gateway
+  namespace: aegis
+spec:
+  selector:
+    app: aegis-edge-gateway
+  ports:
+    - port: 8080
+      targetPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File:argo/edge/edge-gateway-application.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-edge-gateway
+  namespace: argocd
+spec:
+  project: default
+  source:
+    repoURL: 'https://github.com/REPLACE_GITOPS_OWNER/REPLACE_GITOPS_REPO.git'
+    targetRevision: HEAD
+    path: 'overlays/edge-gateway'
+  destination:
+    server: 'https://kubernetes.default.svc'
+    namespace: aegis
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+    syncOptions:
+      - CreateNamespace=true
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/ota_release.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Lightweight OTA release helper:
+#  - Build/push edge image (use ops/edge/build_and_publish_edge_image.sh)
+#  - Update K8s Deployment image for gateway or update edge fleet via API (balena/mender)
+#
+IMAGE_REGISTRY=${IMAGE_REGISTRY:-REPLACE_IMAGE_REGISTRY}
+IMAGE_NAME=${IMAGE_NAME:-aegis-edge-inference}
+TAG=${1:-latest}
+DEPLOYMENT=${DEPLOYMENT:-aegis-edge-gateway}
+NAMESPACE=${NAMESPACE:-aegis}
+
+IMAGE="${IMAGE_REGISTRY}/${IMAGE_NAME}:${TAG}"
+
+echo "[ota] Updating deployment ${DEPLOYMENT} image -> ${IMAGE}"
+kubectl -n "${NAMESPACE}" set image deployment/"${DEPLOYMENT}" "*=${IMAGE}" --record
+echo "[ota] Triggered rolling update for ${DEPLOYMENT}"
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/device_bootstrap.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Device bootstrap script (run on new edge device)
+# - Generates key pair / CSR
+# - POSTs CSR to provisioning endpoint (operator implements server to issue certs)
+# - Stores returned cert and config locally (never commit)
+#
+PROVISION_URL=${PROVISION_URL:-https://provision.example.com/register}
+DEVICE_ID=${DEVICE_ID:-"edge-$(hostname)-$(date +%s)"}
+OUT_DIR=${OUT_DIR:-/etc/aegis-device}
+mkdir -p "${OUT_DIR}"
+
+echo "Generating device keypair..."
+openssl genpkey -algorithm RSA -out "${OUT_DIR}/device.key" -pkeyopt rsa_keygen_bits:2048
+openssl req -new -key "${OUT_DIR}/device.key" -subj "/CN=${DEVICE_ID}" -out "${OUT_DIR}/device.csr"
+
+echo "Uploading CSR to provisioning server (${PROVISION_URL})"
+RESP=$(curl -s -X POST -F "device_id=${DEVICE_ID}" -F "csr=@${OUT_DIR}/device.csr" "${PROVISION_URL}")
+echo "Provision server response: ${RESP}"
+# Expect JSON: {"cert":"-----BEGIN CERT...","bootstrap_config":{...}}
+echo "${RESP}" > "${OUT_DIR}/provision_response.json"
+echo "Bootstrap artifacts written to ${OUT_DIR}. Securely store device key and cert."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/edge_release.yml
+name: Edge Release (build/publish + Argo sync)
+
+on:
+  workflow_dispatch:
+    inputs:
+      tag:
+        description: "Image tag to publish"
+        required: true
+        default: "latest"
+
+jobs:
+  build-and-push:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build and push edge image
+        env:
+          IMAGE_REGISTRY: ${{ secrets.IMAGE_REGISTRY }}
+          IMAGE_NAME: aegis-edge-inference
+          TAG: ${{ github.event.inputs.tag }}
+        run: |
+          ./ops/edge/build_and_publish_edge_image.sh ./artifacts "${{ github.event.inputs.tag }}"
+  argocd-sync:
+    needs: build-and-push
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Update ArgoCD application (if using image tags in manifests)
+        env:
+          ARGOCD_SERVER: ${{ secrets.ARGOCD_SERVER }}
+          ARGOCD_TOKEN: ${{ secrets.ARGOCD_TOKEN }}
+        run: |
+          # Option A: use argocd CLI to sync application (requires ARGOCD_TOKEN)
+          # argocd app sync aegis-edge-gateway --server $ARGOCD_SERVER --auth-token $ARGOCD_TOKEN
+          echo "Trigger ArgoCD sync for aegis-edge-gateway (implement CLI or API call)"
+
*** End Patch
*** Begin Patch
*** Add File:docs/edge_hybrid_runbook.md
+# Edge‑Capable Hybrid Aegis — Runbook
+
+Goal: Keep Aegis cloud/cluster-centric while enabling embedded/edge deployments via a gateway pattern and OTA pipeline.
+
+Key pieces added
+- Model conversion: ops/edge/convert_model.py (ONNX/TorchScript conversion; TFLite placeholder)
+- Edge runtime: ops/edge/edge_runtime.py + Dockerfile.edge, build helper and OTA release helper
+- Gateway deployment manifest: k8s/edge/gateway-deployment.yaml and ArgoCD Application overlay
+- Device bootstrap: ops/edge/device_bootstrap.sh (CSR provisioning pattern)
+- OTA: ops/edge/build_and_publish_edge_image.sh + ops/edge/ota_release.sh + GitHub Actions workflow .github/workflows/edge_release.yml
+- Edge collector / agent integration (edge-collector and edge agent were added earlier)
+
+Operator checklist (step-by-step)
+1) Prepare model artifacts for edge
+   - Run conversion locally in a controlled environment:
+     python ops/edge/convert_model.py --model-path /path/to/pytorch.pt --out-dir ./artifacts
+   - Validate model outputs / numeric parity on representative inputs.
+
+2) Build and publish edge image
+   - Ensure IMAGE_REGISTRY credentials are available on CI or locally (do not commit secrets)
+   - ./ops/edge/build_and_publish_edge_image.sh ./artifacts <tag>
+
+3) Deploy gateway in cluster
+   - Fill replacements in k8s/edge/gateway-deployment.yaml (image registry & tag)
+   - kubectl apply -f k8s/edge/gateway-deployment.yaml
+   - Or use ArgoCD: apply argo/edge/edge-gateway-application.yaml (fill repo & path placeholders) and let ArgoCD reconcile
+
+4) Bootstrap devices
+   - Run ops/edge/device_bootstrap.sh on new device after operator implements provisioning server
+   - Provisioning server should sign CSR and issue cert + initial manifest (store cert in device secure storage)
+
+5) OTA updates
+   - After image push, call ops/edge/ota_release.sh <tag> or use ArgoCD to sync overlay and trigger rolling update
+   - For constrained fleets use balena/Mender: implement balena/mender deployment steps in ops/edge/ota_release.sh
+
+6) Monitoring & fallback
+   - Edge gateway exposes /health endpoint; configure Prometheus scrape and alert on failures
+   - Implement fallback: if gateway unavailable, devices buffer telemetry and retry upload (edge agent supports this)
+
+Security notes
+- Use device identity (certs or TPM) and mutual TLS between device and gateway/collector.
+- Do not store private keys on the image; generate keys on device and protect with TPM or OS keystore.
+- Use ExternalSecrets for all registry keys and provider keys (already configured in earlier patches).
+
+Testing & validation
+- Run latency and accuracy regression tests for converted model before deploying to fleet.
+- Test rollback by deploying a known-bad image and ensure ota_release triggers a rollback to previous image tag.
+- Simulate network partitions and validate buffering/backfill behaviour.
+
*** End Patch
*** End Patch
