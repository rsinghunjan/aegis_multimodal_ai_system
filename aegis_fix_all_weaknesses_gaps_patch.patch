*** Begin Patch
*** Add File:deploy/rekor/postgres-statefulset.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: rekor-postgres-secret
+  namespace: aegis
+type: Opaque
+stringData:
+  postgres-user: rekor
+  postgres-password: "REPLACE_WITH_STRONG_PASSWORD"
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  serviceName: rekor-postgres
+  replicas: 1
+  selector:
+    matchLabels:
+      app: rekor-postgres
+  template:
+    metadata:
+      labels:
+        app: rekor-postgres
+    spec:
+      containers:
+        - name: postgres
+          image: postgres:15
+          env:
+            - name: POSTGRES_USER
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-user
+            - name: POSTGRES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-password
+          ports:
+            - containerPort: 5432
+          volumeMounts:
+            - name: pgdata
+              mountPath: /var/lib/postgresql/data
+  volumeClaimTemplates:
+    - metadata:
+        name: pgdata
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 10Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  ports:
+    - port: 5432
+      targetPort: 5432
+  selector:
+    app: rekor-postgres
+
+#
+# Notes:
+# - Replace the postgres-password secret value before applying.
+# - Consider running Postgres in HA (Patroni) for production; this is a minimal durable deployment using PVC.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/rekor-deployment-postgres.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          env:
+            - name: REKOR_STORAGE_TYPE
+              value: "postgres"
+            - name: REKOR_DATABASE_URL
+              value: "postgres://$(REKOR_USER):$(REKOR_PASSWORD)@rekor-postgres.aegis.svc:5432/rekordb"
+            - name: REKOR_API_PORT
+              value: "3000"
+          ports:
+            - containerPort: 3000
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  selector:
+    app: rekor
+  ports:
+    - name: http
+      port: 3000
+      targetPort: 3000
+
+#
+# Notes:
+# - Ensure a 'rekordb' database exists; provide REKOR_USER / REKOR_PASSWORD via Kubernetes Secret or envFrom.
+# - Add readiness/liveness probes and TLS termination via an ingress or LB in front of this service.
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: signing-proxy-config
+  namespace: aegis
+data:
+  proxy.conf: |
+    # Signing-proxy configuration (scaffold)
+    # - require_client_cert: whether mTLS client cert required
+    require_client_cert: "true"
+    # - vault_addr: endpoint to call to request ephemeral signing tokens
+    vault_addr: "https://vault.aegis.svc:8200"
+    # - rekor_url: where to push transparency entries after signing
+    rekor_url: "https://rekor.aegis.svc:3000"
+    # - audit_bucket: evidence bucket to upload audit trails
+    audit_bucket: "${EVIDENCE_BUCKET}"
+    # - allowed_client_cn_prefix: only client cert CNs with this prefix are permitted (tune for your org)
+    allowed_client_cn_prefix: "cn=ci-runner-"
+    # - signing_command: placeholder indicating signing backend (hsm|cosign-kms)
+    signing_backend: "cosign-kms"
+
+#
+# Implementation: the signing-proxy container should validate client cert CN, map to Vault role
+# and request an ephemeral auth/signing operation. All requests and responses must be logged and
+# uploaded to audit_bucket and mirrored to Rekor.
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/service-and-networkpolicy.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  selector:
+    app: signing-proxy
+  ports:
+    - name: https
+      port: 8443
+      targetPort: 8443
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: signing-proxy-restrict
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: signing-proxy
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: cicd
+        - podSelector:
+            matchLabels:
+              app: gitlab-runner
+      ports:
+        - protocol: TCP
+          port: 8443
+  policyTypes:
+    - Ingress
+
+#
+# Notes: restrict ingress to CI/CD namespaces and known runners; update selectors to match your environment.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ci/signing_proxy_client.py
+#!/usr/bin/env python3
+"""
+Small cosign-signing proxy client using mTLS to request a signature from signing-proxy.
+This client uses a local client cert and key (provided via CI secrets) to authenticate.
+The signing-proxy is expected to return a cosign-compatible signature payload which the client
+will then (optionally) push to Rekor or attach to the artifact.
+"""
+import argparse, requests, json, sys
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--proxy", required=True, help="https://signing-proxy:8443")
+    p.add_argument("--artifact", required=True, help="image:tag or path to manifest")
+    p.add_argument("--client-cert", default="/tmp/client.crt")
+    p.add_argument("--client-key", default="/tmp/client.key")
+    p.add_argument("--ca-cert", default="/tmp/ca.crt")
+    args = p.parse_args()
+    url = args.proxy.rstrip("/") + "/sign"
+    payload = {"artifact": args.artifact}
+    resp = requests.post(url, json=payload, cert=(args.client_cert, args.client_key), verify=args.ca_cert, timeout=60)
+    if resp.status_code != 200:
+        print("Error from signing-proxy:", resp.status_code, resp.text)
+        sys.exit(2)
+    print("Signing response:", resp.text)
+    # Write response to stdout / file for CI to attach
+    open("/tmp/sign_res.json","w").write(resp.text)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/enable-audit-to-s3-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: vault-audit-upload
+  namespace: aegis
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: vault
+          containers:
+            - name: upload
+              image: amazon/aws-cli:latest
+              env:
+                - name: EVIDENCE_BUCKET
+                  value: "${EVIDENCE_BUCKET}"
+              command:
+                - sh
+                - -c
+                - |
+                  set -e
+                  # This assumes Vault is configured to write audit logs to /vault/logs/ ; operator must mount that path
+                  if [ -d /vault/logs ]; then
+                    aws s3 cp --recursive /vault/logs s3://${EVIDENCE_BUCKET}/vault-audit/ || true
+                    find /vault/logs -type f -mtime +7 -delete || true
+                  else
+                    echo "No audit logs found"
+                  fi
+              volumeMounts:
+                - name: vault-audit
+                  mountPath: /vault/logs
+          restartPolicy: OnFailure
+          volumes:
+            - name: vault-audit
+              emptyDir: {}
+
+#
+# Notes: Prefer configuring Vault audit device (s3, socket) natively; this CronJob is a fallback uploader for audit files.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/vault-autounseal-helper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper to patch Helm values with KMS ARN for auto-unseal and install Vault via Helm in HA mode (raft)
+KMS_ARN=${1:-}
+if [ -z "$KMS_ARN" ]; then
+  echo "Usage: $0 <kms-key-arn>"
+  exit 2
+fi
+cat > /tmp/vault_values_override.yaml <<EOF
+unsealer:
+  aws:
+    enabled: true
+    region: us-west-2
+    kmsKeyId: "${KMS_ARN}"
+s3:
+  enabled: false
+server:
+  raft:
+    enabled: true
+tls:
+  enabled: true
+EOF
+
+echo "Installing/upgrading Vault Helm with KMS auto-unseal (override file: /tmp/vault_values_override.yaml)"
+helm upgrade --install vault hashicorp/vault -n aegis -f deploy/vault/ha-values-raft.yaml -f /tmp/vault_values_override.yaml
+echo "Vault helm applied; run deploy/vault/init-vault-job.yaml to initialize if needed."
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/automation/collect_false_positives.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS=${1:-aegis-staging}
+OUT=/tmp/gatekeeper_dryrun_violations_$(date +%s).json
+echo "Collecting Gatekeeper audit reports for namespace ${NS}"
+kubectl get audit -n gatekeeper-system -o json > /tmp/gatekeeper_audit_all.json || true
+kubectl get constraint -A -o json > /tmp/gatekeeper_constraints.json || true
+# This is a simple collector; in production query Gatekeeper Audit API / OPA reporting endpoints
+echo "Saved raw Gatekeeper audit and constraint outputs to /tmp"
+echo "Tip: analyze /tmp/gatekeeper_audit_all.json to detect expected false positives and tune rules"
+echo "Output: ${OUT}"
+
*** End Patch
*** Begin Patch
*** Add File:fal](#)
î€€
