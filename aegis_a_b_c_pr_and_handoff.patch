*** Begin Patch
*** Add File:issues/PRIORITIZED_ISSUES_BACKLOG_FOR_PR.md
+# Prioritized Issues Backlog (ready to create GitHub Issues / import)
+
+Below are discrete issues (title, description, estimate days, priority, suggested assignee). Use this to triage and assign.
+
+- Title: "Confirm telemetry collection & baseline health"
+  Description: |
+    Ensure node-power-exporter DaemonSet running on representative node types.
+    Run `telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 7` and upload the report.
+    Deliverable: S3 report s3://<EVIDENCE_BUCKET>/telemetry/health/ and Prometheus targets healthy.
+  Estimate: 2
+  Priority: high
+  Assignee: @platform
+
+- Title: "Run iterative calibration & CI gating (staging)"
+  Description: |
+    Execute the calibration/validation GitHub Action in staging.
+    Inspect calibration drafts in s3://<EVIDENCE_BUCKET>/calibration/drafts/ and validation report under calibration/validation_reports/.
+    If median_relative_error > 0.10, iterate calibration runs and collect telemetry for longer.
+  Estimate: 3
+  Priority: high
+  Assignee: @ml-platform
+
+- Title: "Stage scheduler extender + Argo admission enforcer; run enforcement harness"
+  Description: |
+    Deploy extender and enforcer manifests to staging, ensure certs via cert-manager corporate ClusterIssuer.
+    Run enforcement/test_harness/enforcement_coverage_harness.py and iterate policy until enforcement_coverage ≥ 0.95.
+    Deliverable: S3 report s3://<EVIDENCE_BUCKET>/enforcement/coverage_reports/
+  Estimate: 7
+  Priority: high
+  Assignee: @k8s-sre
+
+- Title: "Run parquet → manifest → dev DW bulk-load and validate MERGE/upsert"
+  Description: |
+    Run etl/s3_parquet_pipeline_v3.py for a sample prefix, generate manifest, and run loader job (etl/redshift_loader_wrapper.sh) in dev.
+    Validate MERGE/upsert logic (etl/redshift_merge_from_staging.sql) and idempotency (re-run without duplicates).
+    Deliverable: success logs + manifest in S3 + confirmation of idempotent load.
+  Estimate: 5
+  Priority: high
+  Assignee: @data-eng
+
+- Title: "Provision KMS key & corporate PKI for webhooks; rotate cosign key"
+  Description: |
+    Provision KMS key via Terraform (security/kms_kms_policy.tf), create cert-manager ClusterIssuer secret corporate-ca-keypair, rotate cosign (security/cosign_rotate.sh).
+    Deliverable: KMS key ARN in secrets, cert-manager issuer active, rotation validated in staging.
+  Estimate: 7
+  Priority: high
+  Assignee: @security
+
+- Title: "Operationalize DR/resume drills & incident playbooks"
+  Description: |
+    Run operational/dr_resume_test.py and other runbook drills; record results to evidence S3; remediate failures.
+    Deliverable: drill report in s3://<EVIDENCE_BUCKET>/dr_drills/
+  Estimate: 3
+  Priority: medium
+  Assignee: @sre
+
+---
+How to import:
+- Copy each issue into repo Issues; include links to runbooks and script files referenced.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/enforcement_and_dataloader.yml
+name: Enforcement Harness + Parquet → Manifest → Dev DW Loader
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 4 * * 1' # weekly Monday 04:00 UTC
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+  KUBECONFIG: ${{ secrets.KUBECONFIG }}
+  PUSHGATEWAY: ${{ secrets.PUSHGATEWAY }}
+  REDSHIFT_DSN: ${{ secrets.REDSHIFT_DSN }}
+  REDSHIFT_IAM_ROLE_ARN: ${{ secrets.REDSHIFT_IAM_ROLE_ARN }}
+
+jobs:
+  enforcement-and-loader:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+
+      - name: Install deps
+        run: pip install boto3 kubernetes requests pandas pyarrow
+
+      - name: Configure kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+
+      - name: Run enforcement coverage harness (staging)
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          PUSHGATEWAY: ${{ secrets.PUSHGATEWAY }}
+        run: |
+          python3 enforcement/test_harness/enforcement_coverage_harness.py || true
+
+      - name: Run parquet pipeline (sample) and generate manifest
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          python3 etl/s3_parquet_pipeline_v3.py || true
+          python3 etl/copy_manifest_generator.py
+
+      - name: Trigger Redshift COPY Job in cluster (dev)
+        env:
+          MANIFEST_S3_URI: "s3://${{ secrets.EVIDENCE_BUCKET }}/parquet/evidence/manifest.json"
+          REDSHIFT_DSN: ${{ secrets.REDSHIFT_DSN }}
+          REDSHIFT_IAM_ROLE: ${{ secrets.REDSHIFT_IAM_ROLE_ARN }}
+        run: |
+          # Create a Kubernetes Job that runs the loader wrapper in cluster (use existing etl/redshift_copy_job.yaml)
+          kubectl -n aegis apply -f etl/redshift_copy_job.yaml
+          # wait for completion (job name in manifest)
+          kubectl -n aegis wait --for=condition=complete job/redshift-copy-loader --timeout=1800s || true
+
+      - name: Run dbt on dev (optional)
+        env:
+          DBT_PROFILES_DIR: dbt/profiles
+        run: |
+          # this assumes dbt profiles prepared in secrets or mounted; best-effort step
+          if [ -d "dbt" ]; then
+            cd dbt
+            dbt deps || true
+            dbt run || true
+          fi
+
+      - name: Upload summary artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: enforcement-and-loader-summary
+          path: |
+            enforcement/test_harness/*.json
+            /tmp/copy_manifest.json || true
+
*** End Patch
*** Begin Patch
*** Add File:.github/prs/create_pr.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper script to create a branch, commit current changes, push and open a PR using `gh` CLI.
+#
+# Usage:
+#   ./create_pr.sh <repo_owner/repo> <base_branch> "<pr_title>" "<pr_body>"
+#
+REPO=${1:-}
+BASE=${2:-main}
+TITLE=${3:-"Aegis: wire enforcement + loader + operator handoff"}
+BODY=${4:-"This PR wires enforcement harness, dev DW loader workflow, issues backlog and operator handoff."}
+
+if [ -z "$REPO" ]; then
+  echo "Usage: $0 <repo_owner/repo> <base_branch> \"<pr_title>\" \"<pr_body>\""
+  exit 2
+fi
+
+BRANCH="aegis/optimize/enforcement-loader-$(date -u +%Y%m%dT%H%M%SZ)"
+
+git checkout -b "$BRANCH"
+git add .
+git commit -m "$TITLE"
+git push -u origin "$BRANCH"
+
+if command -v gh >/dev/null 2>&1; then
+  gh pr create --repo "$REPO" --base "$BASE" --head "$BRANCH" --title "$TITLE" --body "$BODY"
+else
+  echo "gh CLI not found. Branch pushed to origin/$BRANCH. Create PR manually at https://github.com/$REPO/compare/$BRANCH?expand=1"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/OPERATOR_HANDOFF_COMMANDS.md
+# Operator Handoff — exact commands to run enforcement harness, telemetry/calibration and dev DW loader
+
+Replace placeholders (EVIDENCE_BUCKET, cluster contexts, secrets) with your values.
+
+Prereqs:
+- kubectl configured for staging/dev cluster
+- AWS CLI configured with access to EVIDENCE_BUCKET
+- GitHub Actions secrets set for workflows (EVIDENCE_BUCKET, KUBECONFIG, AWS creds, REDSHIFT_*)
+- `psql` and Redshift network access for dev loader step (or run loader inside cluster)
+
+1) Telemetry healthcheck (run locally or CI runner)
+ - export EVIDENCE_BUCKET=aegis-evidence
+ - export AWS_REGION=us-west-2
+ - python3 telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 7
+ - Check S3: aws s3 ls s3://$EVIDENCE_BUCKET/telemetry/health/
+
+2) Trigger iterative calibration in staging (one-off)
+ - Ensure kubeconfig points to staging:
+   kubectl config current-context
+ - Create a one-off job from CronJob:
+   kubectl -n aegis create job --from=cronjob/aegis-iterative-calibration telemetry-calib-$(date -u +%s)
+ - Wait for completion:
+   kubectl -n aegis wait --for=condition=complete job/telemetry-calib-<ts> --timeout=1800s
+ - Inspect S3:
+   aws s3 ls s3://$EVIDENCE_BUCKET/calibration/
+   aws s3 cp s3://$EVIDENCE_BUCKET/calibration/validation_reports/<file> .
+
+3) Run enforcement coverage harness (staging)
+ - export EVIDENCE_BUCKET=aegis-evidence
+ - export AWS_REGION=us-west-2
+ - python3 enforcement/test_harness/enforcement_coverage_harness.py
+ - Check S3 for coverage report:
+   aws s3 ls s3://$EVIDENCE_BUCKET/enforcement/coverage_reports/
+ - If coverage < 0.95, check enforcer/extender logs:
+   kubectl -n aegis logs -l app=aegis-argo-enforcer
+   kubectl -n kube-system logs -l app=aegis-scheduler-extender
+
+4) Run parquet pipeline locally (sample) and generate manifest
+ - python3 etl/s3_parquet_pipeline_v3.py
+ - python3 etl/copy_manifest_generator.py
+ - The manifest will be at /tmp/copy_manifest.json (or printed)
+
+5) Run dev Redshift loader in cluster (safer)
+ - Ensure etl/redshift_copy_job.yaml is configured with correct env vars (REDSHIFT_IAM_ROLE_ARN, EVIDENCE_BUCKET)
+ - Apply job:
+   kubectl -n aegis apply -f etl/redshift_copy_job.yaml
+ - Wait:
+   kubectl -n aegis wait --for=condition=complete job/redshift-copy-loader --timeout=1800s
+ - Inspect job logs:
+   kubectl -n aegis logs job/redshift-copy-loader
+ - Inspect Redshift staging and verify MERGE/upsert output (check raw.raw_job_runs)
+
+6) Run dbt in dev (if profiles configured)
+ - cd dbt
+ - dbt deps
+ - dbt run --profiles-dir ./profiles
+ - dbt test --profiles-dir ./profiles
+
+7) Validate GE expectations (in cluster or locally)
+ - python3 data_quality/run_expectations.py
+ - Check S3 validations: aws s3 ls s3://$EVIDENCE_BUCKET/data-quality/validations/
+
+8) If validation passes and operator approves promotion
+ - Download approved power_profiles.yaml and sign:
+   cosign sign --key "awskms://$COSIGN_KMS_KEY_ARN" ./power_profiles.yaml
+ - Apply to production cluster (manual):
+   kubectl -n aegis create configmap aegis-power-profiles --from-file=power_profiles.yaml=./power_profiles.yaml --dry-run=client -o yaml | kubectl apply -f -
+ - Record promotion in evidence S3 (upload promotion note) and create Rekor signature via cosign.
+
+Notes:
+- All steps are intentionally manual for auditability. Use CI workflows where available to automate safe parts (we added enforcement_and_dataloader.yml).
+
*** End Patch
*** Begin Patch
*** Add File:.github/prs/PR_DESCRIPTION.md
+Title: Wire enforcement harness, dev DW loader workflow, prioritized issues, operator handoff
+
+Summary:
+- Adds a GitHub Action that runs enforcement coverage harness and a sample parquet->manifest->dev-DW loader flow.
+- Adds a prioritized issues backlog for final optimization tasks.
+- Adds an operator handoff with exact commands to run telemetry healthcheck, calibration, enforcement harness, parquet pipeline and dev loader.
+- Provides a helper script to create a PR using the gh CLI.
+
+Notes:
+- This PR is intended to be reviewed and deployed to staging first. All secrets and cluster contexts must be configured before running workflows.
+
*** End Patch
*** End Patch
