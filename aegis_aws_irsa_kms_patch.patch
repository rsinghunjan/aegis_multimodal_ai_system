*** Begin Patch
*** Add File: scripts/irsa_create_role_and_bind.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# irsa_create_role_and_bind.sh
+# Create an IAM Role for Service Accounts (IRSA) trust relationship for an EKS cluster,
+# attach policies, annotate the Kubernetes ServiceAccount and verify the binding.
+#
+# Usage:
+#  ./scripts/irsa_create_role_and_bind.sh \
+#    --cluster my-eks-cluster --region us-west-2 --account 123456789012 \
+#    --namespace aegis --ksa aegis-operator --role-name aegis-operator-irsa \
+#    --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
+#
+# Notes:
+# - Requires AWS CLI v2 configured and kubectl pointed at the target cluster (or KUBE_CONFIG_DATA env).
+# - This script builds a trust policy for the cluster OIDC provider and the specific KSA.
+# - It is conservative: if the role already exists it will not overwrite it unless --force is provided.
+
+usage() {
+  cat <<EOF
+Usage: $0 --cluster CLUSTER --region REGION --account AWS_ACCOUNT \
+          --namespace K8S_NAMESPACE --ksa KSA_NAME --role-name ROLE_NAME \
+          [--policy-arn POLICY_ARN] [--attach-inline POLICY_JSON] [--force] [--dry-run]
+
+Example:
+  ./scripts/irsa_create_role_and_bind.sh --cluster eks-prod --region us-west-2 --account 123456789012 \\
+    --namespace aegis --ksa aegis-operator --role-name aegis-operator-irsa \\
+    --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
+EOF
+}
+
+CLUSTER=""
+REGION=""
+ACCOUNT=""
+NAMESPACE=""
+KSA=""
+ROLE=""
+POLICY_ARN=""
+INLINE_POLICY_JSON=""
+FORCE="false"
+DRY_RUN="false"
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --cluster) CLUSTER="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --account) ACCOUNT="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --ksa) KSA="$2"; shift 2;;
+    --role-name) ROLE="$2"; shift 2;;
+    --policy-arn) POLICY_ARN="$2"; shift 2;;
+    --attach-inline) INLINE_POLICY_JSON="$2"; shift 2;;
+    --force) FORCE="true"; shift;;
+    --dry-run) DRY_RUN="true"; shift;;
+    -h|--help) usage; exit 0;;
+    *) echo "Unknown arg: $1"; usage; exit 2;;
+  esac
+done
+
+if [ -z "$CLUSTER" ] || [ -z "$REGION" ] || [ -z "$ACCOUNT" ] || [ -z "$NAMESPACE" ] || [ -z "$KSA" ] || [ -z "$ROLE" ]; then
+  usage
+  exit 2
+fi
+
+set -x
+
+# Resolve OIDC issuer for cluster
+OIDC_ISSUER=$(aws eks describe-cluster --name "$CLUSTER" --region "$REGION" --query "cluster.identity.oidc.issuer" --output text)
+if [ -z "$OIDC_ISSUER" ] || [ "$OIDC_ISSUER" = "None" ]; then
+  echo "Unable to determine OIDC issuer for cluster $CLUSTER in $REGION"
+  exit 2
+fi
+
+# Trim https:// when building provider host
+OIDC_HOST=${OIDC_ISSUER#https://}
+OIDC_PROVIDER_ARN="arn:aws:iam::${ACCOUNT}:oidc-provider/${OIDC_HOST}"
+
+TRUST_POLICY_FILE="$(mktemp)"
+cat > "$TRUST_POLICY_FILE" <<EOF
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Principal": {
+        "Federated": "${OIDC_PROVIDER_ARN}"
+      },
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Condition": {
+        "StringEquals": {
+          "${OIDC_HOST}:sub": "system:serviceaccount:${NAMESPACE}:${KSA}"
+        }
+      }
+    }
+  ]
+}
+EOF
+
+echo "OIDC provider: ${OIDC_PROVIDER_ARN}"
+echo "Trust policy written to ${TRUST_POLICY_FILE}"
+
+if [ "$DRY_RUN" = "true" ]; then
+  echo "DRY RUN: would create role ${ROLE} with trust policy and attach policy ${POLICY_ARN}"
+  cat "${TRUST_POLICY_FILE}"
+  exit 0
+fi
+
+# Create role if missing
+if aws iam get-role --role-name "$ROLE" >/dev/null 2>&1; then
+  if [ "$FORCE" = "true" ]; then
+    echo "Role $ROLE exists and --force specified; deleting and recreating (use with caution)"
+    aws iam delete-role --role-name "$ROLE" || true
+  else
+    echo "Role $ROLE already exists. Exiting (use --force to recreate)."
+    exit 1
+  fi
+fi
+
+echo "Creating role ${ROLE}..."
+aws iam create-role --role-name "$ROLE" --assume-role-policy-document file://"${TRUST_POLICY_FILE}"
+
+if [ -n "$POLICY_ARN" ]; then
+  echo "Attaching managed policy ${POLICY_ARN} to ${ROLE}"
+  aws iam attach-role-policy --role-name "$ROLE" --policy-arn "$POLICY_ARN"
+fi
+
+if [ -n "$INLINE_POLICY_JSON" ]; then
+  echo "Putting inline policy into ${ROLE}"
+  aws iam put-role-policy --role-name "$ROLE" --policy-name "${ROLE}-inline" --policy-document file://"${INLINE_POLICY_JSON}"
+fi
+
+echo "Annotating KSA ${NAMESPACE}/${KSA} with iam.amazonaws.com/role-arn"
+ROLE_ARN=$(aws iam get-role --role-name "$ROLE" --query "Role.Arn" --output text)
+kubectl -n "$NAMESPACE" annotate serviceaccount "$KSA" "eks.amazonaws.com/role-arn=${ROLE_ARN}" --overwrite
+
+echo "Verifying pod token projected credentials"
+POD_NAME="irsa-verifier-$(date +%s)"
+kubectl -n "$NAMESPACE" run "$POD_NAME" --restart=Never --rm -i --serviceaccount="$KSA" --image=amazon/aws-cli --command -- /bin/sh -c "aws sts get-caller-identity --output json || true"
+
+echo "IRSA role creation and KSA binding complete. Role ARN: $ROLE_ARN"
+rm -f "$TRUST_POLICY_FILE"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/verify_irsa_s3_access.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# verify_irsa_s3_access.sh
+# Launch a short-lived pod that uses the supplied KSA and attempts to run:
+#   aws sts get-caller-identity
+#   aws s3 ls s3://BUCKET
+#
+# Usage:
+#  ./scripts/verify_irsa_s3_access.sh --namespace N --ksa KSA --bucket my-bucket --timeout 300
+
+NAMESPACE=""
+KSA=""
+BUCKET=""
+TIMEOUT=300
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --ksa) KSA="$2"; shift 2;;
+    --bucket) BUCKET="$2"; shift 2;;
+    --timeout) TIMEOUT="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --namespace N --ksa NAME --bucket BUCKET"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$NAMESPACE" ] || [ -z "$KSA" ] || [ -z "$BUCKET" ]; then
+  echo "Missing required args"
+  exit 2
+fi
+
+POD_NAME="irsa-s3-verify-$(date +%s)"
+cat <<EOF | kubectl -n "$NAMESPACE" apply -f -
+apiVersion: v1
+kind: Pod
+metadata:
+  name: ${POD_NAME}
+spec:
+  serviceAccountName: ${KSA}
+  containers:
+    - name: aws
+      image: amazon/aws-cli
+      command: ["/bin/sh","-c"]
+      args:
+        - aws sts get-caller-identity --output json > /tmp/caller || true
+          ; echo "CALLER:" && cat /tmp/caller || true
+          ; echo "Listing bucket ${BUCKET}" ; aws s3 ls s3://${BUCKET} || echo "s3 ls failed"
+          ; sleep 300
+  restartPolicy: Never
+EOF
+
+kubectl -n "$NAMESPACE" wait --for=condition=Ready pod/"$POD_NAME" --timeout=${TIMEOUT}s || true
+echo "Fetching logs (tail 200):"
+kubectl -n "$NAMESPACE" logs "$POD_NAME" --tail=200 || true
+kubectl -n "$NAMESPACE" delete pod "$POD_NAME" --ignore-not-found
+
+echo "IRSA S3 access verification completed. Check logs above for 'CALLER' JSON and bucket listing."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/irsa_bind_and_verify.yml
+name: IRSA Create Role & Verify (EKS)
+
+on:
+  workflow_dispatch:
+    inputs:
+      cluster:
+        description: "EKS cluster name"
+        required: true
+      region:
+        description: "AWS region"
+        required: true
+      account:
+        description: "AWS account id"
+        required: true
+      namespace:
+        description: "Kubernetes namespace with the KSA"
+        required: true
+      ksa:
+        description: "Kubernetes ServiceAccount name to bind"
+        required: true
+      role_name:
+        description: "IAM Role name to create"
+        required: true
+      policy_arn:
+        description: "Managed policy to attach (optional)"
+        required: false
+      s3_bucket:
+        description: "S3 bucket to verify access (optional)"
+        required: false
+
+permissions:
+  contents: read
+
+jobs:
+  bind-verify:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Configure AWS credentials
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          aws-region: ${{ github.event.inputs.region }}
+
+      - name: Configure kubectl (optional KUBE_CONFIG_DATA)
+        run: |
+          if [ -n "${{ secrets.KUBE_CONFIG_DATA }}" ]; then
+            echo "${{ secrets.KUBE_CONFIG_DATA }}" | base64 --decode > /tmp/kubeconfig
+            mkdir -p $HOME/.kube
+            mv /tmp/kubeconfig $HOME/.kube/config
+          fi
+
+      - name: Create IAM role and bind to KSA
+        run: |
+          chmod +x scripts/irsa_create_role_and_bind.sh scripts/verify_irsa_s3_access.sh
+          ./scripts/irsa_create_role_and_bind.sh \
+            --cluster "${{ github.event.inputs.cluster }}" \
+            --region "${{ github.event.inputs.region }}" \
+            --account "${{ github.event.inputs.account }}" \
+            --namespace "${{ github.event.inputs.namespace }}" \
+            --ksa "${{ github.event.inputs.ksa }}" \
+            --role-name "${{ github.event.inputs.role_name }}" \
+            $( [ -n "${{ github.event.inputs.policy_arn }}" ] && echo --policy-arn "${{ github.event.inputs.policy_arn }}" || true )
+
+      - name: Verify IRSA pod can access S3
+        if: ${{ github.event.inputs.s3_bucket != '' }}
+        run: |
+          ./scripts/verify_irsa_s3_access.sh --namespace "${{ github.event.inputs.namespace }}" --ksa "${{ github.event.inputs.ksa }}" --bucket "${{ github.event.inputs.s3_bucket }}"
+
*** End Patch
*** Begin Patch
*** Add File: infra/overrides/aws/milvus-values-eks.yaml
+# Example Milvus values for EKS with S3 backup + KMS integration
+replicaCount: 3
+persistence:
+  enabled: true
+  storageClass: "gp2"   # Replace with your EKS storage class (gp2, gp3, ebs-sc etc.)
+  size: 200Gi
+tls:
+  enabled: true
+  secretName: milvus-tls-secret-eks
+auth:
+  enabled: true
+
+# Backup configuration using S3 and AWS KMS encryption of objects
+backup:
+  enabled: true
+  provider: s3
+  s3:
+    bucket: "your-milvus-backup-bucket"
+    region: "us-west-2"
+    endpoint: "" # set for custom endpoints
+  encryption:
+    kms:
+      # KMS key ARN or alias, e.g. arn:aws:kms:us-west-2:123456789012:key/abcd-...
+      keyId: "arn:aws:kms:us-west-2:123456789012:key/REPLACE_ME"
+
+# Optionally reference IRSA role via annotation on ServiceAccount used by backup jobs
+serviceAccount:
+  create: false
+  name: milvus-backup-sa
+
*** End Patch
*** Begin Patch
*** Add File: scripts/validate_milvus_aws_values.sh
+#!/usr/bin/env bash
+set -euo pipefail
+VAL_FILE="infra/overrides/aws/milvus-values-eks.yaml"
+
+if [ ! -f "$VAL_FILE" ]; then
+  echo "Values file not found: $VAL_FILE"
+  exit 2
+fi
+
+echo "Validating Milvus EKS values..."
+grep -nE "storageClass|s3:|bucket:|kms:|serviceAccount" "$VAL_FILE" || true
+echo "Please replace placeholder values (bucket, keyId, storageClass) before applying to production."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci_validate_irsa_s3.yml
+name: CI - Validate IRSA & S3 access on PR / Dispatch
+
+on:
+  workflow_dispatch:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+permissions:
+  contents: read
+
+jobs
î€€
