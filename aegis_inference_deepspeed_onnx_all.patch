*** Begin Patch
*** Add File:argo/inference_load_test.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: inference-load-test-
+  namespace: aegis
+spec:
+  entrypoint: load-test
+  arguments:
+    parameters:
+      - name: target_url
+        value: "http://model-serving.aegis.svc/predict"
+      - name: duration
+        value: "60s"
+      - name: rate
+        value: "200"
+  templates:
+    - name: load-test
+      inputs:
+        parameters:
+          - name: target_url
+          - name: duration
+          - name: rate
+      dag:
+        tasks:
+          - name: vegeta-attack
+            template: vegeta-attack
+            arguments:
+              parameters:
+                - name: target_url
+                  value: "{{inputs.parameters.target_url}}"
+                - name: duration
+                  value: "{{inputs.parameters.duration}}"
+                - name: rate
+                  value: "{{inputs.parameters.rate}}"
+          - name: parse-and-log
+            template: parse-and-log
+            dependencies: [vegeta-attack]
+            arguments:
+              parameters:
+                - name: report-path
+                  value: "/tmp/vegeta_report.json"
+
+    - name: vegeta-attack
+      inputs:
+        parameters:
+          - name: target-url
+          - name: duration
+          - name: rate
+      container:
+        image: quay.io/tsenart/vegeta:12.8.4
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            echo "GET {{inputs.parameters.target-url}}" > /tmp/targets.txt
+            vegeta attack -targets=/tmp/targets.txt -duration={{inputs.parameters.duration}} -rate={{inputs.parameters.rate}} | tee /tmp/results.bin
+            vegeta report -type=json /tmp/results.bin > /tmp/vegeta_report.json
+            echo "Wrote /tmp/vegeta_report.json"
+            # optionally upload to MODEL_ARTIFACT_BUCKET if set
+            if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+              if command -v aws >/dev/null 2>&1; then
+                aws s3 cp /tmp/vegeta_report.json s3://${MODEL_ARTIFACT_BUCKET}/benchmarks/vegeta_report_$(date -u +%s).json || true
+              fi
+            fi
+
+    - name: parse-and-log
+      inputs:
+        parameters:
+          - name: report-path
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            pip install mlflow requests
+            python3 - <<'PY'
+import json, os, time, requests, mlflow
+rp = "{{inputs.parameters.report-path}}"
+with open(rp) as f:
+    j = json.load(f)
+# vegeta report JSON contains 'latencies' histogram and summary metrics; compute p50/p95/p99 if available
+metrics = {}
+if 'latencies' in j:
+    l = j['latencies']
+    # vegeta provides mean, p50 etc in 'latencies' as strings in some versions; fallback to 'latency' summary
+try:
+    success = j.get('success', None)
+    results = j.get('bytes_in', {})
+    metrics['requests'] = j.get('requests', 0)
+    metrics['rate'] = j.get('rate', 0)
+    metrics['success'] = success
+    # sometimes vegeta has 'latencies' field with statistical summary
+    lat = j.get('latencies', {})
+    for key in ['mean','50','95','99','p50','p95','p99','max','min']:
+        if key in lat:
+            metrics[f"latency_{key}"] = lat.get(key)
+except Exception:
+    pass
+# Push to MLflow
+mlflow.set_tracking_uri(os.environ.get('MLFLOW_URL', ''))
+with mlflow.start_run(run_name='vegeta_load_test'):
+    for k,v in metrics.items():
+        try:
+            mlflow.log_metric(k, float(v))
+        except Exception:
+            pass
+    mlflow.log_artifact(rp, artifact_path='benchmarks')
+# Push some simple metrics to Prometheus Pushgateway if configured
+pg = os.environ.get('PUSHGATEWAY','')
+if pg:
+    # push a simple JSON metric via a small PUT to pushgateway (Prometheus expects text format)
+    def push(metric, value):
+        data = f"{metric} {value}\n"
+        requests.put(f"{pg}/metrics/job/vegeta_load_test", data=data)
+    try:
+        push('vegeta_requests', metrics.get('requests',0))
+        push('vegeta_rate', metrics.get('rate',0))
+    except Exception:
+        pass
+print('Logged metrics to MLflow and Pushgateway (if configured)')
+PY
+
*** End Patch
*** Begin Patch
*** Add File:benchmarks/vegeta_load_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Helper to run vegeta locally and produce a JSON report suitable for parsing.
+TARGET=${1:-"http://localhost:8080/predict"}
+DURATION=${2:-"60s"}
+RATE=${3:-200}
+OUT=${4:-/tmp/vegeta_report.json}
+TMPBIN=/tmp/vegeta_results.bin
+echo "GET $TARGET" > /tmp/targets.txt
+docker run --rm -v /tmp:/tmp quay.io/tsenart/vegeta:12.8.4 sh -c "vegeta attack -targets=/tmp/targets.txt -duration=${DURATION} -rate=${RATE} | tee ${TMPBIN} && vegeta report -type=json ${TMPBIN} > ${OUT}"
+echo "Wrote report to ${OUT}"
+
*** End Patch
*** Begin Patch
*** Add File:deepspeed/deepspeed_config_zero2.json
+{
+  "train_batch_size": 256,
+  "train_micro_batch_size_per_gpu": 8,
+  "steps_per_print": 100,
+  "optimizer": {
+    "type": "AdamW",
+    "params": {
+      "lr": 1e-4,
+      "betas": [0.9, 0.999],
+      "eps": 1e-8,
+      "weight_decay": 0.01
+    }
+  },
+  "zero_optimization": {
+    "stage": 2,
+    "offload_param": {
+      "device": "cpu",
+      "pin_memory": true
+    },
+    "contiguous_gradients": true,
+    "overlap_comm": true
+  },
+  "fp16": {
+    "enabled": true,
+    "loss_scale": 0,
+    "initial_scale_power": 16
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:deepspeed/deepspeed_config_zero3.json
+{
+  "train_batch_size": 512,
+  "train_micro_batch_size_per_gpu": 4,
+  "steps_per_print": 100,
+  "optimizer": {
+    "type": "AdamW",
+    "params": {
+      "lr": 1e-4,
+      "betas": [0.9, 0.999],
+      "eps": 1e-8,
+      "weight_decay": 0.01
+    }
+  },
+  "zero_optimization": {
+    "stage": 3,
+    "offload_param": {
+      "device": "nvme",
+      "nvme_path": "/local_nvme",
+      "pin_memory": true
+    },
+    "overlap_comm": true,
+    "contiguous_gradients": true
+  },
+  "fp16": {
+    "enabled": true
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:deepspeed/deepspeed_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: deepspeed-train
+  namespace: aegis
+spec:
+  template:
+    spec:
+      # serviceAccount with access to S3/MinIO and MLflow
+      serviceAccountName: feast-service-account
+      containers:
+        - name: deepspeed
+          image: registry.example.com/aegis/deepspeed-trainer:latest
+          command: ["/bin/bash", "-lc"]
+          args:
+            - |
+              set -euo pipefail
+              # Example command: assumes deepspeed and torch are available in image
+              export OMP_NUM_THREADS=4
+              deepspeed --num_gpus=4 training/world_model/deepspeed_entrypoint.py --deepspeed_config deepspeed/deepspeed_config_zero2.json
+          resources:
+            limits:
+              nvidia.com/gpu: 4
+              cpu: "16"
+              memory: "64Gi"
+            requests:
+              nvidia.com/gpu: 4
+              cpu: "8"
+              memory: "32Gi"
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:deepspeed/deepspeed_entrypoint_stub.py
+#!/usr/bin/env python3
+"""
+Entrypoint stub for DeepSpeed jobs. Replace with your model/training code.
+This script demonstrates argument parsing for deepspeed integration.
+"""
+import argparse, json, time
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--deepspeed_config", default="deepspeed/deepspeed_config_zero2.json")
+    args, unknown = p.parse_known_args()
+    print("Starting DeepSpeed training stub with config:", args.deepspeed_config)
+    # Load config
+    with open(args.deepspeed_config) as f:
+        cfg = json.load(f)
+    print("Deepspeed config:", cfg)
+    # Simulate training run
+    for epoch in range(2):
+        print(f"Epoch {epoch} start")
+        time.sleep(5)
+        print(f"Epoch {epoch} done")
+    print("DeepSpeed entrypoint stub complete")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/nightly_benchmarks.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: nightly-benchmarks-
+  namespace: aegis
+spec:
+  entrypoint: nightly
+  templates:
+    - name: nightly
+      steps:
+        - - name: vegeta
+            template: vegeta-attack
+        - - name: deepspeed-smoke
+            template: deepspeed-smoke
+        - - name: parse-and-record
+            template: parse-and-record
+
+    - name: vegeta-attack
+     
î€€
