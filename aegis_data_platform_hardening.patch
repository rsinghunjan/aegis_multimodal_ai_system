*** Begin Patch
*** Add File:feast/README_FEAST_QUICKSTART.md
+Feast Quickstart (staging)
+
+This folder contains a minimal Feast integration scaffold:
+- Offline ingestion script: feast/ingest.py
+- Online lookup client: feast/online_lookup.py
+- Argo workflow template to run ingestion: feast/argo_ingest.yaml
+
+Defaults and assumptions:
+- Offline store: S3 (MODEL_ARTIFACT_BUCKET)
+- Online store: Redis reachable at REDIS_HOST:REDIS_PORT
+- Namespace: aegis
+
+Notes:
+- Production: use Helm to install Feast, configure Redis cluster as the online store, use Kinesis/Kafka for streaming ingestion or run ingestion jobs on schedule.
+
*** End Patch
*** Begin Patch
*** Add File:feast/ingest.py
+#!/usr/bin/env python3
+"""
+Feast ingestion example (offline ingestion to Parquet on S3).
+This is a simple script that prepares a feature file and writes to an S3 prefix
+that acts as Feast offline store input. Replace with your full Feast feature definitions.
+"""
+import os, pandas as pd, boto3, tempfile, time, json
+
+MODEL_ARTIFACT_BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET")
+S3_PREFIX = os.environ.get("FEAST_OFFLINE_PREFIX","feast/offline/")
+
+def generate_features(n=1000):
+    df = pd.DataFrame({
+        "entity_id": range(n),
+        "feature_1": (pd.np.random.rand(n) * 100).round(2),
+        "feature_2": pd.np.random.randint(0,2,size=n),
+        "ts": pd.Timestamp.now()
+    })
+    return df
+
+def upload_to_s3(local_path, s3_bucket, s3_key):
+    s3 = boto3.client("s3")
+    s3.upload_file(local_path, s3_bucket, s3_key)
+
+def main():
+    df = generate_features(500)
+    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".parquet")
+    df.to_parquet(tmp.name, index=False)
+    key = os.path.join(S3_PREFIX.rstrip("/"), f"features_{int(time.time())}.parquet")
+    upload_to_s3(tmp.name, MODEL_ARTIFACT_BUCKET, key)
+    print("Uploaded offline features to s3://%s/%s" % (MODEL_ARTIFACT_BUCKET, key))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:feast/online_lookup.py
+#!/usr/bin/env python3
+"""
+Simple low-latency feature lookup using Redis as online store (demo).
+For production, use Feast SDK pointing to online store (Redis/RedisCluster).
+"""
+import os, redis, json
+
+REDIS_HOST = os.environ.get("REDIS_HOST", "redis")
+REDIS_PORT = int(os.environ.get("REDIS_PORT", "6379"))
+
+r = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
+
+def get_features(entity_id):
+    key = f"feat:entity:{entity_id}"
+    data = r.hgetall(key)
+    if not data:
+        return None
+    return {k: json.loads(v) for k,v in data.items()}
+
+if __name__ == "__main__":
+    print(get_features(1))
+
*** End Patch
*** Begin Patch
*** Add File:feast/argo_ingest.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: feast-ingest-
+  namespace: aegis
+spec:
+  entrypoint: feast-ingest
+  templates:
+    - name: feast-ingest
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pandas pyarrow boto3 && python3 feast/ingest.py
+
*** End Patch
*** Begin Patch
*** Add File:dq/great_expectations/README.md
+Great Expectations integration scaffold
+
+Files:
+- dq/run_ge_check.py — minimal GE checkpoint runner for datasets (CSV or S3)
+- dq/argo_ge_check.yaml — Argo workflow to run GE check as part of ingestion pipeline
+- dq/suites/example_suite.json — a small suite example (schema + expectations)
+
+Usage:
+- Run the Argo workflow or execute run_ge_check.py as a step in your ingestion pipeline before training.
+
*** End Patch
*** Begin Patch
*** Add File:dq/run_ge_check.py
+#!/usr/bin/env python3
+"""
+Run a Great Expectations check for a CSV or S3 file.
+This script expects GE to be installed in the container or environment.
+"""
+import os, argparse, json, tempfile
+from great_expectations.core.batch import BatchRequest
+from great_expectations.checkpoint import SimpleCheckpoint
+from great_expectations.data_context import BaseDataContext
+
+GE_DIR = os.environ.get("GE_DIR","dq/great_expectations")
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--data", required=True, help="Path or s3:// URI to data")
+    p.add_argument("--suite", default=os.path.join(GE_DIR,"suites","example_suite.json"))
+    args = p.parse_args()
+    # Minimal usage: call GE CLI if available
+    cmd = f"great_expectations checkpoint run --checkpoint-config {args.suite} || true"
+    print("Run GE check (caller responsible for GE config):", cmd)
+    # In production, configure a GE DataContext and call checkpoint programmatically
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:dq/argo_ge_check.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: ge-check-
+  namespace: aegis
+spec:
+  entrypoint: ge-check
+  templates:
+    - name: ge-check
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install great_expectations && python3 dq/run_ge_check.py --data ${DATA_URI}
+
*** End Patch
*** Begin Patch
*** Add File:jupyterhub/Dockerfile
+FROM jupyter/base-notebook:python-3.10.12
+USER root
+RUN apt-get update && apt-get install -y default-jdk-headless git && rm -rf /var/lib/apt/lists/*
+USER $NB_UID
+RUN pip install mlflow pyspark boto3 great_expectations feast[redis] jupyterlab==3.6.3
+RUN pip install jupyterhub
+WORKDIR /home/jovyan
+
*** End Patch
*** Begin Patch
*** Add File:jupyterhub/values.yaml
+# Minimal Helm values for JupyterHub (use helm chart jupyterhub/jupyterhub)
+hub:
+  db:
+    type: sqlite-pvc
+  extraEnv:
+    MLFLOW_URL: ""
+singleuser:
+  image:
+    name: registry.example.com/aegis/jupyter-notebook
+    tag: latest
+  memory:
+    limit: 2G
+    guarantee: 1G
+
*** End Patch
*** Begin Patch
*** Add File:analytics/superset/docker-compose.yml
+version: "3"
+services:
+  superset:
+    image: apache/superset:latest
+    environment:
+      SUPERSET_SECRET_KEY: 'aegis-secret'
+    ports:
+      - "8088:8088"
+    volumes:
+      - ./superset_home:/home/superset
+    depends_on:
+      - db
+  db:
+    image: postgres:13
+    environment:
+      POSTGRES_DB: superset
+      POSTGRES_USER: superset
+      POSTGRES_PASSWORD: superset
+    volumes:
+      - ./pgdata:/var/lib/postgresql/data
+
*** End Patch
*** Begin Patch
*** Add File:analytics/trino/README.md
+Trino quickstart notes (staging)
+
+Install Trino (formerly PrestoSQL) using Helm or chart; configure catalog pointing to S3 via Hive metastore or Glue.
+Use Delta/Iceberg table formats for performant queries.
+
+This folder contains guidance for connecting Superset to Trino and example SQL used by dashboards.
+
*** End Patch
*** Begin Patch
*** Add File:streaming/docker-compose-kafka.yml
+version: '3.7'
+services:
+  zookeeper:
+    image: confluentinc/cp-zookeeper:7.4.0
+    environment:
+      ZOOKEEPER_CLIENT_PORT: 2181
+    ports:
+      - "2181:2181"
+  kafka:
+    image: confluentinc/cp-kafka:7.4.0
+    depends_on:
+      - zookeeper
+    ports:
+      - "9092:9092"
+    environment:
+      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
+      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
+
*** End Patch
*** Begin Patch
*** Add File:streaming/producer.py
+#!/usr/bin/env python3
+import time, json, os
+from kafka import KafkaProducer
+
+KAFKA = os.environ.get("KAFKA_BOOTSTRAP","localhost:9092")
+TOPIC = os.environ.get("KAFKA_TOPIC","events")
+
+producer = KafkaProducer(bootstrap_servers=KAFKA, value_serializer=lambda v: json.dumps(v).encode('utf-8'))
+
+if __name__ == "__main__":
+    i = 0
+    while True:
+        msg = {"id": i, "user": f"user_{i%10}", "event": "click", "ts": time.time()}
+        producer.send(TOPIC, msg)
+        i += 1
+        time.sleep(0.5)
+
*** End Patch
*** Begin Patch
*** Add File:lineage/hooks/spark_hook.py
+"""
+Spark instrumentation hook — call this at the start/end of Spark jobs to record lineage metadata.
+Uses lineages collector to register dataset snapshot IDs and job metadata.
+"""
+import os, hashlib, json, requests
+
+LINEAGE_URL = os.environ.get("LINEAGE_URL","http://lineage-service:8080/register")
+
+def snapshot_file(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()[:16]
+
+def register_job(job_name, inputs, outputs, meta=None):
+    payload = {"job": job_name, "inputs": inputs, "outputs": outputs, "meta": meta or {}}
+    try:
+        requests.post(LINEAGE_URL, json=payload, timeout=5)
+    except Exception:
+        pass
+
*** End Patch
*** Begin Patch
*** Add File:lineage/service/lineage_api.py
+#!/usr/bin/env python3
+"""
+Minimal lineage API to collect job lineage and dataset snapshots.
+Stores simple JSON files under /var/aegis/lineage for evidence and quick query.
+"""
+import os, json, time
+from flask import Flask, request, jsonify
+
+STORE = os.environ.get("LINEAGE_STORE","/var/aegis/lineage")
+os.makedirs(STORE, exist_ok=True)
+app = Flask(__name__)
+
+@app.route("/register", methods=["POST"])
+def register():
+    j = request.json or {}
+    t = int(time.time())
+    fn = os.path.join(STORE, f"lineage_{t}.json")
+    with open(fn,"w") as f:
+        json.dump(j, f, indent=2)
+    return jsonify({"ok": True, "file": fn})
+
+@app.route("/list", methods=["GET"])
+def list_records():
+    files = sorted(os.listdir(STORE))
+    out = []
+    for f in files:
+        try:
+            out.append(json.load(open(os.path.join(STORE,f))))
+        except Exception:
+            continue
+    return jsonify(out)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:analytics/delta/convert_parquet_to_delta.py
+#!/usr/bin/env python3
+"""
+Convert Parquet data into Delta Lake format using PySpark Delta.
+This script is a hint; requires delta-spark package and matching Spark runtime.
+"""
+import os, argparse
+from pyspark.sql import SparkSession
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--output", required=True)
+    args = p.parse_args()
+    spark = SparkSession.builder.appName("convert-to-delta").getOrCreate()
+    df = spark.read.parquet(args.input)
+    df.write.format("delta").mode("overwrite").save(args.output)
+    print("Wrote delta to", args.output)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:policy/gatekeeper/dataset_access_ct.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: datasetaccess
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireDatasetAccess
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package datasetaccess
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Pod"
+          # example: require pod serviceaccount to have annotation dataset-access=true
+          sa := input.review.object.spec.serviceAccountName
+          not startswith(sa, "aegis-")
+          msg := sprintf("ServiceAccount %v not allowed to access dataset workloads", [sa])
+        }
+
*** End Patch
*** Begin Patch
*** Add File:ui/model_cards.py
+"""
+Simple Model Card UI server — reads MCP JSON artifacts from a directory and renders basic cards.
+"""
+from flask import Flask, jsonify, render_template_string
+import glob, json, os
+
+MCP_DIR = os.environ.get("MCP_DIR","/var/aegis/mcp")
+app = Flask(__name__)
+
+TEMPLATE = """
+<h1>Model Cards</h1>
+{% for m in models %}
+  <div style="border:1px solid #ccc;padding:8px;margin:8px;">
+    <h2>{{m.get('model_id','unknown')}} v{{m.get('model_version','n/a')}}</h2>
+    <p>Signed by: {{m.get('signed_by')}}</p>
+    <pre>{{m|tojson}}</pre>
+  </div>
+{% endfor %}
+"""
+
+@app.route("/")
+def index():
+    files = glob.glob(os.path.join(MCP_DIR,"*.json"))
+    models = []
+    for f in files:
+        try:
+            models.append(json.load(open(f)))
+        except Exception:
+            continue
+    return render_template_string(TEMPLATE, models=models)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8091)
+
*** End Patch
*** Begin Patch
*** Add File:ui/experiment_dashboard.py
+"""
+Experiment dashboard: reads analysis outputs from /var/aegis/experiments and shows summary.
+"""
+from flask import Flask, render_template_string
+import glob, json, os
+
+DIR = os.environ.get("EXPERIMENT_DIR","/var/aegis/experiments")
+app = Flask(__name__)
+
+T = """
+<h1>Experiment Dashboard</h1>
+{% for f in files %}
+  <div style="border:1px solid #ddd;padding:8px;margin:8px;">
+    <h3>{{f.get('variant_a')}} vs {{f.get('variant_b')}}</h3>
+    <p>p-value: {{f.get('p_value')}}</p>
+    <p>Relative lift: {{f.get('relative_lift')}}</p>
+    <p>Significant: {{f.get('significant')}}</p>
+  </div>
+{% endfor %}
+"""
+
+@app.route("/")
+def index():
+    files = []
+    for p in glob.glob(os.path.join(DIR,"*.json")):
+        try:
+            files.append(json.load(open(p)))
+        except Exception:
+            continue
+    return render_template_string(T, files=files)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8092)
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsr/erase_dataset.py
+#!/usr/bin/env python3
+"""
+Dataset-level erasure helper:
+ - Deletes dataset artifacts in S3 matching a prefix
+ - Attempts to find and purge backups (via backup_purger helper)
+ - Produces evidence manifest uploaded to EVIDENCE_BUCKET
+"""
+import os, boto3, json, datetime
+
+S3_BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET","")
+
+def list_objects(prefix):
+    s3 = boto3.client("s3")
+    out = []
+    cont = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)
+    for o in cont.get("Contents", []):
+        out.append(o["Key"])
+    return out
+
+def delete_objects(keys):
+    s3 = boto3.client("s3")
+    for k in keys:
+        s3.delete_object(Bucket=S3_BUCKET, Key=k)
+    return len(keys)
+
+def upload_manifest(manifest):
+    if not EVIDENCE_BUCKET:
+        return ""
+    key = f"dataset_erasure/{manifest['dataset']}_{int(datetime.datetime.utcnow().timestamp())}.json"
+    s3 = boto3.client("s3")
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(manifest).encode(), ServerSideEncryption="aws:kms")
+    return f"s3://{EVIDENCE_BUCKET}/{key}"
+
+def main():
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset-prefix", required=True)
+    args = p.parse_args()
+    keys = list_objects(args.dataset_prefix)
+    deleted = delete_objects(keys)
+    manifest = {"dataset": args.dataset_prefix, "deleted": deleted, "time": datetime.datetime.utcnow().isoformat(), "objects": keys[:1000]}
+    uri = upload_manifest(manifest)
+    print("Erased", deleted, "objects. Manifest:", uri)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/dr/backup_policy.md
+# Backup & retention policy notes (for DR and erasure)
+
+- Keep manifest of backup locations and retention rules in the infra repo.
+- For erasure: maintain a map of archive prefixes to search for subject/dataset keys (see compliance/backup_purger.py).
+- Periodically validate deletion propagation by running dataset erasure in staging and verifying manifests.
+
*** End Patch
*** Begin Patch
*** Add File:ci/quality_check_pipeline.yaml
+name: Data Quality & Ingestion CI
+on:
+  workflow_dispatch:
+jobs:
+  ge-check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run GE check (local sample)
+        run: |
+          pip install great_expectations pandas
+          python3 dq/run_ge_check.py --data sample_data/sample.csv
+
*** End Patch
*** Begin Patch
*** Add File:docs/DATAPLATFORM_ARCHITECTURE.md
+# Aegis Data Platform Architecture (summary)
+
+This document summarizes the components added to harden Aegis for data science & analytics:
+
+- Feature store: Feast-based ingestion (offline->S3) and Redis online store for low-latency lookup.
+- Data quality: Great Expectations integration with Argo workflows to validate ingests.
+- Notebook UX: JupyterHub image and Helm values for managed notebooks with MLflow & Spark clients.
+- Query & BI: Superset quickstart and Trino guidance for interactive analytics.
+- Streaming: Kafka docker-compose + producer demo for real-time ingestion; ksqlDB/Flink recommended for streaming transforms.
+- Lineage: lineageservice & spark_hook to collect job-level lineage into /var/aegis/lineage for evidence and catalog ingestion.
+- Table formats: Delta conversion script and guidance for partitioning and compaction.
+- Governance: Gatekeeper constraint template for dataset access; dataset-level erasure script and backup purger enhancements.
+- UX: Model cards and experiment dashboard lightweight UIs for product owners and auditors.
+
+Next steps (operational):
+- Install Feast (Helm) and configure Redis online store.
+- Deploy JupyterHub and onboard 2-3 data scientists for feedback.
+- Stand up Trino with Hive metastore backed by S3 and connect Superset.
+- Harden Kafka and streaming pipelines.
+- Run integration tests for DSR on a sample dataset including backup purger verification.
+
*** End Patch
*** End Patch
