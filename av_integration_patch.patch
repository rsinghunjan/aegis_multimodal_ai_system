*** Begin Patch
*** Add File: docs/av_integration_design.md
+# Aegis → Autonomous Vehicle (AV) Integration Design
+
+Purpose
+- Describe how Aegis maps to an AV development & fleet operations stack.
+- Provide concrete manifests, S3 layout, Prometheus metrics, CI/workflow examples to run simulation → train → canary rollout → fleet rollout flows.
+- Focus: safety, auditability, attestation, signed artifacts, canary rollback and evidence capture.
+
+High-level components
+- Data ingestion: secure vehicle/simulator upload gateway → S3 (COMPLIANCE_BUCKET) with provenance metadata.
+- Dataset & feature store: index sensor bundles, labels and scenario metadata; store dataset versions in MLflow or dataset registry.
+- Simulation farm: orchestrate CARLA/LGSVL jobs via Kubernetes Jobs (batch), collect scenario traces to S3.
+- Training & validation: GPU cluster jobs orchestrated by GH Actions / Argo Workflows; checkpoints saved to MLflow and signed.
+- Adversarial & safety harness: adversarial scenario runner + automated scoring; hard CI gate.
+- Artifact signing & promotion: cosign + Rekor for model & firmware releases; promotion gated by production orchestrator.
+- On‑vehicle deployment: A/B partitioned OTA with TPM attestation checks and canary cohorts; rollback on SLO breach.
+
+Security & Safety (must-haves)
+- All model & firmware artifacts cosign-signed; Rekor log entries recorded before deployment.
+- Device attestation via TPM; enroll record stored in S3 + Rekor.
+- HSM/Vault for provider and signing keys; strict Vault policies + network policies.
+- Multi-sig approval for HSM/critical promotions; OPA policy for automated decisions.
+
+Core data flows (concise)
+1. Ingest: vehicle/simulator -> secure gateway -> s3://COMPLIANCE_BUCKET/raw/<vehicle_or_sim>/YYYYMMDD/...
+2. Labeling: labels stored at s3://COMPLIANCE_BUCKET/labels/<dataset>/<version>/
+3. Training: GH/Argo workflow runs training with dataset path -> checkpoints uploaded to MLflow and s3://COMPLIANCE_BUCKET/checkpoints/
+4. Simulation validation: orchestrator schedules simulation batches; results uploaded to s3://COMPLIANCE_BUCKET/simulations/
+5. Gate & sign: after tests pass, cosign model and push Rekor entry; orchestrator triggers canary OTA to fleet cohort
+6. Canary: cohort telemetry -> Aegis anomaly detector -> if OK expand rollout; else run rollback playbook and open incident.
+
+API & Metadata conventions
+- Every upload must include metadata JSON alongside payload:
+  - metadata.json fields: { "source": "vehicle-123"|"simulator", "ts": ISO8601, "commit": "<git sha>", "dataset": "vX", "scenario_id": "s-0001", "signer_rekor": "<rekor-id>" }
+- Filenames: {vehicle|sim}-{YYYYMMDDThhmmss}.tar.gz and metadata JSON with same basename.
+
+Operational guarantees
+- Canary window and UAT: default 24–72 hours depending on risk profile.
+- Abort & rollback: automatic rollback trigger when critical SLOs violated or anomaly count exceeds threshold.
+- Evidence retention: keep artifacts for at least 1 year (configurable; retention policy in S3).
+
+Contacts & roles
+- SRE / FleetOps: deploys orchestrator and handles rollouts.
+- ML / Research: prepares datasets, trains and validates models.
+- Security / Compliance: signs artifacts and validates pen-test results.
+- Legal: approves deployments in regulated regions.
+
+See accompanying manifests and workflows for concrete examples included in this repo.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/av-simulation-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: av-simulation-job
+  namespace: staging
+spec:
+  backoffLimit: 2
+  template:
+    metadata:
+      labels:
+        app: av-simulation
+    spec:
+      restartPolicy: Never
+      containers:
+      - name: carla-runner
+        image: aegis/carla-runner:latest
+        env:
+          - name: SCENARIO_ID
+            value: "scenario-{{JOB_ID}}"
+          - name: OUTPUT_S3
+            value: "s3://{{COMPLIANCE_BUCKET}}/simulations/{{BUILD_ID}}/"
+          - name: AWS_REGION
+            value: "us-west-2"
+        resources:
+          limits:
+            cpu: "4"
+            memory: "12Gi"
+            nvidia.com/gpu: "1"
+        volumeMounts:
+          - name: shared-data
+            mountPath: /data
+      volumes:
+        - name: shared-data
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/av_simulation_train.yml
+name: AV Simulation → Train → Validate
+on:
+  workflow_dispatch:
+    inputs:
+      dataset_path:
+        required: true
+      scenario_batch:
+        required: true
+jobs:
+  simulate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Trigger simulation job (k8s)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          # create a Kubernetes Job from template, pass BUILD_ID
+          BUILD_ID=$(date -u +"%Y%m%dT%H%M%SZ")
+          sed "s/{{BUILD_ID}}/${BUILD_ID}/g; s/{{COMPLIANCE_BUCKET}}/${COMPLIANCE_BUCKET}/g" k8s/manifests/av-simulation-job.yaml > /tmp/sim-job.yaml
+          kubectl --kubeconfig="$KUBECONFIG" apply -f /tmp/sim-job.yaml
+          # wait for completion (simplified)
+          sleep 10
+  train:
+    needs: simulate
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Launch training job
+        env:
+          DATASET_PATH: ${{ github.event.inputs.dataset_path }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          # Example: start training on GPU cluster (placeholder)
+          python3 training/submit_training_job.py --dataset "$DATASET_PATH" --out "s3://${COMPLIANCE_BUCKET}/checkpoints/"
+  validate:
+    needs: train
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run adversarial harness and safety tests
+        env:
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python3 safety/run_adversarial_battery.py --checkpoint s3://${COMPLIANCE_BUCKET}/checkpoints/latest.tar.gz
+          # ensure exit 0 on pass, non-zero on failure
+
*** End Patch
*** Begin Patch
*** Add File: docs/s3_layout.md
+# S3 Layout for AV + Aegis integration
+
+Top-level buckets: use COMPLIANCE_BUCKET (example: aegis-prod-evidence)
+
+Proposed prefixes:
+- raw/
+  - raw/vehicle/<vehicle-id>/<YYYY>/<MM>/<DD>/<vehicle>-<ts>.tar.gz
+  - raw/sim/<sim-run-id>/<YYYYMMDD>/...
+- datasets/
+  - datasets/<name>/v<semver>/
+  - datasets/<name>/v<semver>/manifest.json
+- labels/
+  - labels/<dataset>/<version>/annotations.jsonl
+- checkpoints/
+  - checkpoints/<model-name>/<ts>/ckpt.tar.gz
+  - checkpoints/<model-name>/mlflow/<run-id>/
+- signatures/
+  - signatures/models/<model-name>/<ts>.sig
+  - rekor_entries/<rekor-id>.json
+- simulations/
+  - simulations/<build-id>/<scenario-id>/trace-<ts>.tar.gz
+  - simulations/<build-id>/<scenario-id>/metrics.json
+- attestations/
+  - attestations/<vehicle-id>/attest-<ts>.json
+  - attestations/<vehicle-id>/rekor-<entry-id>.json
+- hsm/
+  - hsm/rotation_<ts>.json
+- reconciliations/
+  - quantum/reconcile/v4/*.json (if used)
+- rlhf/
+  - rlhf/pilot/<run-id>/artifacts/
+- pen_tests/
+  - pen_tests/<vendor>/<scope>/<report>.pdf
+- evidence/
+  - evidence/orchestrator/<run-id>/*
+- backups/
+  - backups/approvals_<ts>.db
+
+Retention & permissions
+- Retain critical evidence for minimum 1 year; lifecycle rules to move to Glacier after 90 days.
+- Enforce bucket policies: object ACLs private; encrypt (SSE-KMS); limited put/get by role (operator, SRE, security).
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/av_rules.yaml
+groups:
+- name: av.safety.rules
+  rules:
+  - alert: AVSimulationFailure
+    expr: increase(simulation_failed_total[10m]) > 0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Simulation failure detected"
+      description: "One or more simulation jobs failed in the last 10m."
+  - alert: AVModelQualityRegression
+    expr: increase(rlhf_validation_failures_total[1h]) > 0
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Model validation failures detected"
+      description: "New model failed validation tests in CI/adversarial harness."
+  - alert: VehicleAttestationMissing
+    expr: count_over_time(av_attestation_missing_total[30m]) > 0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Vehicle attestation missing or failing"
+      description: "One or more vehicles failed attestation checks; investigate device health and network."
+  - alert: CanarySLOBreach
+    expr: increase(canary_critical_violations_total[15m]) > 0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Canary cohort SLO violation"
+      description: "Canary fleet violated critical SLOs; automated rollback should be triggered."
+
*** End Patch
*** Begin Patch
*** Add File: docs/iso26262_checklist.md
+# ISO 26262 / SOTIF / UN R155 Checklist (template)
+
+Use this checklist to collect artifacts required for safety certification. Tailor to OEM / region regulatory needs.
+
+1) System description
+- [ ] High-level system architecture diagrams (control, perception, planning, actuation)
+- [ ] Component responsibilities with safety goals and ASIL levels
+
+2) Requirements & Traceability
+- [ ] Safety requirements (SR) and mapping to system components
+- [ ] Traceability matrix linking SR -> design -> implementation -> test
+
+3) Design & Implementation
+- [ ] Data flow diagrams and timing budgets
+- [ ] Deterministic inference stack documentation (RT constraints)
+- [ ] Failure modes & mitigations documented
+
+4) Verification & Validation artifacts
+- [ ] Unit / integration test reports
+- [ ] Simulation coverage report (scenario coverage)
+- [ ] Adversarial harness results and remediation logs
+- [ ] Test vectors with pass/fail records in s3://<COMPLIANCE_BUCKET>/simulations/
+
+5) Validation on hardware
+- [ ] HIL bench test reports
+- [ ] Shadow mode run logs from vehicles
+- [ ] Canary cohort telemetry and SLO reports
+
+6) Software & Artifact Integrity
+- [ ] Cosign signatures for model & firmware artifacts (signatures in signatures/)
+- [ ] Rekor entries for signed artifacts
+- [ ] Release notes + MLflow checkpoint metadata
+
+7) Process & People
+- [ ] Roles & responsibilities (SRE, ML, Security, Legal)
+- [ ] Approval workflows and multi-sig policies
+- [ ] Incident response & forensic playbooks (runbooks/av_incident_playbook.md)
+
+8) Cybersecurity (UN R155)
+- [ ] Threat model & mitigations
+- [ ] Vulnerability scan reports (images, firmware)
+- [ ] Pen test reports (pen_tests/<vendor>/)
+
+9) Evidence packaging
+- [ ] Package: diagrams, traceability, test artifacts, signatures, pen-tests in one zip:
+  - s3://<COMPLIANCE_BUCKET>/certification/<project>/<version>/
+- [ ] Signed manifest.json describing package contents and signers
+
+Notes:
+- Maintain immutable evidence in S3 and Rekor for auditors.
+- Engage legal and safety early to align on ASIL allocation and required test coverage.
+
*** End Patch
*** Begin Patch
*** Add File: docs/pr_template_iso26262.md
+# PR Template for safety/certification artifacts
+
+Title: cert: add ISO26262 evidence bundle for <feature/branch>
+
+Summary:
+- Brief description of change (e.g., "Add simulation coverage report and HIL bench results for model vX")
+- Attach/point to S3 evidence location: s3://<COMPLIANCE_BUCKET>/certification/<project>/<version>/
+
+Checklist (required before merge):
+- [ ] Architecture & diagrams included
+- [ ] Traceability matrix included (SR -> test)
+- [ ] Simulation & HIL test artifacts available and uploaded
+- [ ] Cosign signatures for artifacts present and rekor entries recorded
+- [ ] Vulnerability scan & pen-test results attached
+- [ ] Legal & Safety reviewers assigned and approve
+
+Reviewers:
+- @safety-engineer @security @legal-team @ops-team
+
+Merge policy:
+- Require approvals from at least two safety reviewers and one security reviewer.
+- Tag release with `certified/<region>/<version>` on successful merge.
+
*** End Patch
*** Begin Patch
*** Add File: docs/av_phased_roadmap.md
+# AV Integration Phased Roadmap — tailored estimates
+
+Assumptions: small pilot fleet = 50 vehicles; medium = 500; large = 5,000+. Region examples: US (DOT + FMVSS + state-by-state), EU (UNECE regs), Japan.
+
+Phase A — Foundations (0–4 weeks)
+- Secure ingestion, S3 layout, simple simulation pipeline, artifact signing.
+- Deliverables: ingestion gateway, S3 prefixes, MLflow integration, cosign baseline.
+- Team: 1 SRE, 1 ML eng, 1 Security
+- Effort: 2–4 weeks
+
+Phase B — Simulation + Training (4–12 weeks)
+- Large simulation farm, training pipelines, checkpointing, adversarial harness.
+- Deliverables: .github/workflows/av_simulation_train.yml, simulation job templates, adversarial test battery.
+- Team: 2 ML, 1 SRE
+- Effort: 4–8 weeks
+
+Phase C — Canary & On‑vehicle Integration (6–16 weeks)
+- Edge attestation, TPM-based enroll, canary OTA playbooks, telemetry ingestion.
+- Deliverables: Ansible playbooks, orchestrator runbooks, device TPM images.
+- Fleet pilot: 10–50 vehicles
+- Team: 1 FleetOps, 1 SRE, 1 Security
+- Effort: 6–12 weeks
+
+Phase D — Shadow & Limited Autonomy Trials (12–28 weeks)
+- Shadow-mode deployment, HIL validation, regulatory engagement.
+- Fleet pilot: 50–500 vehicles in geofenced regions
+- Deliverables: HIL reports, shadow logs, regulatory artifacts
+- Effort: 12–24 weeks (iterative)
+
+Phase E — Production Rollout & Certification (20–52+ weeks)
+- Full certification efforts, pen-tests, legal signoff, scaling OTA.
+- Fleet: scale up to target size
+- Team: cross-functional (ML, SRE, Security, Legal, QA, Ops)
+- Effort: 6–12+ months depending on region and ASIL requirements
+
+Estimate examples by fleet size
+- 50 vehicles (pilot): ~3–6 months to run through Phases A–D (shadow + limited autonomy)
+- 500 vehicles (regional): ~6–12 months incl. deeper certification & pen-tests
+- 5000+ vehicles (production scale): 12–24+ months with full cert & regulatory compliance
+
+Key milestones
+- M1: Simulation & adversarial harness passing (Phase B complete)
+- M2: TPM attestation on reference vehicle + OTA canary (Phase C complete)
+- M3: Shadow mode validation across region + HIL completion (Phase D complete)
+- M4: Regulatory & pen-test signoffs + production rollout plan (Phase E start)
+
+Acceptance gating
+- Each phase gated by tests, pen-tests and legal signoff; no automatic promotions beyond a gate without multi-sig.
+
*** End Patch
*** Begin Patch
*** Add File: ansible/av_canary_playbook.yml
+---
+- name: AV Canary OTA Playbook (example)
+  hosts: canary_vehicles
+  gather_facts: no
+  tasks:
+    - name: Push firmware/model artifact to vehicle
+      ansible.builtin.shell: >
+        scp /tmp/artifacts/{{ artifact_file }} {{ ansible_user }}@{{ inventory_hostname }}:/tmp/
+      register: push_res
+    - name: Trigger install on vehicle (A/B update)
+      ansible.builtin.shell: >
+        ssh {{ ansible_user }}@{{ inventory_hostname }} "sudo /opt/vehicle/bin/install_artifact.sh /tmp/{{ artifact_file }}"
+      register: install_res
+    - name: Health check after install (wait and query)
+      ansible.builtin.shell: >
+        ssh {{ ansible_user }}@{{ inventory_hostname }} "curl -sS http://localhost:9100/health"
+      register: health_res
+    - name: Fail if health not ok
+      ansible.builtin.fail:
+        msg: "Canary health check failed on {{ inventory_hostname }}: {{ health_res.stdout }}"
+      when: "'ok' not in health_res.stdout"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/av_incident_playbook.md
+# AV Incident Playbook (critical incidents)
+
+Scope: use for canary SLO breaches, on‑vehicle critical failures, or rollbacks during OTA.
+
+Immediate actions
+1. Acknowledge alert and note timestamp.
+2. Capture blackbox logs from affected vehicle(s):
+   - Pull logs via secure gateway or request device upload to s3://<COMPLIANCE_BUCKET>/incidents/<run-id>/
+3. Capture system snapshot: kubectl -n staging get pods -o wide; pg_dump (if applicable).
+4. If on-canter cohort -> trigger Automator rollback playbook:
+   - ansible-playbook ansible/av_canary_playbook.yml --limit canary_vehicles -e "artifact_file={{previous_artifact}}"
+5. Open GitHub issue with evidence links and assign SRE + Safety lead.
+
+Investigation
+- Reproduce failure in simulator with same scenario id.
+- Cross-check MLflow checkpoint, signatures, and Rekor entry for artifact.
+- Run offline adversarial harness against suspect checkpoint.
+
+Containment & Recovery
+- If failure is safety-critical: stop rollout, pause orchestrator, notify Legal & Ops.
+- If failure is recoverable: apply rollback and monitor for 60 minutes.
+
+Postmortem
+- Produce incident report: timeline, root cause, evidence (S3 keys), remediation steps.
+- Update runbooks and blocklist if necessary.
+
+Escalation
+- Critical safety incident: page Safety Director and Legal immediately.
+- Regulatory report as required by local law (e.g., serious incidents to authority).
+
*** End Patch
