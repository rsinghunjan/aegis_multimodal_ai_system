diff --git a/k8s/manifests/dgx/nccl-configmap.yaml b/k8s/manifests/dgx/nccl-configmap.yaml
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/k8s/manifests/dgx/nccl-configmap.yaml
@@ -0,0 +1,40 @@
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: dgx-nccl-config
+  namespace: aegis-ml
+data:
+  # Default/safe placeholders. These should be tuned per-cluster using the NCCL tuning artifacts.
+  NCCL_SOCKET_IFNAME: "eth0"
+  NCCL_IB_HCA: "mlx5_0"
+  NCCL_IB_GID_INDEX: "3"
+  NCCL_IB_DISABLE: "0"
+  NCCL_DEBUG: "INFO"
+  NCCL_NET_GDR_LEVEL: "2"
+
+# To update these values:
+#  - Run scripts/dgx/nccl_tuning.sh on DGX nodes
+#  - Run scripts/dgx/generate_nccl_env_suggest.sh to generate a suggested env script
+#  - Use scripts/dgx/apply_nccl_config.sh to create/patch this ConfigMap with tuned values
+
diff --git a/scripts/dgx/apply_nccl_config.sh b/scripts/dgx/apply_nccl_config.sh
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/scripts/dgx/apply_nccl_config.sh
@@ -0,0 +1,200 @@
+#!/usr/bin/env bash
+#
+# Read a generated env suggestion (script output or simple KEY=VALUE lines) and apply to Kubernetes ConfigMap dgx-nccl-config.
+#
+# Usage:
+#   ./scripts/dgx/apply_nccl_config.sh --env-file ./artifacts/nccl/env_suggest.sh --namespace aegis-ml
+
+set -euo pipefail
+
+ENV_FILE=""
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+CM_NAME="${CM_NAME:-dgx-nccl-config}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --env-file) ENV_FILE="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --cm-name) CM_NAME="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$ENV_FILE" || ! -f "$ENV_FILE" ]]; then
+  echo "Provide a --env-file containing exports (e.g., env_suggest.sh)"; exit 2
+fi
+
+echo "Parsing env file: $ENV_FILE"
+declare -A kv
+while IFS= read -r line; do
+  # accept lines like: export KEY=VALUE or KEY=VALUE
+  line="${line#"${line%%[![:space:]]*}"}"   # ltrim
+  [[ -z "$line" || "${line:0:1}" == "#" ]] && continue
+  line="${line#export }"
+  if [[ "$line" == *=* ]]; then
+    key="${line%%=*}"
+    value="${line#*=}"
+    # strip surrounding quotes
+    value="${value%\"}"
+    value="${value#\"}"
+    kv["$key"]="$value"
+  fi
+done < "$ENV_FILE"
+
+if [[ ${#kv[@]} -eq 0 ]]; then
+  echo "No key=value pairs found in $ENV_FILE"; exit 3
+fi
+
+echo "Preparing kubectl patch for ConfigMap $CM_NAME in namespace $NAMESPACE..."
+tmpfile="$(mktemp)"
+{
+  echo "data:"
+  for k in "${!kv[@]}"; do
+    v="${kv[$k]}"
+    # YAML-safe quoting
+    printf '  %s: "%s"\n' "$k" "$v"
+  done
+} > "$tmpfile"
+
+if kubectl get configmap "$CM_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
+  echo "Patching existing ConfigMap..."
+  kubectl patch configmap "$CM_NAME" -n "$NAMESPACE" --patch-file="$tmpfile" || kubectl apply -f "$tmpfile" -n "$NAMESPACE"
+else
+  echo "Creating ConfigMap..."
+  # create a minimal manifest
+  cat > "$tmpfile.manifest" <<EOF
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: $CM_NAME
+  namespace: $NAMESPACE
+$(cat "$tmpfile")
+EOF
+  kubectl apply -f "$tmpfile.manifest"
+  rm -f "$tmpfile.manifest"
+fi
+
+rm -f "$tmpfile"
+echo "ConfigMap $CM_NAME updated."
+exit 0
+
diff --git a/k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml b/k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml
@@ -0,0 +1,160 @@
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: deepspeed-dgx-prod-validate-configmap
+  namespace: aegis-ml
+  labels:
+    app: deepspeed-dgx-prod-validate
+spec:
+  template:
+    metadata:
+      labels:
+        app: deepspeed-dgx-prod-validate
+    spec:
+      restartPolicy: Never
+      nodeSelector:
+        "node.kubernetes.io/dgx": "true"
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      containers:
+        - name: deepspeed-runner
+          image: __IMAGE_FULL__
+          imagePullPolicy: IfNotPresent
+          envFrom:
+            - configMapRef:
+                name: dgx-nccl-config
+          env:
+            - name: PYTHONUNBUFFERED
+              value: "1"
+            - name: TORCH_DISTRIBUTED_DEBUG
+              value: "INFO"
+          command: ["/bin/bash", "-c"]
+          args:
+            - |
+              set -euo pipefail
+              echo "NCCL envs:"
+              env | grep NCCL || true
+              deepspeed --num_gpus 8 examples/deepspeed_smoke/train.py --epochs 1 --batch-size 8 || { rc=$?; echo "Deepspeed failed: $rc"; exit $rc; }
+          resources:
+            limits:
+              nvidia.com/gpu: 8
+            requests:
+              cpu: "8"
+              memory: "32Gi"
+      volumes:
+        - name: checkpoints
+          persistentVolumeClaim:
+            claimName: dgx-checkpoints-pvc
+  backoffLimit: 1
+
+# This manifest references the ConfigMap dgx-nccl-config for NCCL tuning parameters and a PVC (dgx-checkpoints-pvc).
+#
+# Replace __IMAGE_FULL__ with the validated image (fully qualified with registry + tag).
+
diff --git a/k8s/manifests/dgx/storageclass-nvme.yaml b/k8s/manifests/dgx/storageclass-nvme.yaml
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/k8s/manifests/dgx/storageclass-nvme.yaml
@@ -0,0 +1,60 @@
+apiVersion: storage.k8s.io/v1
+kind: StorageClass
+metadata:
+  name: nvme-fast
+provisioner: local.csi.openebs.io  # replace with your NVMe/Lustre CSI driver/provisioner
+parameters:
+  type: nvme
+reclaimPolicy: Retain
+volumeBindingMode: WaitForFirstConsumer
+
+# Note:
+# - Replace provisioner value with the CSI driver available in your DGX environment for NVMe or Lustre.
+# - This StorageClass is a placeholder to indicate "fast" local storage for checkpoints; adapt as needed.
+
diff --git a/scripts/dgx/multi_node_scaling_test.sh b/scripts/dgx/multi_node_scaling_test.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/dgx/multi_node_scaling_test.sh
@@ -0,0 +1,200 @@
+#!/usr/bin/env bash
+#
+# Submit a multi-node DeepSpeed job (k8s) to validate multi-node scaling across DGX nodes.
+#
+# Usage:
+#   ./scripts/dgx/multi_node_scaling_test.sh --image <REGISTRY>/aegis-deepspeed:h100-... --nodes 2 --gpus-per-node 8
+
+set -euo pipefail
+
+IMAGE=""
+NODES=1
+GPUS_PER_NODE=8
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+JOB_NAME="deepspeed-multinode-$(date +%s)"
+TMP_JOB="/tmp/${JOB_NAME}.yaml"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --image) IMAGE="$2"; shift 2;;
+    --nodes) NODES="$2"; shift 2;;
+    --gpus-per-node) GPUS_PER_NODE="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$IMAGE" ]]; then
+  echo "--image required"
+  exit 2
+fi
+
+cat > "$TMP_JOB" <<EOF
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: ${JOB_NAME}
+  namespace: ${NAMESPACE}
+  labels:
+    app: deepspeed-multinode
+spec:
+  template:
+    metadata:
+      labels:
+        app: deepspeed-multinode
+    spec:
+      restartPolicy: Never
+      nodeSelector:
+        "node.kubernetes.io/dgx": "true"
+      containers:
+        - name: runner
+          image: ${IMAGE}
+          command: ["/bin/bash", "-c"]
+          args:
+            - |
+              set -euo pipefail
+              export NCCL_DEBUG=INFO
+              export TORCH_DISTRIBUTED_DEBUG=INFO
+              export NCCL_SOCKET_IFNAME=\${NCCL_SOCKET_IFNAME:-eth0}
+              deepspeed --num_gpus ${GPUS_PER_NODE} examples/deepspeed_multinode/run_multinode.py --nproc_per_node ${GPUS_PER_NODE} --nnodes ${NODES}
+          resources:
+            limits:
+              nvidia.com/gpu: ${GPUS_PER_NODE}
+            requests:
+              cpu: "16"
+              memory: "64Gi"
+      backoffLimit: 1
+EOF
+
+kubectl create ns "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f - || true
+kubectl apply -f "$TMP_JOB"
+echo "Waiting for pod..."
+kubectl -n "$NAMESPACE" wait --for=condition=ready pod -l app=deepspeed-multinode --timeout 600s || true
+POD=$(kubectl -n "$NAMESPACE" get pod -l app=deepspeed-multinode -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+if [[ -n "$POD" ]]; then
+  kubectl -n "$NAMESPACE" logs -f "$POD" || true
+fi
+echo "Multi-node scaling test submitted; logs may show NCCL tune/relay messages. Collect artifacts in ./artifacts."
+exit 0
+
diff --git a/.github/workflows/dgx_nightly_chaos.yml b/.github/workflows/dgx_nightly_chaos.yml
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/.github/workflows/dgx_nightly_chaos.yml
@@ -0,0 +1,220 @@
+name: DGX Nightly Validation & Chaos
+
+on:
+  schedule:
+    - cron: '0 3 * * *'
+  workflow_dispatch:
+    inputs:
+      run_chaos:
+        description: "Run chaos matrix as well"
+        required: false
+        default: "true"
+
+jobs:
+  dgx-nightly:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Restore Kubeconfig
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          if [ -z "$KUBECONFIG_DGX" ]; then echo "KUBECONFIG_DGX missing"; exit 1; fi
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+
+      - name: Run NCCL tuning & smoke
+        run: |
+          chmod +x scripts/dgx/nccl_tuning.sh
+          ./scripts/dgx/nccl_tuning.sh --nodes 1 --gpus-per-node 8 --out ./artifacts/nccl || true
+          chmod +x scripts/dgx/dgx_smoke_test.sh
+          ./scripts/dgx/dgx_smoke_test.sh || true
+
+      - name: Apply tuned NCCL configmap if present
+        run: |
+          if [ -f ./artifacts/nccl/env_suggest.sh ]; then
+            chmod +x scripts/dgx/apply_nccl_config.sh
+            ./scripts/dgx/apply_nccl_config.sh --env-file ./artifacts/nccl/env_suggest.sh --namespace aegis-ml || true
+          else
+            echo "No env_suggest.sh produced; skipping configmap update"
+          fi
+
+      - name: Run chaos matrix (optional)
+        if: ${{ github.event.inputs.run_chaos == 'true' }}
+        run: |
+          if [ -x tools/chaos_matrix_runner.sh ]; then
+            ./tools/chaos_matrix_runner.sh --profile dgx --out artifacts/chaos || true
+          else
+            echo "No chaos_matrix_runner.sh available; skipping"
+          fi
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-nightly-artifacts
+          path: artifacts || .
+
+      - name: Cleanup kubeconfig
+        if: always()
+        run: rm -f "$HOME/.kube/config_dgx" || true
+
diff --git a/docs/dgx/secrets_and_ci.md b/docs/dgx/secrets_and_ci.md
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/docs/dgx/secrets_and_ci.md
@@ -0,0 +1,200 @@
+# DGX CI Secrets & Runner Access (secure setup)
+
+Required repository (or org) secrets
+- KUBECONFIG_DGX: raw kubeconfig YAML for the DGX cluster (or use short-lived kubeconfigs / service account tokens).  
+- DGX_IMAGE_REGISTRY: registry host (e.g., ghcr.io/org).  
+- DGX_REGISTRY_USER / DGX_REGISTRY_PASS: optional registry credentials if not using public images.  
+- COSIGN_KEY: optional cosign private key (or configure OIDC keyless signing).  
+- DECISION_LOG_URL: optional endpoint to POST provenance after validation.
+
+Best practices
+- Limit access to these secrets using GitHub Environments and required reviewers for workflows that use them.  
+- Prefer self-hosted runners with restricted network access and least-privilege access to the DGX cluster.  
+- Rotate secrets and kubeconfigs regularly. Use ephemeral kubeconfigs if your identity provider supports it.
+
+How workflows use these secrets
+- .github/workflows/dgx_nightly_chaos.yml and .github/workflows/dgx_production_validation.yml expect KUBECONFIG_DGX and DGX_IMAGE_REGISTRY to be present.  
+- .github/workflows/dgx_image_scan_and_publish.yml uses COSIGN_KEY and DGX_IMAGE_REGISTRY to sign & push images.
+
diff --git a/docs/runbooks/dgx_sre_alerts.md b/docs/runbooks/dgx_sre_alerts.md
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/docs/runbooks/dgx_sre_alerts.md
@@ -0,0 +1,220 @@
+# DGX SRE Alerts & Runbook
+
+This document explains alerts added for DGX-specific failure modes and recommended SRE responses.
+
+Alerts (mapping to k8s/manifests/monitoring/dgx-prometheus-rules.yaml)
+- DGX_GPU_ECC_Error (severity: critical)
+  - Meaning: ECC volatile errors on GPUs indicate hardware faults.  
+  - Action: cordon the node, collect logs (nvidia-smi -q), file hardware ticket with vendor, and reassign jobs.  
+
+- DGX_NVLINK_Errors (severity: warning)
+  - Meaning: NVLink/NVSwitch errors.  
+  - Action: investigate NVSwitch logs, check cabling/topology, escalate to infra vendor if sustained.
+
+- DGX_Low_GPU_Utilization (severity: warning)
+  - Meaning: low avg GPU utilization indicates scheduling/IO issues.  
+  - Action: check node resource allocator, check checkpoint IO and NVMe bandwidth, run local benchmark to isolate bottleneck.
+
+- DGX_NCCL_Timeouts (severity: critical)
+  - Meaning: NCCL errors likely due to IB/RDMA or driver mismatch.  
+  - Action: run nccl-tests, validate NCCL envs (NCCL_SOCKET_IFNAME, NCCL_IB_HCA), verify driver + CUDA + NCCL ABI compatibility.
+
+- DGX_Checkpoint_PVC_Filling (severity: warning)
+  - Meaning: checkpoint PVC is filling; might block training job.  
+  - Action: offload older checkpoints to S3, increase PVC, or clean stale artifacts; ensure retention policy enforced.
+
+Runbook steps (abbreviated)
+1. Acknowledge alert and determine scope (single node vs cluster).  
+2. For hardware errors: cordon node, kill running jobs, preserve logs, open support ticket.  
+3. For NCCL/IB issues: run scripts/dgx/nccl_tuning.sh, review outputs in artifacts, apply tuned configmap and re-run smoke tests.  
+4. For storage issues: check PVC usage, offload to object store using the aegs artifact uploader script, and re-run job.  
+
+Contact: platform-team@example.com, security@example.com
+
diff --git a/k8s/manifests/monitoring/dgx-prometheus-rules.yaml b/k8s/manifests/monitoring/dgx-prometheus-rules.yaml
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/k8s/manifests/monitoring/dgx-prometheus-rules.yaml
@@ -0,0 +1,220 @@
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: dgx-critical-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: dgx.rules
+      rules:
+        - alert: DGX_GPU_ECC_Error
+          expr: increase(dcgm_gpu_ecc_volatile_total[5m]) > 0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "DGX GPU ECC volatile errors detected"
+            description: "One or more ECC volatile errors on DGX GPUs. Investigate hardware and cordon node."
+
+        - alert: DGX_NVLINK_Errors
+          expr: increase(dcgm_nvlink_error_total[5m]) > 0
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "NVLink/NVSwitch errors observed"
+            description: "NVLink errors detected on DGX nodes. Check NVSwitch and topology."
+
+        - alert: DGX_Low_GPU_Utilization
+          expr: avg_over_time(dcgm_gpu_utilization[30m]) < 10
+          for: 30m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Low GPU utilization on DGX node"
+            description: "GPU utilization below 10% for 30 minutes; investigate scheduling or IO bottlenecks."
+
+        - alert: DGX_NCCL_Timeouts
+          expr: increase(nccl_errors_total[5m]) > 0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "NCCL errors detected"
+            description: "NCCL errors observed; check IB/NIC, NCCL tuning and driver versions."
+
+        - alert: DGX_Checkpoint_PVC_Filling
+          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim="dgx-checkpoints-pvc"} / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="dgx-checkpoints-pvc"} < 0.1
+          for: 15m
+          labels:
+            severity: warning
+          annotations:
+            summary: "DGX checkpoint PVC near capacity"
+            description: "The PVC used for checkpoints is nearly full; ensure checkpoint retention and S3 offload are configured."
+
diff --git a/docs/dgx/IMAGE_MATRIX.md b/docs/dgx/IMAGE_MATRIX.md
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/docs/dgx/IMAGE_MATRIX.md
@@ -0,0 +1,120 @@
+# Validated DGX Image Matrix (canonical record)
+
+Keep this document updated with pinned validated images. Each validated entry must include:
+ - Image reference (registry + tag)
+ - Exact CUDA/cuDNN/NCCL/PyTorch/DeepSpeed versions
+ - Date validated
+ - Validation artifacts location (CI artifact path)
+ - SBOM and cosign signature presence
+
+Example entry (fill in after publishing via scripts/dgx/publish_and_sign_images.sh):
+
+- image: ghcr.io/example/aegis-deepspeed:h100-cuda12.1-pytorch2.2-deepspeed0.9.2
+  cuda: 12.1.1
+  cudnn: 8.9.2
+  nccl: 2.18.0
+  pytorch: 2.2.0
+  deepspeed: 0.9.2
+  validated_on: 2025-12-XX
+  artifacts: artifacts/dgx_prod_validate/...
+  sbom: artifacts/aegis-deepspeed_h100...-sbom.json
+  cosign: true
+
+Record the digest (immutable) and make production promotions reference the digest, not just tag.
+
diff --git a/docs/runbooks/dgx_production_runbook.md b/docs/runbooks/dgx_production_runbook.md
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/docs/runbooks/dgx_production_runbook.md
@@ -0,0 +1,240 @@
+# DGX Production Runbook — Go‑Live Checklist
+
+1) Validate image & provenance
+   - Ensure image exists in DGX_IMAGE_REGISTRY and is signed with cosign.
+   - Confirm SBOM present in CI artifacts. Update docs/dgx/IMAGE_MATRIX.md with digest & details.
+
+2) Driver strategy
+   - If using platform-managed drivers, install GPU Operator with k8s/manifests/dgx/gpu-operator-values-no-driver.yaml (autoInstall=false).
+   - If not, use default gpu-operator-values.yaml (autoInstall=true).
+
+3) NCCL tuning
+   - Run scripts/dgx/nccl_tuning.sh and examine artifacts.
+   - Generate env suggestions and apply via scripts/dgx/apply_nccl_config.sh.
+   - Confirm deepspeed-dgx-job-with-configmap.yaml pulls envFrom the dgx-nccl-config ConfigMap.
+
+4) Storage & checkpoints
+   - Ensure PVC dgx-checkpoints-pvc bound to a fast StorageClass (nvme-fast) or configure S3 offload sidecar.
+   - Configure retention & offload policies.
+
+5) Nightly validation
+   - Confirm .github/workflows/dgx_nightly_chaos.yml runs on self-hosted DGX runners and completes successfully.
+   - Fix any failure, update image matrix and rerun validation.
+
+6) SRE & alerts
+   - Deploy Prometheus rules (k8s/manifests/monitoring/dgx-prometheus-rules.yaml).
+   - Ensure alert routing to on-call channel and runbook accessible.
+
+7) Promote image for production
+   - Promote only digest-pinned image after green validation and add record to IMAGE_MATRIX.md.
+
+End of runbook.
+
