*** Begin Patch
*** Add File: k8s/chaos/chaos-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: agentic-chaos-campaign
+  namespace: aegis
+spec:
+  schedule: "0 3 * * 1" # weekly on Monday at 03:00
+  concurrencyPolicy: Forbid
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: chaos-runner-sa
+          containers:
+            - name: chaos-runner
+              image: aegis/chaos-tools:latest
+              command: ["/bin/sh","-c"]
+              args:
+                - |
+                  set -euo pipefail
+                  /opt/chaos/chaos_matrix_runner.sh --namespace aegis --duration 600
+              volumeMounts:
+                - name: chaos-scripts
+                  mountPath: /opt/chaos
+          restartPolicy: OnFailure
+          volumes:
+            - name: chaos-scripts
+              configMap:
+                name: chaos-scripts
+
*** End Patch
*** Begin Patch
*** Add File: chaos/chaos_matrix_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+usage() {
+  echo "Usage: $0 --namespace <ns> [--duration <secs>]"
+  exit 1
+}
+
+NAMESPACE="aegis"
+DURATION=300
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --duration) DURATION="$2"; shift 2;;
+    *) usage;;
+  esac
+done
+
+echo "Chaos campaign start: namespace=${NAMESPACE}, duration=${DURATION}s"
+end=$((SECONDS + DURATION))
+
+log_start() {
+  mkdir -p /tmp/chaos_evidence
+  kubectl -n ${NAMESPACE} get pods -o wide > /tmp/chaos_evidence/pods_before.txt || true
+  kubectl -n ${NAMESPACE} get pods -l app=transaction-manager -o wide >> /tmp/chaos_evidence/pods_before.txt || true
+}
+
+log_end() {
+  kubectl -n ${NAMESPACE} get pods -o wide > /tmp/chaos_evidence/pods_after.txt || true
+  kubectl -n ${NAMESPACE} logs -l app=transaction-manager --tail=500 > /tmp/chaos_evidence/txmgr_logs.txt || true
+  tar czf /tmp/chaos_evidence.tgz -C /tmp chaos_evidence || true
+  # if S3 env set, upload
+  if [ -n "${COMPLIANCE_BUCKET:-}" ]; then
+    aws s3 cp /tmp/chaos_evidence.tgz s3://${COMPLIANCE_BUCKET}/chaos/$(date +%s)_evidence.tgz || true
+  fi
+}
+
+log_start
+
+while [ $SECONDS -lt $end ]; do
+  # 1) Kill a transaction-manager pod (if exists)
+  TM_POD=$(kubectl -n ${NAMESPACE} get pods -l app=transaction-manager -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+  if [ -n "$TM_POD" ]; then
+    echo "Deleting transaction-manager pod $TM_POD"
+    kubectl -n ${NAMESPACE} delete pod "$TM_POD" --grace-period=0 --force || true
+  fi
+  sleep 15
+
+  # 2) Kill a participant pod
+  PART_POD=$(kubectl -n ${NAMESPACE} get pods -l app=example-tool-adapter -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+  if [ -n "$PART_POD" ]; then
+    echo "Deleting participant pod $PART_POD"
+    kubectl -n ${NAMESPACE} delete pod "$PART_POD" --grace-period=0 --force || true
+  fi
+  sleep 15
+
+  # 3) Simulate node pressure by cordon/uncordon first schedulable node
+  NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+  if [ -n "$NODE" ]; then
+    echo "Cordoning node $NODE"
+    kubectl cordon $NODE || true
+    sleep 10
+    kubectl uncordon $NODE || true
+  fi
+  sleep 10
+done
+
+echo "Chaos campaign finished; collecting evidence"
+log_end
+
*** End Patch
*** Begin Patch
*** Add File: k8s/chaos/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: chaos-scripts
+  namespace: aegis
+data:
+  chaos_matrix_runner.sh: |
+    #!/bin/sh
+    /opt/chaos/chaos_matrix_runner.sh "$@"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/chaos_campaign.yml
+name: Agentic Chaos Campaign (manual)
+on:
+  workflow_dispatch:
+    inputs:
+      namespace:
+        required: false
+        default: "aegis"
+      duration:
+        required: false
+        default: "300"
+
+jobs:
+  run-chaos:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup kubectl & aws
+        run: |
+          sudo apt-get update && sudo apt-get install -y awscli jq
+      - name: Trigger chaos CronJob (k8s)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          kubectl apply -f k8s/chaos/chaos-cronjob.yaml
+          # create a job run now
+          kubectl -n aegis create job --from=cronjob/agentic-chaos-campaign agentic-chaos-run-$(date +%s)
+          # wait for job to complete
+          JOBNAME=$(kubectl -n aegis get jobs -l job-name=agentic-chaos-run- -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+          echo "Launched job: $JOBNAME"
+          kubectl -n aegis wait --for=condition=complete job/$JOBNAME --timeout=600s || true
+      - name: Fetch evidence artifact from S3 (if any)
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          # list recent objects under chaos/
+          aws s3 ls s3://${COMPLIANCE_BUCKET}/chaos/ || true
+
*** End Patch
*** Begin Patch
*** Add File: participant/tuning.yml
+---
+# Participant SDK tuning defaults
+prepare_timeout_seconds: 60
+commit_timeout_seconds: 60
+max_prepare_retries: 3
+max_commit_retries: 3
+backoff_initial_seconds: 1
+backoff_multiplier: 2.0
+metrics_push_interval_seconds: 15
+
*** End Patch
*** Begin Patch
*** Add File: participant/timeout_backoff.py
+#!/usr/bin/env python3
+"""
+Small helper applying tuned retry/backoff semantics for participant SDK operations.
+Read defaults from participant/tuning.yml if present.
+"""
+import yaml, time
+from functools import wraps
+
+DEFAULTS = {
+    "prepare_timeout_seconds": 60,
+    "commit_timeout_seconds": 60,
+    "max_prepare_retries": 3,
+    "max_commit_retries": 3,
+    "backoff_initial_seconds": 1,
+    "backoff_multiplier": 2.0
+}
+
+try:
+    cfg = yaml.safe_load(open("participant/tuning.yml"))
+    DEFAULTS.update(cfg or {})
+except Exception:
+    pass
+
+def retry_with_backoff(max_retries, initial, multiplier):
+    def deco(fn):
+        @wraps(fn)
+        def wrapper(*args, **kwargs):
+            delay = initial
+            for attempt in range(1, max_retries + 1):
+                try:
+                    return fn(*args, **kwargs)
+                except Exception as e:
+                    if attempt == max_retries:
+                        raise
+                    time.sleep(delay)
+                    delay = int(delay * multiplier)
+        return wrapper
+    return deco
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/2pc_integration.yml
+name: 2PC Integration Tests (CI)
+on:
+  workflow_dispatch:
+
+jobs:
+  run-2pc:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        run: python -m pip install --upgrade pip && pip install requests uvicorn fastapi pytest
+      - name: Run 2PC integration script
+        run: |
+          chmod +x tests/run_2pc_integration.sh
+          ./tests/run_2pc_integration.sh
+      - name: Upload logs
+        if: failure()
+        uses: actions/upload-artifact@v4
+        with:
+          name: 2pc-logs
+          path: /tmp/*.log
+
*** End Patch
*** Begin Patch
*** Add File: configs/accelerate_pilot.yaml
+compute_environment: "MULTI_NODE"
+distributed_type: MULTI_GPU
+mixed_precision: "fp16"
+downcast_bfloat16: "no"
+machine_rank: 0
+main_process_ip: "MAIN_PROCESS_IP_PLACEHOLDER"
+main_process_port: 29500
+num_machines: 2
+num_processes: 2
+use_cpu: false
+fsdp: "no"
+# Tuning
+gradient_accumulation_steps: 4
+deepspeed: null
+zero_stage: 0
+checkpoint_interval_minutes: 10
+
*** End Patch
*** Begin Patch
*** Add File: k8s/rlhf/mpi_job_pilot.yaml
+apiVersion: kubeflow.org/v1
+kind: MPIJob
+metadata:
+  name: rlhf-mpi-pilot
+  namespace: aegis-ml
+spec:
+  slotsPerWorker: 1
+  mpiReplicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+        spec:
+          containers:
+            - name: launcher
+              image: aegis/rlhf:latest
+              command:
+                - "/bin/sh"
+                - "-c"
+                - |
+                  set -euo pipefail
+                  accelerate launch --config_file /opt/accelerate/pilot_config.yaml rl/pilot_train.py --model-name "${MODEL_NAME}" --output-dir /tmp/rlhf_out --epochs 1 --per-device-batch-size 2
+              env:
+                - name: MODEL_NAME
+                  value: "distilgpt2"
+                - name: MLFLOW_TRACKING_URI
+                  valueFrom:
+                    secretKeyRef:
+                      name: mlflow-secrets
+                      key: tracking-uri
+          restartPolicy: Never
+    Worker:
+      replicas: 2
+      template:
+        spec:
+          containers:
+            - name: worker
+              image: aegis/rlhf:latest
+              command: ["/bin/sh","-c","sleep 999999"]
+              resources:
+                limits:
+                  nvidia.com/gpu: 1
+
*** End Patch
*** Begin Patch
*** Add File: rl/pilot_train.py
+#!/usr/bin/env python3
+"""
+Pilot training script used for multi-node runs. Includes resume support and MLflow integration.
+"""
+import argparse, os, tempfile, tarfile
+from datasets import Dataset
+from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
+import mlflow
+from rl.checkpoint_validate import sha256
+
+def make_dataset():
+    texts = ["Hello world"] * 100
+    return Dataset.from_list([{"text": t} for t in texts])
+
+def tokenize_function(examples, tokenizer, max_length=128):
+    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_length)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-name", default=os.environ.get("MODEL_NAME","distilgpt2"))
+    p.add_argument("--output-dir", default="/tmp/rlhf_out")
+    p.add_argument("--epochs", type=int, default=1)
+    p.add_argument("--per-device-batch-size", type=int, default=2)
+    p.add_argument("--resume-checkpoint", default=None)
+    args = p.parse_args()
+
+    mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI")
+    if mlflow_uri:
+        mlflow.set_tracking_uri(mlflow_uri)
+
+    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
+    model = AutoModelForCausalLM.from_pretrained(args.model_name)
+
+    ds = make_dataset()
+    ds = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)
+    ds.set_format(type="torch", columns=["input_ids","attention_mask"])
+
+    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
+    training_args = TrainingArguments(
+        output_dir=args.output_dir,
+        per_device_train_batch_size=args.per_device_batch_size,
+        num_train_epochs=args.epochs,
+        logging_steps=10,
+        save_steps=50,
+        save_total_limit=3,
+        fp16=True,
+    )
+
+    trainer = Trainer(model=model, args=training_args, train_dataset=ds, data_collator=data_collator)
+
+    with mlflow.start_run():
+        mlflow.log_param("model_name", args.model_name)
+        if args.resume_checkpoint:
+            trainer.train(resume_from_checkpoint=args.resume_checkpoint)
+            mlflow.log_param("resumed_from", args.resume_checkpoint)
+        else:
+            trainer.train()
+        trainer.save_model(args.output_dir)
+        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".tar.gz")
+        with tarfile.open(tmp.name, "w:gz") as tf:
+            tf.add(args.output_dir, arcname="model")
+        checksum = sha256(tmp.name)
+        mlflow.log_param("checkpoint_sha256", checksum)
+        mlflow.log_artifact(tmp.name, artifact_path="checkpoints")
+        print("Checkpoint created:", tmp.name)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/rlhf_full_pilot.yml
+name: RLHF Full Pilot (pilot + gating)
+on:
+  workflow_dispatch:
+    inputs:
+      mode:
+        required: true
+        default: "k8s" # k8s or local
+      model_name:
+        required: false
+        default: "distilgpt2"
+
+jobs:
+  pilot-run:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install transformers datasets mlflow accelerate
+      - name: Launch pilot
+        env:
+          MODE: ${{ github.event.inputs.mode }}
+          MODEL_NAME: ${{ github.event.inputs.model_name }}
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          if [ "${MODE}" = "k8s" ]; then
+            kubectl apply -f k8s/rlhf/mpi_job_pilot.yaml -n aegis-ml
+            kubectl wait --for=condition=complete mpijob/rlhf-mpi-pilot -n aegis-ml --timeout=2h || true
+            # fetch logs from launcher pod
+            LAUNCHER=$(kubectl -n aegis-ml get pods -l mpi-job-name=rlhf-mpi-pilot,role=launcher -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+            if [ -n "$LAUNCHER" ]; then
+              kubectl -n aegis-ml logs $LAUNCHER --tail=200 || true
+            fi
+          else
+            accelerate launch --num_processes 2 rl/pilot_train.py --model-name "${MODEL_NAME}" --output-dir "/tmp/rlhf_pilot" --epochs 1
+          fi
+      - name: Validate checkpoint and run adversarial gate
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          CKPT=$(ls /tmp/rlhf_pilot/*.tar.gz 2>/dev/null || true)
+          if [ -n "$CKPT" ]; then
+            python rl/checkpoint_validate.py --ckpt "$CKPT"
+            python scripts/adversarial_gate_runner.py
+          else
+            echo "No local checkpoint found; inspect MLflow for artifacts"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/evidence_collector.py
+#!/usr/bin/env python3
+"""
+Evidence collector for chaos and RLHF pilot runs.
+ - Collects logs, DB snapshots, Prometheus queries and uploads to S3 (COMPLIANCE_BUCKET).
+ - Intended to be run from CI or an operator host with access to KUBECONFIG and AWS creds.
+"""
+import os, subprocess, tempfile, json
+import boto3
+from datetime import datetime
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+K8S_NS = os.environ.get("K8S_NS", "aegis")
+PROM_URL = os.environ.get("PROM_URL")
+
+def run(cmd):
+    return subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT).decode()
+
+def collect():
+    outdir = tempfile.mkdtemp(prefix="evidence_")
+    # pods list
+    open(f"{outdir}/pods.txt","w").write(run(f"kubectl -n {K8S_NS} get pods -o wide"))
+    # tx manager logs
+    try:
+        open(f"{outdir}/txmgr_logs.txt","w").write(run(f"kubectl -n {K8S_NS} logs -l app=transaction-manager --tail=500"))
+    except Exception as e:
+        open(f"{outdir}/txmgr_logs.txt","w").write("failed to collect txmgr logs: "+str(e))
+    # participant DB snapshot (if PVC available mount path known)
+    # collect prom metrics for tx failures
+    if PROM_URL:
+        q = 'increase(agent_plan_failures_total[10m])'
+        try:
+            r = run(f"curl -s '{PROM_URL}/api/v1/query?query={q}'")
+            open(f"{outdir}/prom_tx_failures.json","w").write(r)
+        except Exception as e:
+            open(f"{outdir}/prom_tx_failures.json","w").write("prom fetch failed: "+str(e))
+    # tar and upload
+    tgz = f"/tmp/evidence_{int(datetime.utcnow().timestamp())}.tgz"
+    run(f"tar czf {tgz} -C {outdir} .")
+    if COMPLIANCE_BUCKET:
+        s3 = boto3.client("s3")
+        key = f"evidence/{os.path.basename(tgz)}"
+        s3.upload_file(tgz, COMPLIANCE_BUCKET, key)
+        print("Uploaded evidence to s3://{}/{}".format(COMPLIANCE_BUCKET, key))
+    else:
+        print("Evidence available at", tgz)
+
+if __name__=="__main__":
+    collect()
+
*** End Patch
*** Begin Patch
*** Add File: approval/orchestrator_api.py
+#!/usr/bin/env python3
+"""
+Small Approval Orchestrator API patch that adds:
+ - delegation of approvals
+ - expiry timestamps
+ - auto-escalation rules
+
+This is a light-weight in-process example for staging and CI tests.
+"""
+from flask import Flask, request, jsonify
+import time, threading, uuid
+
+app = Flask("approval-orchestrator")
+
+# In-memory store for demo; replace with persistent DB in production.
+APPROVALS = {}
+USERS = {
+    "alice": {"roles": ["approver"]},
+    "bob": {"roles": []},
+    "manager": {"roles": ["approver","escalate"]}
+}
+
+def now_ts():
+    return int(time.time())
+
+@app.post("/request")
+def request_approval():
+    j = request.get_json() or {}
+    model = j.get("model")
+    requested_by = j.get("requested_by", "unknown")
+    expiry = int(j.get("expiry_seconds", 3600))
+    approval_id = str(uuid.uuid4())
+    APP_APPROVAL = {"id": approval_id, "model": model, "requested_by": requested_by, "status":"pending", "created": now_ts(), "expiry_at": now_ts() + expiry, "delegated_to": None, "history": []}
+    APPROVALS[approval_id] = APP_APPROVAL
+    return jsonify({"ok": True, "approval_id": approval_id})
+
+@app.post("/delegate")
+def delegate():
+    j = request.get_json() or {}
+    approval_id = j.get("approval_id")
+    to_user = j.get("to_user")
+    if approval_id not in APPROVALS:
+        return jsonify({"ok": False, "error":"not_found"}), 404
+    APPROVALS[approval_id]["delegated_to"] = to_user
+    APPROVALS[approval_id]["history"].append({"event":"delegated","to":to_user,"ts":now_ts()})
+    return jsonify({"ok": True})
+
+@app.post("/approve")
+def approve():
+    j = request.get_json() or {}
+    approval_id = j.get("approval_id")
+    user = j.get("user")
+    if approval_id not in APPROVALS:
+        return jsonify({"ok": False, "error":"not_found"}), 404
+    apprec = APPROVALS[approval_id]
+    # check delegation / expiry
+    if apprec["delegated_to"] and apprec["delegated_to"] != user and "escalate" not in USERS.get(user, {}).get("roles", []):
+        return jsonify({"ok": False, "error":"not_authorized"}), 403
+    if now_ts() > apprec["expiry_at"]:
+        return jsonify({"ok": False, "error":"expired"}), 410
+    apprec["status"] = "approved"
+    apprec["history"].append({"event":"approved","by":user,"ts":now_ts()})
+    return jsonify({"ok": True})
+
+@app.post("/auto_escalate")
+def auto_escalate():
+    """
+    Trigger auto-escalation: find pending approvals past threshold and mark for escalation / notify
+    """
+    thresh = int(request.args.get("thresh_seconds", "600"))
+    now = now_ts()
+    escalated = []
+    for aid, a in APPROVALS.items():
+        if a["status"] == "pending" and now - a["created"] > thresh:
+            a["history"].append({"event":"auto_escalated","ts":now})
+            a["delegated_to"] = "manager"
+            escalated.append(aid)
+    return jsonify({"ok": True, "escalated": escalated})
+
+@app.get("/status/<approval_id>")
+def status(approval_id):
+    if approval_id not in APPROVALS:
+        return jsonify({"ok": False, "error":"not_found"}), 404
+    return jsonify(APPROVALS[approval_id])
+
+@app.get("/health")
+def health():
+    return jsonify({"ok": True, "pending": sum(1 for a in APPROVALS.values() if a["status"]=="pending")})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=8305)
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/approval_escalation_test.yml
+name: Approval Delegation / Escalation Test (CI)
+on:
+  workflow_dispatch:
+
+jobs:
+  approval-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup deps
+        run: python -m pip install --upgrade pip && pip install requests
+      - name: Start orchestrator (background)
+        run: |
+          python approval/orchestrator_api.py >/tmp/approval.log 2>&1 &
+          sleep 1
+      - name: Request approval
+        run: |
+          python - <<PY
+import requests, time
+r = requests.post("http://127.0.0.1:8305/request", json={"model":"test-model","requested_by":"alice","expiry_seconds":30})
+aid = r.json().get("approval_id")
+print("Approval id:", aid)
+# delegate to bob
+requests.post("http://127.0.0.1:8305/delegate", json={"approval_id":aid,"to_user":"bob"})
+# try approve as bob (should be allowed because delegated)
+resp = requests.post("http://127.0.0.1:8305/approve", json={"approval_id":aid,"user":"bob"})
+print("Bob approve:", resp.status_code, resp.text)
+# create another approval and wait for auto_escalate
+rid = requests.post("http://127.0.0.1:8305/request", json={"model":"slow-model","requested_by":"alice","expiry_seconds":300}).json().get("approval_id")
+print("Second approval:", rid)
+time.sleep(2)
+requests.post("http://127.0.0.1:8305/auto_escalate?thresh_seconds=1")
+print("Escalation triggered")
+print(requests.get("http://127.0.0.1:8305/status/%s" % rid).text)
+PY
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/chaos_and_rlhf_runbook.md
+# Chaos Campaign & RLHF Pilot Runbook (operator)
+
+Purpose
+- Run the scheduled chaos campaign to validate 2PC invariants and then run an RLHF pilot only after chaos results are acceptable.
+
+Preflight
+- Ensure KUBECONFIG_STAGING secret is configured in CI and operator has kube access.
+- Ensure COMPLIANCE_BUCKET is configured and IAM role has PutObject permission.
+- Ensure MLFLOW_TRACKING_URI and aegis/rlhf:latest image are available.
+
+Step 1 — Run chaos campaign (recommended weekly)
+1. From GitHub Actions UI trigger "Agentic Chaos Campaign (manual)" or run kubectl job:
+   kubectl -n aegis create job --from=cronjob/agentic-chaos-campaign agentic-chaos-now-$(date +%s)
+2. Wait for job completion (kubectl -n aegis wait --for=condition=complete job/<name> --timeout=600s)
+3. Evidence artifact will be uploaded to s3://<COMPLIANCE_BUCKET>/chaos/ or can be collected with scripts/evidence_collector.py
+4. Review txmgr_logs.txt and Prometheus metrics. If stuck transactions appear, follow 2PC remediation steps.
+
+Step 2 — Run RLHF pilot (after chaos pass)
+1. Trigger GitHub Action "RLHF Full Pilot (pilot + gating)" with mode=k8s and model_name set.
+2. Monitor MPIJob launcher pod logs (kubectl -n aegis-ml logs <launcher-pod>)
+3. After completion, ensure checkpoint tar was created and pick up MLflow run in UI.
+4. Run adversarial harness (CI step included) — promotion blocked if it fails.
+
+Post-run
+- Upload any artifacts to COMPLIANCE_BUCKET and link to ticketing system for record.
+- If issues found in chaos campaign: pause RLHF pilots until fixes are implemented.
+
*** End Patch
*** End Patch
