*** Begin Patch
*** Add File: k8s/jobs/provider-endtoend-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: provider-endtoend-verify
+  namespace: aegis
+  labels:
+    app: provider-verify
+spec:
+  backoffLimit: 3
+  template:
+    metadata:
+      labels:
+        app: provider-verify
+    spec:
+      # run on a secure node pool for provider access (adjust nodeSelector to your cluster)
+      nodeSelector:
+        aegis/secure-runner: "true"
+      # service account should have least-privilege RBAC to read Vault secret, create Pods if needed, etc.
+      serviceAccountName: quantum-controller
+      restartPolicy: Never
+      containers:
+        - name: provider-verify
+          image: ghcr.io/yourorg/aegis-tools:latest
+          imagePullPolicy: IfNotPresent
+          command: ["python", "scripts/provider_endtoend_verify.py", "--provider", "ibm", "--out", "/results"]
+          env:
+            - name: VAULT_ADDR
+              valueFrom:
+                secretKeyRef:
+                  name: vault-credentials
+                  key: addr
+            - name: VAULT_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: vault-credentials
+                  key: token
+            - name: REKOR_URL
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-credentials
+                  key: url
+            - name: LOCAL_KEY
+              valueFrom:
+                secretKeyRef:
+                  name: signing-key
+                  key: private_key_path  # this can reference a mounted secret path if using CSI
+          volumeMounts:
+            - name: results
+              mountPath: /results
+      volumes:
+        - name: results
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/provider-e2e-selfhosted.yml
+name: Provider E2E Verify (self-hosted)
+on:
+  workflow_dispatch:
+
+jobs:
+  provider-e2e:
+    name: Provider End-to-end Verify
+    runs-on: [self-hosted, qpu-adjacent]
+    environment: self-hosted
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.11"
+
+      - name: Install dependencies
+        run: |
+          python -m pip install --upgrade pip
+          pip install -r aegis_multimodal_ai_system/requirements.txt || true
+          pip install qiskit qiskit-ibm-runtime boto3 || true
+
+      - name: Run provider end-to-end verify
+        env:
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+          BRK_S3_BUCKET: ${{ secrets.BRK_S3_BUCKET }}
+          LOCAL_KEY: ${{ secrets.LOCAL_SIGNING_KEY_PATH }}
+        run: |
+          mkdir -p provider_results
+          python scripts/provider_endtoend_verify.py --provider ibm --out provider_results || true
+          ls -la provider_results
+
+      - name: Upload results
+        uses: actions/upload-artifact@v4
+        with:
+          name: provider-e2e-results
+          path: provider_results/**
+
*** End Patch
*** Begin Patch
*** Add File: terraform/vault/variables.tf
+variable "vault_addr" {
+  type = string
+  description = "Vault server address (example: https://vault.example.com)"
+}
+
+variable "provider_name" {
+  type = string
+  description = "Name of the quantum provider (e.g., ibm, braket)"
+  default = "ibm"
+}
+
+variable "approle_name" {
+  type = string
+  description = "AppRole role name for the controller"
+  default = "quantum-controller"
+}
+
*** End Patch
*** Begin Patch
*** Add File: terraform/vault/provider_policy_and_approle.tf
+terraform {
+  required_providers {
+    vault = {
+      source  = "hashicorp/vault"
+      version = "~> 3.0"
+    }
+  }
+}
+
+provider "vault" {
+  address = var.vault_addr
+  # token should be provided via environment VAULT_TOKEN for admin operator
+}
+
+# Policy to allow reading provider secrets and creating scoped tokens (minimal)
+resource "vault_policy" "provider_read" {
+  name   = "quantum-provider-read"
+  policy = <<-EOT
+path "secret/data/quantum/providers/*" {
+  capabilities = ["read"]
+}
+
+path "auth/token/create" {
+  capabilities = ["update"]
+}
+EOT
+}
+
+# Create an AppRole role for controllers/workers (least-privilege)
+resource "vault_approle_auth_backend_role" "controller" {
+  role_name                   = var.approle_name
+  token_policies              = [vault_policy.provider_read.name]
+  token_ttl                   = "1h"
+  token_max_ttl               = "4h"
+  secret_id_ttl               = "10m"
+  secret_id_num_uses          = 10
+  bind_secret_id              = true
+  period                      = ""
+  enforce_token_policies      = true
+}
+
+# Generate a Secret ID for operator to distribute securely to bootstrap mechanisms
+resource "vault_approle_auth_backend_role_secret_id" "controller_secret_id" {
+  role_name = vault_approle_auth_backend_role.controller.role_name
+  depends_on = [vault_approle_auth_backend_role.controller]
+}
+
+# Data source to fetch the role_id for the created role (may require appropriate privileges)
+data "vault_approle_auth_backend_role_id" "controller_role_id" {
+  role_name = vault_approle_auth_backend_role.controller.role_name
+}
+
+output "approle_role_id" {
+  description = "Role ID for the controller AppRole (use together with secret_id)"
+  value       = data.vault_approle_auth_backend_role_id.controller_role_id.role_id
+}
+
+output "approle_secret_id" {
+  description = "Secret ID (one-time) for the AppRole (sensitive)"
+  value       = vault_approle_auth_backend_role_secret_id.controller_secret_id.secret_id
+  sensitive   = true
+}
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/redis-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-redis
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://charts.bitnami.com/bitnami
+    chart: redis
+    targetRevision: 18.0.0
+    helm:
+      values: |
+        architecture: "replication"
+        replica:
+          replicaCount: 2
+        persistence:
+          enabled: true
+          size: 2Gi
+        resources:
+          requests:
+            memory: "256Mi"
+            cpu: "200m"
+          limits:
+            memory: "1Gi"
+            cpu: "1"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/prometheus-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-prometheus
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: monitoring
+  source:
+    repoURL: https://github.com/prometheus-community/helm-charts
+    chart: kube-prometheus-stack
+    targetRevision: "47.3.0"
+    helm:
+      values: |
+        prometheus:
+          prometheusSpec:
+            retention: "15d"
+            resources:
+              requests:
+                cpu: "500m"
+                memory: "1Gi"
+              limits:
+                cpu: "2"
+                memory: "4Gi"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/grafana-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-grafana
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: observability
+  source:
+    repoURL: https://grafana.github.io/helm-charts
+    chart: grafana
+    targetRevision: 8.5.0
+    helm:
+      values: |
+        persistence:
+          enabled: true
+          size: 5Gi
+        resources:
+          requests:
+            cpu: "200m"
+            memory: "512Mi"
+          limits:
+            cpu: "1"
+            memory: "1Gi"
+        adminUser:
+          existingSecret: grafana-admin
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/oauth2proxy-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-oauth2-proxy
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://oauth2-proxy.github.io/manifests
+    chart: oauth2-proxy
+    targetRevision: "7.4.0"
+    helm:
+      values: |
+        config:
+          provider: "oidc"
+          oidc_issuer_url: "https://accounts.example.com"
+        ingress:
+          enabled: false
+        resources:
+          requests:
+            cpu: "100m"
+            memory: "128Mi"
+          limits:
+            cpu: "500m"
+            memory: "512Mi"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/fluentd-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-fluentd
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: kube-system
+  source:
+    repoURL: https://charts.bitnami.com/bitnami
+    chart: fluentd
+    targetRevision: 2.5.0
+    helm:
+      values: |
+        resources:
+          requests:
+            cpu: "100m"
+            memory: "256Mi"
+          limits:
+            cpu: "500m"
+            memory: "1Gi"
+        persistence:
+          enabled: false
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: docs/deploy_infra_checklist_phase3.md
+# Deploy provider verification CI & infra (checklist)
+
+This document lists the steps and recommended settings to deploy the provider test job, Vault AppRole, and ArgoCD/Helm apps for Redis HA, Prometheus, Grafana, OAuth2 Proxy and Fluentd.
+
+1) Provider E2E Job (Kubernetes)
+ - The manifest k8s/jobs/provider-endtoend-job.yaml is a Kubernetes Job that runs the provider_endtoend_verify script.
+ - Create the following secrets in namespace `aegis`:
+   - vault-credentials (keys: addr, token) — token should be a scoped token with read access to provider secrets (not admin)
+   - rekor-credentials (keys: url) — optional
+   - signing-key (key: private_key_path) — if using local key for sign step
+ - To run manually:
+   kubectl apply -f k8s/jobs/provider-endtoend-job.yaml
+   kubectl logs -f job/provider-endtoend-verify -n aegis
+
+2) GitHub Actions job (self-hosted)
+ - The workflow .github/workflows/provider-e2e-selfhosted.yml runs on your self-hosted runners with label `qpu-adjacent`.
+ - Ensure the runner is provisioned in a secure environment with VM-level network access to providers.
+ - Set repository secrets: VAULT_ADDR, VAULT_TOKEN (scoped), REKOR_URL, BRK_S3_BUCKET, LOCAL_SIGNING_KEY_PATH.
+ - Trigger the workflow from the Actions UI.
+
+3) Vault AppRole (Terraform)
+ - Use terraform/vault/* to provision:
+   - vault_policy "quantum-provider-read"
+   - vault_approle_auth_backend_role "quantum-controller"
+   - vault_approle_auth_backend_role_secret_id (generated one-time secret id)
+ - Provide outputs approle_role_id and approle_secret_id to the operator to bootstrap controller or rotator.
+ - Ensure VAULT_TOKEN used by Terraform is an admin-capable token (operator-only).
+
+4) Deploy infra via ArgoCD (Helm charts)
+ - Apply ArgoCD Application manifests in argo/apps/*.yaml to your ArgoCD instance.
+ - Adjust chart values (persistence sizes, resource requests/limits, replica counts) to match your cluster capacity.
+ - Ensure namespace creation: `aegis`, `monitoring`, `observability`, etc.
+ - Create Kubernetes Secrets for Grafana admin, OAuth2 cookie secret, and Vault credentials before sync.
+
+5) Validation & tests
+ - Run provider_stress_test.py from a self-hosted runner to exercise provider rate limits.
+ - Run scripts/redis_failover_test.py to validate Redis HA and queue persistence.
+ - Run provider_endtoend_verify.py and confirm results artifact, signature and Rekor entry exist.
+
+6) Prometheus / Grafana
+ - After Argo sync, import grafana/dashboards/quantum_dashboard.json and tune panels.
+ - Apply alerting rules prometheus/rules/quantum_alerts.yaml (if using kube-prometheus stack, add as PrometheusRule).
+
+7) RBAC & Security
+ - Ensure ServiceAccount `quantum-controller` is bound to k8s/rbac/quantum-controller-rbac.yaml.
+ - Ensure admin group `aegis-admins` exists and is used for admin UI rolebinding.
+
+Notes:
+ - Provider credentials and AppRole secret_id must be transmitted securely (out of band) to the operator that configures the rotator or controller.
+ - Prefer provider sandboxes for initial testing to avoid billing surprises.
+
*** End Patch
*** End Patch
