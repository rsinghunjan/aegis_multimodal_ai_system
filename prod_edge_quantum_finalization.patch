*** Begin Patch
*** Add File: infra/kms/aws_kms.tf
+/*
+Terraform snippet to provision an AWS KMS key for cosign signing and an IAM role for CI to use the key.
+This is a starter template — update provider configuration, VPC, and policy ARNs to suit your environment.
+*/
+terraform {
+  required_providers {
+    aws = {
+      source  = "hashicorp/aws"
+      version = "~> 4.0"
+    }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_kms_key" "cosign" {
+  description             = "KMS key for cosign signing used by Aegis CI"
+  deletion_window_in_days = 30
+  enable_key_rotation     = true
+  policy = data.aws_iam_policy_document.cosign_kms_policy.json
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign"
+  target_key_id = aws_kms_key.cosign.key_id
+}
+
+data "aws_iam_policy_document" "cosign_kms_policy" {
+  statement {
+    sid = "AllowUseByCI"
+    principals {
+      type        = "AWS"
+      identifiers = [var.ci_service_account_arn]
+    }
+    actions = [
+      "kms:Sign",
+      "kms:Verify",
+      "kms:GetPublicKey",
+      "kms:DescribeKey"
+    ]
+    resources = ["*"]
+  }
+}
+
+output "cosign_kms_key_uri" {
+  value = "awskms:///${aws_kms_key.cosign.key_id}"
+}
+
+variable "aws_region" {
+  type    = string
+  default = "us-west-2"
+}
+
+variable "ci_service_account_arn" {
+  type = string
+}
+
*** End Patch
*** Begin Patch
*** Add File: infra/kms/README.md
+AWS KMS + Cosign deployment notes
+
+- Use infra/kms/aws_kms.tf to provision a KMS key and alias alias/aegis-cosign.
+- After apply, export the COSIGN_KMS_KEY to CI secrets (e.g., GitHub Actions).
+- Ensure the CI service account principal (ci_service_account_arn) has kms:Sign permission as declared in the key policy.
+- Rotate keys using AWS KMS automatic key rotation for symmetric keys (for asym keys, create a new key and update alias).
+- Keep a documented key rotation schedule and an audit of who rotated keys. Use CloudTrail logs for auditing key usage.
+
*** End Patch
*** Begin Patch
*** Add File: infra/kms/rotate_kms_and_update_github.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate cosign KMS key by creating a new KMS key and switching alias to point to it,
+# then update GitHub repo secret COSIGN_KMS_KEY to the new key ARN (requires gh CLI auth).
+#
+NEW_KEY_ID=$(aws kms create-key --description "Rotated cosign key for aegis" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name alias/aegis-cosign --target-key-id "${NEW_KEY_ID}"
+COSIGN_KMS_URI="awskms:///${NEW_KEY_ID}"
+echo "New COSIGN_KMS_KEY: ${COSIGN_KMS_URI}"
+
+# Update GitHub secret (requires GH token with repo:admin)
+if command -v gh >/dev/null 2>&1; then
+  REPO="${1:-owner/repo}"
+  echo "Updating GitHub Actions secret COSIGN_KMS_KEY for ${REPO}"
+  gh secret set COSIGN_KMS_KEY --repo "${REPO}" --body "${COSIGN_KMS_URI}"
+else
+  echo "gh CLI not found: please update CI secret COSIGN_KMS_KEY to ${COSIGN_KMS_URI}"
+fi
+
+echo "Rotation complete. Audit CloudTrail for kms:Sign usage and verify CI jobs still pass."
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/aws_cloudhsm.tf
+/*
+CloudHSM cluster scaffold (AWS). CloudHSM requires VPC/subnet and careful network placement.
+This TF snippet is intentionally minimal — CloudHSM setup often needs manual steps for backups & partitions.
+*/
+resource "aws_cloudhsm_v2_cluster" "aegis" {
+  hsm_type = "hsm1.medium"
+  subnet_ids = var.subnet_ids
+  vpc_id     = var.vpc_id
+}
+
+output "cloudhsm_cluster_id" {
+  value = aws_cloudhsm_v2_cluster.aegis.cluster_id
+}
+
+variable "subnet_ids" {
+  type = list(string)
+}
+variable "vpc_id" {
+  type = string
+}
+
*** End Patch
*** Begin Patch
*** Add File: scripts/runner/ansible/provision_runners.yml
+- name: Provision Jetson/ARM self-hosted runners (secure defaults)
+  hosts: runners
+  become: true
+  vars:
+    runner_user: aegis-runner
+    runner_service: github-runner
+    repo_url: "https://github.com/owner/repo"
+  tasks:
+    - name: Ensure packages installed
+      apt:
+        name:
+          - docker.io
+          - jq
+          - python3-venv
+        state: present
+        update_cache: true
+
+    - name: Create runner user
+      user:
+        name: "{{ runner_user }}"
+        shell: /bin/bash
+        create_home: true
+
+    - name: Add runner_user to docker group
+      user:
+        name: "{{ runner_user }}"
+        groups: docker
+        append: yes
+
+    - name: Download and install GitHub Actions runner
+      become_user: "{{ runner_user }}"
+      shell: |
+        set -e
+        ARCH=$(uname -m)
+        mkdir -p ~/actions-runner && cd ~/actions-runner
+        curl -O -L https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-linux-${ARCH}-2.308.0.tar.gz
+        tar xzf actions-runner-linux-${ARCH}-2.308.0.tar.gz
+      args:
+        creates: "/home/{{ runner_user }}/actions-runner"
+
+    - name: Configure runner (token must be provided via vault/secret)
+      become_user: "{{ runner_user }}"
+      shell: |
+        cd ~/actions-runner
+        ./config.sh --url "{{ repo_url }}" --token "{{ runner_token }}" --name "{{ inventory_hostname }}" --labels "self-hosted,jetson,{{ ansible_architecture }}" --unattended
+      args:
+        creates: "/home/{{ runner_user }}/actions-runner/.credentials"
+
+    - name: Create systemd unit for runner
+      copy:
+        dest: /etc/systemd/system/github-runner.service
+        content: |
+          [Unit]
+          Description=GitHub Actions Runner
+          After=network.target
+
+          [Service]
+          Type=simple
+          User={{ runner_user }}
+          WorkingDirectory=/home/{{ runner_user }}/actions-runner
+          ExecStart=/home/{{ runner_user }}/actions-runner/run.sh
+          Restart=always
+          RestartSec=5
+
+          [Install]
+          WantedBy=multi-user.target
+
+    - name: Start and enable runner service
+      systemd:
+        name: github-runner
+        enabled: yes
+        state: started
+
+    - name: Harden SSH (disable password auth)
+      lineinfile:
+        path: /etc/ssh/sshd_config
+        regexp: '^PasswordAuthentication'
+        line: 'PasswordAuthentication no'
+      notify: Restart ssh
+
+  handlers:
+    - name: Restart ssh
+      service:
+        name: ssh
+        state: restarted
+
*** End Patch
*** Begin Patch
*** Add File: runners/README.md
+Runner provisioning & maintenance
+
+Usage:
+- Populate inventory with runner hosts and create an Ansible vault for runner_token per host.
+- Run: ansible-playbook -i inventory provision_runners.yml --ask-vault-pass
+
+Security & network placement:
+- Place runners in private subnets; restrict outbound traffic to only required provider endpoints (GitHub, artifact storage).
+- Use host-level firewall to restrict inbound SSH to bastion hosts.
+- Run container workloads with rootless Docker where possible.
+
+Maintenance:
+- Automate OS updates during maintenance windows.
+- Monitor runner disk, CPU, and memory usage and rotate runners offline for kernel updates.
+
*** End Patch
*** Begin Patch
*** Add File: quantum/adapters/hardening_wrapper.py
+"""
+Adapter hardening wrapper:
+- Adds timeouts, retries with jitter, exponential backoff, token-bucket throttling, and unified error mapping.
+Use by wrapping existing adapter instances: adapter = HardenedAdapter(original_adapter, throttler, timeout=60)
+"""
+import time
+import random
+import threading
+import logging
+from functools import wraps
+
+LOG = logging.getLogger("aegis.adapter.harden")
+
+class TokenBucket:
+    def __init__(self, rate, burst):
+        self.rate = rate
+        self.burst = burst
+        self.tokens = burst
+        self.lock = threading.Lock()
+        self.last = time.time()
+
+    def consume(self, n=1):
+        with self.lock:
+            now = time.time()
+            elapsed = now - self.last
+            self.tokens = min(self.burst, self.tokens + elapsed * self.rate)
+            self.last = now
+            if self.tokens >= n:
+                self.tokens -= n
+                return True
+            return False
+
+def with_timeout(timeout):
+    def decorator(fn):
+        @wraps(fn)
+        def wrapped(*args, **kwargs):
+            # naive timeout using timeouts in underlying libraries is preferred;
+            # here we just rely on underlying SDK timeouts; we log if call exceeds threshold.
+            start = time.time()
+            res = fn(*args, **kwargs)
+            elapsed = time.time() - start
+            if elapsed > timeout:
+                LOG.warning("call to %s exceeded timeout %s sec (elapsed=%.2f)", fn.__name__, timeout, elapsed)
+            return res
+        return wrapped
+    return decorator
+
+class HardenedAdapter:
+    def __init__(self, adapter, rate=1.0, burst=5, retries=4, timeout=60):
+        self.adapter = adapter
+        self.throttler = TokenBucket(rate, burst)
+        self.retries = retries
+        self.timeout = timeout
+
+    def _retry(self, fn, *args, **kwargs):
+        delay = 1.0
+        for attempt in range(1, self.retries + 1):
+            if not self.throttler.consume():
+                LOG.warning("throttled, sleeping before attempt %s", attempt)
+                time.sleep(0.5)
+            try:
+                return fn(*args, **kwargs)
+            except Exception as e:
+                LOG.warning("adapter call failed attempt %s: %s", attempt, e)
+                if attempt == self.retries:
+                    raise
+                time.sleep(delay + random.random() * 0.1)
+                delay *= 2
+
+    def submit(self, circuit, shots=1024, params=None):
+        return self._retry(self.adapter.submit, circuit, shots, params)
+
+    def status(self, job_id):
+        return self._retry(self.adapter.status, job_id)
+
+    def result(self, job_id):
+        return self._retry(self.adapter.result, job_id)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/transpile/device_catalog.yaml
+# Device catalog: per-device properties used to tune transpilation and mitigation
+devices:
+  ibm_santiago:
+    provider: qiskit
+    n_qubits: 5
+    basis_gates: ["u1","u2","u3","cx"]
+    coupling_map: [[0,1],[1,2],[2,3],[3,4]]
+    calibration_ttl_hours: 24
+    preferred_optimization_level: 1
+
+  ibm_perth:
+    provider: qiskit
+    n_qubits: 27
+    basis_gates: ["u1","u2","u3","cx"]
+    coupling_map: null
+    calibration_ttl_hours: 6
+    preferred_optimization_level: 3
+
+  braket_sv1:
+    provider: braket
+    n_qubits: 1
+    note: "simulator-like"
+
*** End Patch
*** Begin Patch
*** Add File: quantum/transpile/tuning_engine.py
+"""
+Selects transpiler parameters and mitigation strategy based on device catalog and recent calibration history.
+This is a prototype policy engine you can extend with ML models or heuristics.
+"""
+import yaml
+import time
+import logging
+
+LOG = logging.getLogger("aegis.transpile.tuner")
+
+CATALOG_PATH = "quantum/transpile/device_catalog.yaml"
+
+def load_catalog():
+    with open(CATALOG_PATH) as fh:
+        return yaml.safe_load(fh).get("devices", {})
+
+def suggest_params(device_name):
+    catalog = load_catalog()
+    dev = catalog.get(device_name)
+    if not dev:
+        LOG.warning("device %s not in catalog, returning defaults", device_name)
+        return {"optimization_level": 2, "apply_readout_mitigation": False}
+    params = {}
+    params["optimization_level"] = dev.get("preferred_optimization_level", 2)
+    # If calibration TTL is small, lower optimization to reduce transpiler variance
+    if dev.get("calibration_ttl_hours") and dev["calibration_ttl_hours"] < 12:
+        params["optimization_level"] = max(0, params["optimization_level"] - 1)
+    params["apply_readout_mitigation"] = True if dev.get("n_qubits", 0) <= 10 else False
+    return params
+
+def tune_and_transpile(plugin, circuit, backend, device_name):
+    params = suggest_params(device_name)
+    LOG.info("tuning for %s -> params=%s", device_name, params)
+    # plugin is expected to honor optimization_level and mitigation flags
+    if hasattr(plugin, "prepare"):
+        return plugin.prepare(circuit, backend, **params) if "optimization_level" in plugin.prepare.__code__.co_varnames else plugin.prepare(circuit, backend)
+    return circuit
+
*** End Patch
*** Begin Patch
*** Add File: compliance/BAA_template.md
+# Business Associate Agreement (BAA) Template — Aegis
+
+This template is a starting point for drafting a BAA between Aegis and a customer or provider.
+Legal teams MUST review and finalize forms.
+
+1. Parties
+ - Covered Entity: <Customer Name>
+ - Business Associate: <Aegis/Provider Name>
+
+2. Purpose
+ - Define permitted uses and disclosures of Protected Health Information (PHI).
+
+3. Obligations & Activities
+ - Business Associate will implement administrative, physical and technical safeguards to protect PHI as required by HIPAA.
+ - Use of PHI is limited to providing the services described in the Master Services Agreement.
+
+4. Safeguards
+ - Encryption in transit and at rest, access controls, logging & audit, vulnerability management, and key management (HSM/KMS).
+
+5. Reporting & Breach Notification
+ - Timely breach reporting obligations (within 48 hours) and cooperation in incident response.
+
+6. Subcontractors
+ - BA must flow-down obligations to subcontractors that will have access to PHI.
+
+7. Term, Termination & Return/Destruction of PHI
+ - On termination BA shall return or securely delete PHI unless otherwise required.
+
+8. Audit & Evidence
+ - Business Associate shall provide evidence of controls upon reasonable request (logs, cosign/Rekor entries, MLflow run artifacts).
+
*** End Patch
*** Begin Patch
*** Add File: compliance/evidence/assemble_and_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUTDIR="${1:-/tmp/aegis_evidence}"
+mkdir -p "${OUTDIR}"
+echo "Collecting evidence..."
+kubectl logs -l app=aegis-fleet-controller -n aegis --tail=1000 > "${OUTDIR}/controller_logs.txt" || true
+kubectl get pods -n aegis -o yaml > "${OUTDIR}/pods.yaml" || true
+cp manifest.json "${OUTDIR}/" || true
+tar czf "${OUTDIR}/evidence_$(date +%s).tgz" -C "${OUTDIR}" .
+echo "Signing evidence bundle with cosign (COSIGN_KMS_KEY must be in env)"
+BUNDLE="${OUTDIR}/evidence_$(date +%s).tgz"
+if [ -n "${COSIGN_KMS_KEY:-}" ]; then
+  cosign sign --key "${COSIGN_KMS_KEY}" "${BUNDLE}"
+else
+  echo "COSIGN_KMS_KEY not set; produce unsigned bundle at ${BUNDLE}"
+fi
+echo "Upload signed evidence to secure bucket (e.g., S3) and record URI in compliance tracker."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/quantum_tuned_rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-quantum-tuned-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: quantum.sla.rules
+      rules:
+        - alert: QuantumJobQueueGrowing
+          expr: increase(quantum_jobs_queued_total[10m]) > 20
+          for: 10m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Quantum job queue is growing unexpectedly"
+        - alert: QPULatencyHigh
+          expr: histogram_quantile(0.95, sum(rate(quantum_job_submit_duration_seconds_bucket[5m])) by (le)) > 120
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "High QPU submission latency (p95 > 120s)"
+        - alert: QPUErrorRateCritical
+          expr: (increase(quantum_job_failures_total[10m]) / max(1, increase(quantum_job_submissions_total[10m]))) > 0.2
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "High QPU error rate (>20%)"
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/grafana/quantum_dashboard.json
+{
+  "dashboard": {
+    "id": null,
+    "uid": "aegis-quantum",
+    "title": "Aegis Quantum Operations",
+    "panels": [
+      {
+        "type": "graph",
+        "title": "Jobs queued",
+        "targets": [
+          { "expr": "quantum_jobs_queued_total", "legendFormat": "queued" }
+        ],
+        "gridPos": { "x": 0, "y": 0, "w": 12, "h": 6 }
+      },
+      {
+        "type": "graph",
+        "title": "Job submission latency (p95)",
+        "targets": [
+          { "expr": "histogram_quantile(0.95, sum(rate(quantum_job_submit_duration_seconds_bucket[5m])) by (le))", "legendFormat": "p95" }
+        ],
+        "gridPos": { "x": 12, "y": 0, "w": 12, "h": 6 }
+      },
+      {
+        "type": "stat",
+        "title": "Error rate (last 5m)",
+        "targets": [
+          { "expr": "sum(rate(quantum_job_failures_total[5m])) / sum(rate(quantum_job_submissions_total[5m]))", "legendFormat": "err_rate" }
+        ],
+        "gridPos": { "x": 0, "y": 6, "w": 6, "h": 4 }
+      }
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/sre_quantum_tuning.md
+# SRE: Quantum Monitoring & Threshold Tuning
+
+Guidance:
+- Start with conservative thresholds (queue growth 20 in 10min, error rate 20%) and adjust after 2 weeks of telemetry.
+- Use the Grafana dashboard to identify p95 submission latency baseline for each provider and tune the Prometheus rule p95 threshold accordingly.
+- For throttling: monitor provider 429/Throttle errors and set a local token-bucket rate per provider to avoid hitting provider rate limits.
+- Regularly (weekly) review calibration age metrics; if calibration age > X hours, reduce optimization level for better robustness.
+
+Incident steps:
+- If QPUErrorRateCritical fires, run quantum_sla_runbook.md steps: route to simulator, notify provider, collect provenance artifacts, and escalate.
+
*** End Patch
*** End Patch
