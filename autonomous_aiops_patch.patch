*** Begin Patch
*** Add File: autoscale/slo_autoscaler_controller.py
+#!/usr/bin/env python3
+"""
+SLO-driven autoscaler/controller for Aegis (prototype).
+
+- Periodically queries Prometheus for configured SLO metrics.
+- Decides scaling actions and optionally triggers self-healing runbooks if violations persist.
+- Emits simple logs and returns non-zero on fatal errors (use as a k8s CronJob / Deployment).
+
+Environment variables:
+ - PROM_URL (default: http://prometheus-operated.monitoring.svc:9090)
+ - CHECK_INTERVAL_SECONDS (default: 60)
+ - REMEDIATION_RUNBOOK (path to Ansible playbook or script)
+ - NAMESPACE / DEPLOYMENT / SLO_P95_MS / FAILURE_THRESHOLD
+
+Note: This is a controller-lite prototype. Replace subprocess kubectl/ansible calls with operator integrations for production.
+"""
+import os
+import time
+import json
+import math
+import subprocess
+import requests
+from datetime import datetime, timedelta
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus-operated.monitoring.svc:9090")
+CHECK_INTERVAL = int(os.environ.get("CHECK_INTERVAL_SECONDS", "60"))
+REMEDIATION_RUNBOOK = os.environ.get("REMEDIATION_RUNBOOK", "runbooks/self_heal_reprovision.yml")
+FAILURE_WINDOW = int(os.environ.get("FAILURE_WINDOW_MIN", "5"))  # minutes window to evaluate
+FAILURE_THRESHOLD = float(os.environ.get("FAILURE_THRESHOLD", "0.20"))  # fraction of errors to trigger remediation
+
+def query_prometheus(query, prom_url=PROM_URL):
+    try:
+        r = requests.get(f"{prom_url}/api/v1/query", params={"query": query}, timeout=15)
+        r.raise_for_status()
+        j = r.json()
+        if j.get("status") == "success":
+            return j["data"]["result"]
+    except Exception as e:
+        print("Prometheus query error:", e)
+    return []
+
+def deployment_replicas(namespace, deployment):
+    try:
+        out = subprocess.check_output(["kubectl","get","deployment",deployment,"-n",namespace,"-o","json"], stderr=subprocess.DEVNULL)
+        j = json.loads(out)
+        return int(j["spec"].get("replicas",1))
+    except Exception as e:
+        print("Failed to get replicas:", e)
+        return None
+
+def scale_deployment(namespace, deployment, replicas):
+    try:
+        replicas = int(max(1, replicas))
+        subprocess.run(["kubectl","scale","deployment",deployment,"-n",namespace,f"--replicas={replicas}"], check=True)
+        print(f"{datetime.utcnow().isoformat()} scaled {deployment} to {replicas} replicas in {namespace}")
+        return True
+    except Exception as e:
+        print("Scale failed:", e)
+        return False
+
+def run_remediation(playbook=REMEDIATION_RUNBOOK, inventory=None):
+    # If ansible is available run playbook; otherwise attempt a local script
+    if os.path.exists(playbook):
+        cmd = ["ansible-playbook", "-i", (inventory or "ansible/inventory.ini"), playbook]
+        try:
+            subprocess.run(cmd, check=True)
+            print("Remediation runbook completed:", playbook)
+            return True
+        except Exception as e:
+            print("Remediation playbook failed:", e)
+            return False
+    else:
+        print("No runbook found at", playbook)
+        return False
+
+def check_and_act(namespace, deployment, slo_p95_ms):
+    # Query p95 and error rate for the deployment's request metric (names are examples; adapt to your metrics)
+    p95_query = f'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{{deployment="{deployment}"}}[5m])) by (le))'
+    err_query = f'sum(rate(http_requests_total{{deployment="{deployment}",code=~"5.."}}[5m]))'
+    total_query = f'sum(rate(http_requests_total{{deployment="{deployment}"}}[5m]))'
+
+    p95 = None
+    try:
+        res = query_prometheus(p95_query)
+        if res:
+            p95 = float(res[0]["value"][1])
+    except Exception:
+        p95 = None
+
+    # compute error fraction
+    err = 0.0
+    total = 0.0
+    try:
+        er = query_prometheus(err_query)
+        to = query_prometheus(total_query)
+        if er and to:
+            err = float(er[0]["value"][1])
+            total = float(to[0]["value"][1])
+    except Exception:
+        pass
+    err_frac = (err / total) if total > 0 else 0.0
+
+    print(f"[{datetime.utcnow().isoformat()}] deployment={deployment} p95={p95} err_frac={err_frac}")
+
+    # If p95 > target or error fraction > threshold, trigger remediation
+    if (p95 is not None and slo_p95_ms is not None and p95 > slo_p95_ms) or (err_frac > FAILURE_THRESHOLD):
+        print("SLO breach detected; running remediation playbook")
+        run_remediation()
+        return "remediated"
+    return "ok"
+
+def main_loop():
+    # Read configured watch list from env or a mounted config file
+    watches_env = os.environ.get("WATCHES", "")  # JSON array: [{"namespace":"aegis","deployment":"agent","slo_p95_ms":200}]
+    watches = []
+    if watches_env:
+        try:
+            watches = json.loads(watches_env)
+        except Exception:
+            print("Failed to parse WATCHES env; defaulting to empty")
+    # Fallback single watch via env vars
+    if not watches:
+        ns = os.environ.get("NAMESPACE","aegis")
+        dep = os.environ.get("DEPLOYMENT","agent")
+        slo = int(os.environ.get("SLO_P95_MS","200"))
+        watches = [{"namespace":ns,"deployment":dep,"slo_p95_ms":slo}]
+
+    print("Starting SLO controller; watches:", watches)
+    while True:
+        for w in watches:
+            try:
+                check_and_act(w.get("namespace"), w.get("deployment"), w.get("slo_p95_ms"))
+            except Exception as e:
+                print("Error checking watch:", e)
+        time.sleep(CHECK_INTERVAL)
+
+if __name__ == "__main__":
+    try:
+        main_loop()
+    except KeyboardInterrupt:
+        print("Exiting")
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/self_heal_reprovision.yml
+---
+# Runbook: Self-heal - reprovision a failing runner (prototype)
+# This Ansible playbook is intended to run against a runner host that is unhealthy.
+
+- name: Reprovision unhealthy runner
+  hosts: runners
+  become: true
+  vars:
+    jetson_image_url: "{{ jetson_image_url | default('https://example.com/jetson-image.tar.gz') }}"
+  tasks:
+    - name: Mark runner as under maintenance (operator step)
+      debug:
+        msg: "Runner {{ inventory_hostname }} will be reprovisioned. Operator should confirm maintenance window."
+
+    - name: Stop CI runner service (placeholder)
+      ansible.builtin.systemd:
+        name: aegis-runner
+        state: stopped
+      ignore_errors: yes
+
+    - name: Pull fresh image bundle (placeholder)
+      ansible.builtin.shell: |
+        echo "Downloading image from {{ jetson_image_url }} (placeholder)"
+      ignore_errors: yes
+
+    - name: Run reprovision steps (placeholder)
+      ansible.builtin.shell: |
+        echo "Running reprovision scripts..."
+      ignore_errors: yes
+
+    - name: Start CI runner service
+      ansible.builtin.systemd:
+        name: aegis-runner
+        state: started
+      ignore_errors: yes
+
+    - name: Verify runner registers to CI (operator step)
+      debug:
+        msg: "Verify runner registration in CI dashboard and Prometheus metrics."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/canary_rollback.yml
+name: Canary Deploy & Automatic Rollback (prototype)
+on:
+  push:
+    paths:
+      - 'adapters/**'
+      - 'scheduler/**'
+  workflow_dispatch:
+
+jobs:
+  canary-deploy:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Deploy canary (k8s manifest must include a canary label)
+        run: |
+          echo "Applying canary manifests (operator should verify paths)"
+          kubectl apply -f k8s/canary/ || true
+
+      - name: Wait for canary to become ready
+        run: |
+          sleep 15
+          # Example readiness check; customize for real probe
+          kubectl get pods -l canary=true -n aegis || true
+
+      - name: Monitor SLO for 5 minutes then decide
+        env:
+          PROM_URL: ${{ secrets.PROM_URL }}
+        run: |
+          echo "Checking SLO p95 for canary; if exceeded we rollback"
+          python - <<'PY'
+import os, time, requests, sys, json
+PROM=os.environ.get("PROM_URL")
+if not PROM:
+    print("No PROM_URL set; skipping SLO check (assume pass)")
+    sys.exit(0)
+query='histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{deployment=~"adapter-canary.*"}[5m])) by (le))'
+ok=True
+for i in range(5):
+    r=requests.get(f"{PROM}/api/v1/query", params={"query":query}, timeout=10)
+    if r.ok:
+        j=r.json()
+        if j.get("data",{}).get("result"):
+            val=float(j["data"]["result"][0]["value"][1])
+            print("p95:",val)
+            if val>float(os.environ.get("CANARY_SLO_MS","300")):
+                ok=False
+                break
+    time.sleep(60)
+if not ok:
+    print("Canary SLO breached; rolling back")
+    # rollback example
+    import subprocess
+    subprocess.run(["kubectl","rollout","undo","deployment/adapter-canary","-n","aegis"])
+    # create a GitHub issue (requires token)
+    print("Rollback executed")
+    sys.exit(1)
+print("Canary passed")
+PY
+
+      - name: Promote canary to production (if passed)
+        if: ${{ success() }}
+        run: |
+          echo "Promoting canary to production - operator should replace this with release steps"
+          # e.g. kubectl apply -f k8s/production/
+
*** End Patch
*** Begin Patch
*** Add File: aiops/incident_triage.py
+#!/usr/bin/env python3
+"""
+Incident triage automation:
+ - Fetches traces from Jaeger/OTEL collector for a given job_id
+ - Summarizes correlated spans
+ - Optionally opens a GitHub issue linking the summary and triggers a remediation runbook
+
+Environment/requirements:
+ - GITHUB_TOKEN must be set to open issues
+ - JAEGER_URL environment variable (e.g. http://jaeger-query:16686)
+ - REMEDIATION_RUNBOOK path (optional)
+"""
+import os
+import sys
+import json
+import requests
+from datetime import datetime, timedelta
+
+JAEGER_URL = os.environ.get("JAEGER_URL", "http://jaeger-query:16686")
+GITHUB_REPO = os.environ.get("GITHUB_REPO")  # owner/repo
+GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
+REMEDIATION_RUNBOOK = os.environ.get("REMEDIATION_RUNBOOK", "runbooks/self_heal_reprovision.yml")
+
+def fetch_traces(job_id, lookback_minutes=30):
+    url = JAEGER_URL.rstrip("/") + "/api/traces"
+    end = datetime.utcnow()
+    start = end - timedelta(minutes=lookback_minutes)
+    params = {"start": int(start.timestamp()*1000), "end": int(end.timestamp()*1000), "limit": 200}
+    r = requests.get(url, params=params, timeout=30)
+    r.raise_for_status()
+    data = r.json().get("data", [])
+    correlated=[]
+    for t in data:
+        for span in t.get("spans",[]):
+            tags = {x["key"]:x["value"] for x in span.get("tags",[])}
+            if tags.get("job_id")==job_id:
+                correlated.append({"traceID": t.get("traceID"), "operation": span.get("operationName"), "start":span.get("startTime"), "duration_ms": span.get("duration")/1000.0 if span.get("duration") else None, "tags": tags})
+    return correlated
+
+def summarize(correlated):
+    correlated_sorted = sorted(correlated, key=lambda s: s.get("start") or 0)
+    lines=["Incident triage summary:", f"collected_spans: {len(correlated_sorted)}"]
+    for s in correlated_sorted:
+        lines.append(f"- op={s['operation']} dur_ms={s['duration_ms']} tags={s['tags']}")
+    return "\n".join(lines)
+
+def open_github_issue(title, body):
+    if not GITHUB_REPO or not GITHUB_TOKEN:
+        print("GITHUB_REPO or GITHUB_TOKEN not set; cannot open issue")
+        return None
+    api = f"https://api.github.com/repos/{GITHUB_REPO}/issues"
+    headers = {"Authorization": f"token {GITHUB_TOKEN}", "Accept":"application/vnd.github+json"}
+    payload = {"title": title, "body": body, "labels":["aiops","triage"]}
+    r = requests.post(api, json=payload, headers=headers, timeout=30)
+    if r.status_code in (200,201):
+        return r.json().get("html_url")
+    print("Failed to create issue:", r.status_code, r.text)
+    return None
+
+def trigger_remediation():
+    # Attempt to run remediation playbook locally (CI/ops must provide ansible)
+    if os.path.exists(REMEDIATION_RUNBOOK):
+        print("Executing remediation runbook:", REMEDIATION_RUNBOOK)
+        import subprocess
+        try:
+            subprocess.run(["ansible-playbook","-i","ansible/inventory.ini",REMEDIATION_RUNBOOK], check=True)
+            return True
+        except Exception as e:
+            print("Remediation runbook failed:", e)
+            return False
+    else:
+        print("No remediation runbook found; skipping")
+        return False
+
+def main():
+    if len(sys.argv) < 2:
+        print("Usage: incident_triage.py <job_id>")
+        sys.exit(2)
+    job_id = sys.argv[1]
+    traces = fetch_traces(job_id)
+    summary = summarize(traces)
+    print(summary)
+    issue_url = open_github_issue(f"Aegis incident triage for job {job_id}", summary)
+    if issue_url:
+        print("Opened issue:", issue_url)
+    # Optionally auto-trigger remediation (careful in prod)
+    auto = os.environ.get("AUTO_REMEDIATE","false").lower() in ("1","true","yes")
+    if auto:
+        ok = trigger_remediation()
+        print("Remediation triggered:", ok)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: chaos/inject_fault.sh
+#!/usr/bin/env bash
+#
+# Simple chaos injector: randomly deletes a pod with label "chaos-target=true" in namespace aegis.
+# Use with care. Intended to be called from a k8s CronJob or by operators manually.
+set -euo pipefail
+NAMESPACE=${1:-aegis}
+LABEL=${2:-"chaos-target=true"}
+echo "Listing candidate pods in ${NAMESPACE} with label ${LABEL}"
+PODS=$(kubectl get pods -n "${NAMESPACE}" -l "${LABEL}" -o jsonpath='{.items[*].metadata.name}' || true)
+if [ -z "$PODS" ]; then
+  echo "No candidate pods found; exiting"
+  exit 0
+fi
+ARR=($PODS)
+IDX=$((RANDOM % ${#ARR[@]}))
+TARGET=${ARR[$IDX]}
+echo "Deleting pod ${TARGET} in ${NAMESPACE} (chaos)"
+kubectl delete pod "$TARGET" -n "$NAMESPACE" --grace-period=0 --force
+echo "Deleted ${TARGET}"
+
*** End Patch
*** Begin Patch
*** Add File: chaos/cron/chaos-kill-pod-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-chaos-kill-pod
+  namespace: aegis
+spec:
+  schedule: "0 2 * * 1" # weekly on Monday at 02:00 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: chaos
+              image: bitnami/kubectl:1.27
+              command: ["/bin/sh","-c"]
+              args:
+                - kubectl --version && \
+                  /bin/sh -c "kubectl get pods -n aegis -l chaos-target=true -o name | shuf -n1 | xargs -r kubectl delete -n aegis --force --grace-period=0 || true"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: docs/autonomous_ops_README.md
+# Autonomous Operations & AIOps (Aegis)
+
+This patch adds prototypes to enable SLO-driven autoscaling, self-healing runbooks-as-code, canary + automatic rollback pipelines, incident triage automation, and chaos engineering scaffolds.
+
+Files added:
+- autoscale/slo_autoscaler_controller.py — controller-lite that queries Prometheus and triggers remediation playbooks.
+- runbooks/self_heal_reprovision.yml — sample Ansible runbook to reprovision unhealthy runners.
+- .github/workflows/canary_rollback.yml — GitHub Actions prototype for canary deploys with SLO monitoring and automatic rollback.
+- aiops/incident_triage.py — trace correlation + GitHub issue creation + optional remediation execution.
+- chaos/inject_fault.sh and chaos/cron/chaos-kill-pod-cronjob.yaml — chaos injector and k8s CronJob for scheduled fault injection.
+
+Quickstart / usage:
+1. Deploy the SLO controller:
+   - Build a container for autoscale/slo_autoscaler_controller.py or run it as a k8s CronJob / Deployment with env WATCHES containing JSON watches.
+   - Example WATCHES env: '[{"namespace":"aegis","deployment":"agent","slo_p95_ms":200}]'
+
+2. Configure Prometheus access:
+   - Ensure PROM_URL is reachable by the controller and GH Actions (set via secrets).
+
+3. Wire remediation playbooks:
+   - Populate runbooks/self_heal_reprovision.yml and ensure Ansible inventory/credentials are available to the controller or runbook runner.
+
+4. Enable canary rollback workflow:
+   - Place canary k8s manifests under k8s/canary/ and configure PROM_URL secret in repo settings.
+   - On push to adapters/scheduler, the workflow will deploy the canary, monitor SLO, and rollback if necessary.
+
+5. Incident triage:
+   - Use aiops/incident_triage.py <job_id> to fetch traces from Jaeger, create a GitHub issue and optionally trigger remediation (set AUTO_REMEDIATE=true to enable).
+
+6. Chaos experiments:
+   - Tag target pods with `chaos-target=true` label.
+   - The CronJob will randomly delete one such pod weekly. Adjust schedule and label to taste.
+
+Safety notes:
+- These are prototypes. Do not enable AUTO_REMEDIATE in production without strict guardrails and approvals.
+- Use canary mode and shadow deployments when testing learned policies or new broker logic.
+- Chaos experiments should run in controlled windows and be coordinated with SRE on-call rotations.
+
*** End Patch
*** End Patch
   
