*** Begin Patch
*** Add File: hpc/README.md
+# Aegis HPC optimization bundle
+
+This directory contains manifests, configs, benchmark jobs and runbooks to help deploy and validate Kubernetes-based HPC workloads (DGX / multi-GPU systems).
+
+What is included
+- NVIDIA device plugin DaemonSet (hpc/k8s/nvidia-device-plugin-daemonset.yaml)
+- NCCL tuning ConfigMap (hpc/config/nccl-configmap.yaml)
+- kubelet & sysctl tuning recommendation (hpc/runbooks/kubelet_and_sysctl.md)
+- Volcano gang-scheduling example (hpc/scheduler/volcano/gang-scheduling-example.yaml)
+- NCCL local & distributed job scaffolds (benchmarks/nccl)
+- FIO storage benchmark job (benchmarks/fio/fio-job.yaml)
+- Argo workflow to run the lightweight benchmark suite (argo/workflows/hpc-benchmarks-workflow.yaml)
+- Scripts to run the benchmarks and collect artifacts (scripts/hpc/run_benchmarks.sh)
+- Runbook and acceptance criteria (hpc/runbooks/acceptance_criteria.md)
+
+Operator notes
+- These manifests are intended to be applied in a staging cluster first.
+- Replace placeholder image names and storageClass names before applying.
+- Some features (RDMA, NVLink, GPUDirect) require node firmware, drivers and hardware support — follow the runbook.
+
*** End Patch
*** Begin Patch
*** Add File: hpc/k8s/nvidia-device-plugin-daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: nvidia-device-plugin-daemonset
+  namespace: kube-system
+  labels:
+    app: nvidia-device-plugin
+spec:
+  selector:
+    matchLabels:
+      app: nvidia-device-plugin
+  template:
+    metadata:
+      labels:
+        app: nvidia-device-plugin
+    spec:
+      tolerations:
+      - key: nvidia.com/gpu
+        operator: Exists
+        effect: NoSchedule
+      containers:
+      - image: "nvidia/k8s-device-plugin:1.0.0-beta.1" # operator: pin the tested image
+        name: nvidia-device-plugin-ctr
+        securityContext:
+          privileged: true
+        volumeMounts:
+        - name: device-plugin
+          mountPath: /var/lib/kubelet/device-plugins
+      volumes:
+      - name: device-plugin
+        hostPath:
+          path: /var/lib/kubelet/device-plugins
+
+# Note: This is the standard NVIDIA device plugin manifest scaffold.
+# Ensure NVIDIA drivers and GPU operator are installed per your vendor guidance.
+
*** End Patch
*** Begin Patch
*** Add File: hpc/config/nccl-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: aegis-nccl-config
+  namespace: kube-system
+data:
+  NCCL_SOCKET_IFNAME: "eth0"         # operator: set to your high-bandwidth NIC (e.g., ib0)
+  NCCL_IB_DISABLE: "0"               # 0 = enable IB if available
+  NCCL_DEBUG: "WARN"
+  NCCL_P2P_LEVEL: "NVL"
+  NCCL_INFRA_CONFIG: "default"
+
+# These variables should be added to training pod environment or mounted via downward API.
+# Update values according to your topology and test results.
+
*** End Patch
*** Begin Patch
*** Add File: hpc/runbooks/kubelet_and_sysctl.md
+# Kubelet tuning & OS sysctl runbook for HPC (recommended settings)
+
+Before applying: validate with your infra & kernel vendor. Run on each compute node or via cluster configuration management.
+
+Kernel / networking sysctl (example)
+-----------------------------------
+Apply via /etc/sysctl.d/99-aegis-hpc.conf:
+
+```
+net.core.rmem_max=134217728
+net.core.wmem_max=134217728
+net.ipv4.tcp_rmem=4096 87380 134217728
+net.ipv4.tcp_wmem=4096 65536 134217728
+net.core.netdev_max_backlog=250000
+vm.swappiness=10
+```
+
+MTU / Fabric
+------------
+- Set MTU=9000 on RDMA / Ethernet NICs and switches when using InfiniBand or RoCE.
+- Validate with `ip link set dev <iface> mtu 9000` and `ethtool -S`.
+
+Kubelet flags (examples)
+------------------------
+- --feature-gates=DevicePlugins=true
+- --eviction-hard=imagefs.available<5%,nodefs.available<5%
+- --kube-reserved=cpu=500m,memory=1Gi
+- --system-reserved=cpu=500m,memory=1Gi
+
+GPU-specific
+------------
+- Ensure GPU driver and CUDA versions match your container runtime images.
+- Consider Nvidia GPU Operator for driver lifecycle.
+
+Testing & validation
+--------------------
+- Run `nvidia-smi`, `ibv_devinfo` (InfiniBand), `nvcc --version`.
+- Run NCCL microbenchmarks and network throughput tests (ipes/OSU).
+
*** End Patch
*** Begin Patch
*** Add File: hpc/scheduler/volcano/volcano-install.md
+# Volcano installer notes (gang scheduling)
+
+Volcano provides advanced batch scheduling and gang semantics for Kubernetes.
+Install (operator-run):
+
+1) Add helm repo:
+   helm repo add volcano https://volcano.sh/charts
+   helm repo update
+
+2) Install into kube-system or dedicated ns:
+   helm install volcano volcano/volcano -n volcano --create-namespace
+
+3) Validate CRDs:
+   kubectl get crd | grep volcano
+
+4) Example job present: hpc/scheduler/volcano/gang-scheduling-example.yaml
+
+Notes:
+- Use Volcano for multi-pod gang scheduling required by multi-node MPI/DeepSpeed jobs.
+- Ensure NodeSelectors / tolerations are set for GPU node pools.
+
*** End Patch
*** Begin Patch
*** Add File: hpc/scheduler/volcano/gang-scheduling-example.yaml
+apiVersion: batch.volcano.sh/v1alpha1
+kind: Job
+metadata:
+  name: aegis-gang-example
+  namespace: staging
+spec:
+  minAvailable: 2
+  schedulerName: volcano
+  plugins:
+    env: {}
+  tasks:
+  - replicas: 2
+    name: trainer
+    template:
+      spec:
+        containers:
+        - name: trainer
+          image: "REPLACE_WITH_TRAINING_IMAGE" # operator: put validated DeepSpeed / training image
+          command: ["/bin/sh","-c"]
+          args: ["python /workspace/train.py --epochs 1"]
+          resources:
+            limits:
+              nvidia.com/gpu: 4
+        restartPolicy: OnFailure
+
+# This Volcano job requests a gang of 2 pods each with 4 GPUs.
+# Replace image and command with your training run. Use this to validate gang scheduling.
+
*** End Patch
*** Begin Patch
*** Add File: benchmarks/nccl/README.md
+# NCCL benchmark scaffolds
+
+Two benchmark jobs are provided:
+- local-nccl-job.yaml — runs nccl-tests inside a single multi-GPU node (intra-node performance).
+- mpi-nccl-job-example.yaml — example MPIJob (requires kubeflow MPI operator) for multi-node NCCL tests (scaffold, operator must install MPI operator).
+
+Operator steps:
+1. Ensure device plugin and drivers are installed.
+2. Replace IMAGE_NCCL placeholder with a validated nccl-tests image (vendor or built by operator).
+3. Run the Argo workflow or kubectl apply the job.
+
*** End Patch
*** Begin Patch
*** Add File: benchmarks/nccl/local-nccl-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-local-nccl
+  namespace: staging
+spec:
+  template:
+    metadata:
+      name: aegis-local-nccl
+    spec:
+      restartPolicy: Never
+      containers:
+      - name: nccl-tests
+        image: "REPLACE_WITH_NCCL_TESTS_IMAGE" # e.g., nvcr.io/nvidia/nccl-tests:2.15.5-1
+        command: ["/bin/sh","-c"]
+        env:
+        - name: NCCL_SOCKET_IFNAME
+          valueFrom:
+            configMapKeyRef:
+              name: aegis-nccl-config
+              key: NCCL_SOCKET_IFNAME
+        - name: NCCL_IB_DISABLE
+          valueFrom:
+            configMapKeyRef:
+              name: aegis-nccl-config
+              key: NCCL_IB_DISABLE
+        args:
+        - "./build/all_reduce_perf -b 8 -e 512M -f 2 -g 8 || true"
+        resources:
+          limits:
+            nvidia.com/gpu: 8
+      nodeSelector:
+        aegis/hpc: "true"
+      tolerations:
+      - key: "nvidia.com/gpu"
+        operator: "Exists"
+
+# This job runs NCCL all_reduce performance across 8 GPUs on one node (intra-node test).
+# It is safe to run in staging for performance validation.
+
*** End Patch
*** Begin Patch
*** Add File: benchmarks/nccl/mpi-nccl-job-example.yaml
+apiVersion: "kubeflow.org/v1"
+kind: "MPIJob"
+metadata:
+  name: "aegis-mpi-nccl"
+  namespace: staging
+spec:
+  slotsPerWorker: 8
+  runPolicy:
+    cleanPodPolicy: None
+  mpiReplicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+        spec:
+          containers:
+          - image: "REPLACE_WITH_NCCL_TESTS_IMAGE"
+            name: mpi-launcher
+            command: ["/bin/sh", "-c", "mpirun -np $((8 * $MPI_REPLICAS)) --allow-run-as-root ./build/all_reduce_perf -b 8 -e 512M -f 2 -g 8"]
+    Worker:
+      replicas: 2
+      template:
+        spec:
+          containers:
+          - image: "REPLACE_WITH_NCCL_TESTS_IMAGE"
+            name: mpi-worker
+            resources:
+              limits:
+                nvidia.com/gpu: 8
+          restartPolicy: OnFailure
+
+# NOTE: This uses the Kubeflow MPI Operator CRD. Install the MPI operator in your cluster first.
+# This is a multi-node NCCL benchmark example (distributed).
+
*** End Patch
*** Begin Patch
*** Add File: benchmarks/fio/fio-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-fio-benchmark
+  namespace: staging
+spec:
+  template:
+    metadata:
+      name: aegis-fio-benchmark
+    spec:
+      restartPolicy: Never
+      containers:
+      - name: fio
+        image: "ghcr.io/strib/fio:latest" # operator: replace if private policies restrict GHCR
+        command: ["/bin/sh","-c"]
+        args:
+        - |
+          fio --name=write-test --filename=/mnt/testfile --bs=1M --size=10G --direct=1 --rw=write --numjobs=4 --runtime=120 --group_reporting
+        volumeMounts:
+        - name: scratch
+          mountPath: /mnt
+      volumes:
+      - name: scratch
+        persistentVolumeClaim:
+          claimName: REPLACE_WITH_SCRATCH_PVC
+
+# This job performs a 10GB write IO test to the provided PVC. Replace PVC with a high-performance storageClass-backed PVC.
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/hpc-benchmarks-workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-hpc-benchmarks-
+  namespace: staging
+spec:
+  entrypoint: run-benchmarks
+  templates:
+  - name: run-benchmarks
+    steps:
+    - - name: fio
+        template: run-k8s-manifest
+        arguments:
+          parameters:
+          - name: manifest
+            value: "benchmarks/fio/fio-job.yaml"
+    - - name: nccl-local
+        template: run-k8s-manifest
+        arguments:
+          parameters:
+          - name: manifest
+            value: "benchmarks/nccl/local-nccl-job.yaml"
+    - - name: collect-logs
+        template: collect-logs
+
+  - name: run-k8s-manifest
+    inputs:
+      parameters:
+      - name: manifest
+    container:
+      image: bitnami/kubectl:1.26
+      command: ["/bin/sh","-c"]
+      args:
+      - |
+        echo "Applying manifest $1"
+        kubectl apply -f $1
+        # wait for job completion (crude)
+        jobname=$(yq e '.metadata.name' $1)
+        echo "Waiting for job $jobname"
+        kubectl wait --for=condition=complete job/$jobname -n staging --timeout=20m || true
+        kubectl get pods -n staging -l job-name=$jobname -o name | xargs -r -n1 kubectl logs -n staging > /tmp/$jobname.logs || true
+        echo "Logs for $jobname:"
+        tail -n +1 /tmp/$jobname.logs || true
+    inputs:
+      parameters:
+      - name: manifest
+
+  - name: collect-logs
+    container:
+      image: bitnami/kubectl:1.26
+      command: ["/bin/sh","-c"]
+      args:
+      - |
+        mkdir -p /tmp/aegis-bench
+        for p in $(kubectl get pods -n staging -o name); do
+          name=$(echo $p | sed 's#pod/##')
+          kubectl logs -n staging $name --all-containers=true > /tmp/aegis-bench/$name.log || true
+        done
+        tar -czf /tmp/aegis-bench-${HOSTNAME}.tgz -C /tmp aegis-bench || true
+        echo "Created artifact /tmp/aegis-bench-${HOSTNAME}.tgz"
+
+# Notes:
+# - This Argo workflow applies the FIO and local NCCL jobs then collects pod logs.
+# - It relies on 'yq' being available in the workflow runner image if you want to extract names from manifests; the current example simply assumes manifest metadata.name is present.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/hpc/run_benchmarks.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run the Argo HPC benchmark workflow or fallback to kubectl apply jobs directly.
+#
+ARTIFACT_DIR=${ARTIFACT_DIR:-artifacts/hpc_bench_$(date +%Y%m%d-%H%M%S)}
+mkdir -p "$ARTIFACT_DIR"
+
+if command -v argo >/dev/null 2>&1; then
+  echo "Submitting Argo workflow"
+  argo submit -n staging argo/workflows/hpc-benchmarks-workflow.yaml --watch --serviceaccount argo > "$ARTIFACT_DIR/argo_submit.log" 2>&1 || true
+  # collect logs from staging
+  kubectl get pods -n staging -o name | xargs -r -n1 -I{} bash -c 'n={}; kubectl logs -n staging $n --all-containers=true > "'"$ARTIFACT_DIR"'/${n#pod/}.log" || true'
+else
+  echo "Argo CLI not found; applying fio and nccl local jobs directly"
+  kubectl apply -f benchmarks/fio/fio-job.yaml
+  kubectl wait --for=condition=complete job/aegis-fio-benchmark -n staging --timeout=20m || true
+  kubectl logs -n staging job/aegis-fio-benchmark > "$ARTIFACT_DIR/fio.log" || true
+
+  kubectl apply -f benchmarks/nccl/local-nccl-job.yaml
+  kubectl wait --for=condition=complete job/aegis-local-nccl -n staging --timeout=20m || true
+  kubectl logs -n staging job/aegis-local-nccl > "$ARTIFACT_DIR/nccl_local.log" || true
+fi
+
+echo "Benchmarks complete. Artifacts are under: $ARTIFACT_DIR"
+
*** End Patch
*** Begin Patch
*** Add File: hpc/runbooks/acceptance_criteria.md
+# HPC acceptance criteria & validation steps
+
+Baseline tests to validate HPC readiness (apply in staging)
+
+1) Device plugin & drivers
+- Check GPUs visible: nvidia-smi on each node
+- Kube nodes have allocatable GPUs: kubectl describe node | grep -i gpu
+
+2) NCCL intra-node test (local-nccl-job.yaml)
+- Run job and ensure all_reduce_perf completes without errors and prints bandwidth numbers.
+- Acceptance:
+  - All GPUs used
+  - Bandwidth within expected range for your GPU (operator-provided threshold)
+
+3) FIO storage test (fio-job.yaml)
+- Run job writing to the PVC used for checkpoints.
+- Acceptance:
+  - Write throughput and IOPS meet minimal checkpoint throughput needed by your training jobs.
+
+4) Gang scheduling (Volcano)
+- Submit gang job example and verify that both pods start simultaneously when resources become available.
+
+5) Multi-node NCCL (MPI) — optional
+- Install MPI operator and run mpi-nccl-job-example.yaml
+- Acceptance:
+  - NCCL all_reduce across nodes completes and no NCCL transport errors occur.
+
+6) Networking & RDMA tests
+- Run iperf3 and OSU microbenchmarks across nodes to validate IB/RoCE bandwidth and latency.
+
+7) Observability
+- Confirm DCGM exporter or GPU metrics are present in Prometheus.
+- Validate NCCL errors or retries are captured.
+
+If all checks pass, proceed to run a small scale training job (single multi-node job) and validate checkpointing/resume.
+
*** End Patch
*** Begin Patch
*** Add File: hpc/quickstart_apply.txt
+Quickstart (example) - operator steps
+
+1) Install NVIDIA drivers and GPU operator per vendor docs.
+2) Apply device plugin:
+   kubectl apply -f hpc/k8s/nvidia-device-plugin-daemonset.yaml
+3) Apply NCCL config:
+   kubectl apply -f hpc/config/nccl-configmap.yaml
+4) (Optional) Install Volcano for gang scheduling (see hpc/scheduler/volcano/volcano-install.md)
+5) Create a node pool with label `aegis/hpc=true` and attach GPUs.
+6) Edit placeholders in benchmark manifests:
+   - REPLACE_WITH_NCCL_TESTS_IMAGE
+   - REPLACE_WITH_SCRATCH_PVC
+   - REPLACE_WITH_TRAINING_IMAGE (volcano job)
+7) Run the benchmark workflow:
+   ./scripts/hpc/run_benchmarks.sh
+
+Collect artifacts in artifacts/hpc_bench_*/ and inspect logs for NCCL and FIO results.
+
*** End Patch
*** End Patch
