*** Begin Patch
*** Add File:onnx/per_model_export_and_triton_pack.py
+#!/usr/bin/env python3
+"""
+Per-model ONNX export, validation, Triton packaging and load-test generator.
+
+Usage:
+  # For HF seq2seq / causal models:
+  python3 onnx/per_model_export_and_triton_pack.py --model-type nlp --model-path /path/to/hf/model --model-name mymodel --out-repo /tmp/triton_repo
+
+  # For Detectron2:
+  python3 onnx/per_model_export_and_triton_pack.py --model-type cv --cfg /path/to/config.yaml --weights /path/to/weights.pth --model-name cvmodel --out-repo /tmp/triton_repo
+
+This script:
+ - calls the appropriate exporter (decoder/exporter or detectron exporter)
+ - validates ONNX with onnxruntime
+ - writes a tuned Triton config (via triton/generate_triton_config.py)
+ - creates a simple load_test_client.py in the model repo for use in CI / staging load tests
+ - emits a small validation JSON used by CI to decide pass/fail
+"""
+import argparse
+import os
+import json
+import subprocess
+import shutil
+from pathlib import Path
+
+def export_nlp(model_path, onnx_out):
+    # reuse hardened_export_decoder exporter
+    from triton.hardened_export_decoder import export_decoder
+    export_decoder(model_path, onnx_out)
+
+def export_cv(cfg, weights, onnx_out):
+    from triton.hardened_convert_detectron2 import export_detectron
+    export_detectron(cfg, weights, onnx_out)
+
+def validate_onnx(onnx_path, model_type):
+    try:
+        import onnxruntime as ort
+        import numpy as np
+    except Exception as e:
+        return {"ok": False, "error": f"onnxruntime required: {e}"}
+    sess = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
+    inputs = sess.get_inputs()
+    feed = {}
+    if model_type == "nlp":
+        feed[inputs[0].name] = (np.zeros((1,8), dtype=np.int32))
+    else:
+        inp = inputs[0]
+        shape = [1 if (d is None or isinstance(d, str) or d <= 0) else int(d) for d in inp.shape]
+        if len(shape) < 4:
+            shape = [1,3,800,800]
+        feed[inp.name] = np.zeros(tuple(shape), dtype=np.float32)
+    try:
+        out = sess.run(None, feed)
+        return {"ok": True, "outputs": [list(o.shape) for o in out]}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def write_load_client(repo_path, model_name):
+    client = f"""#!/usr/bin/env python3
+import requests, json, sys
+TRITON_URL = "http://triton.aegis.svc:8000/v2/models/{model_name}/infer"
+payload = {{ "inputs":[ {{ "name":"input_ids", "shape":[1,8], "datatype":"INT32", "data":[ [1,2,3,4,5,6,7,8] ] }} ] }}
+r = requests.post(TRITON_URL, json=payload, timeout=10)
+print("status", r.status_code)
+print(r.text[:200])
+"""
+    path = os.path.join(repo_path, "load_test_client.py")
+    with open(path, "w") as f:
+        f.write(client)
+    os.chmod(path, 0o755)
+    return path
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-type", choices=["nlp","cv"], required=True)
+    p.add_argument("--model-path", help="HF model dir for nlp")
+    p.add_argument("--cfg", help="Detectron2 cfg for cv")
+    p.add_argument("--weights", help="Detectron2 weights for cv")
+    p.add_argument("--model-name", required=True)
+    p.add_argument("--out-repo", required=True)
+    p.add_argument("--gpu", action="store_true")
+    p.add_argument("--tensorrt", action="store_true")
+    args = p.parse_args()
+
+    out_repo = Path(args.out_repo)
+    model_dir = out_repo / args.model_name / "1"
+    model_dir.mkdir(parents=True, exist_ok=True)
+    onnx_out = str(model_dir / "model.onnx")
+
+    if args.model_type == "nlp":
+        if not args.model_path:
+            raise SystemExit("model-path required for nlp")
+        export_nlp(args.model_path, onnx_out)
+    else:
+        if not (args.cfg and args.weights):
+            raise SystemExit("cfg and weights required for cv")
+        export_cv(args.cfg, args.weights, onnx_out)
+
+    # validate
+    validation = validate_onnx(onnx_out, args.model_type)
+    (model_dir / "model.onnx.validation.json").write_text(json.dumps(validation, indent=2))
+
+    # generate triton config
+    gen_cmd = [
+        "python3", "triton/generate_triton_config.py",
+        "--model-name", args.model_name,
+        "--out", str(out_repo / args.model_name),
+        "--platform", "onnxruntime_onnx"
+    ]
+    if args.gpu:
+        gen_cmd += ["--gpu"]
+    if args.tensorrt:
+        gen_cmd += ["--tensorrt"]
+    subprocess.run(gen_cmd, check=False)
+
+    # create load test client
+    client_path = write_load_client(str(out_repo), args.model_name)
+
+    # write overall package manifest
+    manifest = {
+        "model_name": args.model_name,
+        "onnx": onnx_out,
+        "validation": str(model_dir / "model.onnx.validation.json"),
+        "load_test_client": client_path,
+        "timestamp": int(os.time() if hasattr(os, "time") else 0)
+    }
+    with open(str(out_repo / args.model_name / "package_manifest.json"), "w") as f:
+        json.dump(manifest, f, indent=2)
+
+    print("Packaging complete. Repo:", out_repo, "model:", args.model_name)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:rl/replay/argo_replay_builder.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rl-replay-builder-
+  namespace: aegis
+spec:
+  entrypoint: build-replay
+  arguments:
+    parameters:
+      - name: collect-prefix
+        value: ""
+      - name: out-prefix
+        value: ""
+  templates:
+    - name: build-replay
+      inputs:
+        parameters:
+          - name: collect-prefix
+          - name: out-prefix
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [python3]
+        args: ["rl/replay/s3_parquet_replay.py", "--prefix", "{{inputs.parameters.collect-prefix}}", "--out-prefix", "{{inputs.parameters.out-prefix}}"]
+        resources:
+          limits:
+            cpu: "1"
+            memory: "1Gi"
+
*** End Patch
*** Begin Patch
*** Add File:ops/redis/redis-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-redis
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-redis
+  template:
+    metadata:
+      labels:
+        app: aegis-redis
+    spec:
+      containers:
+        - name: redis
+          image: redis:6.2-alpine
+          args: ["redis-server","/etc/redis/redis.conf"]
+          volumeMounts:
+            - name: redis-conf
+              mountPath: /etc/redis
+      volumes:
+        - name: redis-conf
+          configMap:
+            name: aegis-redis-config
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: aegis-redis-config
+  namespace: aegis
+data:
+  redis.conf: |
+    bind 0.0.0.0
+    protected-mode no
+    maxmemory 256mb
+    maxmemory-policy allkeys-lru
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-redis
+  namespace: aegis
+spec:
+  selector:
+    app: aegis-redis
+  ports:
+    - port: 6379
+      targetPort: 6379
+
*** End Patch
*** Begin Patch
*** Add File:rl/trainer/rllib_improved_trainer.py
+#!/usr/bin/env python3
+"""
+Improved RLlib trainer scaffold for K8s:
+ - expects Ray cluster available (RayHead + RayWorkers)
+ - submits a Ray Tune job using a provided config file
+ - checkpoints are tarred and uploaded to S3; registry manifest posted
+"""
+import argparse, json, os, subprocess, tempfile
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+REGISTRY_URL = os.environ.get("REGISTRY_URL", "")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN", "")
+
+def run_rllib(config_path, timesteps, s3_prefix):
+    import ray
+    from ray import tune
+    from ray.rllib.agents.ppo import PPOTrainer
+    ray.init(address=os.environ.get("RAY_ADDRESS", "auto"), ignore_reinit_error=True)
+    config = json.load(open(config_path))
+    # register simple env if needed
+    tune.register_env("cartpole", lambda cfg: __import__("gym").make("CartPole-v1"))
+    analysis = tune.run(
+        PPOTrainer,
+        config=config,
+        stop={"timesteps_total": timesteps},
+        checkpoint_at_end=True,
+        local_dir="/tmp/ray_results"
+    )
+    chk = analysis.get_best_checkpoint(metric="episode_reward_mean", mode="max")
+    if not chk:
+        print("No checkpoint found")
+        return
+    tmp = tempfile.mkdtemp()
+    tar = os.path.join(tmp, "checkpoint.tar.gz")
+    subprocess.run(["tar","czf", tar, "-C", os.path.dirname(chk), os.path.basename(chk)], check=False)
+    if EVIDENCE_BUCKET:
+        import boto3
+        key = s3_prefix.rstrip("/") + f"/rllib_{int(os.time() if hasattr(os,'time') else 0)}.tar.gz"
+        s3 = boto3.client("s3")
+        s3.upload_file(tar, EVIDENCE_BUCKET, key)
+        print("Uploaded checkpoint to s3://{}/{}".format(EVIDENCE_BUCKET, key))
+        manifest = {"artifact_id": "rllib-"+str(int(os.time())), "checkpoint_s3": f"s3://{EVIDENCE_BUCKET}/{key}", "ts": int(os.time() if hasattr(os,'time') else 0)}
+        if REGISTRY_URL:
+            try:
+                import requests
+                r = requests.post(f"{REGISTRY_URL}/api/artifacts", json=manifest, timeout=10)
+                print("Registry response:", r.status_code)
+            except Exception as e:
+                print("Registry post failed:", e)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--config", required=True)
+    p.add_argument("--timesteps", type=int, default=200000)
+    p.add_argument("--s3-prefix", default="rl/checkpoints")
+    args = p.parse_args()
+    run_rllib(args.config, args.timesteps, args.s3_prefix)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/rl/rllib_cluster_trainer.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rllib-cluster-train-
+  namespace: aegis
+spec:
+  entrypoint: rllib-cluster
+  templates:
+    - name: rllib-cluster
+      container:
+        image: ghcr.io/yourorg/aegis-rl-rllib:latest
+        command: [python3]
+        args: ["rl/trainer/rllib_improved_trainer.py","--config","/workspace/config.json","--timesteps","200000","--s3-prefix","rl/checkpoints/{{workflow.name}}"]
+      volumeMounts:
+        - name: workspace
+          mountPath: /workspace
+      volumes:
+        - name: workspace
+          emptyDir: {}
+      resources:
+        limits:
+          nvidia.com/gpu: 1
+          memory: "24Gi"
+          cpu: "4"
+
*** End Patch
*** Begin Patch
*** Add File:rl/safety/PROD_SAFETY_GATE.md
+# RL Safety Gate — Production Checklist and Procedures
+
+Purpose
+- Define the production safety gate used to decide whether a trained RL policy may be promoted.
+
+Gate components
+- Deterministic evaluation: run N episodes with deterministic policy (no exploration) and compute avg reward and variance.
+- Safety rule checks: check for out-of-bounds actions, unusually high reward spikes, episode length anomalies, and safety-critical action sequences.
+- Canary environment: run limited canary deployment in a sandbox environment (non-production) and monitor for safety violations.
+- Evidence bundling: produce signed evidence (cosign) and Rekor attestation for the evaluation run and canary logs.
+- Promotion webhook: only invoked after safety gate success; use a service token scoped to promotion-only action.
+
+Operator checklist
+1. Ensure model artifact and manifest available in S3 and registry.
+2. Trigger safety gate Argo workflow (rl/rl_safety_gate_prod.yaml) with model-s3 and manifest-s3 parameters.
+3. Monitor safety-check logs in Argo UI. If failures, collect logs and file an incident.
+4. If safety checks pass, the workflow will upload evidence.s and call the promotion webhook.
+
+Rollback & mitigation
+- If a promoted policy causes issues, use registry to revert to previous model artifact and redeploy previous policy, collect incident evidence, and run root cause analysis.
+
*** End Patch
*** Begin Patch
*** Add File:argo/rl/rl_safety_gate_prod.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rl-safety-gate-prod-
+  namespace: aegis
+spec:
+  entrypoint: safety-gate-prod
+  arguments:
+    parameters:
+      - name: model-s3
+      - name: manifest-s3
+  templates:
+    - name: safety-gate-prod
+      inputs:
+        parameters:
+          - name: model-s3
+          - name: manifest-s3
+      dag:
+        tasks:
+          - name: fetch-model
+            template: aws-sync
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{inputs.parameters.model-s3}}"
+                - name: dst
+                  value: "/workspace/model"
+          - name: run-safety
+            template: safety-checks
+            dependencies: [fetch-model]
+            arguments:
+              parameters:
+                - name: model-path
+                  value: "/workspace/model/model.zip"
+          - name: canary-deploy
+            template: canary
+            dependencies: [run-safety]
+          - name: bundle-evidence
+            template: bundle-evidence
+            dependencies: [canary-deploy]
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+                - name: model-s3
+                  value: "{{inputs.parameters.model-s3}}"
+          - name: promote
+            template: promote
+            dependencies: [bundle-evidence]
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+
+    - name: aws-sync
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: amazon/aws-cli:latest
+        command: [sh, -c]
+        args:
+          - aws s3 cp --recursive "{{inputs.parameters.src}}" "{{inputs.parameters.dst}}"
+
+    - name: safety-checks
+      inputs:
+        parameters:
+          - name: model-path
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [python3]
+        args: ["rl/safety/safety_checks.py","--model","{{inputs.parameters.model-path}}","--env","CartPole-v1","--episodes","20","--min-reward","30"]
+        resources:
+          limits:
+            cpu: "2"
+            memory: "4Gi"
+
+    - name: canary
+      container:
+        image: ghcr.io/yourorg/aegis-rl-canary:latest
+        command: [sh, -c]
+        args:
+          - |
+            # This step should deploy the model to a sandbox namespace for a short canary run
+            # For PoC: run a short local evaluation loop instead (reuse evaluator)
+            python3 rl/eval/evaluator.py --model /workspace/model/model.zip --env CartPole-v1 --episodes 5
+        resources:
+          limits:
+            cpu: "1"
+            memory: "2Gi"
+
+    - name: bundle-evidence
+      inputs:
+        parameters:
+          - name: manifest-s3
+          - name: model-s3
+      container:
+        image: ghcr.io/yourorg/aegis-rl:latest
+        command: [sh, -c]
+        args:
+          - |
+            set -e
+            TMP=/tmp/rl_evidence_$$
+            mkdir -p $TMP
+            aws s3 cp "{{inputs.parameters.manifest-s3}}" $TMP/manifest.json || true
+            aws s3 cp --recursive "{{inputs.parameters.model-s3}}" $TMP/model || true
+            tar czf $TMP/evidence.tgz -C $TMP .
+            if [ -n "${COSIGN_KMS_KEY_ARN:-}" ]; then
+              cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" $TMP/evidence.tgz || true
+            fi
+            aws s3 cp $TMP/evidence.tgz s3://${EVIDENCE_BUCKET}/rl/evidence/ || true
+
+    - name: promote
+      inputs:
+        parameters:
+          - name: manifest-s3
+      container:
+        image: curlimages/curl:7.73.0
+        env:
+          - name: PROMOTION_WEBHOOK
+            value: "{{workflow.parameters.promotion_webhook}}"
+        command: [sh, -c]
+        args:
+          - |
+            # call promotion webhook with minimal payload; the webhook must authenticate & validate
+            curl -X POST -H "Content-Type: application/json" -d '{"type":"promotion","manifest_s3":"{{inputs.parameters.manifest-s3}}"}' ${PROMOTION_WEBHOOK} || true
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_rules_rl_prod.yaml
+groups:
+- name: aegis-rl-prod.rules
+  rules:
+  - alert: AegisRLEvidenceMissing
+    expr: increase(aegis_rl_eval_episodes[1h]) == 0
+    for: 1h
+    labels:
+      severity: warning
+    annotations:
+      summary: "No RL evaluation evidence in the last hour"
+      description: "No evaluation artifacts exported (aegis_rl_eval_episodes == 0). Investigate collectors/evaluator."
+
+  - alert: AegisRLAvgRewardBelowSLO
+    expr: aegis_rl_avg_reward < 20
+    for: 15m
+    labels:
+      severity: page
+    annotations:
+      summary: "RL average reward below SLO"
+      description: "Average reward is below configured threshold (aegis_rl_avg_reward < 20). Check latest eval artifacts and safety gate."
+
+  - alert: AegisRLReplayLagging
+    expr: absent(aegis_rl_replay_size) or aegis_rl_replay_size < 100
+    for: 30m
+    labels:
+      severity: warning
+    annotations:
+      summary: "RL replay buffer low"
+      description: "Replay buffer appears to have fewer than expected transitions; collectors may be failing."
+
+  - alert: AegisRLLatencySpike
+    expr: rate(http_request_duration_seconds_sum{job="triton"}[5m]) / rate(http_request_duration_seconds_count{job="triton"}[5m]) > 1.5
+    for: 10m
+    labels:
+      severity: page
+    annotations:
+      summary: "Triton latency spike detected"
+      description: "Average Triton latency increased more than 50% over 5m. Investigate model performance and instance groups."
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana/rl_dashboard.json
+{
+  "annotations": {"list": []},
+  "title": "Aegis RL Dashboard",
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Average Eval Reward",
+      "targets": [{"expr": "aegis_rl_avg_reward", "legendFormat": "avg_reward"}],
+      "id": 1
+    },
+    {
+      "type": "graph",
+      "title": "Replay Size",
+      "targets": [{"expr": "aegis_rl_replay_size", "legendFormat": "replay_size"}],
+      "id": 2
+    },
+    {
+      "type": "graph",
+      "title": "Eval Episodes",
+      "targets": [{"expr": "aegis_rl_eval_episodes", "legendFormat": "episodes"}],
+      "id": 3
+    },
+    {
+      "type": "graph",
+      "title": "Triton Avg Latency",
+      "targets": [{"expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"triton\"}[5m])) by (le))", "legendFormat": "p95"}],
+      "id": 4
+    }
+  ],
+  "schemaVersion": 16,
+  "version": 0
+}
+
*** End Patch
*** Begin Patch
*** Add File:docs/rl_slos_and_alerting_playbook_prod.md
+# RL SLOs & Alerting — Production Playbook
+
+Overview
+- This document extends the RL playbook with production‑grade SLOs, alerting responses and runbook steps for escalations.
+
+Key SLOs (recommended)
+- Eval average reward: p95(avg_reward) >= 20 (alert if below for 15m)
+- Eval frequency: at least 1 evaluation run per hour (alert if none for 1h)
+- Replay buffer size: > 1000 transitions (warning if below 1000 for 30m)
+- Triton model latency: p95 < 200ms for critical models (page if exceeded for 10m)
+
+On‑call runbook (when alert fires)
+- AegisRLEvidenceMissing:
+  1. Check Argo workflow logs for evaluator and collector.
+  2. Check S3 access and permissions.
+  3. Restart collector deployments if needed and re-run replay builder.
+
+- AegisRLAvgRewardBelowSLO:
+  1. Pull the latest eval artifact from S3 and run a local deterministic eval with rl/eval/evaluator.py.
+  2. If the policy regressed, switch to previous checkpoint via the registry and redeploy.
+  3. Open an incident and notify model owners; include evidence of eval logs and replay data.
+
+- AegisRLReplayLagging:
+  1. Check Redis / S3 for collector payloads.
+  2. Re-run rl/replay/s3_parquet_replay.py to build new replay artifacts.
+  3. If collectors are failing due to infra, scale replicas or fix network issues.
+
+Post‑incident
+- Run full root cause analysis (RCA), capture signed evidence (cosign), and add remediation tasks to PM system.
+
*** End Patch
*** End Patch
