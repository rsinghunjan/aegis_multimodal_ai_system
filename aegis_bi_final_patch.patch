*** Begin Patch
*** Add File:data_quality/great_expectations/great_expectations.yml
+# Great Expectations configuration (minimal scaffold)
+# Operators: customize datasources to point at your warehouse.
+config_version: 3
+datasources:
+  warehouse:
+    class_name: SqlAlchemyDatasource
+    module_name: great_expectations.datasource
+    credentials:
+      url: ${GE_WAREHOUSE_DSN}  # e.g. postgresql://user:pass@host:5432/aegis_dw
+
+stores:
+  expectations_store:
+    class_name: ExpectationsStore
+    store_backend:
+      class_name: FixedLengthTupleFilesystemStoreBackend
+      base_directory: expectations
+  validations_store:
+    class_name: ValidationsStore
+    store_backend:
+      class_name: FixedLengthTupleFilesystemStoreBackend
+      base_directory: validations
+  evaluation_parameter_store:
+    class_name: EvaluationParameterStore
+
+validation_operators:
+  action_list_operator:
+    class_name: ActionListValidationOperator
+    action_list:
+      - name: store_validation_result
+        action:
+          class_name: StoreValidationResultAction
+      - name: update_data_docs
+        action:
+          class_name: UpdateDataDocsAction
+
+data_docs_sites:
+  local_site:
+    class_name: SiteBuilder
+    store_backend:
+      class_name: TupleFilesystemStoreBackend
+      base_directory: uncommitted/data_docs/local_site
+    site_index_builder:
+      class_name: DefaultSiteIndexBuilder
+
*** End Patch
*** Begin Patch
*** Add File:data_quality/great_expectations/run_expectations.py
+#!/usr/bin/env python3
+"""
+Run Great Expectations validation for the core marts (core_job_runs, daily_team_rollup).
+Outputs validation results to S3 evidence bucket and fails non-OK checks.
+"""
+import os, json, sys
+from great_expectations.data_context import DataContext
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+RESULTS_S3_PREFIX = os.environ.get("GE_RESULTS_PREFIX", "data-quality/validations")
+
+def main():
+    ctx = DataContext(context_root_dir="data_quality/great_expectations")
+    suites = ["core_job_runs_suite", "daily_team_rollup_suite"]
+    overall_ok = True
+    for suite in suites:
+        try:
+            batch_request = {
+                "datasource_name": "warehouse",
+                "data_connector_name": "default_runtime_data_connector_name",
+                "data_asset_name": suite,
+                "runtime_parameters": {"query": f"select * from marts.{suite.replace('_suite','')}" },
+                "batch_identifiers": {"default_identifier_name": "validation_run"}
+            }
+            res = ctx.run_validation_operator("action_list_operator", assets_to_validate=[batch_request])
+            # Persist result JSON to local file then upload via aws cli if configured
+            out = f"/tmp/ge_{suite}_result.json"
+            with open(out,"w") as f:
+                json.dump(res, f, indent=2)
+            if EVIDENCE_BUCKET:
+                os.system(f"aws s3 cp {out} s3://{EVIDENCE_BUCKET}/{RESULTS_S3_PREFIX}/{os.path.basename(out)} || true")
+            status = res.get("success", False)
+            if not status:
+                overall_ok = False
+        except Exception as e:
+            print("GE run failed for", suite, e)
+            overall_ok = False
+    if not overall_ok:
+        print("Data quality checks failed")
+        sys.exit(2)
+    print("Great Expectations validations passed")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:datahub/datahub_ingest_recipe.yml
+# DataHub ingestion recipe (ingest lineage from dbt + raw metadata)
+source:
+  type: dbt
+  config:
+    manifest_path: /opt/bi/target/manifest.json
+    catalog_path: /opt/bi/target/catalog.json
+transformations:
+  - type: generic
+    config: {}
+sink:
+  type: datahub-rest
+  config:
+    server: ${DATAHUB_SERVER}  # e.g. http://datahub-gms:8080
+
*** End Patch
*** Begin Patch
*** Add File:ingest/s3_parquet_pipeline.py
+#!/usr/bin/env python3
+"""
+Bulk ingestion helper:
+ - Converts evidence JSON files in S3 to parquet (partition by date) and writes to s3://<bucket>/parquet/... for bulk loading.
+ - Uses pyarrow/pandas to convert. Operator must ensure sufficient memory/compute.
+"""
+import os, boto3, json, tempfile
+import pandas as pd
+import pyarrow as pa
+import pyarrow.parquet as pq
+from urllib.parse import urlparse
+
+S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "aegis-evidence")
+PREFIX = os.environ.get("EVIDENCE_PREFIX", "")
+OUT_PREFIX = os.environ.get("PARQUET_OUT_PREFIX", "parquet/evidence")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def list_keys(prefix):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            yield obj["Key"]
+
+def convert_batch(keys, tmpdir):
+    rows = []
+    for k in keys:
+        if not k.endswith(".json"):
+            continue
+        obj = s3.get_object(Bucket=S3_BUCKET, Key=k)
+        body = obj["Body"].read().decode("utf-8")
+        try:
+            j = json.loads(body)
+            # flatten minimal fields for ETL
+            rows.append({
+                "s3_key": k,
+                "job_id": j.get("job_id"),
+                "team": j.get("team"),
+                "start_ts": j.get("start_ts"),
+                "end_ts": j.get("end_ts"),
+                "estimated_kg": j.get("estimated_kg")
+            })
+        except Exception:
+            continue
+    if not rows:
+        return None
+    df = pd.DataFrame(rows)
+    table = pa.Table.from_pandas(df)
+    out_file = os.path.join(tmpdir, "batch.parquet")
+    pq.write_table(table, out_file)
+    return out_file
+
+def upload_parquet(local_path, out_key):
+    s3.upload_file(local_path, S3_BUCKET, out_key)
+    print("Uploaded", out_key)
+
+def main():
+    tmpdir = tempfile.mkdtemp()
+    keys = list(list_keys(PREFIX))
+    batch = keys[:1000]
+    if not batch:
+        print("No keys")
+        return
+    out_local = convert_batch(batch, tmpdir)
+    if out_local:
+        out_key = f"{OUT_PREFIX}/batch_{int(os.path.getctime(out_local))}.parquet"
+        upload_parquet(out_local, out_key)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ingest/redshift_copy_example.sql
+-- Example Redshift COPY command to bulk load Parquet from S3
+-- Requires IAM role attached to Redshift cluster with S3 read permissions
+COPY raw_table
+FROM 's3://{bucket}/{prefix}/'
+CREDENTIALS 'aws_iam_role={REDSHIFT_IAM_ROLE_ARN}'
+FORMAT AS PARQUET;
+
*** End Patch
*** Begin Patch
*** Add File:airflow/dags/aegis_etl_enhanced.py
+from datetime import datetime, timedelta
+from airflow import DAG
+from airflow.operators.python import PythonOperator
+from airflow.operators.bash import BashOperator
+import subprocess, os
+
+DEFAULT_ARGS = {
+    'owner': 'aegis',
+    'depends_on_past': False,
+    'email_on_failure': False,
+    'retries': 1,
+    'retry_delay': timedelta(minutes=5),
+}
+
+def run_parquet_pipeline(**kwargs):
+    cmd = "python3 /opt/ingest/s3_parquet_pipeline.py"
+    ret = subprocess.run(cmd, shell=True)
+    if ret.returncode != 0:
+        raise RuntimeError("parquet pipeline failed")
+
+def run_great_expectations(**kwargs):
+    cmd = "python3 /opt/data_quality/great_expectations/run_expectations.py"
+    ret = subprocess.run(cmd, shell=True)
+    return ret.returncode
+
+def run_dbt(**kwargs):
+    cmd = "cd /opt/bi && dbt deps && dbt run --profiles-dir /opt/bi/profiles"
+    ret = subprocess.run(cmd, shell=True)
+    if ret.returncode != 0:
+        raise RuntimeError("dbt run failed")
+
+def notify_slack(message):
+    webhook = os.environ.get("SLACK_WEBHOOK_URL")
+    if not webhook:
+        print("No slack webhook")
+        return
+    import requests
+    requests.post(webhook, json={"text": message}, timeout=5)
+
+def on_failure_callback(context):
+    msg = f"ETL DAG failed: {context.get('task_instance').task_id}"
+    notify_slack(msg)
+
+with DAG(
+    dag_id="aegis_etl_enhanced",
+    default_args=DEFAULT_ARGS,
+    schedule_interval="@daily",
+    start_date=datetime(2025,1,1),
+    catchup=False,
+    max_active_runs=1,
+    on_failure_callback=on_failure_callback
+) as dag:
+
+    parquet = PythonOperator(
+        task_id="s3_to_parquet",
+        python_callable=run_parquet_pipeline
+    )
+
+    ge = PythonOperator(
+        task_id="great_expectations_validate",
+        python_callable=run_great_expectations
+    )
+
+    dbt_run = PythonOperator(
+        task_id="dbt_run",
+        python_callable=run_dbt
+    )
+
+    notify = BashOperator(
+        task_id="notify_success",
+        bash_command='echo "ETL pipeline completed"'
+    )
+
+    parquet >> ge >> dbt_run >> notify
+
*** End Patch
*** Begin Patch
*** Add File:governance/superset_sso_rbac.md
+# Superset SSO & RBAC Notes
+
+1) SSO via OIDC (example with Keycloak)
+ - Set environment variables for Superset:
+   - AUTH_TYPE = AUTH_OID, OIDC or configure via config.py
+   - OAUTH_PROVIDERS = [{'name':'keycloak','token_key':'access_token', ...}]
+ - Configure roles mapping from OIDC groups to Superset roles.
+
+2) Row-Level Security (RLS) in Superset (Postgres example)
+ - Create a view in the warehouse that applies RLS predicate. Example:
+   CREATE POLICY team_policy ON marts.core_job_runs USING (team = current_setting('app.team','')::text);
+ - In Superset, use database connection pre-queries to set local parameter:
+   SET app.team = '{{ current_user.custom_attribute.team }}';
+ - Alternatively use dataset permissions to restrict access by role.
+
+3) Dataset-level RBAC
+ - Use Superset's roles and permissions; create roles per-team and grant access to specific datasets/dashboards.
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_rules_bi.yaml
+groups:
+- name: aegis-bi.rules
+  rules:
+  - alert: AirflowDagFailures
+    expr: increase(airflow_dag_run_failed_count[15m]) > 0
+    for: 5m
+    labels:
+      severity: page
+    annotations:
+      summary: "Airflow DAG failures detected"
+
+  - alert: DataQualityFailed
+    expr: increase(aegis_ge_validation_failures_total[1h]) > 0
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Great Expectations validations failing"
+
+  - alert: DataDriftDetected
+    expr: aegis_data_drift_score > 0.5
+    for: 30m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Data drift detected"
+
*** End Patch
*** Begin Patch
*** Add File:bi/models/marts/cost_allocation.sql
+-- Allocate cloud costs to teams using tags and proportionally by resource usage
+with costs as (
+  select service, resource_id, cost_usd, tags->>'team' as team from {{ source('raw','raw_costs') }}
+),
+usage as (
+  select team, sum(cpu_hours + gpu_hours*4) as compute_score from {{ ref('core_job_runs') }} group by team
+),
+alloc as (
+  select c.team, sum(c.cost_usd) as cost_usd
+  from costs c
+  group by c.team
+)
+select
+  coalesce(u.team, a.team) as team,
+  coalesce(sum(a.cost_usd),0) as allocated_cost_usd
+from alloc a
+left join usage u on a.team = u.team
+group by 1
+
*** End Patch
*** Begin Patch
*** Add File:bi/models/marts/model_performance.sql
+-- Per-model performance aggregations
+select
+  model_name,
+  date_trunc('day', start_ts)::date as day,
+  count(*) as requests,
+  avg(duration_s) as avg_latency_s,
+  percentile_cont(0.95) within group (order by duration_s) as p95_latency_s,
+  sum(case when status != 'success' then 1 else 0 end) as errors
+from {{ ref('core_job_runs') }}
+group by 1,2
+
*** End Patch
*** Begin Patch
*** Add File:bi/models/marts/churn_cohort.sql
+-- Minimal churn/corpus example: cohort by model version or workflow
+with users as (
+  select model_name, workflow_name, team, start_ts::date as run_date from {{ ref('core_job_runs') }}
+)
+select model_name, run_date, count(*) as runs
+from users
+group by 1,2
+order by 1,2
+
*** End Patch
*** Begin Patch
*** Add File:bi/models/schema_extra.yml
+version: 2
+models:
+  - name: cost_allocation
+    description: "Allocate cloud costs to teams"
+  - name: model_performance
+    description: "Per model latency and error metrics"
+  - name: churn_cohort
+    description: "Cohort style run counts"
+
*** End Patch
*** Begin Patch
*** Add File:ui/superset_more_dashboards.json
+{
+  "dashboard": {
+    "dashboard_title": "Aegis BI: Finance & Ops",
+    "slices": [
+      {"slice_name":"Team Cost Allocation","viz_type":"pie"},
+      {"slice_name":"Model Performance Overview","viz_type":"line"}
+    ]
+  }
+}
*** End Patch
*** Begin Patch
*** Add File:runbooks/BI_operational_runbook.md
+# BI Operational Runbook (staging â†’ prod)
+
+1) Provision warehouse and credentials. Prefer a managed DW (Snowflake/BigQuery) for scale; Postgres is ok for pilot.
+2) Deploy ingestion containers and schedule Airflow DAG aegis_etl_enhanced.
+3) Ensure Great Expectations configs point to the warehouse and run weekly validations.
+4) Configure DataHub ingestion (dbt manifest) to capture lineage and materialization details.
+5) Configure Superset SSO and dataset RBAC; set row-level security via DB policies or Superset pre-queries.
+6) Monitor Prometheus alerts for DAG failures, data quality failures and drift.
+7) Approve calibration drafts via reconcile approve service; automate only after human vetting.
+8) On acceptance, promote dbt models to production and configure daily reports for FinOps and Sustainability.
+
*** End Patch
*** End Patch
