*** Begin Patch
*** Add File:feature_store/README_FEAST.md
+Feast feature store scaffold for Aegis
+-------------------------------------
+
+Overview:
+- This folder contains a minimal Feast scaffold to register features, materialize them to an online store (e.g., Redis/Bigtable) and produce feature snapshots for training (offline store in S3 / Parquet).
+
+Files:
+- feature_repo/feature_repo.py : Feast feature definitions and entity declarations.
+- feature_repo/docker/Dockerfile : image to run Feast materialization and feature ingestion jobs.
+
+Quickstart (high-level):
+1. Install Feast in your environment (pip install feast).
+2. Configure Feast to use your offline/online stores (see feature_repo/feature_repo.py).
+3. Register entities and features:
+     feast apply --repo feature_store/feature_repo
+4. Ingest feature data (batch): run a job that writes feature vectors to offline store and runs feast materialize.
+5. Materialize to online store for serving: feast materialize-incremental ...
+
+Notes:
+- This scaffold is intentionally minimal. For production, configure access control, encryption, and CI jobs that verify materialized feature snapshots match training inputs (reproducibility).
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/feature_repo/feature_repo.py
+"""
+Minimal Feast repo example (Aegis)
+ - Defines an entity `job_run` keyed by job_id
+ - Defines a simple feature: recent_cpu_usage
+ - Offline store: file-based/Parquet (placeholder); Online store: Redis (placeholder)
+
+Adjust adapter configs to match your infra (S3 for offline, Redis/Bigtable for online).
+"""
+from feast import Entity, Feature, FeatureView, ValueType, FileSource
+from datetime import timedelta
+
+# Define entity
+job_run = Entity(name="job_id", description="Job run id", value_type=ValueType.STRING)
+
+# Define data source - replace path with s3:// or other offline store
+job_run_source = FileSource(
+    path="s3://REPLACE_WITH_EVIDENCE_BUCKET/feature_snapshots/job_run_features.parquet",
+    timestamp_field="event_ts",
+)
+
+# Define feature view
+job_run_fv = FeatureView(
+    name="job_run_features",
+    entities=["job_id"],
+    ttl=timedelta(days=30),
+    features=[
+        Feature(name="recent_cpu_usage", dtype=ValueType.FLOAT),
+        Feature(name="recent_gpu_usage", dtype=ValueType.INT32),
+    ],
+    input=job_run_source,
+    online=True,
+    tags={"team": "aegis"},
+)
+
+# Feast repository requires a list of feature views and entities usually registered via CLI.
+# See Feast docs for repo layout (feature_store/feature_repo is minimal example).
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/feature_repo/docker/Dockerfile
+FROM python:3.11-slim
+WORKDIR /app
+RUN pip install feast boto3 pandas pyarrow
+COPY . /app
+CMD ["bash", "-lc", "echo 'Run feast CLI in mounted repo'"]
+
*** End Patch
*** Begin Patch
*** Add File:explainability/shap_explainer.py
+#!/usr/bin/env python3
+"""
+Compute SHAP explanations for a model and store artifacts to S3 and MLflow.
+Usage:
+  python3 shap_explainer.py --model-path /path/to/artifact --data /path/to/sample.csv --s3-prefix explanations/<run-id>/
+"""
+import os
+import argparse
+import pandas as pd
+import boto3
+import json
+import mlflow
+import joblib
+
+def upload_to_s3(bucket, key, local_path):
+    s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION"))
+    s3.upload_file(local_path, bucket, key)
+    return f"s3://{bucket}/{key}"
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-path", required=True)
+    p.add_argument("--data", required=True)
+    p.add_argument("--s3-prefix", required=True, help="s3 prefix e.g. explanations/run-123/")
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--mlflow-run-id", default=None)
+    args = p.parse_args()
+
+    # Load model and sample data
+    model = joblib.load(args.model_path)
+    X = pd.read_csv(args.data)
+
+    # Lazy import shap to avoid heavy CI cost when not used
+    try:
+        import shap
+    except Exception:
+        raise SystemExit("Install shap (pip install shap) in execution environment")
+
+    explainer = shap.Explainer(model.predict, X)
+    shap_values = explainer(X)
+
+    out_json = args.s3_prefix.rstrip("/") + "/shap_summary.json"
+    tmp = "/tmp/shap_summary.json"
+    # Save a compact summary: mean absolute shap per feature
+    mean_abs = dict(zip(X.columns, list(abs(shap_values.values).mean(axis=0))))
+    with open(tmp, "w") as f:
+        json.dump({"mean_abs_shap": mean_abs, "sample_rows": min(100, len(X))}, f)
+
+    s3uri = upload_to_s3(args.s3_bucket, out_json, tmp)
+    print("Uploaded SHAP summary to", s3uri)
+
+    if args.mlflow_run_id:
+        mlflow.log_artifact(tmp, artifact_path="explainability")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:explainability/notebooks/example_explainability.py
+"""
+Example script for analysts to compute SHAP explanations and store them.
+This is a runnable "notebook" style script.
+"""
+import os
+import pandas as pd
+from explainability.shap_explainer import main as run_shap
+
+if __name__ == "__main__":
+    # Variables to set
+    MODEL_PATH = os.environ.get("MODEL_PATH", "/opt/models/my_model.pkl")
+    SAMPLE_CSV = os.environ.get("SAMPLE_CSV", "/opt/data/sample_features.csv")
+    S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+    S3_PREFIX = os.environ.get("S3_PREFIX", "explanations/run-manual")
+
+    # call SHAP runner
+    import sys
+    sys.argv = ["shap_explainer.py", "--model-path", MODEL_PATH, "--data", SAMPLE_CSV, "--s3-prefix", S3_PREFIX, "--s3-bucket", S3_BUCKET]
+    run_shap()
+
*** End Patch
*** Begin Patch
*** Add File:retrain/trigger_retrain.py
+#!/usr/bin/env python3
+"""
+Checks a Prometheus drift metric and triggers a K8s retrain Job if threshold exceeded.
+ - Uses PROM_ENDPOINT to query current drift score for target model.
+ - If drift_score > THRESH, submits a retrain job manifest to the cluster.
+"""
+import os
+import requests
+import subprocess
+import argparse
+from datetime import datetime
+
+PROM_ENDPOINT = os.environ.get("PROM_ENDPOINT", "http://prometheus.monitoring.svc:9090")
+DRIFT_METRIC = os.environ.get("DRIFT_METRIC", "aegis_model_drift_score")
+MODEL_NAME = os.environ.get("MODEL_NAME", "runtime_model")
+RETRAIN_JOB_MANIFEST = os.environ.get("RETRAIN_JOB_MANIFEST", "retrain/retrain_job.yaml")
+THRESH = float(os.environ.get("DRIFT_THRESHOLD", "0.5"))
+K8S_NAMESPACE = os.environ.get("K8S_NAMESPACE", "aegis")
+
+def query_prometheus(metric):
+    q = f"{metric}"
+    r = requests.get(f"{PROM_ENDPOINT}/api/v1/query", params={"query": q}, timeout=10)
+    r.raise_for_status()
+    data = r.json()
+    if data["status"] != "success":
+        return None
+    results = data["data"]["result"]
+    if not results:
+        return None
+    # take max value
+    vals = [float(x["value"][1]) for x in results]
+    return max(vals)
+
+def trigger_retrain():
+    # apply retrain manifest; retrain job should write artifacts to evidence S3 and MLflow
+    subprocess.run(["kubectl", "-n", K8S_NAMESPACE, "apply", "-f", RETRAIN_JOB_MANIFEST], check=True)
+    print("Triggered retrain job at", datetime.utcnow().isoformat())
+
+def main():
+    drift = query_prometheus(DRIFT_METRIC)
+    print("Drift metric value:", drift)
+    if drift is not None and drift > THRESH:
+        print("Drift threshold exceeded, triggering retrain")
+        trigger_retrain()
+    else:
+        print("Drift below threshold; no action")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:retrain/retrain_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-retrain-job
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-ci-sa
+      containers:
+      - name: retrain
+        image: ghcr.io/yourorg/aegis-retrain:latest
+        env:
+          - name: EVIDENCE_BUCKET
+            value: "REPLACE_WITH_EVIDENCE_BUCKET"
+          - name: MLFLOW_TRACKING_URI
+            value: "http://mlflow.example.com"
+        command: ["/bin/sh","-c"]
+        args:
+          - /opt/retrain/run_retrain.sh
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/drift_trigger_retrain.yml
+name: Drift-triggered retrain checker
+
+on:
+  schedule:
+    - cron: '*/15 * * * *' # every 15 minutes
+  workflow_dispatch:
+
+env:
+  PROM_ENDPOINT: ${{ secrets.PROM_ENDPOINT }}
+  DRIFT_METRIC: "aegis_model_drift_score"
+  DRIFT_THRESHOLD: "0.5"
+  K8S_NAMESPACE: "aegis"
+
+jobs:
+  check-and-trigger:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install requests
+      - name: Run drift checker
+        env:
+          PROM_ENDPOINT: ${{ secrets.PROM_ENDPOINT }}
+          DRIFT_METRIC: ${{ env.DRIFT_METRIC }}
+          DRIFT_THRESHOLD: ${{ env.DRIFT_THRESHOLD }}
+          K8S_NAMESPACE: ${{ env.K8S_NAMESPACE }}
+        run: |
+          python3 retrain/trigger_retrain.py
+
*** End Patch
*** Begin Patch
*** Add File:differential_privacy/dp_aggregate.py
+#!/usr/bin/env python3
+"""
+Simple differentially private aggregator using IBM diffprivlib (if available).
+This file provides an API to compute DP counts/means with epsilon calibration.
+"""
+import os
+import argparse
+try:
+    from diffprivlib.mechanisms import LaplaceTruncated
+    from diffprivlib.tools import mean as dp_mean
+except Exception:
+    LaplaceTruncated = None
+    dp_mean = None
+
+def dp_count(count, epsilon=1.0, sensitivity=1.0):
+    if LaplaceTruncated is None:
+        raise SystemExit("diffprivlib not installed. pip install diffprivlib")
+    mech = LaplaceTruncated(epsilon=epsilon, sensitivity=sensitivity)
+    return mech.randomise(count)
+
+def dp_mean_wrapper(values, epsilon=1.0):
+    if dp_mean is None:
+        raise SystemExit("diffprivlib not installed. pip install diffprivlib")
+    return dp_mean(values, epsilon=epsilon)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--op", choices=["count","mean"], required=True)
+    p.add_argument("--value", type=float, default=0)
+    p.add_argument("--epsilon", type=float, default=1.0)
+    args = p.parse_args()
+    if args.op == "count":
+        print(dp_count(int(args.value), epsilon=args.epsilon))
+    else:
+        # example: run with comma separated sample in environment
+        print(dp_mean_wrapper([args.value], epsilon=args.epsilon))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ci/gated_promotion.yml
+name: Gated Promotion: DQ + Calibration + Cosign check
+
+on:
+  workflow_dispatch:
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  MIN_MEDIAN_THRESHOLD: "0.10"
+
+jobs:
+  validate-and-gate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install boto3
+      - name: Run GE validations (best-effort)
+        run: python3 data_quality/run_expectations.py || true
+      - name: Fetch latest calibration validation
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          aws s3 ls s3://${EVIDENCE_BUCKET}/calibration/validation_reports/ || true
+          LATEST=$(aws s3 ls s3://${EVIDENCE_BUCKET}/calibration/validation_reports/ | tail -n1 | awk '{print $4}')
+          aws s3 cp s3://${EVIDENCE_BUCKET}/calibration/validation_reports/${LATEST} /tmp/latest_validation.json || exit 1
+          cat /tmp/latest_validation.json
+      - name: Check median threshold
+        run: |
+          MEDIAN=$(jq -r '.median_relative_error' /tmp/latest_validation.json)
+          echo "Median = $MEDIAN"
+          python3 - <<PY
+import json,sys
+j=json.load(open('/tmp/latest_validation.json'))
+if j.get('median_relative_error') is None:
+    print('No median found; failing gate'); sys.exit(1)
+if float(j['median_relative_error']) > float(${MIN_MEDIAN_THRESHOLD}):
+    print('Median above threshold; failing gate'); sys.exit(2)
+print('Median within threshold.')
+PY
+      - name: Check cosign signature (operator must upload signed profile to S3)
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          # Expect an approved signed artifact at s3://$EVIDENCE_BUCKET/calibration/approved/power_profiles.yaml.sig
+          aws s3 ls s3://${EVIDENCE_BUCKET}/calibration/approved/power_profiles.yaml.sig || (echo "Signed artifact missing" && exit 3)
+      - name: Manual approval
+        uses: hmarr/auto-approve-action@v2
+        with:
+          github_token: ${{ secrets.GITHUB_TOKEN }}
+
*** End Patch
*** Begin Patch
*** Add File:superset/model_explainability_dashboard.json
+{
+  "dashboard": {
+    "dashboard_title": "Model Explainability & Cost",
+    "position_json": "{}",
+    "slices": [
+      {
+        "slice_name": "Top SHAP Features (Mean Abs)",
+        "viz_type": "table",
+        "params": {
+          "metrics": ["mean_abs_shap"],
+          "groupby": ["feature"]
+        },
+        "data_source_name": "explainability.shap_summary"
+      },
+      {
+        "slice_name": "Model Cost vs P95 Latency",
+        "viz_type": "scatter",
+        "params": {
+          "x": "allocated_cost_usd",
+          "y": "p95_duration_s",
+          "groupby": ["model_name"]
+        },
+        "data_source_name": "marts.model_performance"
+      }
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:issues/FEATURE_ROLLOUT_PLAN.md
+# Feature rollout plan & prioritized issues (Feature store, explainability, retrain triggers, DP)
+
+Summary
+- This plan prioritizes adding:
+  1) Feature store scaffold (Feast) integrated into ETL and training workflows.
+  2) Explainability: SHAP compute and artifact storage linked to MLflow & evidence S3.
+  3) Automated retrain triggers on drift (Prometheus → retrain job).
+  4) Differential privacy options for aggregated analytics and select exports.
+
+Prioritized issues (title / owner / estimate days)
+- "Integrate Feast feature repo & nightly materialization" / @data-eng / 5
+- "Ingest feature snapshots to S3 and register Feast offline store" / @data-eng / 3
+- "Add SHAP explainability runner + MLflow artifact storage" / @ml-platform / 3
+- "Add SHAP summary dashboard to Superset and link to dbt marts" / @analytics / 2
+- "Implement drift monitor → retrain trigger job and CI workflow" / @ml-platform / @sre / 4
+- "Add simple DP aggregator library & integrate into cost reports" / @data-privacy / 3
+- "CI gating: require calibrations + cosign signatures before promotions" / @platform / 2
+- "Run security review for Feast and explainability artifacts (RBAC/KMS)" / @security / 2
+
+Rollout milestones
+- Week 0–1: Feast scaffold + ingest snapshot job; SHAP runner + examples.
+- Week 1–2: Retrain trigger + CI workflow; Superset explainability dashboard.
+- Week 2–4: DP integration for cost/allocation pipelines; security review; get approvals.
+
*** End Patch
*** Begin Patch
*** Add File:notebooks/example_analyst_workflow.py
+"""
+Analyst example: join model_performance with cost_allocation and compute cost per p95
+Run in a notebook or as a script against your DW.
+"""
+import os
+import pandas as pd
+import sqlalchemy
+
+DW_CONN = os.environ.get("DW_CONN", "postgresql://user:pass@host:5432/aegis_dw")
+
+query = """
+WITH cost AS (
+  SELECT team, allocated_cost_usd FROM marts.cost_allocation
+),
+perf AS (
+  SELECT model_name, day, p95_duration_s FROM marts.model_performance WHERE day >= current_date - interval '30 days'
+)
+SELECT p.model_name, p.day, p.p95_duration_s, c.allocated_cost_usd
+FROM perf p
+LEFT JOIN cost c ON c.team = split_part(p.model_name,'-',1) -- example join heuristic
+ORDER BY p.p95_duration_s DESC
+LIMIT 100
+"""
+
+engine = sqlalchemy.create_engine(DW_CONN)
+df = pd.read_sql(query, engine)
+print(df.head())
+
*** End Patch
*** Begin Patch
*** Add File:README_ADDITIONAL_FEATURES.md
+Additional features added in this patch
+ - Feature store (Feast) scaffold: feature definitions + Dockerfile for materialization jobs.
+ - SHAP explainability runner and example "notebook" script; stores summary artifacts to S3 and MLflow.
+ - Drift-trigger retrain: trigger_retrain.py, retrain job manifest, and GitHub workflow to run checks periodically.
+ - Differential privacy aggregator scaffold using diffprivlib (optional dependency).
+ - CI gated promotion workflow: ensures calibration median and cosign-signed promotion artifact present before manual approval.
+ - Superset explainability dashboard JSON and analyst notebook example that joins cost & performance marts.
+ - Prioritized rollout plan and issue list.
+
+How to use
+ - Replace placeholders (S3 bucket, Prometheus endpoint, K8s contexts, etc.) with your environment values.
+ - Deploy retrain job image and ensure it writes artifacts to evidence S3 & MLflow.
+ - Install feast and configure offline/online stores if you want feature serving.
+ - Install diffprivlib in the environment where DP functions will be used if you need DP guarantees.
+
*** End Patch
*** End Patch
