*** Begin Patch
*** Add File:ops/operator_playbook_full.md
+# Aegis Operator Playbook — Full bootstrap, secrets, HA, security, DR and runbooks
+
+Purpose
+- Ready-to-run, fully‑templated operator playbook with exact CLI commands and sample AWS/account/region values, plus checklist and guardrails. Use this to bring a staging Aegis environment to hardened state safely.
+
+Important: This playbook runs destructive or sensitive actions (provisioning, secret updates, backups/restores). Read top‑to‑bottom, run commands interactively and never run blindly in production.
+
+Assumptions & samples
+- AWS account: 123456789012 (replace)
+- AWS region: us-west-2 (replace)
+- GitHub repo: owner/repo (replace)
+- Staging k8s context: kubeconfig at ~/.kube/staging (or KUBECONFIG_STAGING env)
+- GHCR org: ghcr.io/yourorg (replace)
+- Evidence S3 bucket: aegis-evidence-12345 (replace)
+- COSIGN KMS ARN: arn:aws:kms:us-west-2:123456789012:key/REPLACE_KEY_ID
+- REKOR_URL: https://rekor-staging.example.com:3000
+
+Environment variables (set before running interactive sections)
+- export AWS_REGION=us-west-2
+- export AWS_PROFILE=aegis-admin
+- export GITHUB_REPOSITORY=owner/repo
+- export GITHUB_TOKEN=<token with repo:write & secrets permissions>
+- export KUBECONFIG_STAGING=~/.kube/staging
+- export EVIDENCE_BUCKET=aegis-evidence-12345
+- export COSIGN_KMS_ARN=arn:aws:kms:us-west-2:123456789012:key/REPLACE_KEY_ID
+- export REKOR_URL=https://rekor-staging.example.com:3000
+
+Sections
+1) Verify prerequisites
+ - CLI: aws, kubectl, helm, jq, yq, gh, docker, terraform
+ - Local kubeconfig for staging: kubectl --context staging get nodes
+
+2) Terraform & infra outputs (operator-run)
+ - cd infra/terraform
+ - terraform init
+ - terraform plan -var "aws_region=${AWS_REGION:-us-west-2}"
+ - terraform apply -auto-approve -var "aws_region=${AWS_REGION:-us-west-2}"
+ - terraform output -json > /tmp/aegis_tf_output.json
+ - Review outputs:
+   - jq .cosign_kms_arn /tmp/aegis_tf_output.json
+   - jq .evidence_bucket /tmp/aegis_tf_output.json
+
+3) Populate secrets (GitHub, SecretsManager, SSM)
+ - Review ops/populate_secrets_gh_and_aws.sh and run:
+   - chmod +x ops/populate_secrets_gh_and_aws.sh
+   - ./ops/populate_secrets_gh_and_aws.sh /tmp/aegis_tf_output.json
+ - Confirm GitHub secrets present:
+   - gh secret list --repo $GITHUB_REPOSITORY
+ - Confirm AWS SecretsManager/SSM values:
+   - aws secretsmanager get-secret-value --secret-id aegis/cosign
+   - aws ssm get-parameter --name /aegis/evidence_bucket
+
+4) Build & publish pipeline images (GHCR/ECR)
+ - Option A (CI): Trigger GitHub Action:
+   - gh workflow run build_and_push_images.yml --repo $GITHUB_REPOSITORY
+ - Option B (local):
+   - docker build -t ghcr.io/<org>/aegis-tools:latest -f tools/Dockerfile .
+   - docker push ghcr.io/<org>/aegis-tools:latest
+   - Repeat for aegis-train and aegis-transformer
+ - Verify images: docker pull ghcr.io/<org>/aegis-tools:latest
+
+5) ExternalSecrets & IRSA
+ - Apply ExternalSecret manifests:
+   - kubectl apply -f k8s/external-secrets/externalsecret_db.yaml -n aegis
+ - If IRSA role exists, annotate serviceaccount:
+   - ./k8s/irsa/annotate_serviceaccounts.sh cosign-signer security arn:aws:iam::123456789012:role/aegis-cosign-irsa-role
+ - Verify pod access:
+   - kubectl -n security run -it --rm debug --image=amazonlinux:2 -- bash
+   - inside pod: aws sts get-caller-identity
+
+6) PVC/storageClass, Postgres & Velero backups
+ - Tune storage class and values: ops/setup_pvc_and_backups.sh <storageClass> <evidence-bucket>
+   - Example: ./ops/setup_pvc_and_backups.sh gp2 $EVIDENCE_BUCKET us-west-2
+ - Confirm Velero installed: kubectl -n velero get pods
+ - Confirm Postgres helm values applied and PVCs created:
+   - helm upgrade --install postgres bitnami/postgresql -n aegis -f registry/postgres/values.prod.yaml
+   - kubectl -n aegis get pvc
+ - Check CronJob pgdump: kubectl -n aegis get cronjob pgdump-to-s3
+
+7) KServe TLS & MCPx logger auth
+ - Apply cert-manager issuer & Certificate (cert manager must be installed):
+   - kubectl apply -f k8s/kserve/certificate.yaml
+   - kubectl apply -f k8s/kserve/tls_ingress.yaml
+ - Deploy MCPx logger with auth secret:
+   - kubectl create secret generic mcpx-auth -n aegis --from-literal=token=$(openssl rand -hex 32)
+   - kubectl apply -f k8s/mcpx/mcpx-deployment.yaml
+ - Verify service and logs:
+   - kubectl -n aegis rollout status deploy/mcpx-logger
+   - kubectl -n aegis logs deploy/mcpx-logger
+
+8) Verifier eval & thresholds
+ - Run locally or CI:
+   - ./ops/apply_verifier_thresholds.sh
+ - Review PR generated by ops/verifier/apply_threshold_pr.py; merge after peer review.
+ - Apply HPA for verifier:
+   - kubectl -n aegis apply -f mcp/verifier/hpa.yaml
+
+9) Replace demo dataset reads with production lakeFS/S3
+ - Edit pipelines/ssl/*_prod.py and set LABELED_S3_PATH / UNLABELED_S3_PATH env vars before running.
+ - Confirm permissions: aws s3 ls s3://$EVIDENCE_BUCKET/datasets/
+ - Run Argo SSL pipeline in staging via .github/workflows/ssl_demo.yml or argo CLI
+
+10) DR drill (backup & restore)
+ - Create test namespace: kubectl create ns aegis-dr-test
+ - Run Postgres backup (pgdump CronJob will run nightly; run manual backup):
+   - kubectl -n aegis exec deploy/postgres -- pg_dump -U aegis -d aegis_registry | gzip > /tmp/dump.sql.gz
+   - aws s3 cp /tmp/dump.sql.gz s3://$EVIDENCE_BUCKET/backups/postgres/dump.sql.gz
+ - Run restore drill:
+   - ./k8s/postgres/pg_restore_drill.sh $EVIDENCE_BUCKET backups/postgres/dump.sql.gz postgresql-test.aegis-dr.svc.cluster.local aegis aegis_registry
+ - Verify sample queries against restored DB
+
+11) Monitoring baseline, drift & SLO tuning
+ - Generate baseline: export BASELINE_SAMPLE_PATH and run ops/generate_baseline.py
+ - Deploy drift detector and pushgateway then tune monitoring/prometheus rules and alertmanager templates.
+ - Map PagerDuty key into monitoring/alertmanager/alertmanager_production.yml.tpl and apply (operator review only).
+
+12) DSAR & legal process
+ - Ensure legal has access to Approval Service UI and understands runbooks/legal_dsar_playbook.md
+ - Practice a DSAR exercise: ./dsar/automated_dsar.py <sample_subject_hash> in staging and verify audit bundle signed (dsar/sign_audit_bundle.sh)
+
+13) Testing & runbook practice
+ - Run integration tests: pytest tests/integration -q
+ - Execute runbook drills:
+   - Rekor outage: simulate and verify remediation job posts alerts
+   - KMS rotation: run infra/kms/rotate_and_rollover.sh in staging
+   - DSAR exercise: see above
+
+Guardrails & safety checks
+- Always use staging kubeconfig for destructive tests.
+- Do not run KMS rotation in production without a formal maintenance window and backups.
+- When updating GitHub secrets, ensure environment protection rules (approvals) are enabled for the bootstrap workflow.
+- Keep a copy of Terraform state and S3 backups off-site before major changes.
+
+Acceptance checklist (operator to tick)
+- [ ] Terraform outputs saved and validated
+- [ ] GitHub secrets, AWS SecretsManager/SSM populated
+- [ ] ExternalSecrets sync verified in cluster
+- [ ] Images built and pushed to registry
+- [ ] KServe TLS enabled and endpoints reachable
+- [ ] MCPx logger deployed and auth validated
+- [ ] Postgres backups present in S3 and restore drill successful
+- [ ] Drift baseline generated and drift detector deployed
+- [ ] Verifier thresholds PR reviewed and merged
+- [ ] DSAR drill executed and audit bundle signed
+- [ ] Integration tests pass in staging
+
+If you want, I can run the next step and generate the Job/Action scripts referenced in this playbook (e.g., the DR workflow), or create a PR template that wires image names (provide repo + branch) — or both.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/create_repo_wiring_pr.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create a branch in a target repo, replace placeholder image names and non-secret env values,
+# commit and push, and open a pull request. Use interactively.
+#
+# Usage: ./scripts/create_repo_wiring_pr.sh owner/repo base-branch ghcr_org rekor_url evidence_bucket
+#
+REPO=${1:-}
+BASE=${2:-main}
+GHCR_ORG=${3:-ghcr.io/yourorg}
+REKOR_URL=${4:-https://rekor.example.com}
+EVIDENCE_BUCKET=${5:-aegis-evidence-12345}
+
+if [ -z "$REPO" ]; then
+  echo "Usage: $0 owner/repo base-branch ghcr_org rekor_url evidence_bucket"
+  exit 2
+fi
+
+TMPDIR=$(mktemp -d)
+BRANCH="ops/wire-images-$(date +%s)"
+echo "Cloning $REPO to $TMPDIR"
+git clone https://github.com/$REPO.git $TMPDIR
+pushd $TMPDIR
+git checkout $BASE
+git checkout -b $BRANCH
+
+echo "Replacing image placeholders and example env values"
+# Replace ghcr placeholder tags in YAML/helm values
+grep -RIl "ghcr.io/yourorg" . | xargs -r sed -i "s|ghcr.io/yourorg|${GHCR_ORG}|g"
+# Set sample REKOR_URL and EVIDENCE_BUCKET in config files
+grep -RIl "REKOR_URL_PLACEHOLDER" . | xargs -r sed -i "s|REKOR_URL_PLACEHOLDER|${REKOR_URL}|g"
+grep -RIl "REPLACE_EVIDENCE_BUCKET" . | xargs -r sed -i "s|REPLACE_EVIDENCE_BUCKET|${EVIDENCE_BUCKET}|g"
+
+# Add a non-secret config file with inventory of images
+cat > config/images.yaml <<EOF
+images:
+  aegis-tools: ${GHCR_ORG}/aegis-tools:latest
+  aegis-train: ${GHCR_ORG}/aegis-train:latest
+  aegis-transformer: ${GHCR_ORG}/aegis-transformer:latest
+non_secret:
+  rekor_url: ${REKOR_URL}
+  evidence_bucket: ${EVIDENCE_BUCKET}
+EOF
+
+git add -A
+git commit -m "chore(ci): wire GHCR image names and non-secret env values"
+git push origin $BRANCH
+
+if command -v gh >/dev/null 2>&1; then
+  gh pr create --title "Wire images & config (ops)" --body "This PR wires GHCR image names and sets non-secret config values." --base $BASE --head $BRANCH --repo $REPO
+  echo "PR created via gh CLI"
+else
+  echo "Branch pushed: $BRANCH. Create a PR in GitHub UI for $REPO."
+fi
+
+popd
+rm -rf "$TMPDIR"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/mcpx/mcpx-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: mcpx-logger
+  namespace: aegis
+  labels:
+    app: mcpx-logger
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: mcpx-logger
+  template:
+    metadata:
+      labels:
+        app: mcpx-logger
+    spec:
+      containers:
+        - name: mcpx-logger
+          image: ghcr.io/yourorg/mcpx-logger:latest
+          env:
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: evidence_bucket
+            - name: AWS_REGION
+              value: "us-west-2"
+            - name: MCPX_AUTH_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: mcpx-auth
+                  key: token
+          ports:
+            - containerPort: 8080
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+            limits:
+              cpu: "500m"
+              memory: "512Mi"
+      restartPolicy: Always
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: mcpx-logger
+  namespace: aegis
+spec:
+  selector:
+    app: mcpx-logger
+  ports:
+    - port: 8080
+      targetPort: 8080
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: allow-from-modelservers
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: mcpx-logger
+  policyTypes:
+    - Ingress
+  ingress:
+    - from:
+        - podSelector:
+            matchLabels:
+              app: aegis-modelserver
+      ports:
+        - protocol: TCP
+          port: 8080
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kserve/hardened_inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-modelserver-hardened"
+  namespace: "aegis"
+spec:
+  predictor:
+    minReplicas: 2
+    maxReplicas: 8
+    serviceAccountName: "model-server-sa"
+    sklearn:
+      storageUri: "s3://REPLACE_EVIDENCE_BUCKET/models/{{MODEL_NAME}}/v{{MODEL_VERSION}}"
+      resources:
+        requests:
+          cpu: "500m"
+          memory: "1Gi"
+        limits:
+          cpu: "2000m"
+          memory: "4Gi"
+    logger:
+      mode: "all"
+      url: "http://mcpx-logger.aegis.svc.cluster.local:8080/log"
+  transformer:
+    containers:
+      - image: ghcr.io/yourorg/aegis-transformer:latest
+        name: transformer
+        env:
+          - name: MCPX_AUTH_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: mcpx-auth
+                key: token
+  explainer:
+    alibi:
+      image: olivere/alibi-explainer:latest
+      resources:
+        requests:
+          cpu: "200m"
+          memory: "512Mi"
+  # KServe TLS and Istio/Ingress integration expects cert-manager certs; ensure ingress is configured with TLS.
+
+annotations:
+  serving.kserve.io/ingress-class: "istio"
+  # Optional: enable authentication on transformer/requests via env token; transformer should forward token to mcpx logger
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/dr_backup_restore.yml
+name: DR Test — Postgres Backup & Restore
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: write
+
+jobs:
+  dr-test:
+    runs-on: ubuntu-latest
+    env:
+      AWS_REGION: us-west-2
+    steps:
+      - uses: actions/checkout@v4
+      - name: Configure AWS credentials
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          aws-region: ${{ env.AWS_REGION }}
+          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+      - name: Install kubectl and jq
+        run: |
+          sudo apt-get update && sudo apt-get install -y jq
+          curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
+          chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl
+      - name: Setup kubeconfig for staging
+        run: |
+          echo "${{ secrets.KUBECONFIG_STAGING }}" > /tmp/kubeconfig_staging
+          export KUBECONFIG=/tmp/kubeconfig_staging
+          kubectl config current-context
+      - name: Run DR backup & restore script
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          PG_POD_LABEL: "app=postgresql"
+          RESTORE_NS: "aegis-dr-test"
+        run: |
+          chmod +x scripts/dr/pg_backup_and_restore.sh
+          ./scripts/dr/pg_backup_and_restore.sh
+      - name: Post result issue (success)
+        if: success()
+        uses: actions/github-script@v6
+        with:
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          script: |
+            const issue = await github.issues.create({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              title: "DR test succeeded",
+              body: "Postgres backup & restore workflow completed successfully in staging (automated)."
+            });
+      - name: Post result issue (failure)
+        if: failure()
+        uses: actions/github-script@v6
+        with:
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          script: |
+            const issue = await github.issues.create({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              title: "DR test failed",
+              body: "Postgres backup & restore workflow failed. Check workflow logs for details."
+            });
+
*** End Patch
*** Begin Patch
*** Add File:scripts/dr/pg_backup_and_restore.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run inside CI (GitHub Action) or operator workstation with KUBECONFIG pointing to staging.
+# Steps:
+# 1. Find primary Postgres pod and run pg_dump to local file
+# 2. Upload dump to S3 evidence bucket
+# 3. Create test namespace and a test Postgres deployment (or use pre-provisioned test DB)
+# 4. Restore dump into test Postgres and run verification query
+#
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-aegis-evidence-12345}
+PG_POD_LABEL=${PG_POD_LABEL:-app=postgresql}
+RESTORE_NS=${RESTORE_NS:-aegis-dr-test}
+TMP_DUMP="/tmp/aegis_pg_dump_$(date -u +%Y%m%dT%H%M%SZ).sql.gz"
+
+echo "[1] Locating primary Postgres pod"
+POD=$(kubectl -n aegis get pods -l "$PG_POD_LABEL" -o jsonpath='{.items[0].metadata.name}')
+if [ -z "$POD" ]; then
+  echo "Postgres pod not found with label $PG_POD_LABEL"; exit 2
+fi
+echo "Using pod: $POD"
+
+echo "[2] Performing pg_dump (streamed and gzipped)"
+kubectl -n aegis exec -i "$POD" -- bash -lc "pg_dump -U aegis -d aegis_registry" | gzip > "$TMP_DUMP"
+echo "Dump saved to $TMP_DUMP"
+
+echo "[3] Uploading dump to S3 s3://$EVIDENCE_BUCKET/backups/postgres/"
+aws s3 cp "$TMP_DUMP" "s3://$EVIDENCE_BUCKET/backups/postgres/$(basename $TMP_DUMP)"
+
+echo "[4] Prepare restore namespace $RESTORE_NS"
+kubectl create ns "$RESTORE_NS" || true
+
+echo "[5] Deploy a temporary Postgres for restore (helm chart or simple deployment)"
+kubectl -n "$RESTORE_NS" apply -f - <<EOF
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: postgres-restore
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: postgres-restore
+  template:
+    metadata:
+      labels:
+        app: postgres-restore
+    spec:
+      containers:
+        - name: postgres
+          image: bitnami/postgresql:13
+          env:
+            - name: POSTGRESQL_USERNAME
+              value: "aegis"
+            - name: POSTGRESQL_PASSWORD
+              value: "changeme"
+            - name: POSTGRESQL_DATABASE
+              value: "aegis_registry"
+          ports:
+            - containerPort: 5432
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: postgres-restore
+spec:
+  selector:
+    app: postgres-restore
+  ports:
+    - port: 5432
+      targetPort: 5432
+EOF
+
+echo "Waiting for postgres-restore pod to be ready..."
+kubectl -n "$RESTORE_NS" wait --for=condition=ready pod -l app=postgres-restore --timeout=120s
+
+RESTORE_POD=$(kubectl -n "$RESTORE_NS" get pods -l app=postgres-restore -o jsonpath='{.items[0].metadata.name}')
+echo "Restore pod: $RESTORE_POD"
+
+echo "[6] Download dump from S3 into restore pod and restore"
+kubectl -n "$RESTORE_NS" exec -i "$RESTORE_POD" -- bash -lc "apt-get update -y && apt-get install -y awscli gzip" >/dev/null 2>&1 || true
+kubectl -n "$RESTORE_NS" exec -i "$RESTORE_POD" -- bash -lc "aws s3 cp s3://$EVIDENCE_BUCKET/backups/postgres/$(basename $TMP_DUMP) /tmp/dump.sql.gz"
+kubectl -n "$RESTORE_NS" exec -i "$RESTORE_POD" -- bash -lc "gunzip -c /tmp/dump.sql.gz | psql -U aegis -d aegis_registry"
+
+echo "[7] Verification: run a simple query to ensure tables present"
+kubectl -n "$RESTORE_NS" exec -i "$RESTORE_POD" -- bash -lc "psql -U aegis -d aegis_registry -c '\\dt' " > /tmp/restore_tables.txt
+cat /tmp/restore_tables.txt
+
+if grep -q public /tmp/restore_tables.txt; then
+  echo "Restore verification looks good"
+  exit 0
+else
+  echo "Restore verification failed; inspect /tmp/restore_tables.txt"
+  exit 2
+fi
+
*** End Patch
*** Begin Patch
*** Add File:README_OPERATORS_AUTOMATION_AND_DR.md
+# Aegis: Operator automation, repo wiring, hardened KServe and DR automation
+
+This patch adds:
+- ops/operator_playbook_full.md — complete operator playbook with CLI commands, guardrails and acceptance checklist.
+- scripts/create_repo_wiring_pr.sh — helper to open a branch that wires GHCR/ECR image names and non-secret env values into your repo.
+- k8s/mcpx/mcpx-deployment.yaml — authenticated MCPx logger deployment, service and network policy.
+- k8s/kserve/hardened_inferenceservice.yaml — KServe InferenceService template with increased replicas, logger hooking and transformer auth token wiring.
+- .github/workflows/dr_backup_restore.yml — GitHub Action that runs a DR backup & restore test and files a GitHub issue with the result.
+- scripts/dr/pg_backup_and_restore.sh — the script executed by the DR action that performs pg_dump, uploads to S3, deploys a test DB and restores & verifies.
+
+Operator next steps
+1. Edit placeholder values (GHCR org, EVIDENCE_BUCKET, KMS ARNs, REKOR URL) in the files to match your environment.  
+2. Populate GitHub secrets (AWS keys, KUBECONFIG_STAGING, EVIDENCE_BUCKET, GITHUB_TOKEN).  
+3. Build/push the mcpx-logger image (or reference existing image) and update the mcpx-deployment image field.  
+4. Test DR Action by triggering `.github/workflows/dr_backup_restore.yml` in GitHub Actions (requires secrets).  
+5. Follow ops/operator_playbook_full.md to complete the staging hardening runbook.
+
*** End Patch
*** End Patch
