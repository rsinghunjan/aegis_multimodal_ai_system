*** Begin Patch
*** Add File:deploy/vault/values-vault.yaml
+replicaCount: 1
+server:
+  enabled: true
+  standalone:
+    enabled: true
+ui:
+  enabled: true
+injector:
+  enabled: true
+extraEnvironmentVars:
+  - name: VAULT_LOG_LEVEL
+    value: "info"
+serviceAccount:
+  create: true
+  name: vault-sa
+resources:
+  requests:
+    cpu: 100m
+    memory: 256Mi
+  limits:
+    cpu: 500m
+    memory: 1Gi
+
+# NOTE: Replace with production HA values, storage backend and TLS secrets before deploy.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/secretproviderclass.yaml
+apiVersion: secrets-store.csi.x-k8s.io/v1
+kind: SecretProviderClass
+metadata:
+  name: vault-secret-provider
+  namespace: aegis
+spec:
+  provider: "vault"
+  parameters:
+    vaultAddress: "https://vault.aegis.svc:8200"
+    roleName: "aegis-role"
+    objects: |
+      - objectName: "db-creds"
+        secretPath: "secret/data/aegis/db"
+        objectType: "secret"
+
+# Requires HashiCorp Vault Helm chart + CSI provider installed.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/agent_injector_configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: vault-agent-injector-config
+  namespace: kube-system
+data:
+  config: |
+    exit_after_auth: false
+    auto_auth:
+      method:
+        type: kubernetes
+        mount_path: auth/kubernetes
+        config:
+          role: aegis-role
+    sink:
+      type: file
+      config:
+        path: /vault/secrets
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/policies/aegis-policy.hcl
+path "secret/data/aegis/*" {
+  capabilities = ["read","list"]
+}
+path "secret/data/aegis/ci/*" {
+  capabilities = ["read","list"]
+}
+path "auth/token/create" {
+  capabilities = ["create","update"]
+}
+
+# Load into Vault with: vault policy write aegis-policy aegis-policy.hcl
+
*** End Patch
*** Begin Patch
*** Add File:deploy/signing-proxy/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: signing-proxy
+  template:
+    metadata:
+      labels:
+        app: signing-proxy
+    spec:
+      serviceAccountName: signing-proxy-sa
+      containers:
+        - name: signing-proxy
+          image: registry.example.com/aegis/signing-proxy:latest
+          ports:
+            - containerPort: 8080
+          env:
+            - name: HSM_ENDPOINT
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: hsm_endpoint
+            - name: HSM_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: hsm_token
+
*** End Patch
*** Begin Patch
*** Add File:ops/rbac/serviceaccounts_roles.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: signing-proxy-sa
+  namespace: aegis
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: signing-proxy-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get","list"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: signing-proxy-rb
+  namespace: aegis
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: signing-proxy-role
+subjects:
+  - kind: ServiceAccount
+    name: signing-proxy-sa
+    namespace: aegis
+
*** End Patch
*** Begin Patch
*** Add File:governance/gatekeeper/require_signed_image.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: requirecosignattestation
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireCosignAttestation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package requirecosignattestation
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Pod"
+          not input.review.object.metadata.annotations["cosign.sig"]
+          msg := "Pod must have cosign.sig annotation (image must be attested)"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireCosignAttestation
+metadata:
+  name: cosign-required
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
*** End Patch
*** Begin Patch
*** Add File:governance/gatekeeper/no_privileged_pods.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: denyprivilegedpods
+spec:
+  crd:
+    spec:
+      names:
+        kind: DenyPrivilegedPods
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package denyprivilegedpods
+        violation[{"msg": msg}] {
+          input.review.object.spec.containers[_].securityContext.privileged == true
+          msg := "Privileged containers are not allowed"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: DenyPrivilegedPods
+metadata:
+  name: no-privileged
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
*** End Patch
*** Begin Patch
*** Add File:governance/opa/runtime_deny.rego
+package aegis.runtime
+
+deny[msg] {
+  input.proc and sys := input.proc.syscall
+  sys == "ptrace"
+  msg = sprintf("suspicious syscall ptrace detected pid=%v exe=%v", [input.proc.pid, input.proc.exe])
+}
+
+# Runtime agent must feed process syscalls to OPA for enforcement or alerting.
+
*** End Patch
*** Begin Patch
*** Add File:security/falco/falco_rules.yaml
+rules:
+  - rule: Write to /etc/passwd
+    desc: Detect writes to /etc/passwd
+    condition: (evt.type = open) and fd.name = "/etc/passwd"
+    output: "etc/passwd modified (user=%user.name command=%proc.cmdline file=%fd.name)"
+    priority: WARNING
+
+  - rule: Container shell
+    desc: Detect shell spawn in container
+    condition: container and evt.type = execve and proc.name in (bash, sh)
+    output: "Shell in container (container=%container.id user=%user.name proc=%proc.name cmd=%proc.cmdline)"
+    priority: WARNING
+
+  - rule: Falco write to sensitive mount
+    desc: A process wrote to /var/run/secrets
+    condition: open and fd.name startswith "/var/run/secrets"
+    output: "sensitive secrets access (proc=%proc.name file=%fd.name user=%user.name)"
+    priority: NOTICE
+
*** End Patch
*** Begin Patch
*** Add File:security/falco/daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: falco
+  namespace: kube-system
+spec:
+  selector:
+    matchLabels:
+      app: falco
+  template:
+    metadata:
+      labels:
+        app: falco
+    spec:
+      hostNetwork: true
+      containers:
+        - name: falco
+          image: falcosecurity/falco:latest
+          securityContext:
+            privileged: true
+          volumeMounts:
+            - name: dev
+              mountPath: /host/dev
+            - name: proc
+              mountPath: /host/proc
+            - name: varlog
+              mountPath: /host/var/log
+      volumes:
+        - name: dev
+          hostPath:
+            path: /dev
+        - name: proc
+          hostPath:
+            path: /proc
+        - name: varlog
+          hostPath:
+            path: /var/log
+
+# Note: tune rules to reduce false positives and configure alerting webhook.
+
*** End Patch
*** Begin Patch
*** Add File:logging/fluentbit/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluent-bit-config
+  namespace: logging
+data:
+  fluent-bit.conf: |
+    [SERVICE]
+        Flush        5
+        Log_Level    info
+    [INPUT]
+        Name    tail
+        Path    /var/log/containers/*.log
+        Parser  docker
+    [OUTPUT]
+        Name  s3
+        Match *
+        bucket ${EVIDENCE_BUCKET}
+        region us-west-2
+  parsers.conf: |
+    [PARSER]
+        Name        docker
+        Format      json
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-security-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: aegis-security
+      rules:
+        - alert: FalcoAlertFiring
+          expr: sum(falco_alerts_total) > 0
+          for: 1m
+          labels:
+            severity: page
+          annotations:
+            summary: "Falco security alert(s) firing"
+        - alert: BackupFailed
+          expr: increase(velero_backup_failed_total[1h]) > 0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Velero backup failures detected"
+
*** End Patch
*** Begin Patch
*** Add File:backups/velero/velero-credentials-secret.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: velero-creds
+  namespace: velero
+type: Opaque
+data:
+  # base64 AWS credentials placeholder
+  cloud: ""
+
+# Populate with base64-encoded credentials for the cloud backup provider.
+
*** End Patch
*** Begin Patch
*** Add File:backups/velero/backup-schedule.yaml
+apiVersion: velero.io/v1
+kind: Schedule
+metadata:
+  name: daily-backup
+  namespace: velero
+spec:
+  schedule: "0 2 * * *" # daily 02:00 UTC
+  template:
+    ttl: 720h0m0s
+    includedNamespaces:
+      - aegis
+    includedResources:
+      - pods
+      - deployments
+      - persistentvolumeclaims
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/compliance_scan.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: compliance-scan-
+  namespace: aegis
+spec:
+  entrypoint: compliance-scan
+  templates:
+    - name: compliance-scan
+      steps:
+        - - name: run-inspec
+            template: run-inspec
+        - - name: sign-report
+            template: sign-report
+
+    - name: run-inspec
+      container:
+        image: inspec/inspec:latest
+        command: [sh, -c]
+        args:
+          - echo "Running compliance checks (InSpec)"; sleep 2
+            # Add real InSpec profiles in production
+
+    - name: sign-report
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/ci/assemble_evidence.py --sbom sbom.spdx.json --trivy trivy_report.json --out /tmp/compliance_manifest.json || true
+            python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/compliance_manifest.json --proxy ${SIGNING_PROXY_URL:-http://signing-proxy:8080} --out /tmp/compliance_signed.json || true
+            cat /tmp/compliance_signed.json
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/backup_and_restore_test.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: backup-restore-test-
+  namespace: aegis
+spec:
+  entrypoint: backup-restore
+  templates:
+    - name: backup-restore
+      steps:
+        - - name: trigger-backup
+            template: trigger-backup
+        - - name: verify-backup
+            template: verify-backup
+        - - name: test-restore
+            template: test-restore
+
+    - name: trigger-backup
+      container:
+        image: bitnami/velero:latest
+        command: [sh, -c]
+        args:
+          - echo "Triggering on-demand Velero backup (stub)"; sleep 2
+
+    - name: verify-backup
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/backup/verify_backup.sh || true
+
+    - name: test-restore
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - echo "Running restore drill (operator-controlled, stub)"; sleep 2
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/falco_remediate.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: falco-remediate-
+  namespace: aegis
+spec:
+  entrypoint: falco-remediate
+  templates:
+    - name: falco-remediate
+      inputs:
+        parameters:
+          - name: event
+      steps:
+        - - name: collect-evidence
+            template: collect-evidence
+        - - name: quarantine
+            template: quarantine
+        - - name: open-ticket
+            template: open-ticket
+
+    - name: collect-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/forensics/collect_logs_and_sign.sh --event "{{inputs.parameters.event}}" || true
+
+    - name: quarantine
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - echo "Quarantining offending pod (operator: implement selector mapping)" && sleep 1
+
+    - name: open-ticket
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/ci/report_vuln.py --title "Falco Alert" --body "Investigation: see evidence bundle" || true
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/assemble_audit_evidence.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: assemble-evidence-
+  namespace: aegis
+spec:
+  entrypoint: assemble-evidence
+  templates:
+    - name: assemble-evidence
+      inputs:
+        parameters:
+          - name: topic
+      steps:
+        - - name: collect-sbom
+            template: collect-sbom
+        - - name: collect-trivy
+            template: collect-trivy
+        - - name: assemble
+            template: assemble
+
+    - name: collect-sbom
+      container:
+        image: alpine
+        command: [sh, -c]
+        args:
+          - echo "Collecting SBOM artifacts (operator: configure artifact locations)" && sleep 1
+
+    - name: collect-trivy
+      container:
+        image: alpine
+        command: [sh, -c]
+        args:
+          - echo "Collecting Trivy reports" && sleep 1
+
+    - name: assemble
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 evidence/mission_evidence_advanced.py --sbom sbom.spdx.json --checkpoint-sign /tmp/check_sign.json --sim-coverage /tmp/sim_coverage.json --pil-log /tmp/pil_result.json --hil-log /tmp/hil_result.json --uplink-manifest /tmp/signed_manifest.json --out /tmp/mission_evidence.json --evidence-bucket ${EVIDENCE_BUCKET:-} || true
+            cat /tmp/mission_evidence.json || true
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cicd_security.yml
+name: CI Security (SBOM, SCA, Compliance)
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+jobs:
+  sbom-sca:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install Syft
+        run: curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+      - name: Generate SBOM
+        run: syft packages dir:. -o spdx-json > sbom.spdx.json || true
+      - name: Upload SBOM artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom
+          path: sbom.spdx.json
+      - name: Install Trivy
+        run: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+      - name: Run Trivy
+        run: trivy fs --format json --output trivy_report.json --severity HIGH,CRITICAL . || true
+      - name: Upload Trivy report
+        uses: actions/upload-artifact@v4
+        with:
+          name: trivy-report
+          path: trivy_report.json
+  compliance:
+    runs-on: ubuntu-latest
+    needs: sbom-sca
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run InSpec (placeholder)
+        run: echo "Run InSpec profiles (operator configure profiles)" && sleep 2
+      - name: Assemble evidence & sign
+        run: |
+          python3 scripts/ci/assemble_evidence.py --sbom sbom.spdx.json --trivy trivy_report.json --out /tmp/evidence.json
+          python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/evidence.json --proxy ${SIGNING_PROXY_URL:-http://signing-proxy:8080} --out /tmp/evidence_sign.json || true
+      - name: Upload evidence artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: evidence
+          path: /tmp/evidence.json
+
*** End Patch
*** Begin Patch
*** Add File:scripts/kms/encrypt_decrypt_kms.sh
+#!/usr/bin/env bash
+set -euo pipefail
+CMD=${1:-}
+FILE=${2:-}
+if [ -z "$CMD" ] || [ -z "$FILE" ]; then
+  echo "Usage: $0 encrypt|decrypt <file>"
+  exit 2
+fi
+if [ "$CMD" = "encrypt" ]; then
+  aws kms encrypt --key-id $KMS_KEY_ID --plaintext fileb://"$FILE" --output text --query CiphertextBlob | base64 --decode > "${FILE}.enc"
+  echo "Wrote ${FILE}.enc"
+else
+  aws kms decrypt --ciphertext-blob fileb://"$FILE" --output text --query Plaintext | base64 --decode > "${FILE}.dec"
+  echo "Wrote ${FILE}.dec"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:scripts/backup/verify_backup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Verifying Velero backups (stub)."
+velero backup get -o json | jq '.' > /tmp/velero_backups.json || true
+if [ -s /tmp/velero_backups.json ]; then
+  echo "Backups present. Please inspect /tmp/velero_backups.json"
+else
+  echo "No backups detected or Velero not configured"
+  exit 1
+fi
+
*** End Patch
*** Begin Patch
*** Add File:scripts/forensics/collect_logs_and_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+EVENT=${1:-"unknown"}
+OUTDIR=${2:-/tmp/aegis_forensics}
+mkdir -p "$OUTDIR"
+kubectl get pods --all-namespaces -o json > "$OUTDIR/pods.json" || true
+kubectl get events --all-namespaces > "$OUTDIR/events.txt" || true
+tar -czf /tmp/aegis_forensics.tar.gz -C "$OUTDIR" .
+echo "Collected forensic bundle /tmp/aegis_forensics.tar.gz"
+if [ -n "${SIGNING_PROXY_URL:-}" ]; then
+  python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/aegis_forensics.tar.gz --proxy ${SIGNING_PROXY_URL} --out /tmp/forensics_sign.json || true
+  echo "Signed forensic bundle /tmp/forensics_sign.json"
+fi
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  aws s3 cp /tmp/aegis_forensics.tar.gz s3://${EVIDENCE_BUCKET}/forensics/ || true
+  aws s3 cp /tmp/forensics_sign.json s3://${EVIDENCE_BUCKET}/forensics/ || true
+fi
+
*** End Patch
*** Begin Patch
*** Add File:docs/SECURITY_RUNBOOK.md
+# Aegis Security & Compliance Runbook (Defaults)
+
+This runbook explains the new IAM, auditing, compliance, threat detection, encryption and backup scaffolding added to Aegis.
+
+Quick reference
+- Vault: deploy via Helm using deploy/vault/values-vault.yaml. Create policies in deploy/vault/policies.
+- Secrets injection: use SecretProviderClass deploy/vault/secretproviderclass.yaml and the Vault Agent Injector configmap.
+- Signing proxy: deploy deploy/signing-proxy/deployment.yaml and provide HSM credentials in aegis-secrets.
+- Gatekeeper: apply governance/gatekeeper/*.yaml to enforce signed images and prevent privileged pods.
+- Falco: apply security/falco/daemonset.yaml and tune rules in security/falco/falco_rules.yaml.
+- Fluent Bit: logging/fluentbit/configmap.yaml to archive logs to ${EVIDENCE_BUCKET}.
+- Velero: configure backups and schedule in backups/velero.
+- Argo workflows: compliance, backup tests, falco remediation, assemble evidence.
+- CI: .github/workflows/cicd_security.yml runs SBOM/trivy and assembles signed evidence.
+
+Operational steps
+1. Vault
+   - Helm install hashicorp/vault with values-vault.yaml.
+   - Create Vault policies (deploy/vault/policies/aegis-policy.hcl).
+   - Configure Kubernetes auth role mapping and test Vault Agent injection on a demo pod.
+
+2. Signing & provenance
+   - Deploy signing-proxy and secure it (mTLS + network policy).
+   - Use COSIGN + Rekor in CI where possible; CI configured to call signing-proxy.
+   - Gatekeeper ensures images have cosign.sig annotation before production schedule.
+
+3. Detection & response
+   - Deploy Falco daemonset and connect webhook to Argo remediation (argo/workflows/falco_remediate.yaml).
+   - Configure Prometheus alerts in monitoring/prometheus/rules.yaml and connect Alertmanager receivers.
+
+4. Backups & restores
+   - Install Velero with provider credentials (backups/velero/*) and create daily schedule.
+   - Use argo/workflows/backup_and_restore_test.yaml to run backup/restore drills.
+
+5. Compliance automation
+   - Run argo/workflows/compliance_scan.yaml regularly; CI runs InSpec placeholder.
+   - Archive signed compliance reports with evidence workflow.
+
+6. Encryption & KMS
+   - Use scripts/kms/encrypt_decrypt_kms.sh for envelope operations in CI/ops.
+   - Ensure S3 buckets use SSE-KMS and enforce IAM policies to limit access.
+
+Incident response
+- On Falco alert: workflow falco_remediate will collect evidence, optionally quarantine pod, and create a ticket.
+- For backups failing: backup_and_restore_test will generate artifacts and alert on Prometheus.
+- For compliance drift: compliance_scan will sign and produce evidence; open a ticket via CI job.
+
+Notes & next steps
+- Replace scaffolds with vendor-specific connectors (HSM SDK, SIEM endpoints, ground station details).
+- Harden signing-proxy: mTLS, RBAC, audit logging, and rate-limiting.
+- Tune Falco rules & Prometheus queries to reduce false positives.
+- Make EVIDENCE_BUCKET immutable (S3 object lock) for auditability.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/health/check_aegis_health.sh
+#!/usr/bin/env bash
+# Quick health-check for Aegis security stack (Vault, signing-proxy, Gatekeeper, Falco, Velero, Prometheus, Fluent Bit)
+# Usage: MODEL_ARTIFACT_BUCKET=... EVIDENCE_BUCKET=... SIGNING_PROXY_URL=https://signing-proxy.aegis.svc:8443 PROMETHEUS_ENDPOINT=http://prometheus.monitoring.svc:9090 ./scripts/health/check_aegis_health.sh
+set -euo pipefail
+export PATH=/usr/local/bin:$PATH
+
+info() { printf "\n\033[1;34m[INFO]\033[0m %s\n" "$*"; }
+warn() { printf "\n\033[1;33m[WARN]\033[0m %s\n" "$*"; }
+err()  { printf "\n\033[1;31m[ERROR]\033[0m %s\n" "$*"; }
+
+# quick helper to check kubectl context
+if ! command -v kubectl >/dev/null 2>&1; then
+  err "kubectl not found in PATH"
+  exit 2
+fi
+
+NAMESPACES=(aegis monitoring kube-system logging velero)
+info "Checking Kubernetes namespaces"
+for ns in "${NAMESPACES[@]}"; do
+  if kubectl get ns "${ns}" >/dev/null 2>&1; then
+    info "namespace ${ns}: OK"
+  else
+    warn "namespace ${ns}: not found"
+  fi
+done
+
+info "Checking Vault pod(s) in namespace 'aegis'"
+if kubectl get deploy -n aegis | grep -q vault; then
+  kubectl get pods -n aegis -l app.kubernetes.io/name=vault -o wide
+  if kubectl exec -n aegis -l app.kubernetes.io/name=vault -- vault status >/dev/null 2>&1; then
+    info "Vault status: reachable (vault status returned OK)"
+  else
+    warn "Vault present but 'vault status' failed. Check Vault unseal / TLS / service account"
+  fi
+else
+  warn "Vault deployment not found in namespace aegis"
+fi
+
+info "Checking signing-proxy Deployment & Service"
+if kubectl get deploy signing-proxy -n aegis >/dev/null 2>&1; then
+  kubectl rollout status deploy/signing-proxy -n aegis --timeout=30s || warn "signing-proxy rollout not ready"
+  kubectl get svc signing-proxy -n aegis -o wide || warn "signing-proxy service missing"
+  if [ -n "${SIGNING_PROXY_URL:-}" ]; then
+    info "Testing signing-proxy endpoint ${SIGNING_PROXY_URL}"
+    if curl --insecure --silent --fail -X GET "${SIGNING_PROXY_URL}/health" -o /dev/null 2>&1; then
+      info "Signing-proxy health: OK"
+    else
+      warn "Signing-proxy health check failed (curl) — check TLS and service"
+    fi
+  else
+    warn "SIGNING_PROXY_URL not set; skipping external signing-proxy check"
+  fi
+else
+  warn "signing-proxy Deployment not found in aegis"
+fi
+
+info "Checking Gatekeeper & constraints"
+if kubectl get pods -n gatekeeper-system >/dev/null 2>&1; then
+  kubectl get constraint -A || warn "Gatekeeper constraints not listed (cluster may be missing or constrained)"
+else
+  warn "Gatekeeper not installed in cluster (recommended for policy enforcement)"
+fi
+
+info "Checking Falco daemonset"
+if kubectl get daemonset falco -n kube-system >/dev/null 2>&1; then
+  kubectl get pods -n kube-system -l app=falco -o wide || warn "Falco pods missing"
+else
+  warn "Falco daemonset not found"
+fi
+
+info "Checking Fluent Bit logging config and sample S3 write"
+if kubectl get configmap fluent-bit-config -n logging >/dev/null 2>&1; then
+  info "Fluent Bit config exists in logging namespace"
+else
+  warn "Fluent Bit configmap missing (logging/fluentbit/configmap.yaml)"
+fi
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  if command -v aws >/dev/null 2>&1; then
+    if aws s3 ls "s3://${EVIDENCE_BUCKET}" >/dev/null 2>&1; then
+      info "EVIDENCE_BUCKET (${EVIDENCE_BUCKET}) accessible"
+    else
+      warn "Cannot access EVIDENCE_BUCKET (${EVIDENCE_BUCKET}) — check creds/permissions"
+    fi
+  else
+    warn "aws CLI not found; skipping bucket access check"
+  fi
+else
+  warn "EVIDENCE_BUCKET env not set; skipping S3 accessibility check"
+fi
+
+info "Checking Velero (backup) presence"
+if kubectl get deploy -n velero velero >/dev/null 2>&1; then
+  kubectl get backups -n velero || warn "Velero backups cannot be listed (check RBAC)"
+else
+  warn "Velero not installed in velero namespace"
+fi
+
+info "Checking Prometheus endpoint (optional)"
+if [ -n "${PROMETHEUS_ENDPOINT:-}" ]; then
+  if curl --silent --fail "${PROMETHEUS_ENDPOINT}/-/ready" >/dev/null 2>&1; then
+    info "Prometheus ready"
+  else
+    warn "Prometheus not reachable at ${PROMETHEUS_ENDPOINT}"
+  fi
+else
+  warn "PROMETHEUS_ENDPOINT not set; skipping Prometheus checks"
+fi
+
+info "Quick policy check: attempt to create a test Pod without cosign annotation (expect Gatekeeper to deny if enabled)"
+cat <<EOF > /tmp/aegis_test_pod.yaml
+apiVersion: v1
+kind: Pod
+metadata:
+  name: aegis-policy-test
+  namespace: default
+spec:
+  containers:
+    - name: nginx
+      image: nginx:alpine
+EOF
+if kubectl apply -f /tmp/aegis_test_pod.yaml >/dev/null 2>&1; then
+  warn "Test pod created — Gatekeeper constraint may not be active or was not enforced"
+  kubectl delete pod aegis-policy-test -n default || true
+else
+  info "Policy enforcement likely active (pod creation rejected)"
+fi
+rm -f /tmp/aegis_test_pod.yaml
+
+info "Completed checks. Review WARN/ERROR lines above and consult docs/SECURITY_RUNBOOK.md and docs/VALIDATION_RUNBOOK.md for remediation steps."
+
*** End Patch
*** Begin Patch
*** Add File:deploy/signing-proxy/hardening.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis
+---
+# ServiceAccount + namespaced Role for signing-proxy (least privilege)
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: signing-proxy-sa
+  namespace: aegis
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: signing-proxy-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: signing-proxy-rb
+  namespace: aegis
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: signing-proxy-role
+subjects:
+  - kind: ServiceAccount
+    name: signing-proxy-sa
+    namespace: aegis
+---
+# TLS secret placeholder - fill with actual cert and key
+# kubectl create secret tls signing-proxy-tls -n aegis --cert=server.crt --key=server.key
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: signing-proxy
+  template:
+    metadata:
+      labels:
+        app: signing-proxy
+    spec:
+      serviceAccountName: signing-proxy-sa
+      containers:
+        - name: signing-proxy
+          image: registry.example.com/aegis/signing-proxy:latest
+          args: ["--port","8443","--tls-cert","/tls/tls.crt","--tls-key","/tls/tls.key"]
+          ports:
+            - containerPort: 8443
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 8443
+              scheme: HTTPS
+            initialDelaySeconds: 10
+            periodSeconds: 10
+          livenessProbe:
+            httpGet:
+              path: /health
+              port: 8443
+              scheme: HTTPS
+            initialDelaySeconds: 30
+            periodSeconds: 20
+          resources:
+            requests:
+              cpu: 100m
+              memory: 128Mi
+            limits:
+              cpu: 500m
+              memory: 512Mi
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+            - name: hsm-creds
+              mountPath: /etc/hsm
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: signing-proxy-tls
+        - name: hsm-creds
+          secret:
+            secretName: aegis-secrets
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  type: ClusterIP
+  selector:
+    app: signing-proxy
+  ports:
+    - name: https
+      port: 8443
+      targetPort: 8443
+---
+# NetworkPolicy to restrict who can talk to signing-proxy
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: signing-proxy-allow-from-namespace
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: signing-proxy
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: aegis
+        - namespaceSelector:
+            matchLabels:
+              name: cicd
+      ports:
+        - protocol: TCP
+          port: 8443
+  policyTypes:
+    - Ingress
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/aws-values-ha.yaml
+# Helm values for HashiCorp Vault with AWS S3 storage backend and KMS auto-unseal (production placeholders)
+# Usage: helm repo add hashicorp https://helm.releases.hashicorp.com
+# helm install vault hashicorp/vault -f deploy/vault/aws-values-ha.yaml --namespace aegis
+server:
+  dataStorage:
+    enabled: true
+    # For production use proper PVs / storage classes or use S3 backend below
+  ha:
+    enabled: false
+  standalone:
+    enabled: false
+
+# Use S3 backend (ensure IAM policy for bucket access)
+unsealer:
+  aws:
+    enabled: true
+    region: "us-west-2"                  # << REPLACE
+    kmsKeyId: "arn:aws:kms:...:key/..."  # << REPLACE (KMS Key ARN used for auto-unseal)
+s3:
+  enabled: true
+  bucket: "aegis-vault-storage-bucket"  # << REPLACE
+  region: "us-west-2"                   # << REPLACE
+
+ui:
+  enabled: true
+
+# Service account and RBAC (operator should provision the service account and annotate if using IRSA)
+serviceAccount:
+  create: true
+  name: vault
+
+server:
+  extraEnvVars:
+    - name: VAULT_LOG_LEVEL
+      value: "info"
+  resources:
+    requests:
+      cpu: 200m
+      memory: 512Mi
+    limits:
+      cpu: 1
+      memory: 2Gi
+
+# NOTES:
+# - Create an IAM policy that gives access to S3 bucket and KMS decrypt/encrypt for the Vault node role.
+# - This values file is a starting point; for production use Vault HA (consul or raft), TLS certs and storage options.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/iam-policy.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowS3",
+      "Effect": "Allow",
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject",
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::aegis-vault-storage-bucket",
+        "arn:aws:s3:::aegis-vault-storage-bucket/*"
+      ]
+    },
+    {
+      "Sid": "AllowKMSAutoUnseal",
+      "Effect": "Allow",
+      "Action": [
+        "kms:Decrypt",
+        "kms:Encrypt",
+        "kms:GenerateDataKey*",
+        "kms:DescribeKey"
+      ],
+      "Resource": [
+        "arn:aws:kms:us-west-2:123456789012:key/EXAMPLE"   # << REPLACE
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:deploy/velero/velero-aws-secret.yaml.template
+# Template for Velero AWS credentials. Base64-encode or use kubectl create secret --from-file cloud=credentials-velero
+apiVersion: v1
+kind: Secret
+metadata:
+  name: cloud-credentials
+  namespace: velero
+type: Opaque
+stringData:
+  cloud: |
+    [default]
+    aws_access_key_id = YOUR_AWS_ACCESS_KEY_ID
+    aws_secret_access_key = YOUR_AWS_SECRET_ACCESS_KEY
+# Replace placeholders. Alternatively, create with:
+# kubectl create secret generic cloud-credentials --namespace velero --from-file=cloud=./credentials-velero
+
*** End Patch
*** Begin Patch
*** Add File:deploy/velero/install-aws.md
+# Velero AWS install notes (quick)
+# 1) Create velero namespace
+#    kubectl create namespace velero
+#
+# 2) Create AWS credentials file and Kubernetes secret (see template deploy/velero/velero-aws-secret.yaml.template)
+#    kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=./credentials-velero
+#
+# 3) Install Velero (example using Velero CLI):
+#    velero install \
+#      --provider aws \
+#      --bucket <YOUR_BACKUP_BUCKET> \
+#      --secret-file ./credentials-velero \
+#      --use-volume-snapshots=false \
+#      --backup-location-config region=<REGION>
+#
+# 4) Verify:
+#    velero backup get
+#
+# Notes:
+# - If you want PV snapshot support, install provider plugin and configure IAM role for snapshots.
+# - S3 bucket should have lifecycle and retention policies per compliance. Consider enabling object lock.
+
*** End Patch
*** Begin Patch
*** Add File:ops/rbac/signing-proxy-rbac.yaml
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: signing-proxy-min-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["list"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: signing-proxy-min-rolebinding
+  namespace: aegis
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: signing-proxy-min-role
+subjects:
+  - kind: ServiceAccount
+    name: signing-proxy-sa
+    namespace: aegis
+
*** End Patch
*** Begin Patch
*** Add File:docs/VALIDATION_RUNBOOK.md
+# Validation runbook — signing-proxy, Vault (AWS), Velero (AWS) & health checks
+#
+# Purpose
+#  - Deploy and validate hardening and production scaffolds for signing-proxy, Vault (S3 + KMS auto-unseal) and Velero backups.
+#  - Run the health-check to confirm baseline readiness.
+#
+# Preconditions
+#  - kubectl connected to target cluster with sufficient permissions
+#  - Helm (v3) installed locally
+#  - AWS CLI configured if using AWS resources
+#  - Replace all placeholders in YAML / templates before applying (bucket names, ARNs, secrets)
+#
+# 1) Create required S3 buckets & KMS keys (example)
+#    AWS:
+#      aws s3api create-bucket --bucket aegis-vault-storage-bucket --region us-west-2
+#      aws kms create-key --description "Vault auto-unseal key"  --> record KeyId/ARN
+#
+# 2) Create IAM policy / role for Vault nodes
+#    - Use deploy/vault/iam-policy.json and attach to the instance profile / IRSA role used by Vault.
+#
+# 3) Install Vault (Helm) with AWS values
+#    helm repo add hashicorp https://helm.releases.hashicorp.com
+#    kubectl create ns aegis || true
+#    helm install vault hashicorp/vault -n aegis -f deploy/vault/aws-values-ha.yaml
+#    Follow Vault post-install steps: initialize/unseal or configure auto-unseal (if using KMS)
+#
+# 4) Deploy Signing-proxy hardening manifest
+#    - Create TLS secret:
+#      kubectl create secret tls signing-proxy-tls -n aegis --cert=server.crt --key=server.key
+#    - Create aegis-secrets with HSM credentials:
+#      kubectl create secret generic aegis-secrets -n aegis --from-literal=hsm_endpoint="https://hsm.example" --from-literal=hsm_token="REPLACE"
+#    - Apply:
+#      kubectl apply -f deploy/signing-proxy/hardening.yaml
+#      kubectl apply -f ops/rbac/signing-proxy-rbac.yaml
+#
+# 5) Install Gatekeeper and apply constraints
+#    - Install Gatekeeper (see Gatekeeper docs) then:
+#      kubectl apply -f governance/gatekeeper/require_signed_image.yaml
+#      kubectl apply -f governance/gatekeeper/no_privileged_pods.yaml
+#
+# 6) Install Falco
+#    kubectl apply -f security/falco/falco_rules.yaml
+#    kubectl apply -f security/falco/daemonset.yaml
+#
+# 7) Install Fluent Bit (logging) and configure EVIDENCE_BUCKET
+#    - Apply logging/fluentbit/configmap.yaml (adjust bucket and region)
+#
+# 8) Install Velero
+#    - Create velero namespace and secret per deploy/velero instructions
+#    - velero install ... (see deploy/velero/install-aws.md)
+#    - Apply backups/velero/backup-schedule.yaml
+#
+# 9) CI Integration
+#    - Ensure SIGNING_PROXY_URL, MODEL_ARTIFACT_BUCKET, EVIDENCE_BUCKET, REKOR_URL are set in GitHub secrets
+#    - The CI workflow .github/workflows/cicd_security.yml will produce SBOM/trivy and evidence
+#
+# 10) Run the health-check script
+#    export SIGNING_PROXY_URL="https://signing-proxy.aegis.svc:8443"
+#    export EVIDENCE_BUCKET="aegis-evidence-bucket"
+#    export MODEL_ARTIFACT_BUCKET="aegis-model-bucket"
+#    export PROMETHEUS_ENDPOINT="http://prometheus.monitoring.svc:9090"
+#    bash scripts/health/check_aegis_health.sh
+#
+# Interpreting results
+#  - WARN lines indicate missing components or misconfigurations; follow remediation steps above.
+#  - If Vault status cannot be read, check TLS and service account or unseal keys.
+#  - If signing-proxy fails health, verify TLS secret and pod logs (kubectl logs -n aegis deploy/signing-proxy).
+#
+# Remediation tips
+#  - Vault: check vault logs, ensure KMS key policy allows decrypt for Vault role, confirm S3 bucket permissions.
+#  - Signing-proxy: validate TLS secret, service account RBAC, and that the HSM endpoint is reachable from cluster.
+#  - Velero: ensure S3 credentials are correct and bucket policy allows PutObject/GetObject for Velero role.
+#  - Falco: if many false positives, tune security/falco/falco_rules.yaml and lower priority for noisy rules.
+#
+# Notes
+#  - Everything shipped is scaffolded; production hardening requires secret management, mTLS, network policies, and IAM lock-down.
+#  - Consider applying PodSecurityPolicies / Pod Security Admission for further hardening.
+
*** End Patch
*** End Patch
