*** Begin Patch
*** Add File: edge/quantize_and_validate.py
+#!/usr/bin/env python3
+"""
+Quantize and validate a PyTorch model into ONNX and an optimized quantized ONNX Runtime artifact.
+Performs:
+ - export PyTorch -> ONNX
+ - run ONNX Runtime quantization (dynamic/static, depending on calibration dataset)
+ - run a validation loop comparing original and quantized model outputs on sample inputs
+ - produce a validation report (JSON) suitable for CI and OTA ingest
+ - (optional) sign artifact using cosign (operator config)
+
+This script is intended to run in CI (self-hosted runner with GPUs) or on a build host.
+"""
+import os
+import json
+import torch
+import numpy as np
+from pathlib import Path
+import subprocess
+import tempfile
+
+try:
+    import onnx
+    import onnxruntime as ort
+except Exception:
+    onnx = None
+    ort = None
+
+MODEL_PTH = os.environ.get("MODEL_PTH", "/workspace/model.pt")
+EXPORT_ONNX = os.environ.get("EXPORT_ONNX", "/tmp/model.onnx")
+QUANT_ONNX = os.environ.get("QUANT_ONNX", "/tmp/model_quant.onnx")
+CALIB_DATA_DIR = os.environ.get("CALIB_DATA_DIR", "/workspace/calib")
+VALIDATION_DATA_DIR = os.environ.get("VALIDATION_DATA_DIR", "/workspace/val")
+REPORT_OUT = os.environ.get("REPORT_OUT", "/tmp/quant_report.json")
+COSIGN_KEY = os.environ.get("COSIGN_KEY")  # optional path to cosign key for signing artifact
+OTA_UPLOAD_CMD = os.environ.get("OTA_UPLOAD_CMD")  # optional command to upload artifact to OTA (s3/cli)
+
+def export_to_onnx(model_pth, onnx_out, sample_input_shape=(1,3,224,224)):
+    # loads a PyTorch model and exports to ONNX using a dummy input
+    model = torch.load(model_pth, map_location="cpu")
+    model.eval()
+    dummy = torch.randn(*sample_input_shape)
+    torch.onnx.export(model, dummy, onnx_out, opset_version=14, do_constant_folding=True,
+                      input_names=['input'], output_names=['output'])
+    print("Exported ONNX:", onnx_out)
+
+def run_dynamic_quant(onnx_in, onnx_out):
+    # uses onnxruntime quantization tool (dynamic) if available
+    cmd = ["python","-m","onnxruntime.tools.convert_onnx_models", onnx_in, onnx_out]  # placeholder
+    # fallback to onnxruntime quantization script if available
+    try:
+        # Try using onnxruntime's quantization (if installed)
+        from onnxruntime.quantization import quantize_dynamic, QuantType
+        quantize_dynamic(onnx_in, onnx_out, weight_type=QuantType.QInt8)
+        print("Performed dynamic quantization ->", onnx_out)
+        return True
+    except Exception as e:
+        print("Dynamic quant quant failed, attempted fallback:", e)
+        return False
+
+def validate_models(onnx_orig, onnx_quant, val_dir, sample_count=20, atol=1e-3, rtol=1e-2):
+    # Run inference with ONNXRuntime on both models and compute similarity metrics
+    if not ort:
+        raise RuntimeError("onnxruntime not available")
+    sess_orig = ort.InferenceSession(onnx_orig, providers=['CPUExecutionProvider'])
+    sess_quant = ort.InferenceSession(onnx_quant, providers=['CPUExecutionProvider'])
+    rng = np.random.RandomState(0)
+    diffs = []
+    for i in range(sample_count):
+        # In production use real validation inputs read from val_dir; here we synthesize
+        inp = (rng.rand(1,3,224,224).astype(np.float32) - 0.5) * 2.0
+        o1 = sess_orig.run(None, {"input": inp})[0]
+        o2 = sess_quant.run(None, {"input": inp})[0]
+        # compute relative diff
+        nom = np.linalg.norm(o1 - o2)
+        denom = np.linalg.norm(o1) + 1e-12
+        rel = float(nom / denom)
+        diffs.append(rel)
+    stats = {"mean_rel_diff": float(np.mean(diffs)), "max_rel_diff": float(np.max(diffs)), "samples": len(diffs)}
+    # pass/fail
+    passed = stats["max_rel_diff"] <= rtol or stats["mean_rel_diff"] <= atol
+    return passed, stats
+
+def sign_artifact(path):
+    # sign using cosign if COSIGN_KEY provided
+    if not COSIGN_KEY:
+        return None
+    try:
+        out = subprocess.check_output(["cosign","sign","-key", COSIGN_KEY, path], stderr=subprocess.STDOUT).decode()
+        return out
+    except Exception as e:
+        print("cosign sign failed:", e)
+        return None
+
+def upload_artifact(path):
+    if not OTA_UPLOAD_CMD:
+        return None
+    try:
+        out = subprocess.check_output(OTA_UPLOAD_CMD.split() + [path], stderr=subprocess.STDOUT).decode()
+        return out
+    except Exception as e:
+        print("upload failed:", e)
+        return None
+
+def main():
+    report = {"steps": {}, "ts": None}
+    report["ts"] = __import__("time").time()
+    # export
+    export_to_onnx(MODEL_PTH, EXPORT_ONNX)
+    report["steps"]["export"] = {"onnx": EXPORT_ONNX}
+    # quantize
+    ok = run_dynamic_quant(EXPORT_ONNX, QUANT_ONNX)
+    report["steps"]["quantize"] = {"ok": ok, "quant_onnx": QUANT_ONNX}
+    # validate
+    try:
+        passed, stats = validate_models(EXPORT_ONNX, QUANT_ONNX, VALIDATION_DATA_DIR)
+    except Exception as e:
+        passed = False
+        stats = {"error": str(e)}
+    report["steps"]["validate"] = {"passed": passed, "stats": stats}
+    # sign
+    sig = sign_artifact(QUANT_ONNX)
+    report["steps"]["sign"] = {"signature": True if sig else False, "cosign_out": sig}
+    # upload to OTA if configured
+    upl = upload_artifact(QUANT_ONNX)
+    report["steps"]["upload"] = {"uploaded": True if upl else False, "upload_out": upl}
+    # write report
+    Path(REPORT_OUT).parent.mkdir(parents=True, exist_ok=True)
+    with open(REPORT_OUT, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote report", REPORT_OUT)
+    if not passed:
+        raise SystemExit(2)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: ci/quantize_and_validate.yml
+name: Quantize & Validate Model (CI)
+on:
+  workflow_dispatch:
+    inputs:
+      model_path:
+        required: true
+      sample_shape:
+        required: false
+        default: "1,3,224,224"
+
+jobs:
+  quantize-validate:
+    runs-on: self-hosted
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python & deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install torch onnx onnxruntime onnxruntime-tools numpy cosign
+      - name: Run quantize & validate
+        env:
+          MODEL_PTH: ${{ github.event.inputs.model_path }}
+          EXPORT_ONNX: /tmp/model.onnx
+          QUANT_ONNX: /tmp/model_quant.onnx
+          REPORT_OUT: /tmp/quant_report.json
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY_PATH }}
+          OTA_UPLOAD_CMD: ${{ secrets.OTA_UPLOAD_CMD }}
+        run: |
+          python edge/quantize_and_validate.py
+      - name: Upload report
+        uses: actions/upload-artifact@v4
+        with:
+          name: quant-report
+          path: /tmp/quant_report.json
+
*** End Patch
*** Begin Patch
*** Add File: ota/ota_service_v2.py
+#!/usr/bin/env python3
+"""
+OTA Service v2
+- stores artifact metadata + versions
+- supports canary cohort assignment, staged rollout, promote and rollback operations
+- verifies cosign signatures on submitted artifacts (operator provides public key or Vault supply)
+- exposes endpoints:
+  - POST /register_artifact {name, version, s3_path, checksum, signature_blob(optional)}
+  - POST /start_rollout {artifact_name, version, cohort_policy}
+  - POST /promote {artifact_name, version}
+  - POST /rollback {artifact_name, version}
+  - GET  /devices (list) /device/{id}
+  - POST /device/{id}/checkin (report state & get next artifact)
+
+Storage is in-memory (dict) for this example; operator should use DB.
+"""
+import os, json, hashlib
+from flask import Flask, request, jsonify
+from datetime import datetime
+try:
+    import boto3
+except Exception:
+    boto3 = None
+import subprocess
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+COSIGN_PUBKEY = os.environ.get("COSIGN_PUBKEY", "/etc/cosign/pubkey.pem")  # mounted via CSI in prod
+
+app = Flask("aegis-ota")
+
+# In-memory stores
+ARTIFACTS = {}  # name -> versions -> metadata
+DEVICES = {}    # device_id -> info {version, cohort, last_checkin, ip}
+ROLLOUTS = {}   # artifact -> rollout state
+
+def verify_cosign_signature(s3_path):
+    # placeholder: use cosign verify-blob or verify command; assume cosign binary present on OTA host
+    if not os.path.exists(COSIGN_PUBKEY):
+        return False, "no public key"
+    try:
+        out = subprocess.check_output(["cosign","verify-blob","-key", COSIGN_PUBKEY, s3_path], stderr=subprocess.STDOUT).decode()
+        return True, out
+    except Exception as e:
+        return False, str(e)
+
+@app.route("/register_artifact", methods=["POST"])
+def register_artifact():
+    j = request.get_json()
+    name = j.get("name")
+    version = j.get("version")
+    s3 = j.get("s3_path")
+    checksum = j.get("checksum")
+    if not all([name, version, s3]):
+        return jsonify({"error":"name,version,s3_path required"}), 400
+    ARTIFACTS.setdefault(name, {})[version] = {"s3": s3, "checksum": checksum, "registered_at": datetime.utcnow().isoformat(), "validated": False}
+    # verify signature optionally
+    ok, out = verify_cosign_signature(s3)
+    ARTIFACTS[name][version]["signature_verified"] = ok
+    ARTIFACTS[name][version]["signature_out"] = out
+    # optionally upload metadata to compliance bucket
+    if boto3 and COMPLIANCE_BUCKET:
+        s3_client = boto3.client("s3")
+        key = f"ota/metadata/{name}/{version}.json"
+        s3_client.put_object(Bucket=COMPLIANCE_BUCKET, Key=key, Body=json.dumps(ARTIFACTS[name][version]))
+    return jsonify({"ok":True,"artifact":ARTIFACTS[name][version]})
+
+@app.route("/start_rollout", methods=["POST"])
+def start_rollout():
+    j = request.get_json()
+    name = j.get("name"); version = j.get("version"); policy = j.get("policy", {"type":"canary","cohort_size":5})
+    if not ARTIFACTS.get(name,{}).get(version):
+        return jsonify({"error":"unknown artifact"}), 404
+    ROLLOUTS[(name,version)] = {"policy": policy, "started": datetime.utcnow().isoformat(), "state":"canary", "assigned":[]}
+    # assign initial canary cohort devices (simple selection)
+    candidates = [d for d in DEVICES.keys() if DEVICES[d].get("cohort") is None]
+    cohort = candidates[: policy.get("cohort_size",5)]
+    for d in cohort:
+        DEVICES[d]["target"] = {"name":name, "version": version}
+        DEVICES[d]["cohort"] = "canary"
+        ROLLOUTS[(name,version)]["assigned"].append(d)
+    return jsonify({"ok":True,"assigned": cohort})
+
+@app.route("/promote", methods=["POST"])
+def promote():
+    j = request.get_json()
+    name = j.get("name"); version = j.get("version")
+    rollout = ROLLOUTS.get((name,version))
+    if not rollout:
+        return jsonify({"error":"no rollout"}), 404
+    # promote canary to full: assign all devices to target
+    for d in DEVICES.keys():
+        DEVICES[d]["target"] = {"name":name, "version": version}
+    rollout["state"] = "promoted"
+    rollout["promoted_at"] = datetime.utcnow().isoformat()
+    return jsonify({"ok":True, "state":"promoted"})
+
+@app.route("/rollback", methods=["POST"])
+def rollback():
+    j = request.get_json()
+    name = j.get("name"); version = j.get("version")
+    # find devices that have target == name/version and set their target to previous_version if exists
+    prev = None
+    if ARTIFACTS.get(name):
+        versions = sorted(ARTIFACTS[name].keys())
+        if version in versions:
+            idx = versions.index(version)
+            if idx>0:
+                prev = versions[idx-1]
+    for d,info in DEVICES.items():
+        t = info.get("target")
+        if t and t.get("name")==name and t.get("version")==version:
+            if prev:
+                DEVICES[d]["target"] = {"name": name, "version": prev}
+            else:
+                DEVICES[d].pop("target", None)
+    # mark rollout as rolled back
+    ROLLOUTS[(name,version)] = ROLLOUTS.get((name,version),{})
+    ROLLOUTS[(name,version)]["state"] = "rolled_back"
+    ROLLOUTS[(name,version)]["rolled_at"] = datetime.utcnow().isoformat()
+    return jsonify({"ok":True, "rolled_to": prev})
+
+@app.route("/device/<device_id>/checkin", methods=["POST"])
+def device_checkin(device_id):
+    j = request.get_json() or {}
+    version = j.get("version")
+    ip = request.remote_addr
+    DEVICES.setdefault(device_id, {})["version"] = version
+    DEVICES[device_id]["last_checkin"] = datetime.utcnow().isoformat()
+    DEVICES[device_id]["ip"] = ip
+    # if device has a target assigned, return it
+    target = DEVICES[device_id].get("target")
+    return jsonify({"ok":True, "target": target, "device_info": DEVICES[device_id]})
+
+@app.route("/devices", methods=["GET"])
+def list_devices():
+    return jsonify({"devices": DEVICES})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8205")))
+
*** End Patch
*** Begin Patch
*** Add File: ota/k8s/ota-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-ota
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-ota
+  template:
+    metadata:
+      labels:
+        app: aegis-ota
+    spec:
+      containers:
+        - name: ota
+          image: aegis/ota-service:latest
+          ports:
+            - containerPort: 8205
+          env:
+            - name: COMPLIANCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: compliance-bucket
+            - name: COSIGN_PUBKEY
+              value: "/etc/cosign/pubkey.pem"
+          volumeMounts:
+            - name: cosign-pubkey
+              mountPath: /etc/cosign
+              readOnly: true
+      volumes:
+        - name: cosign-pubkey
+          secret:
+            secretName: cosign-public
+
*** End Patch
*** Begin Patch
*** Add File: edge/device_agent_v2.py
+#!/usr/bin/env python3
+"""
+Device agent v2:
+ - polls OTA service for target artifact
+ - downloads artifact with bandwidth-aware chunked downloader
+ - verifies cosign signature locally using mounted public key
+ - performs atomic install (write to versioned directory + symlink)
+ - supports rollback to previous version if install fails
+ - reports telemetry to telemetry collector
+"""
+import os, requests, shutil, hashlib, time
+from pathlib import Path
+import subprocess
+
+OTA_URL = os.environ.get("OTA_URL", "http://ota.aegis.svc:8205")
+DEVICE_ID = os.environ.get("DEVICE_ID", "device-demo-1")
+INSTALL_DIR = os.environ.get("INSTALL_DIR", "/opt/models")
+COSIGN_PUBKEY = os.environ.get("COSIGN_PUBKEY", "/etc/cosign/pubkey.pem")  # mounted via secret
+TELEMETRY_URL = os.environ.get("TELEMETRY_URL", "http://telemetry.aegis.svc:8215/report")
+DOWNLOAD_CHUNK = int(os.environ.get("DOWNLOAD_CHUNK", "1024")) * 1024  # bytes
+
+def sha256(path):
+    import hashlib
+    h = hashlib.sha256()
+    with open(path,"rb") as fh:
+        for chunk in iter(lambda: fh.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def verify_cosign_blob(local_path):
+    if not os.path.exists(COSIGN_PUBKEY):
+        return False, "no cosign pubkey"
+    try:
+        out = subprocess.check_output(["cosign","verify-blob","-key", COSIGN_PUBKEY, local_path], stderr=subprocess.STDOUT).decode()
+        return True, out
+    except Exception as e:
+        return False, str(e)
+
+def chunked_download(url, out_path):
+    # streaming GET and write in chunks; simple rate-limiter throttle (sleep)
+    r = requests.get(url, stream=True, timeout=60)
+    r.raise_for_status()
+    with open(out_path, "wb") as fh:
+        for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK):
+            if not chunk:
+                continue
+            fh.write(chunk)
+            fh.flush()
+            # simple bandwidth control: sleep briefly
+            time.sleep(0.05)
+    return out_path
+
+def atomic_install(local_artifact, name, version):
+    vdir = Path(INSTALL_DIR) / f"{name}-{version}"
+    prev_symlink = Path(INSTALL_DIR) / f"{name}-current"
+    # create vdir
+    vdir.mkdir(parents=True, exist_ok=True)
+    extract_dir = vdir / "files"
+    extract_dir.mkdir(parents=True, exist_ok=True)
+    # for simplicity assume artifact is a tarball
+    import tarfile
+    with tarfile.open(local_artifact) as tf:
+        tf.extractall(path=extract_dir)
+    # run validation hook (optional)
+    # atomic switch
+    tmp_link = Path(INSTALL_DIR) / f".{name}-tmp"
+    if tmp_link.exists():
+        tmp_link.unlink()
+    tmp_link.symlink_to(vdir)
+    tmp_link.replace(prev_symlink)
+    return True
+
+def rollback(name):
+    # find previous version directory and switch symlink
+    p = Path(INSTALL_DIR)
+    dirs = sorted([d for d in p.iterdir() if d.is_dir() and d.name.startswith(name+"-")], reverse=True)
+    if len(dirs) < 2:
+        return False
+    prev = dirs[1]
+    symlink = Path(INSTALL_DIR) / f"{name}-current"
+    if symlink.exists():
+        symlink.unlink()
+    symlink.symlink_to(prev)
+    return True
+
+def report_telemetry(payload):
+    try:
+        requests.post(TELEMETRY_URL, json=payload, timeout=3)
+    except Exception:
+        pass
+
+def main_loop():
+    while True:
+        try:
+            r = requests.post(f"{OTA_URL}/device/{DEVICE_ID}/checkin", json={"version": "current"}, timeout=10)
+            j = r.json()
+            t = j.get("target")
+            if t:
+                name = t["name"]; version = t["version"]
+                # download s3 path via signed URL provider or OTA -> assume/expect direct URL returned
+                s3 = ARTIFACT_URL = f"https://s3.example/{name}/{version}.tar.gz"  # replace with proper resolved URL
+                local = f"/tmp/{name}-{version}.tar.gz"
+                chunked_download(ARTIFACT_URL, local)
+                # verify checksum or cosign
+                ok, out = verify_cosign_blob(local)
+                if not ok:
+                    report_telemetry({"device": DEVICE_ID, "event": "verify_failed", "detail": out})
+                    # report and skip
+                else:
+                    # atomic install
+                    try:
+                        atomic_install(local, name, version)
+                        report_telemetry({"device": DEVICE_ID, "event": "install_ok", "name":name, "version":version})
+                    except Exception as e:
+                        report_telemetry({"device": DEVICE_ID, "event": "install_failed", "error": str(e)})
+                        rollback(name)
+            time.sleep(60)
+        except Exception as e:
+            report_telemetry({"device": DEVICE_ID, "event": "agent_error", "error": str(e)})
+            time.sleep(30)
+
+if __name__=="__main__":
+    Path(INSTALL_DIR).mkdir(parents=True, exist_ok=True)
+    main_loop()
+
*** End Patch
*** Begin Patch
*** Add File: edge/telemetry_collector.py
+#!/usr/bin/env python3
+"""
+Simple telemetry collector for edge devices.
+ - Accepts POST /report from device agents
+ - Stores telemetry to Redis list and (optionally) S3 for long-term retention
+ - Emits simple alerts when devices report install failures
+"""
+import os, json
+from flask import Flask, request, jsonify
+import redis
+import boto3
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/7")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+ALERT_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+r = redis.from_url(REDIS_URL)
+app = Flask("telemetry")
+
+@app.post("/report")
+def report():
+    j = request.get_json() or {}
+    device = j.get("device")
+    ev = j.get("event")
+    ts = j.get("ts") or __import__("time").time()
+    rec = {"device": device, "event": ev, "ts": ts, "payload": j}
+    r.lpush("telemetry:events", json.dumps(rec))
+    # upload critical events
+    if ev in ("install_failed","verify_failed","agent_error"):
+        try:
+            if ALERT_WEBHOOK:
+                import requests
+                requests.post(ALERT_WEBHOOK, json=rec, timeout=3)
+        except Exception:
+            pass
+    # optional S3 archival
+    if COMPLIANCE_BUCKET:
+        try:
+            s3 = boto3.client("s3")
+            key = f"telemetry/{device}/{int(ts)}.json"
+            s3.put_object(Bucket=COMPLIANCE_BUCKET, Key=key, Body=json.dumps(rec))
+        except Exception:
+            pass
+    return jsonify({"ok":True})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8215")))
+
*** End Patch
*** Begin Patch
*** Add File: edge/fleet_manager.py
+#!/usr/bin/env python3
+"""
+Fleet manager:
+ - registers devices and maintains cohorts (canary, staged, all)
+ - tracks rollout progress and metrics (via telemetry in Redis)
+ - decides promotions based on success metrics (success rate, bandwidth, failure rate)
+ - exposes HTTP API for operator to query & manually trigger rollouts/promote/rollback
+"""
+import os, json, time
+from flask import Flask, request, jsonify
+import redis
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/8")
+r = redis.from_url(REDIS_URL)
+OTA_API = os.environ.get("OTA_API", "http://ota.aegis.svc:8205")
+
+app = Flask("fleet")
+
+@app.post("/register")
+def register():
+    j = request.get_json() or {}
+    device = j.get("device_id")
+    meta = j.get("meta",{})
+    if not device:
+        return jsonify({"error":"device_id required"}), 400
+    r.hset("devices:meta", device, json.dumps(meta))
+    return jsonify({"ok":True})
+
+@app.get("/devices")
+def list_devices():
+    allk = r.hgetall("devices:meta")
+    return jsonify({"devices": {k.decode(): json.loads(v.decode()) for k,v in allk.items()}})
+
+@app.post("/rollout")
+def rollout():
+    j = request.get_json() or {}
+    artifact = j.get("artifact")
+    version = j.get("version")
+    policy = j.get("policy", {"type":"canary","cohort_size":5})
+    # register rollout state
+    rollout_key = f"rollout:{artifact}:{version}"
+    r.hset(rollout_key, mapping={"policy": json.dumps(policy), "state":"canary", "started":time.time()})
+    # choose cohort
+    devices = list(map(lambda x: x.decode(), r.hkeys("devices:meta")))
+    cohort = devices[:policy.get("cohort_size",5)]
+    r.hset(rollout_key, "cohort", json.dumps(cohort))
+    # request OTA to start rollout
+    import requests
+    requests.post(f"{OTA_API}/start_rollout", json={"name": artifact, "version": version, "policy": policy})
+    return jsonify({"ok":True, "cohort": cohort})
+
+@app.post("/promote")
+def promote():
+    j = request.get_json() or {}
+    artifact = j.get("artifact"); version = j.get("version")
+    r.hset(f"rollout:{artifact}:{version}", "state", "promoting")
+    import requests
+    requests.post(f"{OTA_API}/promote", json={"name": artifact, "version": version})
+    return jsonify({"ok":True})
+
+@app.post("/rollback")
+def rollback():
+    j = request.get_json() or {}
+    artifact = j.get("artifact"); version = j.get("version")
+    r.hset(f"rollout:{artifact}:{version}", "state", "rolling_back")
+    import requests
+    requests.post(f"{OTA_API}/rollback", json={"name": artifact, "version": version})
+    return jsonify({"ok":True})
+
+@app.get("/rollout_status/<artifact>/<version>")
+def status(artifact, version):
+    k = f"rollout:{artifact}:{version}"
+    d = r.hgetall(k)
+    return jsonify({k: {kk.decode(): vv.decode() for kk,vv in d.items()}})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8220")))
+
*** End Patch
*** Begin Patch
*** Add File: edge/hardware_validation.sh
+#!/usr/bin/env bash
+# Run on a representative device to validate quantized artifact performance & correctness
+set -euo pipefail
+ARTIFACT_TAR="$1"
+WORKDIR=${2:-/tmp/hwval}
+mkdir -p "${WORKDIR}"
+tar -xzf "${ARTIFACT_TAR}" -C "${WORKDIR}"
+# assume model files are in model.onnx
+MODEL="${WORKDIR}/model.onnx"
+if [ ! -f "${MODEL}" ]; then
+  echo "Model not found in artifact"
+  exit 2
+fi
+# run a small ONNX runtime inference and measure latency
+python - <<PY
+import time, onnxruntime as ort, numpy as np
+sess = ort.InferenceSession("${MODEL}", providers=['CPUExecutionProvider'])
+inp = (np.random.rand(1,3,224,224).astype('float32')-0.5)*2.0
+lat = []
+for i in range(10):
+    t0=time.time()
+    _=sess.run(None, {"input": inp})
+    lat.append(time.time()-t0)
+print("median_latency", sorted(lat)[len(lat)//2])
+print("mean_latency", sum(lat)/len(lat))
+PY
+echo "Hardware validation completed"
+
*** End Patch
*** Begin Patch
*** Add File: security/cosign_verify_helper.sh
+#!/usr/bin/env bash
+# Verify an artifact blob with cosign public key stored in /etc/cosign/pubkey.pem
+set -euo pipefail
+if [ "$#" -lt 1 ]; then
+  echo "usage: $0 <artifact-path>"
+  exit 2
+fi
+ART=$1
+PUB="/etc/cosign/pubkey.pem"
+if [ ! -f "$PUB" ]; then
+  echo "cosign public key not found at $PUB"
+  exit 3
+fi
+cosign verify-blob -key "$PUB" "$ART"
+echo "cosign verification succeeded"
+
*** End Patch
*** Begin Patch
*** Add File: docs/edge_rollout_playbook.md
+# Edge OTA & Rollout Playbook
+
+This playbook describes the safe flow for quantized model packaging, canary OTA rollout and rollback.
+
+1. Build & Quantize
+   - Run CI job `Quantize & Validate Model (CI)` which runs edge/quantize_and_validate.py.
+   - Ensure validation report passes (max_rel_diff and mean_rel_diff within thresholds).
+
+2. Register Artifact
+   - Use OTA API `POST /register_artifact` with name, version, s3_path and checksum.
+   - OTA will verify cosign signature and record metadata to compliance bucket.
+
+3. Start Canary
+   - Use fleet manager `POST /rollout` with artifact/version and policy {"type":"canary","cohort_size":N}.
+   - Fleet manager chooses N devices and OTA assigns target.
+
+4. Monitor Telemetry
+   - Device agents report events to telemetry collector.
+   - Watch errors (install_failed, verify_failed). Billing and bandwidth monitors should be observed.
+
+5. Promote or Rollback
+   - If canary metrics stable (low failure rate, acceptable latency), operator runs `POST /promote` or fleet manager auto-promotes.
+   - If failures observed, operator triggers `POST /rollback`.
+
+6. Hardware Validation
+   - For new device classes run edge/hardware_validation.sh on sample devices to confirm latency/accuracy.
+
+Security notes:
+ - Cosign public key must be provisioned via Kubernetes secret (CSI) to devices or OTA hosts.
+ - Operator signing service (HSM) must be used for production signing. CI must not be able to sign artifacts.
+
*** End Patch
*** End Patch
