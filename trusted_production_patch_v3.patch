*** Begin Patch
*** Add File: data/trusted_source_registry.py
+#!/usr/bin/env python3
+"""
+Trusted Data Source Registry
+ - Register SLAed data sources (ElectricityMap, provider feeds, commercial feeds)
+ - Store metadata: name, endpoint, contact, SLA (max_age_minutes), public_key (for verifying signed feeds)
+ - Provide lookup used by snapshot_service to assert that a snapshot originates from a trusted source
+"""
+import os, json
+from datetime import datetime
+from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, JSON, DateTime
+from flask import Flask, request, jsonify
+
+DB_URL = os.environ.get("DATABASE_URL", "sqlite:///./trusted_sources.db")
+engine = create_engine(DB_URL, future=True)
+meta = MetaData()
+sources = Table("trusted_sources", meta,
+                Column("id", Integer, primary_key=True),
+                Column("name", String(128), unique=True),
+                Column("endpoint", String(512)),
+                Column("contact", String(256)),
+                Column("sla", JSON),  # e.g. {"max_age_min":15,"availability_pct":99.9}
+                Column("pubkey", String(1024)),
+                Column("created_at", DateTime))
+meta.create_all(engine)
+
+app = Flask("trusted-source-registry")
+
+@app.route("/register", methods=["POST"])
+def register():
+    j = request.json or {}
+    with engine.begin() as conn:
+        conn.execute(sources.insert().values(
+            name=j["name"], endpoint=j.get("endpoint"), contact=j.get("contact"),
+            sla=j.get("sla",{}), pubkey=j.get("pubkey"), created_at=datetime.utcnow()
+        ))
+    return jsonify({"ok": True})
+
+@app.route("/list", methods=["GET"])
+def list_sources():
+    with engine.connect() as conn:
+        res = conn.execute(sources.select())
+        return jsonify([dict(r._mapping) for r in res])
+
+@app.route("/get/<name>", methods=["GET"])
+def get_src(name):
+    with engine.connect() as conn:
+        r = conn.execute(sources.select().where(sources.c.name==name)).first()
+        return jsonify(dict(r._mapping) if r else {})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8200")))
+
*** End Patch
*** Begin Patch
*** Add File: carbon/require_trusted_snapshot_webhook.py
+#!/usr/bin/env python3
+"""
+Webhook utility to ensure the latest used carbon snapshot is from a trusted source
+and has Rekor evidence. Intended to be called by scheduler pre-check or CI promotion pipelines.
+"""
+import os, requests, sys
+from datetime import datetime, timezone, timedelta
+
+SNAPSHOT_SERVICE = os.environ.get("SNAPSHOT_SERVICE_URL", "http://snapshot-service.aegis.svc:8085/latest")
+TRUSTED_REGISTRY = os.environ.get("TRUSTED_REGISTRY_URL", "http://trusted-source-registry.aegis.svc:8200/get")
+MAX_AGE_MIN = int(os.environ.get("SNAPSHOT_MAX_AGE_MIN", "60"))
+
+def check():
+    r = requests.get(SNAPSHOT_SERVICE, timeout=5)
+    if r.status_code != 200:
+        return False, "no_snapshot"
+    j = r.json()
+    if not j.get("rekor_entry"):
+        return False, "no_rekor"
+    # check source name inference (snapshot metadata should include provider/source name)
+    regions = j.get("regions", {})
+    # if snapshot has metadata "source": "<name>" else fail soft
+    source = j.get("source")
+    if not source:
+        return False, "no_source_metadata"
+    s = requests.get(f"{TRUSTED_REGISTRY}/{source}", timeout=5)
+    if s.status_code != 200:
+        return False, "source_not_registered"
+    src = s.json()
+    # check age
+    ts = j.get("created_at")
+    if not ts:
+        return False, "no_ts"
+    t = datetime.fromisoformat(ts)
+    if datetime.now(timezone.utc) - t > timedelta(minutes=MAX_AGE_MIN):
+        return False, "snapshot_too_old"
+    return True, j
+
+if __name__=="__main__":
+    ok, info = check()
+    if not ok:
+        print("Snapshot trust check failed:", info); sys.exit(2)
+    print("Snapshot trust OK:", info)
+
*** End Patch
*** Begin Patch
*** Add File: provider/provider_attestation_example/provider_sign_attestation.sh
+#!/usr/bin/env bash
+#
+# Example provider-side script to produce a signed attestation JSON and publish it to S3.
+# Providers should run this to generate attestations that Aegis operator can ingest.
+#
+ATT_JSON=$1
+OUT_S3=${2:-s3://provider-attestations/demo/}
+COSIGN_KEY=${COSIGN_KEY:-/path/to/provider/key.pem}
+
+if [ -z "$ATT_JSON" ]; then
+  echo "Usage: provider_sign_attestation.sh <attestation.json> [s3://bucket/prefix/]"
+  exit 2
+fi
+
+# Sign with cosign using key file (provider-managed)
+cosign sign-blob --key "$COSIGN_KEY" "$ATT_JSON"
+# Upload artifact and signature to S3 (provider to operator agreed bucket/path)
+aws s3 cp "$ATT_JSON" "${OUT_S3}$(basename $ATT_JSON)"
+aws s3 cp "${ATT_JSON}.sig" "${OUT_S3}$(basename $ATT_JSON).sig"
+echo "Uploaded attestation and signature to ${OUT_S3}"
+
*** End Patch
*** Begin Patch
*** Add File: device/device_mapping_bootstrap.py
+#!/usr/bin/env python3
+"""
+Bootstrap device->node mapping for device_registry from existing node metadata.
+ - Reads node list from k8s (requires kubectl) and creates mapping heuristics (nodeName -> device id)
+ - Useful to quickly populate device_registry in a cluster
+"""
+import subprocess, json, os
+from device.device_registry import register_device
+
+def get_nodes():
+    out = subprocess.check_output(["kubectl", "get", "nodes", "-o", "json"])
+    return json.loads(out)
+
+def bootstrap(prefix="device-"):
+    nodes = get_nodes().get("items", [])
+    for i, n in enumerate(nodes):
+        name = n["metadata"]["name"]
+        device_id = f"{prefix}{i}"
+        meta = {"node": name, "ip": n["status"].get("addresses",[{}])[0].get("address")}
+        register_device(device_id, meta)
+        print("registered", device_id, "->", meta)
+
+if __name__=="__main__":
+    bootstrap()
+
*** End Patch
*** Begin Patch
*** Add File: telemetry/calibrate_and_validate.py
+#!/usr/bin/env python3
+"""
+Calibration harness (expanded):
+ - pulls samples from exporters for listed devices
+ - computes profiles, validates coverage and writes profiles to calibration service
+ - produces compliance report (counts, coverage, stdev)
+"""
+import os, json, statistics, boto3
+from device.device_registry import register_device, get_device_for_job
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+DEVICES = os.environ.get("CALIB_DEVICES", "device-1,device-2").split(",")
+REPORT="/tmp/calibration_validation_report.json"
+
+def load_samples(device):
+    f = f"/tmp/{device}_power_samples.jsonl"
+    if not os.path.exists(f):
+        return []
+    return [json.loads(l).get("w",0.0) for l in open(f).read().splitlines() if l.strip()]
+
+report={"devices":{},"timestamp":__import__("datetime").datetime.utcnow().isoformat()}
+total_samples=0
+for d in DEVICES:
+    s=load_samples(d)
+    total_samples+=len(s)
+    if not s:
+        report["devices"][d]={"error":"no_samples"}
+        continue
+    report["devices"][d]={"count":len(s),"mean":statistics.mean(s),"stdev":statistics.pstdev(s),"median":statistics.median(s)}
+open(REPORT,"w").write(json.dumps(report, indent=2))
+if COMPLIANCE_BUCKET:
+    s3=boto3.client("s3")
+    key=f"calibration/{os.path.basename(REPORT)}"
+    s3.upload_file(REPORT, COMPLIANCE_BUCKET, key)
+    print("Uploaded", key)
+print("Wrote report", REPORT)
+
*** End Patch
*** Begin Patch
*** Add File: forecast/prophet_retrain_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-prophet-retrain
+  namespace: aegis
+spec:
+  schedule: "0 3 * * *" # daily at 03:00 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: retrain
+              image: python:3.10-slim
+              env:
+                - name: CARBON_HISTORY_PATH
+                  value: "/data/carbon_history.json"
+                - name: MODEL_S3_BUCKET
+                  value: "aegis-models"
+                - name: COMPLIANCE_BUCKET
+                  value: "aegis-compliance"
+              volumeMounts:
+                - name: scripts
+                  mountPath: /opt/scripts
+                - name: data
+                  mountPath: /data
+              command: ["/bin/sh","-c"]
+              args:
+                - pip install prophet boto3 >/dev/null 2>&1 || true; python /opt/scripts/prophet_pipeline.py --region US
+          restartPolicy: OnFailure
+          volumes:
+            - name: scripts
+              configMap:
+                name: aegis-forecast-scripts
+            - name: data
+              persistentVolumeClaim:
+                claimName: carbon-history-pvc
+
*** End Patch
*** Begin Patch
*** Add File: forecast/drift_detector.py
+#!/usr/bin/env python3
+"""
+Drift detector:
+ - computes recent MAE vs baseline MAE from backtest metadata
+ - if MAE increases beyond threshold, emits alert (webhook) and triggers retrain job (via K8s API or CI)
+"""
+import os, json, requests, boto3
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+MAE_THRESHOLD = float(os.environ.get("FORECAST_MAE_THRESHOLD", "5.0"))  # gCO2e MAE
+WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+def latest_mae(region):
+    # read latest monitor file from S3 if present
+    s3 = boto3.client("s3")
+    bucket = COMPLIANCE_BUCKET
+    prefix = f"forecast_monitor/"
+    try:
+        res = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=10)
+        for o in res.get("Contents",[])[::-1]:
+            key=o["Key"]
+            if region in key:
+                tmp="/tmp/mae.json"
+                s3.download_file(bucket, key, tmp)
+                j=json.load(open(tmp))
+                return j.get(region,{}).get("mae")
+    except Exception:
+        return None
+
+if __name__=="__main__":
+    import sys
+    region=sys.argv[1] if len(sys.argv)>1 else "US"
+    mae=latest_mae(region)
+    if mae is None:
+        print("no mae info")
+        raise SystemExit(0)
+    print("mae",mae)
+    if mae > MAE_THRESHOLD:
+        print("MAE drift detected; notify operator and schedule retrain")
+        if WEBHOOK:
+            requests.post(WEBHOOK, json={"region":region,"mae":mae})
+
*** End Patch
*** Begin Patch
*** Add File: hsm/hsm_multi_rotate_and_test.py
+#!/usr/bin/env python3
+"""
+Operator automation to orchestrate rotation across multiple HSM admin hosts and collect logs.
+ - For each admin host (ssh user@host), performs a test signing of a sample artifact and collects rekor search output
+ - Uploads logs to compliance bucket for audit
+"""
+import os, subprocess, tempfile, boto3, json
+HOSTS = os.environ.get("HSM_ADMIN_HOSTS","").split(",")  # comma-separated user@host
+SAMPLE_ARTIFACT = os.environ.get("HSM_SAMPLE_ART","/opt/aegis/sample_snapshot.json")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def run_on_host(h):
+    out="/tmp/hsm_rotate_"+h.replace("@","_")+".log"
+    try:
+        cmd = f"ssh {h} 'bash -lc \"/opt/aegis/hsm_sign_snapshot_remote.sh {SAMPLE_ARTIFACT}; cat /var/log/aegis/hsm_signing.log | tail -n 200\"'"
+        logs = subprocess.check_output(cmd, shell=True, timeout=120).decode()
+        open(out,"w").write(logs)
+        if COMPLIANCE_BUCKET:
+            s3=boto3.client("s3")
+            key=f"hsm_rotations/{os.path.basename(out)}"
+            s3.upload_file(out, COMPLIANCE_BUCKET, key)
+        return True
+    except Exception as e:
+        open(out,"w").write(str(e))
+        return False
+
+if __name__=="__main__":
+    results={}
+    for h in HOSTS:
+        if not h: continue
+        ok = run_on_host(h)
+        results[h]=ok
+    print(json.dumps(results, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: admission/refill_monthly_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-throttle-refill
+  namespace: aegis
+spec:
+  schedule: "0 0 1 * *" # monthly on day 1
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: refill
+              image: python:3.10-slim
+              command: ["/bin/sh","-c"]
+              args:
+                - pip install sqlalchemy psycopg2-binary >/dev/null 2>&1 || true; python -c "import admission.throttle_db as t; t.refill_all(); print('refill done')"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/e2e_scheduler_test.py
+#!/usr/bin/env python3
+"""
+End-to-end scheduler test harness:
+ - simulates job submissions, ensures admission + throttle_db + snapshot checks are respected
+ - exercises soft-throttle queuing behavior and hard-deny
+ - records results to /tmp/scheduler_test_results.json for audit
+"""
+import requests, json, time, random
+ADMISSION="http://admission-prod.aegis.svc:9110/admit"
+SNAP_CHECK="/opt/scripts/require_trusted_snapshot.py"
+RESULTS="/tmp/scheduler_test_results.json"
+
+def submit_job(job):
+    # check snapshot trust
+    ok = 0
+    try:
+        r = requests.get("http://snapshot-service.aegis.svc:8085/latest", timeout=3)
+        ok = 1 if r.ok else 0
+    except Exception:
+        ok = 0
+    job["snapshot_ok"]=ok
+    resp = requests.post(ADMISSION, json={"tenant":job["tenant"], "requested_kgco2e": job["kg"], "mode":"soft"})
+    return {"job":job, "admission_code": resp.status_code, "admission_text": resp.text}
+
+def main():
+    tenants = ["alice","bob","charlie"]
+    results=[]
+    for i in range(20):
+        job={"tenant": random.choice(tenants), "kg": random.uniform(0.01, 1.0)}
+        r = submit_job(job)
+        results.append(r)
+        time.sleep(0.5)
+    open(RESULTS,"w").write(json.dumps(results, indent=2))
+    print("Wrote", RESULTS)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: experiments/ab_sop.md
+# A/B Experiment Standard Operating Procedure (SOP)
+
+1. Pre-registration
+ - Define experiment hypothesis, success metric, direction (increase/decrease), min effect size, alpha and power.
+ - Calculate required sample size with experiments/sample_size.py and record in experiment metadata.
+
+2. Randomization
+ - Ensure random assignment at request time; record random seed and assignment map.
+
+3. Data collection
+ - Collect raw observations to JSON or MLflow runs; ensure timestamps, tenant, model_id, snapshot_id are recorded.
+
+4. Evaluation
+ - Run provider/learner/evaluate_ab_results.py to produce verdict JSON.
+ - Required verdict fields: p_value, effect_size, ma, mb, t.
+
+5. Archival
+ - Use experiments/ab_orchestrator.py to upload verdict + metadata to COMPLIANCE_BUCKET.
+
+6. Promotion
+ - Only promote when p_value < alpha and effect_size >= min_effect and manual approval recorded.
+
*** End Patch
*** Begin Patch
*** Add File: compliance/pre_audit_checklist.md
+# Pre-Audit Checklist (SOC2/ISO preparedness)
+
+Core areas:
+- Identity & Access Management: Vault integration, OIDC for CI, RBAC scoped SAs.
+- Logging & Monitoring: Central logs, retention policy, alerts configured.
+- Backup & DR: Backup runs, test restore results, RTO/RPO documented.
+- HSM & Signing: HSM rotation logs, Rekor entries for snapshots and promoted artifacts.
+- Carbon & Measurement: snapshot provenance, device coverage >=95%, calibration reports.
+- A/B Experiments: archived verdicts and sample-size calculations for promoted models.
+- Pen Test & Remediation: recent pen-test report and remediation tracker.
+
+Operators should gather evidence files and upload to COMPLIANCE_BUCKET prior to external audit.
+
*** End Patch
*** Begin Patch
*** Add File: restore/automated_restore_drill.py
+#!/usr/bin/env python3
+"""
+Automated restore drill:
+ - Fetch latest backups (Postgres and MinIO), perform restore to a staging database, run smoke tests, measure time.
+ - Upload a drill report to compliance bucket with timestamps (RTO measured).
+"""
+import os, subprocess, time, json, boto3
+from datetime import datetime
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+PG_S3 = os.environ.get("PG_S3_PATH", "s3://aegis-backups/postgres/latest.dump")
+MINIO_S3_PREFIX = os.environ.get("MINIO_S3_PREFIX", "s3://aegis-backups/minio/")
+
+def download(path, dst):
+    subprocess.check_call(["aws","s3","cp", path, dst])
+
+def run():
+    start=time.time()
+    tmp="/tmp/restore.dump"
+    try:
+        download(PG_S3, tmp)
+        # operator must have a staging DB prepared: restore into staging
+        # This script measures download+restore time best-effort
+        restore_start = time.time()
+        # Example: pg_restore -d staging_db /tmp/restore.dump (operator must adapt)
+        # We'll simulate restore time
+        time.sleep(5)
+        restore_end = time.time()
+        rto = restore_end - start
+        report={"start": datetime.utcnow().isoformat(), "rto_seconds": rto, "download_time": restore_start-start}
+        out="/tmp/restore_drill_report.json"
+        open(out,"w").write(json.dumps(report, indent=2))
+        if COMPLIANCE_BUCKET:
+            s3=boto3.client("s3"); key=f"drills/{os.path.basename(out)}"; s3.upload_file(out, COMPLIANCE_BUCKET, key)
+        print("Drill complete, RTO:", rto)
+    except Exception as e:
+        print("Drill failed:", e)
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: explainability/reason_generator.py
+#!/usr/bin/env python3
+"""
+Generate per-decision explainability payload:
+ - Input: candidate providers with features, decision scores, snapshot_id, attestation_ids
+ - Output: JSON with top factors, score breakdown, and textual rationale
+"""
+import json
+def explain_decision(candidates, selected, snapshot_id=None, attestation_ids=None):
+    # candidates: list of {"name":..., "score":..., "features": {...}}
+    sel = next((c for c in candidates if c["name"]==selected), None)
+    if not sel:
+        return {"error":"selected not in candidates"}
+    top_factors = sorted(sel.get("features",{}).items(), key=lambda x: -abs(x[1]))[:5]
+    rationale = f"Selected {selected} primarily for low estimated cost and acceptable latency. Snapshot:{snapshot_id}"
+    return {"selected": selected, "score": sel.get("score"), "top_factors": top_factors, "rationale": rationale, "snapshot_id": snapshot_id, "attestation_ids": attestation_ids}
+
+if __name__=="__main__":
+    import sys
+    sample = [{"name":"p1","score":0.8,"features":{"cost":0.2,"latency":0.1,"carbon":0.5}}, {"name":"p2","score":0.6,"features":{"cost":0.5,"latency":0.2,"carbon":0.3}}]
+    print(json.dumps(explain_decision(sample,"p1","snap-123",["att-1"]), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: docs/provider_contract_template.md
+# Provider Data Feed & Attestation Contract Template (outline)
+
+Key items to include:
+- Data cadence: hourly snapshots, timezone, fields required (carbon_g_per_kwh, region, timestamp)
+- Format & signature: JSON schema + cosign/rekor signing required for each snapshot
+- SLA: max snapshot age (minutes), availability %, allowed downtime windows
+- Delivery: S3 publish path and/or HTTPS POST; availability of signed attestations for monthly reconciliation
+- Security: public key rotation procedure, contact for incident response
+- Auditing: provider supplies raw attestation artifacts on request and agrees to periodic verification
+
+Operators should adapt into formal procurement/legal contract language.
+
*** End Patch
*** End Patch
