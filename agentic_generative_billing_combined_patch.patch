*** Begin Patch
*** Add File: k8s/rlhf/accelerate_distributed_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: rlhf-accelerate-distributed
+  namespace: aegis-ml
+spec:
+  template:
+    spec:
+      containers:
+        - name: accelerate-launcher
+          image: aegis/rlhf:latest
+          command:
+            - "/bin/sh"
+            - "-c"
+            - |
+              set -euo pipefail
+              /opt/rl/accelerate_launcher.sh
+          env:
+            - name: NUM_MACHINES
+              value: "2"
+            - name: NUM_PROCESSES
+              value: "2"
+            - name: MAIN_PROCESS_IP
+              value: ""
+            - name: MAIN_PROCESS_PORT
+              value: "29500"
+            - name: MODEL_NAME
+              value: "distilgpt2"
+            - name: OUTPUT_DIR
+              value: "/tmp/rlhf_out"
+            - name: MLFLOW_TRACKING_URI
+              valueFrom:
+                secretKeyRef:
+                  name: mlflow-secrets
+                  key: tracking-uri
+            - name: ARTIFACT_S3_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: artifact-bucket
+          resources:
+            requests:
+              cpu: "4"
+              memory: "16Gi"
+            limits:
+              cpu: "8"
+              memory: "48Gi"
+      restartPolicy: Never
+  backoffLimit: 1
+
+---
+# Notes:
+# - For multi-node: set NUM_MACHINES>1 and ensure MAIN_PROCESS_IP points to the launcher's node ip (or use headless service).
+# - Use a proper scheduler (MPIJob, KubeRay, or a job controller) for large multi-node experiments in production.
+
*** End Patch
*** Begin Patch
*** Add File: rl/accelerate_launcher.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Build an accelerate config dynamically and launch the distributed training script.
+NUM_MACHINES=${NUM_MACHINES:-2}
+NUM_PROCESSES=${NUM_PROCESSES:-2}
+MAIN_PROCESS_IP=${MAIN_PROCESS_IP:-}
+MAIN_PROCESS_PORT=${MAIN_PROCESS_PORT:-29500}
+MODEL_NAME=${MODEL_NAME:-"distilgpt2"}
+OUTPUT_DIR=${OUTPUT_DIR:-"/tmp/rlhf_out"}
+
+ACCEL_CONF=/tmp/accelerate_config.yaml
+
+cat > ${ACCEL_CONF} <<YAML
+compute_environment: LOCAL_MACHINE
+distributed_type: MULTI_GPU
+downcast_bfloat16: "no"
+mixed_precision: "fp16"
+machine_rank: 0
+main_process_ip: "${MAIN_PROCESS_IP}"
+main_process_port: ${MAIN_PROCESS_PORT}
+num_machines: ${NUM_MACHINES}
+num_processes: ${NUM_PROCESSES}
+use_cpu: false
+YAML
+
+echo "Generated accelerate config:"
+cat ${ACCEL_CONF}
+
+echo "Launching accelerate with config ${ACCEL_CONF}..."
+accelerate launch --config_file ${ACCEL_CONF} rl/distributed_train.py --model-name "${MODEL_NAME}" --output-dir "${OUTPUT_DIR}" --epochs 1 --per-device-batch-size 2
+
+echo "Accelerate launch finished."
+
*** End Patch
*** Begin Patch
*** Add File: rl/distributed_train.py
+#!/usr/bin/env python3
+"""
+Distributed training entrypoint (works under `accelerate launch`).
+ - Uses HuggingFace Trainer for a small supervised-style run as a RLHF pilot step.
+ - Logs parameters/artifacts to MLflow and uploads checkpoint via checkpoint_manager helper.
+"""
+import os, argparse, tempfile
+from datasets import Dataset
+from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
+import mlflow
+try:
+    from rl.checkpoint_manager import upload_checkpoint
+except Exception:
+    def upload_checkpoint(path, s3_path, metadata=None):
+        return {"s3_path": s3_path, "local": path, "meta": metadata}
+from rl.checkpoint_validate import sha256
+
+def make_toy_dataset():
+    texts = ["Q: What is 2+2? A: 4", "Q: Capital of France? A: Paris", "Q: Who wrote Hamlet? A: Shakespeare"]
+    return Dataset.from_list([{"text": t} for t in texts])
+
+def tokenize_function(examples, tokenizer, max_length=128):
+    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_length)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-name", default=os.environ.get("MODEL_NAME","distilgpt2"))
+    p.add_argument("--output-dir", default=os.environ.get("OUTPUT_DIR","/tmp/rlhf_out"))
+    p.add_argument("--epochs", type=int, default=1)
+    p.add_argument("--per-device-batch-size", type=int, default=2)
+    p.add_argument("--save-steps", type=int, default=50)
+    args = p.parse_args()
+
+    mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI")
+    if mlflow_uri:
+        mlflow.set_tracking_uri(mlflow_uri)
+
+    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
+    model = AutoModelForCausalLM.from_pretrained(args.model_name)
+
+    ds = make_toy_dataset()
+    ds = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)
+    ds.set_format(type="torch", columns=["input_ids","attention_mask"])
+
+    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
+
+    training_args = TrainingArguments(
+        output_dir=args.output_dir,
+        per_device_train_batch_size=args.per_device_batch_size,
+        num_train_epochs=args.epochs,
+        logging_steps=10,
+        save_steps=args.save_steps,
+        save_total_limit=3,
+        fp16=True,
+    )
+
+    trainer = Trainer(model=model, args=training_args, train_dataset=ds, data_collator=data_collator)
+
+    with mlflow.start_run():
+        mlflow.log_param("model_name", args.model_name)
+        mlflow.log_param("epochs", args.epochs)
+        mlflow.log_param("per_device_batch_size", args.per_device_batch_size)
+        trainer.train()
+        trainer.save_model(args.output_dir)
+        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".tar.gz")
+        import tarfile
+        with tarfile.open(tmp.name, "w:gz") as tf:
+            tf.add(args.output_dir, arcname="model")
+        checksum = sha256(tmp.name)
+        mlflow.log_param("checkpoint_sha256", checksum)
+        mlflow.log_artifact(tmp.name, artifact_path="checkpoints")
+        s3_rec = upload_checkpoint(tmp.name, f"rlhf/checkpoints/{os.path.basename(tmp.name)}", metadata={"model": args.model_name})
+        print("Uploaded checkpoint:", s3_rec)
+    print("Distributed training script finished.")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/rlhf_full_staging.yml
+name: RLHF Full Staging Run (multi-node)
+on:
+  workflow_dispatch:
+    inputs:
+      mode:
+        description: "run-mode: 'k8s' or 'local'"
+        required: true
+        default: "local"
+      num_machines:
+        required: false
+        default: "2"
+      num_processes:
+        required: false
+        default: "2"
+      model_name:
+        required: false
+        default: "distilgpt2"
+
+jobs:
+  run:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python and tools
+        run: |
+          python -m pip install --upgrade pip
+          pip install accelerate transformers datasets mlflow boto3
+      - name: Run staging RLHF
+        env:
+          MODE: ${{ github.event.inputs.mode }}
+          NUM_MACHINES: ${{ github.event.inputs.num_machines }}
+          NUM_PROCESSES: ${{ github.event.inputs.num_processes }}
+          MODEL_NAME: ${{ github.event.inputs.model_name }}
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+        run: |
+          if [ "${MODE}" = "k8s" ]; then
+            kubectl apply -f k8s/rlhf/accelerate_distributed_job.yaml
+            kubectl wait --for=condition=complete job/rlhf-accelerate-distributed -n aegis-ml --timeout=2h
+          else
+            python -m pip install --upgrade accelerate
+            accelerate launch --num_processes 2 rl/distributed_train.py --model-name "${MODEL_NAME}" --output-dir "/tmp/rlhf_smoke" --epochs 1 --per-device-batch-size 1 --save-steps 10
+          fi
+      - name: Collect artifacts and validate checkpoint
+        if: success()
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          CKPT=$(ls /tmp/rlhf_smoke/*.tar.gz 2>/dev/null || true)
+          if [ -n "$CKPT" ]; then
+            python rl/checkpoint_validate.py --ckpt "$CKPT"
+          else
+            python - <<PY
+import mlflow, os
+mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI"))
+client = mlflow.tracking.MlflowClient()
+runs = client.search_runs(experiment_ids=None, filter_string="", max_results=10)
+print("Recent runs:", [r.info.run_id for r in runs])
+PY
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/rlhf_operator_playbook.md
+# RLHF Operator Playbook (Staging multi-node)
+
+Purpose
+- Guide to run, monitor, resume, and validate distributed RLHF runs in staging.
+
+Prereqs
+- K8s staging cluster with GPU nodepool and network connectivity for multi-node rendezvous.
+- MLflow tracking server reachable from training pods.
+- Image aegis/rlhf:latest built with CUDA & deps.
+- Secrets: mlflow-secrets, aegis-secrets in aegis-ml namespace.
+
+Run (k8s mode)
+1. Update k8s/rlhf/accelerate_distributed_job.yaml env:
+   - NUM_MACHINES, NUM_PROCESSES, MAIN_PROCESS_IP, MODEL_NAME, OUTPUT_DIR.
+2. Apply job:
+   kubectl apply -f k8s/rlhf/accelerate_distributed_job.yaml -n aegis-ml
+3. Monitor logs:
+   kubectl logs job/rlhf-accelerate-distributed -n aegis-ml --follow
+4. Retrieve MLflow artifacts and validate checkpoint using rl/checkpoint_validate.py
+5. Run adversarial harness before promotion.
+
+Run (local/CI smoke)
+1. Use GitHub Action RLHF Full Staging Run (mode=local) or:
+   accelerate launch --num_processes 2 rl/distributed_train.py --model-name <model> --output-dir /tmp/rlhf_smoke --epochs 1
+2. Validate checkpoint and run adversarial harness.
+
+Resuming
+- Trainer.resume_from_checkpoint should be used in future training args.
+
*** End Patch
*** Begin Patch
*** Add File: llm/quota.lua
+-- Redis Lua token bucket
+-- KEYS[1] = bucket_key (e.g. "quota:{tenant}:{model}")
+-- ARGV[1] = requested_tokens
+-- ARGV[2] = capacity
+-- ARGV[3] = refill_per_sec
+-- ARGV[4] = now_ms
+local key = KEYS[1]
+local req = tonumber(ARGV[1])
+local capacity = tonumber(ARGV[2])
+local refill = tonumber(ARGV[3])
+local now_ms = tonumber(ARGV[4])
+
+local state = redis.call("HMGET", key, "tokens", "last_ts")
+local tokens = tonumber(state[1]) or capacity
+local last_ts = tonumber(state[2]) or now_ms
+
+local elapsed = math.max(0, now_ms - last_ts)
+local refill_amount = (elapsed / 1000.0) * refill
+tokens = math.min(capacity, tokens + refill_amount)
+
+if tokens >= req then
+  tokens = tokens - req
+  redis.call("HMSET", key, "tokens", tostring(tokens), "last_ts", tostring(now_ms))
+  redis.call("PEXPIRE", key, 86400000)
+  return {1, tostring(tokens)}
+else
+  redis.call("HMSET", key, "tokens", tostring(tokens), "last_ts", tostring(now_ms))
+  redis.call("PEXPIRE", key, 86400000)
+  return {0, tostring(tokens)}
+end
+
*** End Patch
*** Begin Patch
*** Add File: llm/gateway_quota_middleware.py
+#!/usr/bin/env python3
+import os, time, pkgutil
+import redis
+from prometheus_client import Counter, Gauge
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+redis_client = redis.from_url(REDIS_URL)
+
+_quota_lua = pkgutil.get_data(__name__, "quota.lua")
+if _quota_lua is None:
+    try:
+        _quota_lua = open(os.path.join(os.path.dirname(__file__), "quota.lua"), "rb").read()
+    except Exception:
+        _quota_lua = b""
+_quota_sha = None
+try:
+    if _quota_lua:
+        _quota_sha = redis_client.script_load(_quota_lua)
+except Exception:
+    _quota_sha = None
+
+MET_TOKENS_CONSUMED = Counter("aegis_gateway_tokens_consumed_total", "Tokens consumed by gateway", ["model", "tenant"])
+MET_TOKENS_REJECTED = Counter("aegis_gateway_tokens_rejected_total", "Tokens rejected due to quota", ["model", "tenant"])
+MET_MODEL_THROTTLED = Gauge("aegis_model_throttled", "Model throttle state 1=throttled", ["model"])
+
+DEFAULT_CAPACITY = int(os.environ.get("QUOTA_DEFAULT_CAPACITY", "10000"))
+DEFAULT_REFILL = float(os.environ.get("QUOTA_DEFAULT_REFILL_PER_SEC", "1000"))
+
+def _eval_quota(bucket_key, requested, capacity, refill):
+    now_ms = int(time.time() * 1000)
+    try:
+        if _quota_sha:
+            res = redis_client.evalsha(_quota_sha, 1, bucket_key, requested, capacity, refill, now_ms)
+        else:
+            script = open(os.path.join(os.path.dirname(__file__), "quota.lua")).read()
+            res = redis_client.eval(script, 1, bucket_key, requested, capacity, refill, now_ms)
+        ok = int(res[0]) == 1
+        remaining = float(res[1])
+        return ok, remaining
+    except redis.exceptions.ResponseError:
+        return False, 0.0
+    except Exception:
+        return False, 0.0
+
+def check_and_consume(model_id: str, tenant_id: str, requested_tokens: int = 1, capacity: int = None, refill_per_sec: float = None) -> bool:
+    if capacity is None:
+        capacity = DEFAULT_CAPACITY
+    if refill_per_sec is None:
+        refill_per_sec = DEFAULT_REFILL
+    bucket_key = f"quota:{tenant_id}:{model_id}"
+    throttle_key = f"model:throttle:{model_id}"
+    if redis_client.get(throttle_key):
+        MET_TOKENS_REJECTED.labels(model=model_id, tenant=tenant_id).inc()
+        MET_MODEL_THROTTLED.labels(model=model_id).set(1)
+        return False
+    ok, remaining = _eval_quota(bucket_key, requested_tokens, capacity, refill_per_sec)
+    if ok:
+        MET_TOKENS_CONSUMED.labels(model=model_id, tenant=tenant_id).inc(requested_tokens)
+    else:
+        MET_TOKENS_REJECTED.labels(model=model_id, tenant=tenant_id).inc()
+    MET_MODEL_THROTTLED.labels(model=model_id).set(0)
+    return ok
+
+def quota_dependency(required_tokens: int = 1):
+    from fastapi import Request, HTTPException
+    async def dep(request: Request):
+        model = request.headers.get("X-Model-Id") or request.query_params.get("model")
+        tenant = request.headers.get("X-Tenant-Id") or request.query_params.get("tenant") or "default"
+        if not model:
+            raise HTTPException(status_code=400, detail="model id required")
+        allowed = check_and_consume(model, tenant, required_tokens)
+        if not allowed:
+            raise HTTPException(status_code=429, detail="quota exceeded")
+    return dep
+
*** End Patch
*** Begin Patch
*** Add File: prometheus/alerts-billing.rules.yaml
+groups:
+- name: aegis-billing-alerts
+  rules:
+  - alert: BillingSpikeDetected
+    expr: increase(aegis_gateway_tokens_consumed_total[5m]) > 100000
+    for: 5m
+    labels:
+      severity: page
+    annotations:
+      summary: "Large billing spike detected"
+      description: "Total tokens consumed across models increased rapidly in the last 5 minutes."
+
+  - alert: ModelHighTokenRate
+    expr: increase(aegis_gateway_tokens_consumed_total{model=~".+"}[5m]) / 300 > 1000
+    for: 5m
+    labels:
+      severity: warning
+    annotations:
+      summary: "High token rate for one or more models"
+      description: "Investigate model token consumption and consider throttling."
+
+  - alert: ModelQuotaExhausted
+    expr: increase(aegis_gateway_tokens_rejected_total{model=~".+"}[5m]) > 10
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Quota rejections observed"
+      description: "Clients are being rejected due to quota exhaustion â€” check quotas and potential runaway models."
+
*** End Patch
*** Begin Patch
*** Add File: billing/alert_reconcile_worker.py
+#!/usr/bin/env python3
+import os, time, json, requests
+import boto3
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.monitoring.svc:9090")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+def prom_query(expr):
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr}, timeout=10)
+    r.raise_for_status()
+    return r.json().get("data", {}).get("result", [])
+
+def fetch_recent_token_usage(minutes=60):
+    expr = f"sum by (model) (increase(aegis_gateway_tokens_consumed_total[{minutes}m]))"
+    res = prom_query(expr)
+    usage = {}
+    for item in res:
+        model = item["metric"].get("model","unknown")
+        val = float(item["value"][1])
+        usage[model] = val
+    return usage
+
+def fetch_invoices_from_s3(prefix="billing/invoices/"):
+    if not COMPLIANCE_BUCKET:
+        return {}
+    s3 = boto3.client("s3")
+    objs = s3.list_objects_v2(Bucket=COMPLIANCE_BUCKET, Prefix=prefix)
+    inv = {}
+    for o in objs.get("Contents", []):
+        key = o["Key"]
+        tmp = f"/tmp/{key.replace('/','_')}"
+        s3.download_file(COMPLIANCE_BUCKET, key, tmp)
+        batch = json.load(open(tmp))
+        for rec in batch:
+            model = rec.get("model","unknown")
+            inv.setdefault(model, 0.0)
+            inv[model] += rec.get("tokens", 0)
+    return inv
+
+def detect_anomalies():
+    usage = fetch_recent_token_usage(60)
+    invoices = fetch_invoices_from_s3()
+    anomalies = []
+    for model, tokens in usage.items():
+        inv_tokens = invoices.get(model, 0)
+        if tokens > max(1000, inv_tokens * 2):
+            anomalies.append({"model": model, "usage_tokens": tokens, "invoice_tokens": inv_tokens})
+    if anomalies and OPERATOR_WEBHOOK:
+        try:
+            requests.post(OPERATOR_WEBHOOK, json={"event":"billing_anomalies","items": anomalies}, timeout=5)
+        except Exception:
+            pass
+    return anomalies
+
+if __name__ == "__main__":
+    print("Running billing reconcile worker...")
+    a = detect_anomalies()
+    print("Anomalies:", json.dumps(a, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: services/auto_throttle_manager.py
+#!/usr/bin/env python3
+import os, time, json, redis, requests, boto3
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.monitoring.svc:9090")
+MODEL_REGISTRY_API = os.environ.get("MODEL_REGISTRY_API")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+THRESH_TOKENS_PER_SEC = float(os.environ.get("THRESH_TOKENS_PER_SEC", "2000"))
+COOLDOWN_SEC = int(os.environ.get("THROTTLE_COOLDOWN_SEC", "300"))
+POLL_SEC = int(os.environ.get("THROTTLE_POLL_SEC", "30"))
+
+redis_client = redis.from_url(REDIS_URL)
+s3 = boto3.client("s3") if COMPLIANCE_BUCKET else None
+
+def prom_query(expr):
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr}, timeout=10)
+    r.raise_for_status()
+    return r.json().get("data", {}).get("result", [])
+
+def get_model_token_rates(window="1m"):
+    expr = f"sum by (model) (rate(aegis_gateway_tokens_consumed_total[{window}]))"
+    res = prom_query(expr)
+    out = {}
+    for item in res:
+        model = item["metric"].get("model","unknown")
+        rate = float(item["value"][1])
+        out[model] = rate
+    return out
+
+def set_throttle(model, reason):
+    key = f"model:throttle:{model}"
+    redis_client.set(key, "1", ex=3600)
+    if MODEL_REGISTRY_API:
+        try:
+            requests.post(f"{MODEL_REGISTRY_API}/models/throttle", json={"model": model, "throttled": True, "reason": reason}, timeout=5)
+        except Exception:
+            pass
+    rec = {"model": model, "action": "throttle", "reason": reason, "ts": int(time.time())}
+    if s3:
+        key = f"throttle/actions/{model}_{int(time.time())}.json"
+        try:
+            s3.put_object(Bucket=COMPLIANCE_BUCKET, Key=key, Body=json.dumps(rec))
+        except Exception:
+            pass
+    if OPERATOR_WEBHOOK:
+        try:
+            requests.post(OPERATOR_WEBHOOK, json=rec, timeout=5)
+        except Exception:
+            pass
+
+def clear_throttle(model):
+    key = f"model:throttle:{model}"
+    redis_client.delete(key)
+    if MODEL_REGISTRY_API:
+        try:
+            requests.post(f"{MODEL_REGISTRY_API}/models/throttle", json={"model": model, "throttled": False}, timeout=5)
+        except Exception:
+            pass
+    rec = {"model": model, "action": "unthrottle", "ts": int(time.time())}
+    if s3:
+        key = f"throttle/actions/{model}_{int(time.time())}.json"
+        try:
+            s3.put_object(Bucket=COMPLIANCE_BUCKET, Key=key, Body=json.dumps(rec))
+        except Exception:
+            pass
+    if OPERATOR_WEBHOOK:
+        try:
+            requests.post(OPERATOR_WEBHOOK, json=rec, timeout=5)
+        except Exception:
+            pass
+
+def monitor_loop():
+    print("Starting auto-throttle manager...")
+    while True:
+        try:
+            rates = get_model_token_rates(window="1m")
+            for model, rate in rates.items():
+                if rate >= THRESH_TOKENS_PER_SEC:
+                    set_throttle(model, f"token_rate {rate} >= {THRESH_TOKENS_PER_SEC}")
+                else:
+                    key = f"model:throttle:{model}"
+                    if redis_client.get(key):
+                        clear_throttle(model)
+        except Exception as e:
+            print("Error in throttle manager:", e)
+        time.sleep(POLL_SEC)
+
+if __name__ == "__main__":
+    monitor_loop()
+
*** End Patch
*** Begin Patch
*** Add File: tests/simulate_gateway_requests.py
+#!/usr/bin/env python3
+import argparse, requests, concurrent.futures, time
+
+def make_request(url, model, tenant):
+    try:
+        headers = {"X-Model-Id": model, "X-Tenant-Id": tenant}
+        r = requests.post(url, headers=headers, json={"prompt":"hello"}, timeout=10)
+        return r.status_code, r.text[:200]
+    except Exception as e:
+        return 0, str(e)
+
+def run(url, model, tenant, total, concurrency):
+    ok=0; fail=0
+    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as ex:
+        futures = [ex.submit(make_request, url, model, tenant) for _ in range(total)]
+        for f in concurrent.futures.as_completed(futures):
+            code, body = f.result()
+            if code == 200:
+                ok += 1
+            else:
+                fail += 1
+    return ok, fail
+
+if __name__=="__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--url", required=True)
+    p.add_argument("--model", default="test-model")
+    p.add_argument("--tenant", default="tenantA")
+    p.add_argument("--concurrency", type=int, default=50)
+    p.add_argument("--requests", type=int, default=500)
+    args = p.parse_args()
+    start = time.time()
+    ok, fail = run(args.url, args.model, args.tenant, args.requests, args.concurrency)
+    print("Done in", time.time()-start, "ok=", ok, "fail=", fail)
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/billing_mitigation_simulation.yml
+name: Billing Mitigation Simulation
+on:
+  workflow_dispatch:
+    inputs:
+      gateway_url:
+        required: true
+      model:
+        required: false
+        default: "test-model"
+      tenant:
+        required: false
+        default: "tenantA"
+
+jobs:
+  simulate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests
+      - name: Run request storm against gateway
+        run: |
+          python tests/simulate_gateway_requests.py --url "${{ github.event.inputs.gateway_url }}" --model "${{ github.event.inputs.model }}" --tenant "${{ github.event.inputs.tenant }}" --concurrency 100 --requests 5000
+      - name: Sleep to allow throttle manager to act
+        run: sleep 120
+      - name: Query throttle state in Redis (optional)
+        env:
+          REDIS_URL: ${{ secrets.REDIS_URL }}
+        run: |
+          python - <<PY
+import redis,os
+r=redis.from_url(os.environ["REDIS_URL"])
+print("Throttled keys:", r.keys("model:throttle:*"))
+PY
+
*** End Patch
*** Begin Patch
*** Add File: docs/billing_mitigation_test_plan.md
+# Billing & Throttle Mitigation Test Plan
+
+Objectives
+- Validate gateway quota enforcement under high load.
+- Verify Prometheus metrics and billing_reconcile alerts trigger.
+- Verify auto_throttle_manager marks models throttled and gateway rejects requests when throttled.
+
+Steps
+1. Baseline: ensure gateway healthy and quotas exist for "test-model".
+2. Run request storm (use billing mitigation simulation workflow).
+3. Observe billing_reconcile logs; ensure no anomalies initially.
+4. Force high sustained token rate to trigger auto_throttle_manager.
+5. Confirm gateway rejects requests (HTTP 429).
+6. Stop storm, wait COOLDOWN_SEC and ensure throttle clears.
+
*** End Patch
*** Begin Patch
*** Add File: participant/participant_sdk.py
+#!/usr/bin/env python3
+import os, sqlite3, json, time, requests
+from functools import wraps
+
+DB = os.environ.get("PART_DB", "/data/participant_state.sqlite")
+METRICS_PUSHGATEWAY = os.environ.get("PUSHGATEWAY", None)
+
+def ensure_db(path=DB):
+    os.makedirs(os.path.dirname(path), exist_ok=True)
+    c = sqlite3.connect(path, check_same_thread=False)
+    cur = c.cursor()
+    cur.execute("""CREATE TABLE IF NOT EXISTS participant_state (
+        tx_id TEXT PRIMARY KEY,
+        state TEXT,
+        payload TEXT,
+        updated_at INTEGER
+    )""")
+    c.commit()
+    return c
+
+class Participant:
+    def __init__(self, name, coordinator_url=None):
+        self.name = name
+        self.coordinator = coordinator_url or os.environ.get("TX_COORDINATOR", "http://transaction-manager.aegis.svc:8301")
+        self.db = ensure_db()
+
+    def _write_state(self, tx_id, state, payload=None):
+        now = int(time.time())
+        cur = self.db.cursor()
+        cur.execute("INSERT OR REPLACE INTO participant_state (tx_id,state,payload,updated_at) VALUES (?,?,?,?)",
+                    (tx_id, state, json.dumps(payload or {}), now))
+        self.db.commit()
+
+    def _read_state(self, tx_id):
+        cur = self.db.cursor()
+        cur.execute("SELECT state, payload, updated_at FROM participant_state WHERE tx_id=?", (tx_id,))
+        r = cur.fetchone()
+        if not r:
+            return None
+        return {"state": r[0], "payload": json.loads(r[1] or "{}"), "updated_at": r[2]}
+
+    def call_coordinator(self, endpoint, tx_id, timeout=10, max_retries=3):
+        url = f"{self.coordinator}/tx/{tx_id}/{endpoint}"
+        delay = 1
+        for attempt in range(1, max_retries + 1):
+            try:
+                r = requests.post(url, json={"participant": self.name}, timeout=timeout)
+                if 200 <= r.status_code < 300:
+                    return True, r.text
+                if 500 <= r.status_code < 600:
+                    time.sleep(delay); delay *= 2; continue
+                return False, r.text
+            except requests.RequestException:
+                time.sleep(delay); delay *= 2
+        return False, "max_retries_exceeded"
+
+    def push_metric(self, name, value, labels=None):
+        if not METRICS_PUSHGATEWAY:
+            return
+        try:
+            payload = f"# TYPE {name} gauge\n{name}{('{' + ','.join([f'{k}=\"{v}\"' for k,v in (labels or {}).items()]) + '}') if labels else ''} {value}\n"
+            requests.post(f"{METRICS_PUSHGATEWAY}/metrics/job/participant/name/{self.name}", data=payload, timeout=5)
+        except Exception:
+            pass
+
*** End Patch
*** Begin Patch
*** Add File: participants/example_tool_adapter.py
+#!/usr/bin/env python3
+from fastapi import FastAPI
+from pydantic import BaseModel
+import os
+from participant.participant_sdk import Participant
+
+app = FastAPI(title="Example Tool Adapter")
+PART = Participant(name=os.environ.get("PART_NAME","tool-adapter-1"))
+
+class TxPayload(BaseModel):
+    action: str = None
+    data: dict = {}
+    tx_id: str = None
+
+@app.post("/prepare")
+async def prepare(p: TxPayload):
+    tx_id = p.tx_id or p.data.get("tx_id")
+    st = PART._read_state(tx_id)
+    if st and st["state"] == "prepared":
+        return {"ok": True, "status": "already_prepared"}
+    try:
+        PART._write_state(tx_id, "prepared", {"reserved": True, "data": p.data})
+        PART.push_metric("participant_prepares_total", 1, labels={"participant": PART.name})
+        return {"ok": True, "status": "prepared"}
+    except Exception as e:
+        PART._write_state(tx_id, "failed_prepare", {"error": str(e)})
+        PART.push_metric("participant_prepare_failures_total", 1, labels={"participant": PART.name})
+        return {"ok": False, "error": str(e)}
+
+@app.post("/commit")
+async def commit(p: TxPayload):
+    tx_id = p.tx_id or p.data.get("tx_id")
+    st = PART._read_state(tx_id)
+    if st and st["state"] == "committed":
+        return {"ok": True, "status": "already_committed"}
+    try:
+        PART._write_state(tx_id, "committed", {"committed": True})
+        PART.push_metric("participant_commits_total", 1, labels={"participant": PART.name})
+        return {"ok": True, "status": "committed"}
+    except Exception as e:
+        PART._write_state(tx_id, "failed_commit", {"error": str(e)})
+        PART.push_metric("participant_commit_failures_total", 1, labels={"participant": PART.name})
+        return {"ok": False, "error": str(e)}
+
+@app.post("/abort")
+async def abort(p: TxPayload):
+    tx_id = p.tx_id or p.data.get("tx_id")
+    PART._write_state(tx_id, "aborted", {"aborted": True})
+    PART.push_metric("participant_aborts_total", 1, labels={"participant": PART.name})
+    return {"ok": True, "status": "aborted"}
+
+@app.get("/health")
+def health():
+    return {"ok": True}
+
*** End Patch
*** Begin Patch
*** Add File: tests/participant_hardening_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+PART_PORT=8102
+PART_URL="http://127.0.0.1:${PART_PORT}"
+
+echo "Starting participant..."
+uvicorn participants.example_tool_adapter:app --host 127.0.0.1 --port ${PART_PORT} >/tmp/participant.log 2>&1 &
+PPID=$!
+sleep 1
+echo "Participant PID: $PPID"
+
+TXMGR="${TXMGR:-http://localhost:8301}"
+
+echo "Starting transaction (success path)..."
+TX_ID=$(curl -s -X POST "${TXMGR}/tx/start" | jq -r '.tx_id')
+echo "TX_ID=${TX_ID}"
+curl -s -X POST "${TXMGR}/tx/${TX_ID}/participant" -H "Content-Type: application/json" -d "{\"name\":\"example\",\"prepare_url\":\"${PART_URL}/prepare\",\"commit_url\":\"${PART_URL}/commit\",\"abort_url\":\"${PART_URL}/abort\"}" | jq .
+curl -s -X POST "${TXMGR}/tx/${TX_ID}/prepare" | jq .
+curl -s -X POST "${TXMGR}/tx/${TX_ID}/commit" | jq .
+curl -s "${TXMGR}/tx/${TX_ID}" | jq .
+
+echo "Starting transaction (prepare failure -> abort)..."
+kill ${PPID} || true
+FAIL_PREPARE=1 uvicorn participants.example_tool_adapter:app --host 127.0.0.1 --port ${PART_PORT} >/tmp/participant_fail.log 2>&1 &
+PPID=$!
+sleep 1
+TX_ID2=$(curl -s -X POST "${TXMGR}/tx/start" | jq -r '.tx_id')
+curl -s -X POST "${TXMGR}/tx/${TX_ID2}/participant" -H "Content-Type: application/json" -d "{\"name\":\"example\",\"prepare_url\":\"${PART_URL}/prepare\",\"commit_url\":\"${PART_URL}/commit\",\"abort_url\":\"${PART_URL}/abort\"}" | jq .
+set +e
+curl -s -X POST "${TXMGR}/tx/${TX_ID2}/prepare" | jq .
+set -e
+curl -s "${TXMGR}/tx/${TX_ID2}" | jq .
+
+echo "Cleaning up participant..."
+kill ${PPID} || true
+echo "Participant hardening tests complete."
+
*** End Patch
*** Begin Patch
*** Add File: chaos/chaos_killer.py
+#!/usr/bin/env python3
+import argparse, random, subprocess, time
+
+def kubectl(cmd):
+    p = subprocess.run(["kubectl"] + cmd, capture_output=True, text=True)
+    if p.returncode != 0:
+        raise RuntimeError(p.stderr)
+    return p.stdout.strip()
+
+def list_pods(namespace, selector=None):
+    cmd = ["get", "pods", "-n", namespace, "-o", "jsonpath={.items[*].metadata.name}"]
+    if selector:
+        cmd = ["get", "pods", "-n", namespace, "-l", selector, "-o", "jsonpath={.items[*].metadata.name}"]
+    out = kubectl(cmd)
+    return out.split() if out else []
+
+def delete_pod(name, namespace):
+    print("Deleting pod", name)
+    try:
+        kubectl(["delete","pod", name, "-n", namespace, "--grace-period=0", "--force"])
+    except Exception as e:
+        print("delete failed:", e)
+
+def run(namespace, selector, count, interval, duration):
+    end = time.time() + duration
+    while time.time() < end:
+        pods = list_pods(namespace, selector)
+        if not pods:
+            print("No pods found")
+            time.sleep(interval); continue
+        targets = random.sample(pods, min(count, len(pods)))
+        for t in targets:
+            delete_pod(t, namespace)
+        time.sleep(interval)
+
+if __name__=="__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--namespace", required=True)
+    p.add_argument("--label", required=False)
+    p.add_argument("--count", type=int, default=1)
+    p.add_argument("--interval", type=int, default=10)
+    p.add_argument("--duration", type=int, default=60)
+    args = p.parse_args()
+    run(args.namespace, args.label, args.count, args.interval, args.duration)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/chaos/chaos-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: chaos-killer
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: chaos
+          image: bitnami/kubectl:latest
+          command: ["/bin/sh","-c"]
+          args:
+            - /chaos/chaos_killer.sh
+          env:
+            - name: NAMESPACE
+              value: "aegis"
+            - name: LABEL
+              value: ""
+            - name: DURATION
+              value: "120"
+      restartPolicy: Never
+      volumes:
+        - name: chaos-scripts
+          configMap:
+            name: chaos-scripts
+  backoffLimit: 0
+
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: chaos-scripts
+  namespace: aegis
+data:
+  chaos_killer.sh: |
+    #!/bin/sh
+    set -e
+    python /opt/chaos/chaos_killer.py --namespace ${NAMESPACE} --label "${LABEL}" --count 1 --interval 5 --duration ${DURATION}
+  chaos_killer.py: |
+    echo "Please mount chaos_killer.py or build an image with it present."
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/agent_chaos_runbook.md
+# Agentic Chaos Runbook
+
+Purpose:
+- Validate 2PC robustness, sandbox isolation, agent recovery, and SLO adherence using controlled chaos experiments.
+
+Steps:
+1. Baseline: run tests/participant_hardening_test.sh to establish baseline metrics.
+2. Start chaos job: kubectl apply -f k8s/chaos/chaos-job.yaml
+3. Observe Prometheus alerts (alerts-agent-slo.rules.yaml) and transaction-manager logs.
+4. Verify that transactions either commit or abort cleanly and no stuck transactions remain.
+5. If stuck transactions observed, abort via transaction-manager API and collect participant logs.
+
+Automation:
+- Schedule chaos experiments weekly in staging; record results and tune timeouts/retries in participant SDK.
+
*** End Patch
*** Begin Patch
*** Add File: prometheus/alerts-agent-slo.rules.yaml
+groups:
+- name: aegis-agent-slo
+  rules:
+  - alert: AgentPlanLatencySLOViolation
+    expr: histogram_quantile(0.95, sum(rate(agent_plan_duration_seconds_bucket[5m])) by (le)) > 600
+    for: 15m
+    labels:
+      severity: page
+    annotations:
+      summary: "Agent plan latency SLO breach (95th > 10m)"
+      description: "Agent plans are taking too long to complete. Investigate orchestrator backlog or participant failures."
+
+  - alert: AgentPlanFailureSpike
+    expr: increase(agent_plan_failures_total[10m]) > 5
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Agent plan failure spike"
+      description: "More than 5 agent plan failures in 10m. Check participants and transaction manager."
+
*** End Patch
*** End Patch
