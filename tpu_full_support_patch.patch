*** Begin Patch
*** Add File: argo/workflows/tpu_provision_and_train.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: tpu-provision-train
+  namespace: staging
+spec:
+  entrypoint: tpu-train
+  templates:
+  - name: tpu-train
+    steps:
+    - - name: create-tpu
+        template: gcloud-step
+        arguments:
+          parameters:
+            - name: cmd
+              value: |
+                gcloud compute tpus tpu-vm create {{workflow.parameters.tpuName}} \
+                  --zone={{workflow.parameters.zone}} \
+                  --accelerator-type={{workflow.parameters.acceleratorType}} \
+                  --version={{workflow.parameters.tpuVersion}} --quiet
+    - - name: scp-and-run
+        template: gcloud-step
+        arguments:
+          parameters:
+            - name: cmd
+              value: |
+                gcloud compute tpus tpu-vm scp training/tpu_training.py {{workflow.parameters.tpuName}}:~/ --zone={{workflow.parameters.zone}}
+                gcloud compute tpus tpu-vm ssh {{workflow.parameters.tpuName}} --zone={{workflow.parameters.zone}} --command "python3 ~/tpu_training.py"
+    - - name: delete-tpu
+        template: gcloud-step
+        arguments:
+          parameters:
+            - name: cmd
+              value: |
+                gcloud compute tpus tpu-vm delete {{workflow.parameters.tpuName}} --zone={{workflow.parameters.zone}} --quiet
+  - name: gcloud-step
+    inputs:
+      parameters:
+      - name: cmd
+    container:
+      image: google/cloud-sdk:slim
+      command: [sh, -c]
+      args:
+        - |
+          set -xe
+          echo "Running: {{inputs.parameters.cmd}}"
+          gcloud auth activate-service-account --key-file=/secrets/gcp-key.json
+          gcloud config set project $GCP_PROJECT
+          {{inputs.parameters.cmd}}
+    volumeMounts:
+      - name: gcp-key
+        mountPath: /secrets
+        readOnly: true
+  volumes:
+    - name: gcp-key
+      secret:
+        secretName: gcp-service-account
+  arguments:
+    parameters:
+      - name: tpuName
+        value: tpuvm-train
+      - name: zone
+        value: us-central1-b
+      - name: acceleratorType
+        value: v4-8
+      - name: tpuVersion
+        value: v4
+
+# Notes:
+# - Requires a Kubernetes secret named `gcp-service-account` with the service account JSON at key 'gcp-key.json'.
+# - The training script `training/tpu_training.py` is copied from this repo to the TPU VM and executed. It should save checkpoints locally and push to your agreed storage (S3/GCS).
+
*** End Patch
*** Begin Patch
*** Add File: training/tpu_training.py
+"""
+Minimal TPU-capable training script (example).
+- Uses JAX if available; creates a tiny "checkpoint" and uploads to S3 if AWS creds are present.
+"""
+import os
+import time
+import json
+
+def dummy_train_and_checkpoint():
+    # Simulate training and write a minimal checkpoint file
+    ckpt = {"trained_steps": 10, "timestamp": int(time.time())}
+    path = "/tmp/tpu_ckpt.json"
+    with open(path, "w") as f:
+        json.dump(ckpt, f)
+    return path
+
+def upload_to_s3(local_path, s3_uri):
+    try:
+        import boto3
+        from urllib.parse import urlparse
+        p = urlparse(s3_uri)
+        bucket = p.netloc
+        key = p.path.lstrip("/")
+        s3 = boto3.client("s3")
+        s3.upload_file(local_path, bucket, key)
+        print(f"Uploaded {local_path} -> s3://{bucket}/{key}")
+    except Exception as e:
+        print("S3 upload failed (ensure AWS creds configured):", e)
+
+def main():
+    out = dummy_train_and_checkpoint()
+    print("Wrote checkpoint:", out)
+    s3_dest = os.environ.get("TRAINING_CHECKPOINT_S3")
+    if s3_dest:
+        upload_to_s3(out, s3_dest)
+    else:
+        print("TRAINING_CHECKPOINT_S3 not set; skipping upload")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: k8s/device-plugin/edge-tpu-hostpath-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: edge-tpu-hostpath-smoke
+  namespace: staging
+spec:
+  template:
+    spec:
+      restartPolicy: Never
+      nodeSelector:
+        edge_tpu: "true"   # label nodes that have Edge TPU attached
+      containers:
+      - name: edge-tpu-test
+        image: ghcr.io/yourorg/edge-tpu-test:latest
+        volumeMounts:
+          - name: usb-dev
+            mountPath: /dev/bus/usb
+            readOnly: true
+        resources:
+          limits:
+            cpu: "500m"
+            memory: "256Mi"
+      volumes:
+        - name: usb-dev
+          hostPath:
+            path: /dev/bus/usb
+            type: Directory
+
+# To run this in-cluster, ensure at least one node is labeled:
+# kubectl label node <node-name> edge_tpu=true
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/edge_tpu_incluster_smoke.yml
+name: Edge TPU In-Cluster Smoke Test
+on:
+  workflow_dispatch:
+    inputs:
+      job_manifest:
+        description: "Manifest path for job"
+        default: k8s/device-plugin/edge-tpu-hostpath-job.yaml
+
+jobs:
+  smoke:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.27.0'
+
+      - name: Configure KUBECONFIG
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+        shell: bash
+
+      - name: Apply Job manifest
+        run: |
+          kubectl --kubeconfig=/tmp/kubeconfig apply -f ${{ github.event.inputs.job_manifest }}
+          JOB_NAME=$(yq e '.metadata.name' ${{ github.event.inputs.job_manifest }})
+          echo "Waiting for job $JOB_NAME to complete..."
+          kubectl --kubeconfig=/tmp/kubeconfig wait --for=condition=complete job/$JOB_NAME --timeout=10m
+
+      - name: Fetch pod logs (if failed)
+        if: failure()
+        run: |
+          POD=$(kubectl --kubeconfig=/tmp/kubeconfig get pods -l job-name=$(yq e '.metadata.name' ${{ github.event.inputs.job_manifest }}) -o jsonpath='{.items[0].metadata.name}')
+          kubectl --kubeconfig=/tmp/kubeconfig logs $POD --tail=200 || true
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/build_push_tpu_images.yml
+name: Build & Push TPU Images
+on:
+  workflow_dispatch:
+    inputs:
+      tag:
+        required: false
+        default: ${{ github.sha }}
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up QEMU and Docker Buildx
+        uses: docker/setup-buildx-action@v2
+
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.REGISTRY_HOST }}
+          username: ${{ secrets.REGISTRY_USER }}
+          password: ${{ secrets.REGISTRY_TOKEN }}
+
+      - name: Build and push cloud TPU image
+        id: cloud_build
+        uses: docker/build-push-action@v4
+        with:
+          context: ./cloud_tpu
+          file: ./cloud_tpu/Dockerfile.tpu
+          push: true
+          tags: ${{ secrets.REGISTRY_HOST }}/aegis-cloud-tpu:${{ github.event.inputs.tag }}
+
+      - name: Get cloud image digest
+        run: |
+          IMAGE=${{ secrets.REGISTRY_HOST }}/aegis-cloud-tpu:${{ github.event.inputs.tag }}
+          docker pull $IMAGE
+          DIGEST=$(docker image inspect --format='{{index .RepoDigests 0}}' $IMAGE | awk -F@ '{print $2}')
+          echo "cloud_digest=$DIGEST" >> $GITHUB_OUTPUT
+
+      - name: Build and push edge TPU image
+        id: edge_build
+        uses: docker/build-push-action@v4
+        with:
+          context: ./edge_tpu/docker
+          file: ./edge_tpu/docker/edge_tpu_test.Dockerfile
+          push: true
+          tags: ${{ secrets.REGISTRY_HOST }}/aegis-edge-tpu:${{ github.event.inputs.tag }}
+
+      - name: Get edge image digest
+        run: |
+          IMAGE=${{ secrets.REGISTRY_HOST }}/aegis-edge-tpu:${{ github.event.inputs.tag }}
+          docker pull $IMAGE
+          DIGEST=$(docker image inspect --format='{{index .RepoDigests 0}}' $IMAGE | awk -F@ '{print $2}')
+          echo "edge_digest=$DIGEST" >> $GITHUB_OUTPUT
+
+      - name: Summary
+        run: |
+          echo "Cloud image: ${{ secrets.REGISTRY_HOST }}/aegis-cloud-tpu:${{ github.event.inputs.tag }}@${{ steps.cloud_build.outputs.cloud_digest || github.outputs.cloud_digest || '' }}"
+          echo "Edge image: ${{ secrets.REGISTRY_HOST }}/aegis-edge-tpu:${{ github.event.inputs.tag }}@${{ steps.edge_build.outputs.edge_digest || github.outputs.edge_digest || '' }}"
+
+# Required secrets:
+# - REGISTRY_HOST (e.g., ghcr.io/yourorg)
+# - REGISTRY_USER
+# - REGISTRY_TOKEN
+
*** End Patch
*** Begin Patch
*** Add File: docs/tpu_full_support_next_steps.md
+# TPU Full Support â€” next steps to finalize production readiness
+
+What this patch adds
+- Argo workflow to provision TPU VMs, run training, and tear down (argo/workflows/tpu_provision_and_train.yaml)
+- Example training script that writes a checkpoint and uploads to S3 if configured (training/tpu_training.py)
+- In-cluster Edge TPU hostPath Job example (k8s/device-plugin/edge-tpu-hostpath-job.yaml)
+- GitHub workflow to run the in-cluster Edge TPU smoke test (edge_tpu_incluster_smoke.yml)
+- GH Action to build and push TPU images (build_push_tpu_images.yml)
+- Docs and runbook to guide operators
+
+To finalize production-grade TPU support:
+1. Configure secrets and infra:
+   - GCP: create k8s secret `gcp-service-account` with the service account JSON (key filename: gcp-key.json) and ensure the Argo cluster can mount it.
+   - AWS S3: ensure AWS creds available on TPU VM (or use GCS for TPU-native checkpointing).
+   - Container registry: set REGISTRY_HOST, REGISTRY_USER, REGISTRY_TOKEN secrets.
+   - KUBECONFIG secret for in-cluster jobs (for GitHub Actions that run kubectl).
+
+2. Build and push validated TPU images using `Build & Push TPU Images` workflow.
+3. Label a node with `edge_tpu=true` and run the `Edge TPU In-Cluster Smoke Test` workflow to validate Edge containers.
+4. Run the Argo TPU workflow (submit in Argo UI) in staging; ensure the secret `gcp-service-account` exists and TRAINING_CHECKPOINT_S3 env is set on the TPU VM or the training script uploads checkpoints.
+5. Harden:
+   - Replace dummy training with your distributed TPU job and checkpointing logic.
+   - Add monitoring for TPU jobs and cost-control automation (auto-teardown if stuck).
+   - Add adversarial & checkpoint-restore tests to the training step before promotion.
+
*** End Patch
