*** Begin Patch
*** Add File: provider/broker_state_postgres.py
+#!/usr/bin/env python3
+"""
+Provider Broker persistent store backed by Postgres and S3 for model artifacts.
+ - Uses SQLAlchemy for broker state and telemetry
+ - Stores trained model metadata and pointer to model artifact in object store (S3/MinIO)
+
+Environment:
+ - DATABASE_URL (SQLAlchemy URL, e.g. postgres://user:pass@host:5432/aegis)
+ - MODEL_BUCKET (s3 bucket name or MinIO bucket)
+ - AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY (or use IAM/OIDC in production)
+
+Usage patterns:
+ - create tables: python provider/broker_state_postgres.py --init
+ - save telemetry: import and call persist_telemetry(...)
+ - list models: list_models()
+"""
+import os
+import json
+from datetime import datetime
+from sqlalchemy import (
+    create_engine, MetaData, Table, Column, Integer, String, JSON, DateTime, Float, select, Text
+)
+from sqlalchemy.engine.url import make_url
+import boto3
+
+DATABASE_URL = os.environ.get("DATABASE_URL", "sqlite:///./broker_state.db")
+MODEL_BUCKET = os.environ.get("MODEL_BUCKET", "aegis-models")
+
+engine = create_engine(DATABASE_URL, future=True)
+meta = MetaData()
+
+broker_telemetry = Table(
+    "broker_telemetry", meta,
+    Column("id", Integer, primary_key=True),
+    Column("ts", DateTime, default=datetime.utcnow),
+    Column("job_id", String(128)),
+    Column("payload", JSON),
+)
+
+broker_models = Table(
+    "broker_models", meta,
+    Column("id", Integer, primary_key=True),
+    Column("created_at", DateTime, default=datetime.utcnow),
+    Column("model_name", String(256)),
+    Column("s3_key", String(512)),
+    Column("metadata", JSON),
+)
+
+def init_db():
+    meta.create_all(engine)
+    print("Initialized DB schema at", DATABASE_URL)
+
+def persist_telemetry(job_id, payload):
+    with engine.begin() as conn:
+        conn.execute(broker_telemetry.insert().values(job_id=job_id, payload=payload))
+
+def save_model_artifact(local_path, model_name, metadata=None):
+    # upload to S3/MinIO
+    s3 = boto3.client(
+        "s3",
+        aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
+        aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
+        endpoint_url=os.environ.get("S3_ENDPOINT")
+    )
+    key = f"models/{model_name}/{os.path.basename(local_path)}"
+    s3.upload_file(local_path, MODEL_BUCKET, key)
+    with engine.begin() as conn:
+        conn.execute(broker_models.insert().values(model_name=model_name, s3_key=key, metadata=(metadata or {})))
+    return key
+
+def list_models(limit=20):
+    with engine.connect() as conn:
+        res = conn.execute(select(broker_models).order_by(broker_models.c.created_at.desc()).limit(limit))
+        return [dict(r._mapping) for r in res]
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--init", action="store_true")
+    p.add_argument("--list", action="store_true")
+    args = p.parse_args()
+    if args.init:
+        init_db()
+    if args.list:
+        for m in list_models():
+            print(json.dumps(m, default=str, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: provider/db/migrate.py
+#!/usr/bin/env python3
+"""
+Simple DB migration helper: creates required tables for broker and ledger.
+(Production: use Alembic; this script is a convenience.)
+"""
+from provider.broker_state_postgres import init_db
+from billing.job_ledger_db import init_db as init_ledger_db
+
+if __name__ == "__main__":
+    init_db()
+    init_ledger_db()
+    print("Migration completed.")
+
*** End Patch
*** Begin Patch
*** Add File: billing/job_ledger_db.py
+#!/usr/bin/env python3
+"""
+Postgres-backed job ledger for production.
+ - Uses SQLAlchemy to persist ledger entries
+ - Designed to be called by scheduler/adapters at job end
+"""
+import os
+from sqlalchemy import (
+    create_engine, MetaData, Table, Column, Integer, String, Float, DateTime, JSON
+)
+from datetime import datetime
+
+DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@localhost:5432/aegis")
+engine = create_engine(DATABASE_URL, future=True)
+meta = MetaData()
+
+job_ledger = Table(
+    "job_ledger", meta,
+    Column("id", Integer, primary_key=True),
+    Column("job_id", String(128), nullable=False, index=True),
+    Column("tenant", String(128), index=True),
+    Column("provider", String(128), index=True),
+    Column("cost_usd", Float),
+    Column("runtime_secs", Float),
+    Column("device_power_w", Float),
+    Column("kwh", Float),
+    Column("kgco2e", Float),
+    Column("region", String(64)),
+    Column("created_at", DateTime, default=datetime.utcnow),
+    Column("extra", JSON)
+)
+
+def init_db():
+    meta.create_all(engine)
+    print("Initialized ledger DB at", DATABASE_URL)
+
+def write_entry(entry):
+    with engine.begin() as conn:
+        conn.execute(job_ledger.insert().values(
+            job_id=entry["job_id"],
+            tenant=entry.get("tenant"),
+            provider=entry.get("provider"),
+            cost_usd=entry.get("cost_usd"),
+            runtime_secs=entry.get("runtime_secs"),
+            device_power_w=entry.get("device_power_w"),
+            kwh=entry.get("kwh"),
+            kgco2e=entry.get("kgco2e"),
+            region=entry.get("region"),
+            extra=entry.get("extra", {})
+        ))
+
+if __name__ == "__main__":
+    init_db()
+
*** End Patch
*** Begin Patch
*** Add File: mlflow/production/docker-compose.yml
+version: '3.7'
+services:
+  postgres:
+    image: postgres:15
+    environment:
+      POSTGRES_USER: mlflow
+      POSTGRES_PASSWORD: mlflow
+      POSTGRES_DB: mlflow
+    volumes:
+      - mlflow_postgres:/var/lib/postgresql/data
+    healthcheck:
+      test: ["CMD-SHELL", "pg_isready -U mlflow"]
+      interval: 10s
+      retries: 5
+
+  minio:
+    image: minio/minio:RELEASE.2025-01-01T00-00-00Z
+    environment:
+      MINIO_ROOT_USER: minio
+      MINIO_ROOT_PASSWORD: minio123
+    command: server /data
+    ports:
+      - "9000:9000"
+    volumes:
+      - mlflow_minio:/data
+
+  mlflow:
+    image: python:3.10-slim
+    depends_on:
+      - postgres
+      - minio
+    environment:
+      BACKEND_STORE_URI: postgresql+psycopg2://mlflow:mlflow@postgres:5432/mlflow
+      ARTIFACT_ROOT: s3://mlflow-artifacts/
+      AWS_ACCESS_KEY_ID: minio
+      AWS_SECRET_ACCESS_KEY: minio123
+      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
+    volumes:
+      - ./mlflow_server:/opt/mlflow
+    command: >
+      sh -c "pip install mlflow boto3 sqlalchemy psycopg2-binary && 
+      mlflow server --backend-store-uri $BACKEND_STORE_URI --default-artifact-root $ARTIFACT_ROOT --host 0.0.0.0 --port 5000"
+    ports:
+      - "5000:5000"
+
+volumes:
+  mlflow_postgres:
+  mlflow_minio:
+
*** End Patch
*** Begin Patch
*** Add File: hsm/cosign_pkcs11_sign.sh
+#!/usr/bin/env bash
+#
+# Operator script: sign artifact with cosign using PKCS11 HSM module.
+#
+# Requires:
+# - cosign installed on HSM admin host
+# - environment variables:
+#    PKCS11_MODULE (path to pkcs11 lib)
+#    PKCS11_PIN
+#    PKCS11_KEY_LABEL (key label on HSM)
+# Usage:
+# ./cosign_pkcs11_sign.sh <artifact-path> <s3-or-local-upload-target>
+set -euo pipefail
+ART=$1
+TARGET=${2:-"$ART"}
+
+if [ -z "${PKCS11_MODULE:-}" ] || [ -z "${PKCS11_PIN:-}" ] || [ -z "${PKCS11_KEY_LABEL:-}" ]; then
+  echo "Set PKCS11_MODULE, PKCS11_PIN, PKCS11_KEY_LABEL in environment"
+  exit 2
+fi
+
+export COSIGN_PKCS11_MODULE="$PKCS11_MODULE"
+export COSIGN_PKCS11_PIN="$PKCS11_PIN"
+export COSIGN_PKCS11_KEY_LABEL="$PKCS11_KEY_LABEL"
+
+# Use cosign to sign artifact using HSM key reference
+cosign sign-blob --key "pkcs11:object=$COSIGN_PKCS11_KEY_LABEL" "$ART" || (echo "cosign sign failed" && exit 3)
+
+echo "Signed $ART with HSM key label $COSIGN_PKCS11_KEY_LABEL"
+
+# optional: upload signed artifact or bundle to target (S3/registry) - left as operator step
+echo "Operator should now upload $ART and cosign signature to target: $TARGET"
+
*** End Patch
*** Begin Patch
*** Add File: zero_trust/istio/namespace_mtls_strict.yaml
+apiVersion: security.istio.io/v1beta1
+kind: PeerAuthentication
+metadata:
+  name: mtls-strict-aegis
+  namespace: aegis
+spec:
+  mtls:
+    mode: STRICT
+
+---
+apiVersion: security.istio.io/v1beta1
+kind: PeerAuthentication
+metadata:
+  name: mtls-strict-istio-system
+  namespace: istio-system
+spec:
+  mtls:
+    mode: STRICT
+
*** End Patch
*** Begin Patch
*** Add File: zero_trust/attestation/attestation_worker.py
+#!/usr/bin/env python3
+"""
+Scaleable TPM enrollment worker (operator-run).
+ - Reads inventory (Ansible style or CSV)
+ - Runs remote tpm2_quote and uploads to attestation server concurrently
+ - Retries & records results to a local JSON report for audit
+"""
+import os
+import json
+import concurrent.futures
+from subprocess import run, PIPE
+from datetime import datetime
+import requests
+
+ATTESTATION_SERVER = os.environ.get("ATTESTATION_SERVER", "https://attest.example/attest")
+INVENTORY = os.environ.get("TPM_INVENTORY", "ansible/inventory_federated_example.ini")
+
+def read_inventory(path):
+    # For prototype: read static list from file (one hostname per line)
+    if not os.path.exists(path):
+        return []
+    out = []
+    with open(path) as fh:
+        for l in fh:
+            l = l.strip()
+            if l and not l.startswith("#"):
+                out.append(l)
+    return out
+
+def collect_quote(host):
+    # Operator must have SSH access (setup previously)
+    try:
+        cmd = f"ssh {host} 'tpm2_quote -c 0x81010001 -l sha256:0x0004 -m /tmp/tpm_quote.bin && cat /tmp/tpm_quote.bin | base64 -w0'"
+        r = run(cmd, shell=True, stdout=PIPE, stderr=PIPE, timeout=120)
+        if r.returncode == 0:
+            return {"host": host, "quote_b64": r.stdout.decode().strip(), "ok": True}
+        return {"host": host, "ok": False, "error": r.stderr.decode()}
+    except Exception as e:
+        return {"host": host, "ok": False, "error": str(e)}
+
+def upload_quote(q):
+    try:
+        payload = {"client_id": q["host"], "quote_b64": q.get("quote_b64")}
+        r = requests.post(ATTESTATION_SERVER, json=payload, timeout=30, verify=True)
+        return {"host": q["host"], "status": r.status_code, "ok": r.ok}
+    except Exception as e:
+        return {"host": q["host"], "ok": False, "error": str(e)}
+
+def main():
+    hosts = read_inventory(os.environ.get("TPM_INVENTORY", "ansible/inventory_hosts.txt"))
+    results = []
+    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:
+        futures = {ex.submit(collect_quote, h): h for h in hosts}
+        for fut in concurrent.futures.as_completed(futures):
+            res = fut.result()
+            if res.get("ok"):
+                up = upload_quote(res)
+                results.append(up)
+            else:
+                results.append(res)
+    out = {"ts": datetime.utcnow().isoformat(), "results": results}
+    with open("/tmp/attestation_batch_report.json", "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote report to /tmp/attestation_batch_report.json")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/broker_canary_pipeline.yml
+name: Learned Broker Shadow→Train→Canary Pipeline
+on:
+  workflow_dispatch:
+    inputs:
+      canary-manifest:
+        description: "Path to k8s canary manifest (must include canary label)"
+        required: true
+      prom-url:
+        description: "Prometheus URL for SLO checks"
+        required: true
+
+jobs:
+  shadow:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run shadow collection (operator must adapt scheduler to call this)
+        run: |
+          echo "Shadow collection is expected to be long running in production; this step triggers an offline snapshot"
+          # Example: curl broker shadow endpoint to collect recent policy decisions
+          curl -X POST -H "Content-Type: application/json" -d '{"mode":"shadow"}' http://provider-broker-learned.aegis.svc/select || true
+
+  train:
+    needs: shadow
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Train offline policy
+        run: |
+          python provider/learner/train_policy.py --state /workspace/provider_state.json --out /workspace/policy_model.pkl || true
+      - name: Upload model artifact for operator review
+        uses: actions/upload-artifact@v4
+        with:
+          name: trained-policy
+          path: /workspace/policy_model.pkl
+
+  canary-deploy:
+    needs: train
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Apply canary manifest
+        run: |
+          kubectl apply -f "${{ github.event.inputs.canary-manifest }}"
+      - name: Monitor SLO for canary (5m)
+        env:
+          PROM_URL: ${{ github.event.inputs.prom-url }}
+        run: |
+          python - <<'PY'
+import os, time, requests, sys
+PROM=os.environ.get("PROM_URL")
+query='histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{deployment=~"provider-broker-learned.*"}[5m])) by (le))'
+ok=True
+for i in range(5):
+    r=requests.get(f"{PROM}/api/v1/query", params={"query":query}, timeout=10)
+    if r.ok and r.json().get("data",{}).get("result"):
+        val=float(r.json()["data"]["result"][0]["value"][1])
+        print("p95:",val)
+        if val>300:
+            ok=False
+            break
+    time.sleep(60)
+if not ok:
+    print("Canary SLO breached; rollback")
+    import subprocess
+    subprocess.run(["kubectl","rollout","undo","deployment/provider-broker-learned"], check=False)
+    sys.exit(1)
+print("Canary OK")
+PY
+
*** End Patch
*** Begin Patch
*** Add File: federated/production/flower_orchestrator.py
+#!/usr/bin/env python3
+"""
+Federated learning production pilot using Flower.
+ - Flower server collects client models from devices (Jetsons) running Flower clients
+ - This is a scaffold: secure transport (TLS) and secure aggregation (TI/DP) must be configured in production
+
+Usage:
+ - Deploy server on a cloud-hosted instance reachable by clients
+ - Clients run federated/client_runner.py adapted to Flower
+"""
+import socket
+from flwr.server import start_server
+from flwr.server.strategy import FedAvg
+from datetime import datetime
+
+def main():
+    print("Starting Flower server (production pilot)")
+    strategy = FedAvg()
+    start_server(server_address="0.0.0.0:8080", config={"num_rounds": 10}, strategy=strategy)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: federated/production/client_flower_template.py
+#!/usr/bin/env python3
+"""
+Flower client template for Jetson/edge devices.
+ - For production, wrap in a service and ensure TLS + mTLS between server/clients.
+ - Add local secure storage for model updates and differential privacy noise if required.
+"""
+import flwr as fl
+import numpy as np
+
+class SimpleClient(fl.client.NumPyClient):
+    def get_parameters(self):
+        return []
+    def fit(self, parameters, config):
+        # load local data, train, and return updated params
+        return [], 1, {}
+    def evaluate(self, parameters):
+        return 0.1, 1, {}
+
+if __name__ == "__main__":
+    fl.client.start_numpy_client(server_address="localhost:8080", client=SimpleClient())
+
*** End Patch
*** Begin Patch
*** Add File: edge/power_exporter.py
+#!/usr/bin/env python3
+"""
+Prometheus exporter for device power telemetry (simple).
+ - Exposes gauge `aegis_device_power_w{device="..."}`
+ - For production, this should read from a measured telemetry source (IPMI, INA, DC power monitor)
+"""
+from prometheus_client import start_http_server, Gauge
+import random
+import time
+import os
+
+DEVICE = os.environ.get("DEVICE_ID", "device-1")
+PORT = int(os.environ.get("POWER_EXPORTER_PORT", "9101"))
+g = Gauge("aegis_device_power_w", "Device power in Watts", ["device"])
+
+def measure():
+    # placeholder: sample random value; replace with real sensor read
+    return 30.0 + random.random()*10.0
+
+if __name__ == "__main__":
+    start_http_server(PORT)
+    while True:
+        val = measure()
+        g.labels(device=DEVICE).set(val)
+        time.sleep(10)
+
*** End Patch
*** Begin Patch
*** Add File: carbon/ingest_and_sign_snapshot.sh
+#!/usr/bin/env bash
+#
+# Operator script to fetch ElectricityMap data, write JSON snapshot, upload to S3, and sign snapshot via cosign+HSM admin host.
+#
+# Requires:
+# - ELECTRICITYMAP_API_KEY env var set
+# - AWS CLI configured for S3 upload or use MinIO CLI
+# - cosign PKCS11 signing is performed on HSM admin host (script invokes remote ssh command as example)
+set -euo pipefail
+OUT=/tmp/aegis_carbon_snapshot.json
+REGIONS=${CARBON_REGIONS:-"US,EU"}
+API_KEY=${ELECTRICITYMAP_API_KEY:-}
+if [ -z "$API_KEY" ]; then
+  echo "ELECTRICITYMAP_API_KEY not set; aborting"
+  exit 2
+fi
+python - <<PY
+import os,json,requests
+regions=os.environ.get("CARBON_REGIONS","US,EU").split(",")
+api=os.environ.get("ELECTRICITYMAP_API_URL","https://api.electricitymap.org/v3/zone")
+key=os.environ["ELECTRICITYMAP_API_KEY"]
+out={"ts":__import__("datetime").datetime.utcnow().isoformat(),"regions":{}}
+for r in regions:
+    try:
+        h={"Accept":"application/json","auth-token":key}
+        resp=requests.get(f"{api}/{r}",headers=h,timeout=10)
+        resp.raise_for_status()
+        j=resp.json()
+        out["regions"][r]=j
+    except Exception as e:
+        out["regions"][r]={"error":str(e)}
+open("$OUT","w").write(json.dumps(out,indent=2))
+print("Wrote snapshot to $OUT")
+PY
+
+# Upload snapshot (operator should configure S3 bucket)
+if [ -n "${CARBON_S3_BUCKET:-}" ]; then
+  aws s3 cp "$OUT" "s3://${CARBON_S3_BUCKET}/carbon_snapshots/$(basename $OUT)" || true
+fi
+
+echo "You must sign the snapshot on HSM admin host. Example (run on admin host):"
+echo "  cosign sign-blob --key 'pkcs11:object=KEY_LABEL' $OUT"
+echo "Or use the hsm/cosign_pkcs11_sign.sh script on admin host."
+
*** End Patch
*** Begin Patch
*** Add File: signoff/generate_audit_package.py
+#!/usr/bin/env python3
+"""
+Generate auditor-ready package with evidence.
+ - Collects MLflow run artifacts, Rekor entries, attestation reports, HSM rotation logs, and ledger summaries
+ - Produces tarball signoff/audit_package_<ts>.tar.gz
+"""
+import argparse, os, tarfile, json
+from datetime import datetime
+
+def collect(paths, tg):
+    for p in paths:
+        if os.path.exists(p):
+            tg.add(p, arcname=os.path.basename(p))
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--dest", default="signoff/audit_package.tar.gz")
+    args=p.parse_args()
+    os.makedirs(os.path.dirname(args.dest), exist_ok=True)
+    tg=tarfile.open(args.dest,"w:gz")
+    paths=["/tmp/attestation_batch_report.json","/tmp/cloudhsm_audit.json","/tmp/qpu_sla_summary.json","/tmp/reconcile_report.json"]
+    collect(paths,tg)
+    # include signoff markdown if present
+    if os.path.exists("signoff/aegis_signoff.md"):
+        tg.add("signoff/aegis_signoff.md", arcname="aegis_signoff.md")
+    tg.close()
+    print("Wrote audit package to", args.dest)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: docs/productionize_README.md
+# Productionization Plan & Artifacts (Aegis)
+
+This patch introduces key productionization artifacts:
+
+- Durable storage & scaling:
+  - provider/broker_state_postgres.py (Postgres + S3 model storage)
+  - provider/db/migrate.py (simple migration helper)
+  - billing/job_ledger_db.py (Postgres-backed job ledger)
+  - mlflow/production/docker-compose.yml (Postgres + MinIO + MLflow server)
+
+- Secure HSM signing workflow:
+  - hsm/cosign_pkcs11_sign.sh : operator script to sign artifacts using PKCS11 HSM key (run on HSM admin host)
+
+- Zero-trust enforcement:
+  - zero_trust/istio/namespace_mtls_strict.yaml : PeerAuthentication to enforce mTLS in namespace
+  - zero_trust/attestation/attestation_worker.py : scaleable TPM enrollment worker for operator to collect quotes and upload to attestation server
+
+- Learned broker safety & rollout:
+  - .github/workflows/broker_canary_pipeline.yml : GH Action that runs shadow, triggers offline training, deploys canary, monitors SLO and rolls back on violation
+  - provider/broker_state_postgres.py and provider/learner/train_policy.py (existing) integrate for durable training data and model storage
+
+- Federated production pilot:
+  - federated/production/flower_orchestrator.py and client template (Flower integration scaffold)
+
+- Accurate energy measurement & carbon provenance:
+  - edge/power_exporter.py : Prometheus exporter for device power telemetry
+  - carbon/ingest_and_sign_snapshot.sh : operator script to snapshot ElectricityMap and operator-signed snapshot instructions
+
+- Audit & compliance:
+  - signoff/generate_audit_package.py : collect evidence into tarball
+
+Operational notes:
+- Environment variables and secrets (DATABASE_URL, S3 credentials, ELECTRICITYMAP_API_KEY, PKCS11_* variables) must be configured in secure operator environment (Vault, KMS).
+- HSM cosign signing must run on an HSM-admin host with PKCS11 access; CI verifies public key + Rekor entries.
+- Broker & MLflow use Postgres + object storage for durability. Replace local file usage with DB-backed store and ensure backups.
+- mTLS enforcement via Istio PeerAuthentication requires Istio control plane and proper CA (cert-manager).
+
+Next recommended steps:
+1. Deploy Postgres & MinIO (or S3) and start MLflow production stack.
+2. Run provider/db/migrate.py and billing/job_ledger_db.init_db() to prepare DBs.
+3. Setup HSM admin host, populate PKCS11 env variables and perform end-to-end signing drill.
+4. Enable Istio mTLS strict PeerAuthentication for aegis namespace and test internal service connectivity.
+5. Start attestation_worker to enroll a subset of devices and collect /tmp/attestation_batch_report.json for audit.
+6. Wire scheduler/adapters to call billing/job_ledger_db.write_entry at job completion.
+
*** End Patch
*** End Patch
