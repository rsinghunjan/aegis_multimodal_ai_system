*** Begin Patch
*** Add File: docs/request_provider_creds.md
+Title: Request for Provider Sandbox Credentials or Secure Self‑Hosted Runner
+
+Purpose
+-------
+Use this template to request either:
+- provider sandbox credentials (IBM/Braket) or
+- a secure self-hosted runner for running provider E2E verification.
+
+What we need from you
+---------------------
+Option A — Provider sandbox credentials (recommended for quick runs)
+- IBM (example):
+  - A minimal runtime API token that can be used against the IBM sandbox (or test account).
+  - Prefer storing token in Vault at path: secret/data/quantum/providers/ibm
+  - If you cannot use Vault, provide an encrypted JSON using credentials/encrypt_credentials.sh (operator_pub.pem required).
+
+- AWS Braket (example):
+  - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY with limited permissions (Braket S3 write + Braket task submit).
+  - BRK_S3_BUCKET (a test bucket) or permission to create one.
+  - Prefer storing in Vault at secret/data/quantum/providers/braket.
+
+Option B — Secure self-hosted runner (recommended if you prefer not to expose provider tokens)
+- A VM with:
+  - Network access to provider endpoints
+  - GitHub Actions runner registered with labels: self-hosted,qpu-adjacent
+  - Python 3.11, kubectl, docker (optional)
+  - Private signing key placed at a local path (e.g., /opt/aegis/secrets/signing_key.pem, chmod 600)
+
+How to deliver credentials securely
+----------------------------------
+1) Vault (preferred)
+   - Add provider secret under secret/data/quantum/providers/<provider>.
+   - Use the AppRole approach (terraform/vault/) so the controller/runner can fetch via scoped token.
+
+2) Encrypted handoff (operator-assisted)
+   - Encrypt using: ./credentials/encrypt_credentials.sh --in provider_ibm.json --pubkey operator_pub.pem --out-dir ./secure_creds
+   - Deliver .enc and .key.enc to operator via secure channel (SFTP, secure email).
+   - Operator will decrypt with private key and use create_k8s_secret_from_tf_outputs.sh or operator bootstrap scripts.
+
+What we'll do with the credentials/runner
+-----------------------------------------
+- Run provider_endtoend_verify.py and provider_stress_test.py.
+- Adapt Qiskit/Braket adapters to handle real provider error responses, throttling, and async job APIs, then open PRs to update adapters and controller retry behavior.
+- Produce signed artifact + Rekor submission to prove end‑to‑end veracity.
+
+Security & billing notes
+-----------------------
+- Use provider sandboxes or limited test accounts to avoid unintended billing.
+- Tokens/keys must be rotated after tests.
+- Do NOT commit credentials into source control.
+
+Example contact message (copy/paste)
+-----------------------------------
+Please provide one of:
+ - Vault entry at secret/data/quantum/providers/ibm and/or secret/data/quantum/providers/braket, or
+ - Encrypted credentials bundle using our operator public key (instructions in repo/credentials).
+Alternatively, provide a secure self-hosted runner with labels [self-hosted,qpu-adjacent] and the runner registration token; we will register and use it for provider E2E runs.
+
+Thank you — once we receive credentials or the runner we will execute the provider E2E verification and share results.
+
*** End Patch
*** Begin Patch
*** Add File: automation/run_provider_e2e_on_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run provider end-to-end verification on a secured self-hosted runner (local or remote).
+# Expected environment variables / inputs:
+#   - KUBECONFIG (or kube access on the runner)
+#   - VAULT_ADDR / VAULT_TOKEN or vault-credentials secret already in cluster
+#   - LOCAL_SIGNING_KEY_PATH (path to private signing key on runner) OR Rekor configured
+# Usage:
+#   ./automation/run_provider_e2e_on_runner.sh --provider ibm --namespace aegis --out ./provider_results --wait 600
+
+PROVIDER="ibm"
+NAMESPACE="aegis"
+OUT_DIR="./provider_results"
+WAIT_SECONDS=600
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --provider) PROVIDER="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    --wait) WAIT_SECONDS="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 [--provider ibm|braket] [--namespace aegis] [--out ./provider_results] [--wait 600]"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "Creating/patching job manifest for provider=$PROVIDER..."
+# patch the job manifest to use chosen provider by env substitution (simple approach)
+TEMP_MANIFEST="$(mktemp)"
+sed "s/--provider ibm/--provider ${PROVIDER}/g" k8s/jobs/provider-endtoend-job.yaml > "$TEMP_MANIFEST"
+
+echo "Applying job manifest..."
+kubectl -n "$NAMESPACE" delete job provider-endtoend-verify --ignore-not-found || true
+kubectl -n "$NAMESPACE" apply -f "$TEMP_MANIFEST"
+rm -f "$TEMP_MANIFEST"
+
+echo "Waiting up to ${WAIT_SECONDS}s for job completion..."
+START=$(date +%s)
+END=$((START + WAIT_SECONDS))
+while [ "$(date +%s)" -lt "$END" ]; do
+  succeeded=$(kubectl -n "$NAMESPACE" get job provider-endtoend-verify -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "")
+  failed=$(kubectl -n "$NAMESPACE" get job provider-endtoend-verify -o jsonpath='{.status.failed}' 2>/dev/null || echo "")
+  if [ "$succeeded" = "1" ]; then
+    echo "Job completed successfully"
+    break
+  fi
+  if [ -n "$failed" ] && [ "$failed" != "0" ]; then
+    echo "Job failed (failed=$failed). Fetching logs..."
+    kubectl -n "$NAMESPACE" logs job/provider-endtoend-verify --all-containers || true
+    exit 3
+  fi
+  echo -n "."
+  sleep 5
+done
+
+if [ "$(kubectl -n "$NAMESPACE" get job provider-endtoend-verify -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "")" != "1" ]; then
+  echo "Job did not complete within ${WAIT_SECONDS}s"
+  kubectl -n "$NAMESPACE" get pods -l job-name=provider-endtoend-verify -o wide
+  kubectl -n "$NAMESPACE" logs job/provider-endtoend-verify --all-containers || true
+  exit 4
+fi
+
+POD=$(kubectl -n "$NAMESPACE" get pods -l job-name=provider-endtoend-verify -o jsonpath='{.items[0].metadata.name}')
+mkdir -p "$OUT_DIR"
+kubectl -n "$NAMESPACE" cp "$POD":/results "$OUT_DIR" || echo "kubectl cp failed; check pod logs"
+echo "Results copied to $OUT_DIR"
+
+echo "Provider E2E run complete. Please inspect $OUT_DIR for summary.json and signature files."
+
*** End Patch
*** Begin Patch
*** Add File: operator/start_rotator_and_verify.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy vault-token-rotator CronJob (if not already deployed) and verify it populates vault-credentials secret with a runtime token.
+#
+# Usage:
+#   ./operator/start_rotator_and_verify.sh --namespace aegis --secret-name vault-credentials --wait 120
+
+NAMESPACE="aegis"
+SECRET_NAME="vault-credentials"
+WAIT_SEC=120
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --secret-name) SECRET_NAME="$2"; shift 2;;
+    --wait) WAIT_SEC="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 [--namespace aegis] [--secret-name vault-credentials] [--wait 120]"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+echo "Applying CronJob manifest for rotator..."
+kubectl apply -f k8s/cronjobs/vault-token-rotator-cronjob.yaml
+
+echo "Waiting up to ${WAIT_SEC}s for CronJob to run and populate secret ${SECRET_NAME}..."
+end=$((SECONDS + WAIT_SEC))
+while [ $SECONDS -lt $end ]; do
+  if kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.token}' >/dev/null 2>&1; then
+    echo "Secret ${SECRET_NAME} contains a token (base64). Rotator succeeded."
+    kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o yaml
+    exit 0
+  fi
+  echo -n "."
+  sleep 5
+done
+
+echo ""
+echo "Timed out waiting for secret ${SECRET_NAME} to be populated. Inspect CronJob and its logs."
+kubectl get cronjob -n "$NAMESPACE" vault-token-rotator -o yaml || true
+kubectl get jobs -n "$NAMESPACE" --selector=job-name -o wide || true
+exit 2
+
*** End Patch
*** Begin Patch
*** Add File: argo/sync_and_run_tests_cli.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Sync Argo apps in argo/apps/* and run staging tests (wrapper).
+# Requires argocd CLI configured or ArgoCD auto-sync enabled.
+#
+# Usage:
+#   ARGOCD_SERVER=... ARGOCD_TOKEN=... ./argo/sync_and_run_tests_cli.sh --results-dir ./provider_staging_results
+
+RESULTS_DIR="./provider_staging_results"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --results-dir) RESULTS_DIR="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 [--results-dir ./provider_staging_results]"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+echo "Syncing Argo apps from argo/apps/..."
+for f in argo/apps/*.yaml; do
+  appname="$(basename "$f" .yaml)"
+  echo "Sync attempt for $appname"
+  if command -v argocd >/dev/null 2>&1; then
+    argocd app sync "$appname" || echo "argocd sync failed or app not found: $appname"
+    argocd app wait "$appname" --health --timeout 300s || echo "wait timed out for $appname"
+  else
+    echo "argocd CLI not found. Ensure ArgoCD auto-sync is enabled or sync via ArgoCD UI."
+  fi
+done
+
+echo "Calling staging test runner..."
+chmod +x scripts/argo_sync_and_test.sh
+./scripts/argo_sync_and_test.sh
+
+echo "Collecting test outputs to $RESULTS_DIR"
+mkdir -p "$RESULTS_DIR"
+cp -r provider_staging_results/* "$RESULTS_DIR" 2>/dev/null || true
+echo "Done. Results in $RESULTS_DIR"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/run-staging-tests-dispatch.yml
+name: Run Staging Tests (dispatch)
+on:
+  workflow_dispatch:
+
+jobs:
+  dispatch:
+    runs-on: [self-hosted, qpu-adjacent]
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure kubectl & argocd (optional)
+        run: |
+          if ! command -v kubectl >/dev/null 2>&1; then
+            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
+            chmod +x kubectl; sudo mv kubectl /usr/local/bin/
+          fi
+          if ! command -v argocd >/dev/null 2>&1; then
+            echo "argocd CLI not present (optional)"
+          fi
+      - name: Sync Argo Apps and Run Tests
+        env:
+          ARGOCD_SERVER: ${{ secrets.ARGOCD_SERVER }}
+          ARGOCD_TOKEN: ${{ secrets.ARGOCD_TOKEN }}
+        run: |
+          chmod +x argo/sync_and_run_tests_cli.sh
+          ./argo/sync_and_run_tests_cli.sh --results-dir provider_staging_results
+      - name: Upload staging results
+        uses: actions/upload-artifact@v4
+        with:
+          name: staging-test-results
+          path: provider_staging_results/**
+
*** End Patch
*** End Patch
