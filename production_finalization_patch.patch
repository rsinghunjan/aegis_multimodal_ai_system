*** Begin Patch
*** Add File: mlops/ha/postgres_operator_cr_ha.yaml
+---
+# Zalando Postgres Operator example CR for HA Postgres for MLflow metadata.
+# Requires Zalando Postgres Operator installed in the cluster.
+apiVersion: "acid.zalan.do/v1"
+kind: postgresql
+metadata:
+  name: mlflow
+  namespace: aegis-ml
+spec:
+  teamId: "aegis"
+  volume:
+    size: 200Gi
+    storageClass: "gp2" # replace with your production storage class
+  numberOfInstances: 3
+  users:
+    mlflow_user:  # database user
+    - superuser
+    - createdb
+  databases:
+    mlflow: mlflow_user
+  postgresql:
+    version: "14"
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "1"
+      memory: "2Gi"
+  patroni:
+    initdb:
+      encoding: "UTF8"
+      locale: "en_US.UTF-8"
+  podAntiAffinity: preferred
+
+---
+# Notes:
+# - This CR creates a 3-node HA Postgres cluster with replication and automatic failover.
+# - Ensure operator has permissions and StorageClass exists. Configure backups (pgbackrest or cronjobs).
+
*** End Patch
*** Begin Patch
*** Add File: mlops/ha/rds_provision_template.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# rds_provision_template.sh
+# Template to provision a Multi-AZ RDS Postgres instance and an automated Read Replica using AWS CLI.
+# Fill placeholders and run with AWS credentials that have RDS permissions.
+#
+DB_IDENTIFIER="${DB_IDENTIFIER:-aegis-mlflow}"
+DB_NAME="${DB_NAME:-mlflow}"
+DB_INSTANCE_CLASS="${DB_INSTANCE_CLASS:-db.m6g.large}"
+MASTER_USERNAME="${MASTER_USERNAME:-mlflow_admin}"
+MASTER_PASSWORD="${MASTER_PASSWORD:-REPLACE_WITH_STRONG_PASSWORD}"
+ALLOCATED_STORAGE="${ALLOCATED_STORAGE:-100}"
+SUBNET_GROUP="${SUBNET_GROUP:-default}"
+SECURITY_GROUP_IDS="${SECURITY_GROUP_IDS:-sg-0123456789abcdef0}"
+REGION="${AWS_REGION:-us-west-2}"
+
+echo "Creating Multi-AZ RDS PostgreSQL instance: $DB_IDENTIFIER (this is a template, review before running)"
+aws rds create-db-instance \
+  --db-instance-identifier "$DB_IDENTIFIER" \
+  --allocated-storage "$ALLOCATED_STORAGE" \
+  --db-instance-class "$DB_INSTANCE_CLASS" \
+  --engine postgres \
+  --engine-version 14.6 \
+  --master-username "$MASTER_USERNAME" \
+  --master-user-password "$MASTER_PASSWORD" \
+  --multi-az \
+  --backup-retention-period 7 \
+  --publicly-accessible false \
+  --vpc-security-group-ids $SECURITY_GROUP_IDS \
+  --db-subnet-group-name "$SUBNET_GROUP" \
+  --region "$REGION"
+
+echo "After instance creation, create read replicas if desired using 'aws rds create-db-instance-read-replica'."
+echo "Remember to enable automated backups, point-in-time recovery (PITR), and configure monitoring/maintenance windows."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/ha/pgbackrest_deploy_notes.md
+# pgBackRest & PITR notes
+
+- For operator-managed Postgres, use pgBackRest (or cloud-managed snapshot/PITR) for reliable backups and continuous archiving.
+- Consider the following:
+  - Use a dedicated S3/GCS bucket with CMK encryption for WAL and base backups.
+  - Configure retention and archive settings: e.g., full backup weekly, incremental daily, WAL archived continuously.
+  - Test PITR restore monthly with documented time-to-restore measurement.
+
+This repo includes a simple pg_dump CronJob example (mlops/k8s/postgres/pg_backup_cronjob.yaml) but for high durability use pgBackRest for consistent WAL-based backups.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/dr/run_restore_and_measure.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# run_restore_and_measure.sh
+# Automates a DR restore run for MLflow metadata and measures restore time. Produces an artifact for signoff.
+#
+# Env inputs:
+#  - MLFLOW_BACKUP_BUCKET: S3 bucket with pg_dump backups
+#  - AWS_REGION
+#  - PGHOST/PGUSER/PGPASSWORD/PGDATABASE env used for the restored DB target (test cluster)
+#  - OUTPUT_DIR: where to place artifacts (defaults /tmp/aegis-dr)
+#
+OUTPUT_DIR="${OUTPUT_DIR:-/tmp/aegis-dr}"
+mkdir -p "$OUTPUT_DIR"
+
+if [ -z "${MLFLOW_BACKUP_BUCKET:-}" ]; then
+  echo "Set MLFLOW_BACKUP_BUCKET"
+  exit 2
+fi
+
+echo "Listing backups in s3://$MLFLOW_BACKUP_BUCKET/backups/mlflow-db/"
+LATEST=$(aws s3 ls "s3://$MLFLOW_BACKUP_BUCKET/backups/mlflow-db/" --region "${AWS_REGION:-us-west-2}" | sort | tail -n1 | awk '{print $4}')
+if [ -z "$LATEST" ]; then
+  echo "No backups found"
+  exit 1
+fi
+S3PATH="s3://$MLFLOW_BACKUP_BUCKET/backups/mlflow-db/$LATEST"
+echo "Latest backup: $S3PATH"
+
+START_TS=$(date +%s)
+echo "Downloading backup..."
+aws s3 cp "$S3PATH" "$OUTPUT_DIR/$LATEST" --region "${AWS_REGION:-us-west-2}"
+echo "Restore start: $(date -u -d @$START_TS)"
+
+echo "Restoring to PG target (ensure env PGHOST/PGUSER/PGPASSWORD/PGDATABASE set to test instance)"
+gunzip -c "$OUTPUT_DIR/$LATEST" | psql "$PGDATABASE" -h "$PGHOST" -U "$PGUSER"
+
+END_TS=$(date +%s)
+ELAPSED=$((END_TS - START_TS))
+echo "Restore completed in $ELAPSED seconds" | tee "$OUTPUT_DIR/restore_time.txt"
+
+echo "Collecting verification: sample query of experiments"
+psql -h "$PGHOST" -U "$PGUSER" -d "$PGDATABASE" -c "SELECT count(*) FROM experiments;" > "$OUTPUT_DIR/experiments_count.txt" || true
+
+echo "DR artifacts available at $OUTPUT_DIR"
+tar -czf "$OUTPUT_DIR/aegis-dr-artifacts-$(date -u +%Y%m%dT%H%M%SZ).tgz" -C "$OUTPUT_DIR" .
+echo "DR run complete. Upload artifacts for signoff."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/auth/oauth2_full_setup.md
+# OAuth2 / SSO Setup & RBAC mapping for MLflow
+
+Summary
+- This document shows recommended steps to finish OAuth2 SSO integration for MLflow using oauth2-proxy (or API Gateway) and to map SSO groups to Kubernetes/RBAC roles and MLflow access.
+
+Steps
+1. Create OAuth client in your IdP (Okta/Keycloak/GitHub/Google):
+   - Redirect URI: https://mlflow.<your-domain>/oauth2/callback
+   - Scopes: openid email profile groups
+   - Capture client_id and client_secret
+
+2. Create k8s secret `oauth2-proxy-secret` with keys: client_id, client_secret, cookie_secret (base64), issuer_url
+
+3. Deploy oauth2-proxy (manifest in mlops/k8s/mlflow_ingress_oauth2.yaml) and configure ingress annotation to point to oauth2-proxy auth endpoint.
+
+4. Map IdP groups to GitHub/Keycloak groups, and configure application RBAC:
+   - Use group claims to grant "mlflow-admin", "mlflow-user" groups.
+   - Optionally front MLflow with an auth-proxy that injects X-Remote-User and X-Remote-Group headers for MLflow to consume.
+
+5. Implement MLflow RBAC:
+   - MLflow does not ship with enterprise RBAC by default. Use oauth2-proxy + an API Gateway (or a simple proxy) to implement access control rules.
+   - Example: allow /api/2.0/ preview endpoints only to mlflow-admin group; /api/2.0/ artifacts to mlflow-user group.
+
+6. Audit logs:
+   - Configure Fluentd/FluentBit to collect oauth2-proxy and MLflow logs and ship to your SIEM (Elastic/Datadog/CloudWatch).
+   - Ensure log retention policy (e.g., 90 days) and access control for logs.
+
+7. Test:
+   - Login as a user in mlflow-admin and confirm admin actions (delete, create) succeed.
+   - Login as a mlflow-user and ensure admin endpoints are forbidden.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/auth/rbac_mapping_example.yaml
+---
+# Example mapping for SSO groups to MLflow roles (conceptual).
+# Implement as part of oauth2-proxy/group-to-header mapping or via API gateway policies.
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: mlflow-rbac-mapping
+  namespace: aegis-ml
+data:
+  mappings.json: |
+    {
+      "mlflow-admin": {
+        "users": ["group:mlflow-admin"],
+        "permissions": ["create","delete","manage"]
+      },
+      "mlflow-user": {
+        "users": ["group:mlflow-user"],
+        "permissions": ["read","write"]
+      }
+    }
+
+---
+# Use these mappings in your proxy or API gateway to block/allow requests. This file is advisory and requires
+# integration code in the proxy layer to enforce.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/data/lakefs_install_and_verify.md
+# LakeFS Install & Argo Validation Runbook
+
+Install LakeFS (Helm)
+- helm repo add treeverse https://charts.treeverse.io
+- helm repo update
+- helm install lakefs treeverse/lakefs -f mlops/lakefs/helm-values-production.yaml --namespace aegis-ml --create-namespace
+
+Create LakeFS user & keys
+- Use `lakefs auth create` or API to create access key/secret and store as k8s secret.
+
+Argo pipeline validation
+- Ensure Argo's snapshot step runs lakefs_commit_and_tag.sh and that the pipeline writes snapshot_id to workflow outputs.
+- After a pipeline run, verify:
+  - The snapshot id exists in lakeFS: `lakefs api repositories/<repo>/commits/<commit_id>`
+  - MLflow run has tag `lakefs.commit` (use mlflow UI or `mlflow.tracking.MlflowClient()`).
+
+If any step fails, the Argo snapshot step must fail (it already exits non-zero on commit failure) and the pipeline should not continue to train.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/ci/argo_wait_fetch_promote_robust.yml
+name: Argo Robust Wait, Fetch Artifacts, Validate & Promote
+
+on:
+  workflow_dispatch:
+    inputs:
+      workflow_prefix:
+        description: "Argoworkflow name prefix (e.g., aegis-ml-train-prod-)"
+        required: true
+
+permissions:
+  contents: read
+  id-token: write
+
+jobs:
+  wait-validate-promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Restore kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG_BASE64 }}" | base64 --decode > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+        env:
+          KUBECONFIG_BASE64: ${{ secrets.KUBECONFIG_BASE64 }}
+
+      - name: Install Argo CLI
+        run: |
+          curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo-workflows/releases/latest/download/argo-linux-amd64
+          chmod +x /usr/local/bin/argo
+
+      - name: Find latest workflow & wait
+        id: wf
+        run: |
+          PREFIX="${{ github.event.inputs.workflow_prefix }}"
+          NAMESPACE="${{ secrets.K8S_NAMESPACE }}"
+          WF=$(kubectl -n "$NAMESPACE" get wf -o name | grep "$PREFIX" | tail -n1 | sed 's|workflow.argoproj.io/||')
+          if [ -z "$WF" ]; then echo "No workflow found"; exit 1; fi
+          echo "workflow=$WF" >> $GITHUB_OUTPUT
+          argo wait -n "$NAMESPACE" "$WF" --timeout 2h || (echo "workflow failed or timed out"; exit 1)
+          argo get -n "$NAMESPACE" "$WF" -o json > /tmp/argo_${WF}.json
+          echo "::set-output name=workflow_json_path::/tmp/argo_${WF}.json"
+
+      - name: Extract run_id & snapshot_id robustly
+        id: extract
+        run: |
+          JSON_PATH="${{ steps.wf.outputs.workflow_json_path }}"
+          RUN_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="run_id") .value' "$JSON_PATH" || echo "")
+          if [ -z "$RUN_ID" ]; then
+            # fallback: search for run id pattern in logs/artifacts (e.g., uuid-looking string or runs:/)
+            RUN_ID=$(jq -r '.. | objects | .uri? // empty' "$JSON_PATH" | grep -Eo 'runs:/[^/]+/[^/"]+' | tail -n1 | sed 's|runs:/||' || echo "")
+          fi
+          SNAPSHOT_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="snapshot_id") .value' "$JSON_PATH" || echo "")
+          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
+          echo "snapshot_id=$SNAPSHOT_ID" >> $GITHUB_OUTPUT
+
+      - name: Validate MLflow run metadata & provenance
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          RUN_ID: ${{ steps.extract.outputs.run_id }}
+        run: |
+          if [ -z "$RUN_ID" ]; then echo "No run id found"; exit 1; fi
+          pip install mlflow
+          python3 - <<PY
+import os,sys
+from mlflow.tracking import MlflowClient
+client = MlflowClient(tracking_uri=os.environ.get("MLFLOW_TRACKING_URI"))
+runid = os.environ.get("RUN_ID")
+run = client.get_run(runid)
+tags = run.data.tags
+required = ["git.sha","lakefs.commit","trainer.image","image.digest"]
+missing = [r for r in required if not tags.get(r)]
+if missing:
+    print("Missing provenance tags:", missing)
+    sys.exit(2)
+print("Provenance tags present")
+PY
+
+      - name: Download artifacts and run validation
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          RUN_ID: ${{ steps.extract.outputs.run_id }}
+        run: |
+          pip install mlflow boto3
+          python3 - <<PY
+import os,mlflow,tempfile,tarfile
+from mlflow.tracking import MlflowClient
+client = MlflowClient(tracking_uri=os.environ.get("MLFLOW_TRACKING_URI"))
+run_id = os.environ.get("RUN_ID")
+artifacts = client.list_artifacts(run_id, path='')
+print("Artifacts:", artifacts)
+# Download a model artifact for validation (example)
+local = tempfile.mkdtemp()
+client.download_artifacts(run_id, "model", local)
+print("Downloaded model to", local)
+PY
+          # Run the validation script using the downloaded run_id
+          python3 mlops/scripts/validate_model.py --run-id "${{ steps.extract.outputs.run_id }}" --min-accuracy 0.80
+
+      - name: If validation fails -> create rollback issue and abort promotion
+        if: failure()
+        run: |
+          RUN_ID=${{ steps.extract.outputs.run_id }}
+          gh issue create --title "ML validation failed for run ${RUN_ID}" --body "Automated validation failed. Run ${RUN_ID} did not meet validation criteria. Please investigate and rollback canary if deployed." || true
+          exit 1
+
+      - name: Create promotion PR (on success)
+        if: success()
+        run: |
+          RUN_ID=${{ steps.extract.outputs.run_id }}
+          SNAP=${{ steps.extract.outputs.snapshot_id }}
+          gh pr create --title "Promote model run ${RUN_ID} to staging/canary" --body "Automated promotion PR for run ${RUN_ID}\n\nsnapshot: ${SNAP}" || true
+
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: argo-artifacts
+          path: /tmp/argo_*.json || true
+
+# Requirements: GH CLI installed and authenticated via secrets, KUBECONFIG_BASE64, K8S_NAMESPACE, MLFLOW_TRACKING_URI
+
*** End Patch
*** Begin Patch
*** Add File: mlops/serving/destinationrule_and_vs.yaml
+apiVersion: networking.istio.io/v1alpha3
+kind: DestinationRule
+metadata:
+  name: aegis-ml-canary-drule
+  namespace: aegis-ml
+spec:
+  host: aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local
+  subsets:
+  - name: stable
+    labels:
+      version: stable
+  - name: canary
+    labels:
+      version: canary
+
+---
+apiVersion: networking.istio.io/v1alpha3
+kind: VirtualService
+metadata:
+  name: aegis-ml-canary-vs
+  namespace: aegis-ml
+spec:
+  hosts:
+  - "aegis-ml-canary.aegis-ml.svc.cluster.local"
+  http:
+  - route:
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: stable
+      weight: 90
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: canary
+      weight: 10
+
+---
+# Note: Define Deployment labels for predictor pods to include version: stable|canary.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/serving/canary_rollback.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# canary_rollback.sh
+# Usage: ./canary_rollback.sh --namespace aegis-ml --service aegis-ml-canary
+
+NAMESPACE="${1:-aegis-ml}"
+SERVICE="${2:-aegis-ml-canary-predictor-default}"
+
+echo "Rolling back canary for $SERVICE in namespace $NAMESPACE"
+# Scale down canary subset pods by selecting label version=canary -> set replicas 0
+kubectl -n "$NAMESPACE" get deploy -l version=canary -o name | xargs -r -n1 kubectl -n "$NAMESPACE" scale --replicas=0
+# Ensure stable pods scale up to desired state (this depends on HPA / Deployment settings)
+kubectl -n "$NAMESPACE" get deploy -l version=stable -o name | xargs -r -n1 kubectl -n "$NAMESPACE" scale --replicas=2
+echo "Rollback actions executed. Monitor pods and service endpoints."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/scrape_inference_config.yaml
+---
+# Prometheus scrape config snippet for inference services
+scrape_configs:
+- job_name: 'aegis-model-inference'
+  metrics_path: /metrics
+  scrape_interval: 15s
+  static_configs:
+  - targets:
+    - aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local:8000
+    - aegis-ml-stable-predictor-default.aegis-ml.svc.cluster.local:8000
+
+#
+# Ensure your inference containers expose Prometheus metrics at /metrics (see mlops/scripts/instrument_inference.py).
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/cron/drift_detector_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-drift-detector
+  namespace: monitoring
+spec:
+  schedule: "*/5 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-monitoring-sa
+          containers:
+          - name: drift-detector
+            image: ghcr.io/${{ github.repository_owner }}/aegis-monitoring:latest
+            command: ["python3", "/app/mlops/scripts/drift_alert_runner.py"]
+            env:
+            - name: PROM_URL
+              value: "http://prometheus.monitoring.svc.cluster.local"
+            - name: DRIFT_BASELINE_FILE
+              value: "/etc/aegis/baseline.json"
+            - name: DRIFT_THRESHOLD
+              value: "0.1"
+            - name: DRIFT_SLACK_WEBHOOK
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-monitoring-secrets
+                  key: drift_slack_webhook
+            volumeMounts:
+            - name: baseline
+              mountPath: /etc/aegis
+          restartPolicy: OnFailure
+          volumes:
+          - name: baseline
+            configMap:
+              name: aegis-drift-baseline
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/sbom_trivy_block_and_issue.yml
+name: SBOM & Trivy Enforcement (block & create issue)
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+permissions:
+  contents: read
+
+jobs:
+  sbom-enforce:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build trainer image
+        run: |
+          IMAGE=ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:pr-${{ github.event.number }}
+          docker build -t $IMAGE -f mlops/docker/Dockerfile .
+          echo $IMAGE > image.txt
+      - name: Generate SBOM (Syft)
+        run: |
+          IMAGE=$(cat image.txt)
+          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+          syft $IMAGE -o json > sbom.json
+      - name: Trivy scan (block on HIGH/CRITICAL)
+        id: trivy
+        run: |
+          IMAGE=$(cat image.txt)
+          curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+          trivy image --exit-code 1 --severity HIGH,CRITICAL --format json --output trivy-report.json $IMAGE || true
+          jq '.Results[] | select(.Vulnerabilities != null) | .Vulnerabilities[] | select(.Severity=="CRITICAL" or .Severity=="HIGH")' trivy-report.json > criticals.json || true
+          if [ -s criticals.json ]; then
+             echo "Vulnerabilities found"
+             gh issue create --title "Security Vulnerabilities in trainer image (PR #${{ github.event.number }})" --body "Trivy found HIGH/CRITICAL vulnerabilities. See artifacts." || true
+             exit 1
+          fi
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom-trivy
+          path: sbom.json,trivy-report.json || true
+
+# To enforce as a required check, enable this workflow's job name as required in Branch Protection UI.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/multi_tenant/namespace_onboard_gatekeeper_constraint.yaml
+---
+# OPA Gatekeeper ConstraintTemplate + Constraint (example) to require tenant label on namespaces.
+# Requires Gatekeeper installed.
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequirednamespacelabel
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequiredNamespaceLabel
+  targets:
+  - target: admission.k8s.gatekeeper.sh
+    rego: |
+      package k8srequirednamespacelabel
+      violation[{"msg": msg}] {
+        input.request.kind.kind == "Namespace"
+        not input.request.object.metadata.labels.tenant
+        msg := "Namespace must have label 'tenant'"
+      }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequiredNamespaceLabel
+metadata:
+  name: require-tenant-label
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Namespace"]
+
+# Enforced: every namespace must include label tenant=<tenant-id> to proceed. This supports tenant onboarding validation.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/provenance/enforce_provenance_ci.yml
+name: Enforce Provenance on Promotion PRs
+
+on:
+  pull_request:
+    types: [opened, reopened, labeled]
+
+permissions:
+  contents: read
+
+jobs:
+  check-provenance:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Check PR body for run_id and lakefs snapshot
+        run: |
+          BODY="${{ github.event.pull_request.body }}"
+          echo "PR Body: $BODY"
+          if ! echo "$BODY" | grep -E "run[ _-]?id[: ]|runs:/" >/dev/null; then
+            echo "Promotion PR missing MLflow run id in body. Please include run id and lakefs.commit."
+            gh pr comment ${{ github.event.pull_request.number }} --body "Promotion PR missing MLflow run id and provenance tags. Add 'run_id: <id>' and 'lakefs.commit: <commit>' to PR body." || true
+            exit 1
+          fi
+      - name: Require CODEOWNERS approval (reminder)
+        run: |
+          echo "Ensure CODEOWNERS reviews infra and security signoff before merging. Branch protection should enforce CODEOWNERS."
+
*** End Patch
*** Begin Patch
*** Add File: docs/prod_finalization_runbook.md
+# Production Finalization Runbook â€” Aegis MLOps
+
+Purpose: Steps to verify HA, Auth, LakeFS, CI/CD automation, serving, observability, security, multi-tenant and DR readiness.
+
+1) HA Postgres validation
+ - Confirm Zalando PostgresOperator cluster (mlflow CR) shows 3 healthy instances.
+ - Induce failover: cordon & delete primary pod and confirm a replica promotes within timeframe. Record failover time.
+ - Execute PITR or pgBackRest restore test and run run_restore_and_measure.sh to capture restore time artifacts.
+
+2) OAuth2 / RBAC validation
+ - Complete IdP client config, set oauth2-proxy secrets, and test login for admin and user groups.
+ - Confirm MLflow endpoints reject unauthorized users and admin actions require mlflow-admin group.
+ - Validate audit logs are forwarded to SIEM and retention policy applied.
+
+3) LakeFS & lineage validation
+ - Run Argo pipeline and confirm lakefs.commit tag present on MLflow run.
+ - Test a missing commit scenario: make snapshot step fail and confirm Argo pipeline halts.
+
+4) CI/CD automation validation
+ - Trigger training & confirm argo_wait_fetch_promote_robust.yml extracts run_id, validates provenance and creates promotion PR.
+ - Simulate validation failure and confirm rollback issue created and canary not promoted.
+
+5) Serving & canary tests
+ - Deploy canary and run canary traffic test (load) to validate autoscaling & traffic split.
+ - Trigger canary_rollback.sh and confirm service reverts to stable.
+
+6) Observability & drift
+ - Deploy instrumented inference; verify Prometheus scrape and Grafana panels populate.
+ - Trigger drift scenario (manipulate feature means) and confirm alert hits Slack/Pager.
+
+7) SBOM & Security gating
+ - Open PR with vulnerable base image (test) and confirm SBOM/Trivy workflow fails and creates issue.
+ - Triage and remediate vulnerabilities, then confirm workflow passes.
+
+8) Multi-tenant & billing
+ - Create namespace with and without tenant label; Gatekeeper should block unlabeled namespace.
+ - Run tenant_billing_report.sh to collect resource usage and confirm chargeback report.
+
+9) DR exercise
+ - Run run_restore_and_measure.sh, bring up test MLflow pointing to restored DB & artifacts, validate a model load and inference.
+ - Collect artifacts and require SRE & Security signoff in repository.
+
+Signoff template
+- SRE lead:
+- Security lead:
+- Date:
+- Key metrics (failover time, restore time, validation pass rates):
+
*** End Patch
*** End Patch
