*** Begin Patch
*** Add File:ops/nvidia/setup_nvidia_drivers.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Install NVIDIA drivers, CUDA toolkit, and NVIDIA Container Toolkit on an Ubuntu node.
+# Idempotent (will skip if already installed).
+#
+# Usage (operator/bastion):
+#   sudo ./ops/nvidia/setup_nvidia_drivers.sh --cuda 12.1 --driver 535
+#
+CUDA_VERSION=${1:-"12.1"}
+NVIDIA_DRIVER_VERSION=${2:-"535"}
+
+if command -v nvidia-smi >/dev/null 2>&1; then
+  echo "nvidia-smi already present - driver likely installed. Skipping driver install."
+else
+  echo "Installing NVIDIA driver ${NVIDIA_DRIVER_VERSION}..."
+  # Add NVIDIA package repo
+  distribution=$(lsb_release -cs)
+  sudo apt-get update
+  sudo apt-get install -y wget gnupg lsb-release ca-certificates curl
+  curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
+  curl -s -L https://nvidia.github.io/libnvidia-container/ubuntu${distribution}/libnvidia-container.list | \
+    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
+  sudo apt-get update
+  # Install driver package (this may vary by distro; adjust for your distro)
+  sudo apt-get install -y --no-install-recommends "nvidia-driver-${NVIDIA_DRIVER_VERSION}"
+fi
+
+if ! dpkg -l | grep -q nvidia-container-toolkit; then
+  echo "Installing NVIDIA Container Toolkit (for containerd / Docker integration)"
+  # Install NVIDIA container toolkit
+  curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
+  curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu${distribution}/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
+  sudo apt-get update
+  sudo apt-get install -y nvidia-container-toolkit
+  # Restart container runtime (containerd assumed)
+  if systemctl is-active --quiet containerd; then
+    sudo systemctl restart containerd
+  else
+    echo "containerd not found; if using Docker restart docker"
+    sudo systemctl restart docker || true
+  fi
+fi
+
+echo "Verify that NVIDIA driver and container toolkit are operational:"
+nvidia-smi || true
+echo "Install complete. Consider baking these steps into your node image (AMI/GKE image) for repeatability."
+
*** End Patch
*** Begin Patch
*** Add File:ops/nvidia/nvidia-device-plugin-daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: nvidia-device-plugin-daemonset
+  namespace: kube-system
+  labels:
+    k8s-app: nvidia-device-plugin
+spec:
+  selector:
+    matchLabels:
+      name: nvidia-device-plugin-ds
+  template:
+    metadata:
+      labels:
+        name: nvidia-device-plugin-ds
+    spec:
+      tolerations:
+      - key: nvidia.com/gpu
+        operator: Exists
+        effect: NoSchedule
+      containers:
+      - name: nvidia-device-plugin-ctr
+        image: "nvidia/k8s-device-plugin:1.0.0-beta.0"
+        env:
+        - name: FAIL_ON_INIT_ERROR
+          value: "true"
+        resources:
+          limits:
+            cpu: "100m"
+            memory: "100Mi"
+        volumeMounts:
+        - name: device-plugin
+          mountPath: /var/lib/kubelet/device-plugins
+      volumes:
+      - name: device-plugin
+        hostPath:
+          path: /var/lib/kubelet/device-plugins
+
*** End Patch
*** Begin Patch
*** Add File:ops/nvidia/bake_gpu_image.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple helper to bootstrap GPU node images by running the driver install and installing common DL tools.
+# Intended to be used inside your image build pipeline (cloud-init / Packer step).
+#
+# Usage:
+#   ./ops/nvidia/bake_gpu_image.sh <cuda_version> <nvidia_driver_version>
+
+CUDA_VERSION=${1:-12.1}
+NVIDIA_DRIVER_VERSION=${2:-535}
+
+echo "Running GPU image bootstrap: CUDA=${CUDA_VERSION} DRIVER=${NVIDIA_DRIVER_VERSION}"
+apt-get update
+apt-get install -y build-essential curl wget git ca-certificates
+# run the driver installer (uses install script above)
+./ops/nvidia/setup_nvidia_drivers.sh "${CUDA_VERSION}" "${NVIDIA_DRIVER_VERSION}"
+
+# Install common deep learning packages for image (no heavy libs here; prefer runtime in containers)
+pip3 install --upgrade pip
+pip3 install numpy==1.24.0
+
+echo "GPU image bootstrap complete. Bake the image (AMI/GCE image) using your normal toolchain."
+
*** End Patch
*** Begin Patch
*** Add File:ops/ray/ray-operator-deploy.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: ray-operator
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: ray-operator
+  template:
+    metadata:
+      labels:
+        app: ray-operator
+    spec:
+      containers:
+      - name: ray-operator
+        image: rayproject/ray-operator:latest
+        resources:
+          limits:
+            cpu: "500m"
+            memory: "512Mi"
+
*** End Patch
*** Begin Patch
*** Add File:ops/deepspeed/deepspeed-launcher-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: deepspeed-train-job
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: deepspeed
+          image: ghcr.io/yourorg/deepspeed-train:latest
+          command: [ "/bin/bash", "-c" ]
+          args:
+            - |
+              set -e
+              export NCCL_DEBUG=INFO
+              export NCCL_IB_DISABLE=0
+              export NCCL_SOCKET_IFNAME=eth0
+              export OMPI_MCA_btl_vader_single_copy_mechanism=none
+              # NCCL tuning knobs (example)
+              export NCCL_MIN_NRINGS=4
+              # Launch via deepspeed launcher; workers will use GPU resources assigned by K8s
+              deepspeed --num_gpus=1 train.py --config ds_config.json --output_dir /workspace/model
+          env:
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: EVIDENCE_BUCKET
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              memory: "32Gi"
+              cpu: "4"
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:triton/tensorrt/convert_onnx_to_tensorrt.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Convert ONNX model to TensorRT plan using trtexec (NVIDIA TensorRT must be installed on the host/container).
+# Usage:
+#   ./triton/tensorrt/convert_onnx_to_tensorrt.sh /path/to/model.onnx /out/engine.plan --int8 false --fp16 true
+
+ONNX_PATH=${1:?onnx path}
+OUT_PLAN=${2:?out plan}
+FP16=${3:-"true"}
+INT8=${4:-"false"}
+
+CMD=(trtexec --onnx="${ONNX_PATH}" --saveEngine="${OUT_PLAN}" --workspace=4096)
+if [ "${FP16}" = "true" ]; then
+  CMD+=(--fp16)
+fi
+if [ "${INT8}" = "true" ]; then
+  CMD+=(--int8)
+fi
+
+echo "Running TensorRT conversion: ${CMD[*]}"
+"${CMD[@]}"
+
+echo "TensorRT engine generated at ${OUT_PLAN} (validate with Triton or TRT runtime)"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/training/reproducible_train_wrapper.py
+#!/usr/bin/env python3
+"""
+Training wrapper to enforce deterministic/reproducible settings and mixed-precision policies.
+Wrap your training script like:
+  python3 scripts/training/reproducible_train_wrapper.py --train-cmd "python train.py --epochs 3" --seed 42 --amp true --loss-scaling dynamic
+
+The wrapper:
+ - sets PYTHONHASHSEED
+ - sets torch and numpy seeds
+ - sets cudnn deterministic flags
+ - configures AMP environment variables if requested
+ - captures hyperparams and writes an artifact manifest (manifest.json) in working dir
+"""
+import argparse
+import os
+import subprocess
+import json
+import time
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--train-cmd", required=True, help="Command to run training (quoted)")
+    p.add_argument("--seed", type=int, default=42)
+    p.add_argument("--amp", action="store_true")
+    p.add_argument("--loss-scaling", choices=["dynamic","static","none"], default="dynamic")
+    p.add_argument("--out-dir", default="/workspace/model")
+    args = p.parse_args()
+
+    os.environ["PYTHONHASHSEED"] = str(args.seed)
+    os.environ["TORCH_DETERMINISTIC"] = "1"
+    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
+    # Set optional AMP env
+    if args.amp:
+        os.environ["USE_AMP"] = "1"
+        os.environ["AMP_LOSS_SCALING"] = args.loss_scaling
+
+    # export seeds for common libs (training scripts should read these)
+    env = os.environ.copy()
+    env["DEFAULT_SEED"] = str(args.seed)
+
+    manifest = {
+        "seed": args.seed,
+        "amp": args.amp,
+        "loss_scaling": args.loss_scaling,
+        "train_cmd": args.train_cmd,
+        "timestamp": int(time.time())
+    }
+    os.makedirs(args.out_dir, exist_ok=True)
+    manifest_path = os.path.join(args.out_dir, "training_manifest.json")
+    with open(manifest_path, "w") as f:
+        json.dump(manifest, f, indent=2)
+
+    print("Running training with reproducible wrapper. Manifest:", manifest_path)
+    rc = subprocess.call(args.train_cmd, shell=True, env=env)
+    if rc != 0:
+        raise SystemExit(rc)
+
+    print("Training completed. Manifest saved to", manifest_path)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:triton/validation/tensorrt_validate.py
+#!/usr/bin/env python3
+"""
+Validate TensorRT engine correctness vs ONNX reference using small test inputs.
+ - Loads ONNX via onnxruntime CPU EP and TensorRT engine via TRT Inference Server (or TRT-EP) if available.
+ - Compares outputs numerically (L2 or max abs) within tolerances.
+
+Usage:
+  python3 triton/validation/tensorrt_validate.py --onnx /tmp/model.onnx --engine /tmp/engine.plan
+"""
+import argparse
+import numpy as np
+import subprocess
+import tempfile
+import json
+
+def run_onnx(onnx_path, input_feed):
+    import onnxruntime as ort
+    sess = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
+    out = sess.run(None, input_feed)
+    return out
+
+def run_trt_engine_placeholder(engine_path, input_feed):
+    # Placeholder: in production run inference via Triton server or TRT runtime; this is a best-effort stub
+    # For robust validation, spin up a Triton server with the TRT engine and call /v2/models/.../infer
+    return None
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--onnx", required=True)
+    p.add_argument("--engine", required=True)
+    args = p.parse_args()
+
+    # build a small synthetic input (assume INT32 input_ids for NLP or FP32 image for CV)
+    # Attempt to infer input shape by inspecting ONNX (simple heuristic)
+    import onnx
+    model = onnx.load(args.onnx)
+    input0 = model.graph.input[0]
+    dims = [d.dim_value if (d.dim_value > 0) else 1 for d in input0.type.tensor_type.shape.dim]
+    # if int type, use int32; else float32
+    dtype = np.int32 if input0.type.tensor_type.elem_type in (7, 8) else np.float32
+    feed = {input0.name: np.zeros(tuple(dims), dtype=dtype)}
+
+    ref_out = run_onnx(args.onnx, feed)
+    trt_out = run_trt_engine_placeholder(args.engine, feed)
+    if trt_out is None:
+        print("TRT runtime validation is environment dependent. Consider validating via Triton server inference.")
+    else:
+        # Compare numeric differences
+        diffs = []
+        for r, t in zip(ref_out, trt_out):
+            diffs.append(float(np.max(np.abs(np.array(r) - np.array(t)))))
+        print("Max diffs:", diffs)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:data/pipeline/shard_to_parquet.py
+#!/usr/bin/env python3
+"""
+Create sharded Parquet files from a dataset directory (images or examples) for high-throughput training.
+ - For image datasets: reads images, serializes to parquet with bytes columns and labels.
+ - For TFRecord workflows, provides a helper to split existing TFRecord files into shards.
+
+Usage:
+  python3 data/pipeline/shard_to_parquet.py --input-dir /data/images --out s3://bucket/datasets/myds --shards 64
+"""
+import argparse
+import os
+import pyarrow as pa
+import pyarrow.parquet as pq
+import tempfile
+import boto3
+from PIL import Image
+import io
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def images_to_parquet(input_dir, out_prefix, shards=16):
+    imgs = []
+    for root, _, files in os.walk(input_dir):
+        for f in files:
+            if f.lower().endswith((".png",".jpg",".jpeg")):
+                imgs.append(os.path.join(root,f))
+    total = len(imgs)
+    per_shard = max(1, total // shards)
+    s3 = boto3.client("s3")
+    bucket, prefix = out_prefix.replace("s3://","").split("/",1)
+    tmpdir = tempfile.mkdtemp()
+    for i in range(shards):
+        start = i * per_shard
+        end = start + per_shard if i < shards-1 else total
+        rows = []
+        for p in imgs[start:end]:
+            with open(p,"rb") as fd:
+                raw = fd.read()
+            label = os.path.basename(os.path.dirname(p))
+            rows.append({"image_bytes": raw, "label": label, "path": p})
+        if not rows:
+            continue
+        table = pa.Table.from_pylist(rows)
+        local = os.path.join(tmpdir, f"shard-{i}.parquet")
+        pq.write_table(table, local, compression="SNAPPY")
+        s3_key = f"{prefix}/shard-{i}.parquet"
+        s3.upload_file(local, bucket, s3_key)
+        print("Uploaded shard to s3://{}/{}".format(bucket, s3_key))
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input-dir", required=True)
+    p.add_argument("--out", required=True, help="s3://bucket/prefix")
+    p.add_argument("--shards", type=int, default=16)
+    args = p.parse_args()
+    images_to_parquet(args.input_dir, args.out, args.shards)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:profiling/nsys/profile_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run a Python training script under NVIDIA Nsight Systems (nsys) to generate a profile.
+# Usage:
+#   ./profiling/nsys/profile_runner.sh --cmd "python train.py" --out /tmp/profile
+
+CMD=${1:-}
+OUT=${2:-/tmp/nsys_profile}
+
+if [ -z "$CMD" ]; then
+  echo "Usage: $0 \"python train.py ...\" /tmp/profileprefix"
+  exit 2
+fi
+
+nsys profile --output "${OUT}" --trace=cuda,nvtx,osrt --sample=cpu,fp64 --capture-range=cudaProfilerApi --launch-mode=missing --capture-cli "$CMD"
+
+echo "Profile generated: ${OUT}.qdrep (use nsys UI or nsys stats to inspect)"
+
*** End Patch
*** Begin Patch
*** Add File:profiling/analyze_profile.py
+#!/usr/bin/env python3
+"""
+Simple analyzer for Nsight Systems profile outputs (PoC).
+Produces basic recommendations for batch_size and instance choices based on GPU utilization heuristics.
+
+This is a heuristic script: use it as a starting point and iteratively refine with real profilers.
+"""
+import argparse, json, subprocess, os
+
+def extract_basic_metrics(qdrep):
+    # Placeholder: use `nsys stats` in a real environment and parse CSV; here we mock recommended heuristics
+    # Real implementation would call `nsys stats --format json` and parse GPU utilization, kernel times, memcpy times.
+    print("Note: real parsing of qdrep requires nsys CLI available; returning heuristic recommendations.")
+    return {"gpu_util": 0.6, "avg_kernel_ms": 3.2, "memcpy_ms": 0.5}
+
+def recommend(cfg):
+    util = cfg["gpu_util"]
+    rec = {}
+    if util < 0.5:
+        rec["suggested_batch_increase_factor"] = 2
+        rec["note"] = "GPU underutilized -> increase batch size or add concurrent instances"
+    elif util < 0.85:
+        rec["suggested_batch_increase_factor"] = 1
+        rec["note"] = "Moderate utilization -> fine tune batch size"
+    else:
+        rec["suggested_batch_increase_factor"] = 0.5
+        rec["note"] = "High utilization -> consider adding instances or model parallelism"
+    return rec
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--profile", required=True, help="nsys .qdrep file")
+    args = p.parse_args()
+    metrics = extract_basic_metrics(args.profile)
+    recs = recommend(metrics)
+    out = {"metrics": metrics, "recommendations": recs}
+    print(json.dumps(out, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/auto_tuning/suggest_triton_config.py
+#!/usr/bin/env python3
+"""
+Generate a recommended Triton config snippet from profiling recommendations.
+Input: output from profiling/analyze_profile.py
+Output: config suggestions: max_batch_size, instance_group count, memory_mb, enable_tensorrt flag
+"""
+import argparse, json
+
+def suggest(recs):
+    factor = recs.get("recommendations", {}).get("suggested_batch_increase_factor", 1)
+    note = recs.get("recommendations", {}).get("note", "")
+    # simple heuristic mapping
+    base_batch = 8
+    max_batch = max(1, int(base_batch * factor))
+    if factor >= 2:
+        instances = 1
+    elif factor >= 1:
+        instances = 2
+    else:
+        instances = 4
+    return {"max_batch_size": max_batch, "instance_count": instances, "memory_mb": 12288, "enable_tensorrt": True, "note": note}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--profile-recs", required=True, help="JSON file with profiling recommendations")
+    args = p.parse_args()
+    recs = json.load(open(args.profile_recs))
+    out = suggest(recs)
+    print(json.dumps(out, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ci/workflows/tensorrt_convert_and_validate.yml
+name: Convert ONNX -> TensorRT & Validate
+on:
+  workflow_dispatch:
+jobs:
+  convert-validate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install deps (light)
+        run: |
+          python -m pip install --upgrade pip
+          pip install onnx onnxruntime numpy
+      - name: Convert ONNX to TensorRT (must run on GPU runner in prod)
+        run: |
+          echo "Note: TensorRT conversion requires NVIDIA toolkit on runner; this step is a placeholder to show intended CI job."
+      - name: Validate ONNX equivalence
+        run: |
+          python3 triton/validation/tensorrt_validate.py --onnx ./model/model.onnx --engine ./model/engine.plan || true
+
*** End Patch
*** End Patch
