*** Begin Patch
*** Add File:docs/CERTIFICATION_RUNBOOK.md
+# Aegis Certification Runbook
+
+Purpose
+- Provide an automated and manual runbook to validate Aegis for enterprise certification (HSM, Feast/Redis HA & SLOs, DeepSpeed multi-node training, DSR/erasure discovery, evidence bundling and legal handoff).
+
+Scope
+- Runs a set of automated checks and collects evidence into an evidence bundle for legal/security review.
+
+Prereqs
+- kubectl + argo CLI available (or GitHub Actions with KUBECONFIG secret)
+- AWS CLI configured (if using S3/MinIO) OR equivalent S3-compatible client
+- K8s cluster with GPU nodes and MPI Operator for DeepSpeed multi-node
+- Vendor HSM CLI/tools on a bastion or operator container for rotation commands
+- Helm and permissions to install Feast/Redis in staging namespace
+
+High-level Steps
+1) Prepare environment and secrets
+   - Provide KUBECONFIG for staging cluster
+   - Set MODEL_ARTIFACT_BUCKET and EVIDENCE_BUCKET env vars
+   - Set HSM_VENDOR and point to vendor CLI on bastion/container
+
+2) Run automated checks
+   - HSM vendor validation suite (ops/hsm/vendor_validation_suite.py)
+   - HSM vendor rotation (dry-run) using ops/hsm/vendor_rotation_runner.sh (operator must review vendor-specific scripts before enabling)
+   - Feast online SLO validation (argo/feast_slo_validation_workflow.yaml)
+   - Redis/Feast stream processor HA bench (argo/benchmarks/feast_redis_benchmark.yaml)
+   - DeepSpeed multi-node validation (deepspeed/mpi/deepspeed_mpi_job.yaml; MPI Operator required)
+   - Erasure discovery (ops/erasure/discover_and_erase_all.py --dry-run)
+
+3) Collect evidence
+   - assemble_audit_bundle.py collects MCPs, HSM reports, GE results, bench outputs and restores
+   - Upload bundle to s3://${EVIDENCE_BUCKET}/
+
+4) Legal handoff
+   - Create issue using compliance/audit_issue_template.md and attach evidence bundle
+   - Walk legal through artifacts (HSM validation report, erasure manifests, SLO reports)
+
+Acceptance Criteria (example)
+- HSM: sign success rate >= 99% and p95 latency <= vendor SLA in staging
+- Feast: online lookup P95 < 50ms under target QPS (bench)
+- DeepSpeed: multi-node job completes and reports throughput; checkpoint restore time < agreed RTO
+- Erasure: discover_and_erase_all.py covers known prefixes and produces manifest (dry-run before deletion)
+- Evidence bundle: contains all artifacts and uploaded to EVIDENCE_BUCKET
+
+Notes
+- Vendor-specific HSM rotation MUST be reviewed and enabled by operators; templates provided under ops/hsm/vendor_rotation_templates/.
+- Run the automated script scripts/certification/run_certification_checks.sh in CI or manually on a bastion.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/certification/run_certification_checks.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run the full certification automated checks and assemble evidence.
+#
+NAMESPACE="${NAMESPACE:-aegis}"
+EVIDENCE_BUCKET="${EVIDENCE_BUCKET:-}"
+MODEL_BUCKET="${MODEL_ARTIFACT_BUCKET:-}"
+OUT_DIR="${OUT_DIR:-/tmp/aegis_cert_checks}"
+mkdir -p "$OUT_DIR"
+
+echo "Starting Aegis certification checks..."
+echo "Environment: NAMESPACE=$NAMESPACE, EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-<not-set>}"
+
+echo "1) Run HSM healthcheck & validation suite"
+python3 ops/hsm/hsm_healthcheck.py --json > "$OUT_DIR"/hsm_health.json || true
+python3 ops/hsm/vendor_validation_suite.py --runs 20 > "$OUT_DIR"/hsm_validation.json || true
+
+echo "2) Run vendor rotation dry-run (operator must confirm vendor scripts first)"
+bash ops/hsm/vendor_rotation_runner.sh --dry-run >> "$OUT_DIR"/hsm_rotation_runner.log 2>&1 || true
+
+echo "3) Run Feast SLO validation (Argo)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/feast_slo_validation_workflow.yaml -n "$NAMESPACE" --watch --wait || true
+else
+  echo "argo CLI not found; skippig Argo-run. Please run argo/feast_slo_validation_workflow.yaml manually in $NAMESPACE"
+fi
+
+echo "4) Launch Redis/Feast bench (Argo)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/benchmarks/feast_redis_benchmark.yaml -n "$NAMESPACE" --watch --wait || true
+fi
+
+echo "5) Launch DeepSpeed multi-node validation (MPIJob)"
+# apply the MPIJob; requires MPI operator installed
+kubectl apply -f deepspeed/mpi/deepspeed_mpi_job.yaml -n "$NAMESPACE" || true
+
+echo "6) Erasure discovery (dry-run)"
+python3 ops/erasure/discover_and_erase_all.py --bucket "${MODEL_BUCKET}" --dry-run --out "$OUT_DIR/erasure_discovery.json" || true
+
+echo "7) Assemble audit bundle"
+python3 compliance/assemble_audit_bundle.py || true
+
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  echo "Uploading artifacts to s3://${EVIDENCE_BUCKET}/certification/"
+  aws s3 cp "$OUT_DIR"/hsm_validation.json "s3://${EVIDENCE_BUCKET}/certification/" || true
+  aws s3 cp /tmp/aegis_audit_bundle_*.tar.gz "s3://${EVIDENCE_BUCKET}/certification/" || true
+fi
+
+echo "Certification checks complete. Evidence stored in $OUT_DIR and uploaded to ${EVIDENCE_BUCKET:-<not-set>}"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/certification_checks.yml
+name: Certification checks
+on:
+  workflow_dispatch:
+
+jobs:
+  run-cert-checks:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install deps
+        run: pip install boto3 requests
+      - name: Run certification checks
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+          MODEL_ARTIFACT_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          echo "$KUBECONFIG" | base64 --decode > /github/home/.kube/config
+          bash scripts/certification/run_certification_checks.sh
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/vendor_rotation_templates/rotation_yubihsm.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example YubiHSM rotation script template.
+# Operators MUST test and adapt the commands below to their environment.
+#
+# Prereqs:
+# - yubihsmctl installed and configured on the runner
+# - appropriate authentication keys available (operator-managed)
+
+echo "YubiHSM rotation START at $(date -u)"
+echo "NOTE: The following commands are examples and are commented out. Uncomment and adapt carefully."
+
+cat <<'EOF'
+# Example commands (DO NOT RUN as-is):
+# Rotate key by creating new key and updating key label mapping for signing service.
+# NEW_KEY_ID=$(yubihsmctl -a change-auth-key ... create-asymmetric -a sign -t object-attached -l "aegis-signing-rot" ...)
+# Export or map new key into signing service config (e.g., update PKCS11 label in operator secret)
+# Verify new key can sign:
+# yubihsmctl sign -k $NEW_KEY_ID --data-file /tmp/health_payload -o /tmp/health_sig
+# Update services to use new label, perform smoke sign flows
+# Revoke / archive old key via vendor process
+EOF
+
+echo "YubiHSM rotation template complete. Replace placeholders and uncomment vendor commands. Log outputs to /tmp/hsm_rotation/"
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/vendor_rotation_templates/rotation_thales.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Thales / nShield rotation template (example). Replace placeholders with vendor commands and credentials.
+#
+echo "Thales (nShield) rotation template - fill vendor commands"
+cat <<'EOF'
+# Example pseudo-steps:
+# 1) Use vendor CLI to create new key container
+# 2) Export public key to operator signing config
+# 3) Run smoke sign tests via HSM-backed PKCS#11
+# 4) Update Kubernetes secret that points to HSM key/session
+# 5) Revoke old key container per vendor instructions
+EOF
+
+echo "Template complete. Edit with real vendor CLI steps and test in staging."
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/vendor_rotation_templates/rotation_utimaco.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Utimaco HSM rotation template - adapt per vendor docs.
+#
+echo "Utimaco rotation template - please populate with actual commands"
+cat <<'EOF'
+# Example:
+# utimaco-cli create-key --label aegis-sign-rot --type RSA --size 3072
+# utimaco-cli export-public --label aegis-sign-rot --out /tmp/new_pub.pem
+# Update signing service to use /tmp/new_pub.pem or label in PKCS#11 config
+# Validate by signing a health payload via signing helper
+EOF
+
+echo "Template ready. Replace with vendor-specific commands and validate in staging."
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/vendor_rotation_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper to run vendor-specific rotation script. Use HSM_VENDOR env var to select template.
+# Usage: ops/hsm/vendor_rotation_runner.sh [--dry-run]
+#
+DRY_RUN=0
+if [ "${1:-}" = "--dry-run" ]; then DRY_RUN=1; fi
+HSM_VENDOR="${HSM_VENDOR:-yubihsm}"
+LOGDIR=${LOGDIR:-/tmp/hsm_rotation}
+mkdir -p "$LOGDIR"
+TIMESTAMP=$(date -u +%Y%m%dT%H%M%SZ)
+LOG="$LOGDIR/rotation_runner_${HSM_VENDOR}_${TIMESTAMP}.log"
+
+echo "Rotation runner invoked for vendor=$HSM_VENDOR (dry_run=$DRY_RUN)" | tee "$LOG"
+case "$HSM_VENDOR" in
+  yubihsm)
+    SCRIPT="ops/hsm/vendor_rotation_templates/rotation_yubihsm.sh"
+    ;;
+  thales|nshield)
+    SCRIPT="ops/hsm/vendor_rotation_templates/rotation_thales.sh"
+    ;;
+  utimaco)
+    SCRIPT="ops/hsm/vendor_rotation_templates/rotation_utimaco.sh"
+    ;;
+  *)
+    echo "Unknown HSM_VENDOR=$HSM_VENDOR. Valid options: yubihsm, thales, utimaco" | tee -a "$LOG"
+    exit 2
+    ;;
+esac
+
+if [ "$DRY_RUN" -eq 1 ]; then
+  echo "DRY RUN: printing script contents for $SCRIPT" | tee -a "$LOG"
+  sed -n '1,200p' "$SCRIPT" | tee -a "$LOG"
+  exit 0
+fi
+
+echo "Executing vendor rotation script $SCRIPT" | tee -a "$LOG"
+bash "$SCRIPT" 2>&1 | tee -a "$LOG" || {
+  echo "Vendor rotation script failed; check $LOG" | tee -a "$LOG"
+  exit 3
+}
+
+echo "Rotation runner completed, log: $LOG"
+
*** End Patch
*** Begin Patch
*** Add File:argo/staged_validation_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: staged-validation-
+  namespace: aegis
+spec:
+  entrypoint: staged-validate
+  templates:
+    - name: staged-validate
+      steps:
+        - - name: feast-slo-validate
+            template: feast-slo-validate
+        - - name: redis-bench
+            template: redis-bench
+        - - name: deepspeed-validate
+            template: deepspeed-validate
+        - - name: erasure-discovery
+            template: erasure-dryrun
+        - - name: assemble-evidence
+            template: assemble-evidence
+
+    - name: feast-slo-validate
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests && python3 - <<'PY'
+import os,subprocess
+print("Running Feast SLO validation (Argo internal step).")
+# If external Argo job exists, you can call argo CLI; otherwise call the validation script or endpoint.
+os.system("python3 argo/feast_slo_validation_workflow.py 2>/dev/null || true")
+PY
+
+    - name: redis-bench
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install redis && python3 benchmarks/redis_latency_bench.py --host redis-headless.aegis.svc --port 6379 --duration 60 --qps 200 --output /tmp/redis_bench.json && cat /tmp/redis_bench.json
+      outputs:
+        artifacts:
+          - name: redis-bench-result
+            path: /tmp/redis_bench.json
+
+    - name: deepspeed-validate
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - kubectl apply -f deepspeed/mpi/deepspeed_mpi_job.yaml -n aegis || true && sleep 5 && kubectl get mpijob -n aegis || true
+
+    - name: erasure-dryrun
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 ops/erasure/discover_and_erase_all.py --bucket ${MODEL_ARTIFACT_BUCKET:-your-bucket} --dry-run --out /tmp/erasure_discovery.json && cat /tmp/erasure_discovery.json
+      outputs:
+        artifacts:
+          - name: erasure-discovery
+            path: /tmp/erasure_discovery.json
+
+    - name: assemble-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 compliance/assemble_audit_bundle.py || true
+
*** End Patch
*** Begin Patch
*** Add File:docs/STAGED_VALIDATION_PLAN.md
+# Staged Validation Plan (Feast/Redis & DeepSpeed)
+
+Overview
+- This plan describes a staged approach to validate Feast online HA & streaming guarantees and DeepSpeed multi-node training at scale.
+
+Phases
+Phase 0 — Prep (1–2 days)
+ - Provision staging cluster with GPU nodes, RDMA-capable network if available.
+ - Install MPI Operator, Helm, Redis storage class (fast NVMe), Prometheus adapter and Pushgateway.
+ - Create MODEL_ARTIFACT_BUCKET and EVIDENCE_BUCKET and ensure access.
+
+Phase 1 — Feast/Redis validation (2–4 days)
+ - Deploy RedisCluster with production storage class and PDB (redis/README_HELM_INSTALL.md).
+ - Deploy Feast with helm/feast/values-ha-redis-streaming.yaml.
+ - Deploy feast-stream-processor (feast/feast_stream_processor_high_availability.yaml).
+ - Run argo/feast_slo_validation_workflow.yaml and argo/benchmarks/feast_redis_benchmark.yaml.
+ - Tune Redis replicas, CPU/memory and placement until P95 < 50ms at target QPS.
+ - Acceptance: SLO met for 1 hour sustained under load test; metrics uploaded to MLflow.
+
+Phase 2 — DeepSpeed multi-node validation (3–7 days)
+ - Ensure MPI Operator installed and GPU nodes available (at least 2 nodes with 4 GPUs each for testing).
+ - Run deepspeed/mpi/deepspeed_mpi_job.yaml and monitor GPU utilization, communication overhead, time per step.
+ - Test ZeRO stage2 then stage3 (deepspeed/deepspeed_config_zero2.json, deepspeed/deepspeed_config_zero3.json).
+ - Measure checkpoint write time and restore drill (ops/dr/restore_drill.sh).
+ - Acceptance: job completes; GPU util >= 70% and checkpoint restore time < RTO.
+
+Phase 3 — Integrated run & evidence (1–2 days)
+ - Run argo/staged_validation_pipeline.yaml to orchestrate Feast SLO, Redis bench, DeepSpeed test and erasure discovery.
+ - Assemble audit bundle and hand off to legal for review.
+
+Notes
+- For RDMA/100Gbps testing, ensure network fabric and cloud instance types support it. Consider using bare metal or specialized instance types.
+- Record all bench outputs and upload to EVIDENCE_BUCKET for audit and reproducibility.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/feast/scale_redis.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple helper to patch RedisCluster replica count or statefulset replicas in the aegis namespace.
+#
+NAMESPACE=${NAMESPACE:-aegis}
+REPLICAS=${1:-3}
+echo "Scaling Redis replicas to ${REPLICAS} in namespace ${NAMESPACE}"
+
+# If using Bitnami redis-cluster helm release, patch via kubectl scaling statefulset or helm upgrade with values.
+kubectl -n "${NAMESPACE}" get statefulset -l app.kubernetes.io/name=redis-cluster -o name | while read ss; do
+  echo "Patching ${ss} to replicas=${REPLICAS}"
+  kubectl -n "${NAMESPACE}" scale "${ss}" --replicas="${REPLICAS}" || true
+done
+
+echo "Scaling complete. Verify pod readiness:"
+kubectl -n "${NAMESPACE}" get pods -l app.kubernetes.io/name=redis-cluster
+
*** End Patch
*** Begin Patch
*** Add File:README_CERTIFICATION_QUICKSTART.md
+# Quickstart: Run Certification Checks Locally / CI
+
+1) Set env vars:
+   export KUBECONFIG_BASE64=$(base64 -w0 ~/.kube/config)
+   export MODEL_ARTIFACT_BUCKET=<your-bucket>
+   export EVIDENCE_BUCKET=<your-evidence-bucket>
+   export HSM_VENDOR=yubihsm
+
+2) Run locally (requires tools):
+   pip install -r requirements.txt  # repo requirements if any
+   bash scripts/certification/run_certification_checks.sh
+
+3) Or dispatch GitHub Actions workflow "Certification checks" via UI (requires secrets configured)
+
+4) Inspect evidence bundle in /tmp and s3://${EVIDENCE_BUCKET}/certification/
+
*** End Patch
*** End Patch
