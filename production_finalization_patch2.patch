*** Begin Patch
*** Add File: mlops/ha/postgres_operator_cr_ha.yaml
+---
+# Zalando Postgres Operator CR for HA Postgres (MLflow metadata)
+# Requires: Zalando Postgres Operator installed.
+apiVersion: "acid.zalan.do/v1"
+kind: postgresql
+metadata:
+  name: mlflow
+  namespace: aegis-ml
+spec:
+  teamId: "aegis"
+  volume:
+    size: 200Gi
+    storageClass: "gp2" # replace with your production storage class
+  numberOfInstances: 3
+  users:
+    mlflow_user:
+    - superuser
+    - createdb
+  databases:
+    mlflow: mlflow_user
+  postgresql:
+    version: "14"
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "1"
+      memory: "2Gi"
+  patroni:
+    initdb:
+      encoding: "UTF8"
+      locale: "en_US.UTF-8"
+  podAntiAffinity: preferred
+
+---
+# Notes:
+# - This CR creates a 3-node HA Postgres with replication and automatic failover.
+# - Configure pgbackrest (or cloud-managed PITR) for WAL archiving and point-in-time recovery.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/ha/rds_provision_template.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# rds_provision_template.sh
+# Template to provision a Multi-AZ RDS Postgres instance (AWS CLI). Review before running.
+#
+DB_IDENTIFIER="${DB_IDENTIFIER:-aegis-mlflow}"
+DB_NAME="${DB_NAME:-mlflow}"
+DB_INSTANCE_CLASS="${DB_INSTANCE_CLASS:-db.m6g.large}"
+MASTER_USERNAME="${MASTER_USERNAME:-mlflow_admin}"
+MASTER_PASSWORD="${MASTER_PASSWORD:-REPLACE_WITH_STRONG_PASSWORD}"
+ALLOCATED_STORAGE="${ALLOCATED_STORAGE:-100}"
+SUBNET_GROUP="${SUBNET_GROUP:-default}"
+SECURITY_GROUP_IDS="${SECURITY_GROUP_IDS:-sg-0123456789abcdef0}"
+REGION="${AWS_REGION:-us-west-2}"
+
+echo "Creating Multi-AZ RDS PostgreSQL instance: $DB_IDENTIFIER"
+aws rds create-db-instance \
+  --db-instance-identifier "$DB_IDENTIFIER" \
+  --allocated-storage "$ALLOCATED_STORAGE" \
+  --db-instance-class "$DB_INSTANCE_CLASS" \
+  --engine postgres \
+  --engine-version 14.6 \
+  --master-username "$MASTER_USERNAME" \
+  --master-user-password "$MASTER_PASSWORD" \
+  --multi-az \
+  --backup-retention-period 7 \
+  --publicly-accessible false \
+  --vpc-security-group-ids $SECURITY_GROUP_IDS \
+  --db-subnet-group-name "$SUBNET_GROUP" \
+  --region "$REGION"
+
+echo "After creation, enable automated backups and configure CloudWatch metrics/alarms for failover detection."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/ha/pgbackrest_notes.md
+pgBackRest & PITR notes
+------------------------
+
+- For operator-managed Postgres, use pgBackRest for consistent backups and WAL archiving.
+- Recommendations:
+  - Store WAL and base backups in a CMK-encrypted bucket.
+  - Configure full weekly backups + daily incremental + continuous WAL archive.
+  - Test PITR restores monthly and record restore time.
+  - Use a separate test namespace/cluster for restores to avoid overwriting production.
+
+This repository includes a pg_dump CronJob example, but pgBackRest (or managed cloud PITR) is recommended for production.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/auth/oauth2_proxy_full.yaml
+---
+# oauth2-proxy deployment to front MLflow and enforce SSO / group claims
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-ml
+
+---
+apiVersion: v1
+kind: Secret
+metadata:
+  name: oauth2-proxy-secret
+  namespace: aegis-ml
+type: Opaque
+stringData:
+  client_id: "REPLACE_WITH_CLIENT_ID"
+  client_secret: "REPLACE_WITH_CLIENT_SECRET"
+  cookie_secret: "REPLACE_WITH_BASE64_32_BYTES"
+  issuer_url: "https://idp.example.com"
+
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: oauth2-proxy
+  namespace: aegis-ml
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: oauth2-proxy
+  template:
+    metadata:
+      labels:
+        app: oauth2-proxy
+    spec:
+      containers:
+      - name: oauth2-proxy
+        image: quay.io/oauth2-proxy/oauth2-proxy:v7.4.0
+        ports:
+        - containerPort: 4180
+        env:
+        - name: OAUTH2_PROXY_PROVIDER
+          value: "oidc"
+        - name: OAUTH2_PROXY_CLIENT_ID
+          valueFrom:
+            secretKeyRef:
+              name: oauth2-proxy-secret
+              key: client_id
+        - name: OAUTH2_PROXY_CLIENT_SECRET
+          valueFrom:
+            secretKeyRef:
+              name: oauth2-proxy-secret
+              key: client_secret
+        - name: OAUTH2_PROXY_COOKIE_SECRET
+          valueFrom:
+            secretKeyRef:
+              name: oauth2-proxy-secret
+              key: cookie_secret
+        - name: OAUTH2_PROXY_OIDC_ISSUER_URL
+          valueFrom:
+            secretKeyRef:
+              name: oauth2-proxy-secret
+              key: issuer_url
+        - name: OAUTH2_PROXY_UPSTREAMS
+          value: "http://mlflow:5000/"
+        - name: OAUTH2_PROXY_EMAIL_DOMAINS
+          value: "*"
+        args:
+          - --provider=oidc
+          - --redirect-url=https://mlflow.example.com/oauth2/callback
+          - --cookie-secure=true
+          - --set-authorization-header=true
+          - --pass-access-token=true
+          - --pass-user-headers=true
+          - --skip-provider-button=true
+          - --scope="openid email profile groups"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: oauth2-proxy
+  namespace: aegis-ml
+spec:
+  selector:
+    app: oauth2-proxy
+  ports:
+    - port: 4180
+      targetPort: 4180
+
+---
+# Ingress routing MLflow via oauth2-proxy (nginx example)
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: mlflow-ingress
+  namespace: aegis-ml
+  annotations:
+    kubernetes.io/ingress.class: nginx
+    nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.aegis-ml.svc.cluster.local:4180/oauth2/auth"
+    nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$request_uri"
+spec:
+  rules:
+  - host: mlflow.example.com
+    http:
+      paths:
+      - path: /
+        pathType: Prefix
+        backend:
+          service:
+            name: oauth2-proxy
+            port:
+              number: 4180
+
+---
+# Notes:
+# - Replace mlflow.example.com with your domain; configure DNS and TLS (cert-manager or external LB).
+# - Configure IdP client with redirect to https://<host>/oauth2/callback and request groups claim.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/auth/fluentbit_audit_config.yaml
+---
+# Fluent Bit config to collect oauth2-proxy and mlflow logs and forward to SIEM (HTTP endpoint or cloud)
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluent-bit-config
+  namespace: kube-system
+data:
+  fluent-bit.conf: |
+    [SERVICE]
+        Flush        1
+        Daemon       Off
+        Log_Level    info
+
+    [INPUT]
+        Name              tail
+        Path              /var/log/containers/*oauth2-proxy*.log,/var/log/containers/*mlflow*.log
+        Parser            docker
+        Tag               audit.*
+
+    [FILTER]
+        Name              kubernetes
+        Match             audit.*
+
+    [OUTPUT]
+        Name              http
+        Match             audit.*
+        Host              siem.example.com
+        Port              443
+        URI               /ingest
+        TLS               On
+        Format            json
+
+---
+# Deploy Fluent Bit DaemonSet using this config (not included here). Ensure SIEM endpoint and auth (API key) are set.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/lakefs/helm-values-production.yaml
+---
+# LakeFS Helm values (production)
+service:
+  type: ClusterIP
+
+storage:
+  type: s3
+  s3:
+    region: "{{AWS_REGION}}"
+    bucket: "{{MLFLOW_ARTIFACT_BUCKET}}"
+    endpoint: "{{MLFLOW_S3_ENDPOINT}}"
+
+server:
+  replicas: 2
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "1"
+      memory: "2Gi"
+
+database:
+  postgres:
+    host: postgres-mlflow.aegis-ml.svc.cluster.local
+    user: lakefs
+    password: "{{LAKEFS_DB_PASSWORD}}"
+    dbname: lakefs
+
+auth:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: mlops/argo/train_pipeline_prod_with_lakefs.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-ml-train-prod-
+spec:
+  entrypoint: train-validate-register
+  serviceAccountName: aegis-ml-operator
+  templates:
+  - name: train-validate-register
+    outputs:
+      parameters:
+      - name: snapshot_id
+      - name: run_id
+    steps:
+    - - name: snapshot-data
+        template: snapshot-data
+    - - name: train
+        template: train
+    - - name: validate
+        template: validate
+    - - name: register
+        template: register
+
+  - name: snapshot-data
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          ./scripts/lakefs_commit_and_tag.sh
+          cat /tmp/snapshot_id
+    outputs:
+      parameters:
+      - name: snapshot_id
+        valueFrom:
+          path: /tmp/snapshot_id
+
+  - name: train
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID=$(python3 scripts/train_and_register.py --mode train-only | tail -n1)
+          echo "$RUN_ID" > /tmp/run_id
+    outputs:
+      parameters:
+      - name: run_id
+        valueFrom:
+          path: /tmp/run_id
+    resources:
+      requests:
+        cpu: "4"
+        memory: "16Gi"
+        nvidia.com/gpu: 1
+    nodeSelector:
+      aegis/gpu: "true"
+    tolerations:
+    - key: "nvidia.com/gpu"
+      operator: "Exists"
+
+  - name: validate
+    inputs:
+      parameters:
+      - name: run_id
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID="{{inputs.parameters.run_id}}"
+          python3 mlops/scripts/validate_model.py --run-id $RUN_ID --min-accuracy 0.80
+
+  - name: register
+    inputs:
+      parameters:
+      - name: run_id
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID="{{inputs.parameters.run_id}}"
+          python3 scripts/train_and_register.py --mode register --run-id $RUN_ID
+
+  volumes:
+  - name: work
+    emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/argo_wait_fetch_promote_robust.yml
+name: Argo Robust Wait, Fetch Artifacts, Validate & Promote
+
+on:
+  workflow_dispatch:
+    inputs:
+      workflow_prefix:
+        description: "Argoworkflow name prefix (e.g., aegis-ml-train-prod-)"
+        required: true
+
+permissions:
+  contents: read
+  id-token: write
+
+jobs:
+  wait-validate-promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Restore kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG_BASE64 }}" | base64 --decode > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+        env:
+          KUBECONFIG_BASE64: ${{ secrets.KUBECONFIG_BASE64 }}
+
+      - name: Install Argo CLI
+        run: |
+          curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo-workflows/releases/latest/download/argo-linux-amd64
+          chmod +x /usr/local/bin/argo
+
+      - name: Find latest workflow & wait
+        id: wf
+        run: |
+          PREFIX="${{ github.event.inputs.workflow_prefix }}"
+          NAMESPACE="${{ secrets.K8S_NAMESPACE }}"
+          WF=$(kubectl -n "$NAMESPACE" get wf -o name | grep "$PREFIX" | tail -n1 | sed 's|workflow.argoproj.io/||')
+          if [ -z "$WF" ]; then echo "No workflow found"; exit 1; fi
+          echo "workflow=$WF" >> $GITHUB_OUTPUT
+          argo wait -n "$NAMESPACE" "$WF" --timeout 2h || (echo "workflow failed or timed out"; exit 1)
+          argo get -n "$NAMESPACE" "$WF" -o json > /tmp/argo_${WF}.json
+          echo "::set-output name=workflow_json_path::/tmp/argo_${WF}.json"
+
+      - name: Extract run_id & snapshot_id robustly
+        id: extract
+        run: |
+          JSON_PATH="${{ steps.wf.outputs.workflow_json_path }}"
+          RUN_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="run_id") .value' "$JSON_PATH" || echo "")
+          if [ -z "$RUN_ID" ]; then
+            RUN_ID=$(jq -r '.. | objects | .value? // empty' "$JSON_PATH" | grep -Eo '[0-9a-f\-]{8,}' | tail -n1 || echo "")
+          fi
+          SNAPSHOT_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="snapshot_id") .value' "$JSON_PATH" || echo "")
+          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
+          echo "snapshot_id=$SNAPSHOT_ID" >> $GITHUB_OUTPUT
+
+      - name: Validate MLflow run metadata & provenance
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          RUN_ID: ${{ steps.extract.outputs.run_id }}
+        run: |
+          if [ -z "$RUN_ID" ]; then echo "No run id found"; exit 1; fi
+          pip install mlflow
+          python3 - <<PY
+import os,sys
+from mlflow.tracking import MlflowClient
+client = MlflowClient(tracking_uri=os.environ.get("MLFLOW_TRACKING_URI"))
+runid = os.environ.get("RUN_ID")
+run = client.get_run(runid)
+tags = run.data.tags
+required = ["git.sha","lakefs.commit","trainer.image","image.digest"]
+missing = [r for r in required if not tags.get(r)]
+if missing:
+    print("Missing provenance tags:", missing)
+    sys.exit(2)
+print("Provenance tags present")
+PY
+
+      - name: Download artifacts and run validation
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          RUN_ID: ${{ steps.extract.outputs.run_id }}
+        run: |
+          pip install mlflow boto3
+          python3 - <<PY
+import os,mlflow,tempfile
+from mlflow.tracking import MlflowClient
+client = MlflowClient(tracking_uri=os.environ.get("MLFLOW_TRACKING_URI"))
+run_id = os.environ.get("RUN_ID")
+local = tempfile.mkdtemp()
+client.download_artifacts(run_id, "model", local)
+print("Downloaded model to", local)
+PY
+          python3 mlops/scripts/validate_model.py --run-id "${{ steps.extract.outputs.run_id }}" --min-accuracy 0.80
+
+      - name: If validation fails -> create rollback issue and abort promotion
+        if: failure()
+        run: |
+          RUN_ID=${{ steps.extract.outputs.run_id }}
+          gh issue create --title "ML validation failed for run ${RUN_ID}" --body "Validation failed. Please investigate and rollback if necessary." || true
+          exit 1
+
+      - name: Create promotion PR (on success)
+        if: success()
+        run: |
+          RUN_ID=${{ steps.extract.outputs.run_id }}
+          SNAP=${{ steps.extract.outputs.snapshot_id }}
+          gh pr create --title "Promote model run ${RUN_ID} to staging/canary" --body "Automated promotion PR for run ${RUN_ID}\n\nsnapshot: ${SNAP}" || true
+
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: argo-artifacts
+          path: /tmp/argo_*.json || true
+
*** End Patch
*** Begin Patch
*** Add File: mlops/serving/destinationrule_and_vs.yaml
+apiVersion: networking.istio.io/v1alpha3
+kind: DestinationRule
+metadata:
+  name: aegis-ml-canary-drule
+  namespace: aegis-ml
+spec:
+  host: aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local
+  subsets:
+  - name: stable
+    labels:
+      version: stable
+  - name: canary
+    labels:
+      version: canary
+
+---
+apiVersion: networking.istio.io/v1alpha3
+kind: VirtualService
+metadata:
+  name: aegis-ml-canary-vs
+  namespace: aegis-ml
+spec:
+  hosts:
+  - "aegis-ml-canary.aegis-ml.svc.cluster.local"
+  http:
+  - route:
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: stable
+      weight: 90
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: canary
+      weight: 10
+
+---
+# Note: Ensure predictor Deployment pods carry label version=stable|canary for subsets to match.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/serving/keda_hpa_examples.yaml
+---
+# KEDA ScaledObject example for model autoscaling (requires KEDA installed)
+apiVersion: keda.sh/v1alpha1
+kind: ScaledObject
+metadata:
+  name: aegis-model-scaledobject
+  namespace: aegis-ml
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-ml-canary-predictor-default
+  pollingInterval: 30
+  cooldownPeriod: 300
+  minReplicaCount: 1
+  maxReplicaCount: 10
+  triggers:
+  - type: prometheus
+    metadata:
+      serverAddress: http://prometheus.monitoring.svc.cluster.local
+      metricName: model_inference_requests_total
+      query: sum(rate(model_inference_requests_total{service="aegis-ml-canary"}[1m]))
+      threshold: "50"
+
+---
+# HPA fallback using CPU utilization
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-ml-canary-hpa
+  namespace: aegis-ml
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-ml-canary-predictor-default
+  minReplicas: 1
+  maxReplicas: 5
+  metrics:
+  - type: Resource
+    resource:
+      name: cpu
+      target:
+        type: Utilization
+        averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/production_drift_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-production-drift-alerts
+  namespace: monitoring
+spec:
+  groups:
+  - name: aegis-drift.rules
+    rules:
+    - alert: FeatureDriftHigh
+      expr: |
+        avg_over_time(ml_feature_mean_diff[10m]) > 0.1
+      for: 10m
+      labels:
+        severity: warning
+      annotations:
+        summary: "High per-feature drift detected"
+        description: "Feature mean difference exceeds threshold; investigate and consider rollback."
+
+    - alert: ModelLatencyHigh
+      expr: histogram_quantile(0.95, sum(rate(model_inference_latency_seconds_bucket[5m])) by (le)) > 1
+      for: 5m
+      labels:
+        severity: critical
+      annotations:
+        summary: "Model 95th percentile latency > 1s"
+        description: "Investigate serving performance and autoscaling."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/scrape_inference_config.yaml
+---
+# Prometheus scrape config snippet for inference services
+scrape_configs:
+- job_name: 'aegis-model-inference'
+  metrics_path: /metrics
+  scrape_interval: 15s
+  static_configs:
+  - targets:
+    - aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local:8000
+    - aegis-ml-stable-predictor-default.aegis-ml.svc.cluster.local:8000
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/instrument_inference.py
+#!/usr/bin/env python3
+"""
+Instrumentation for inference container to export Prometheus metrics:
+ - per-feature running mean (Gauge)
+ - request counts and latency (Counter/Histogram)
+
+Integrate into your inference server to update metrics on every request.
+"""
+from prometheus_client import start_http_server, Counter, Histogram, Gauge
+import time
+import threading
+
+REQUEST_COUNT = Counter('model_inference_requests_total', 'Total inference requests', ['model', 'service'])
+REQUEST_LATENCY = Histogram('model_inference_latency_seconds', 'Inference latency seconds', ['model', 'service'])
+FEATURE_MEAN = Gauge('model_feature_mean', 'Feature mean (latest)', ['feature'])
+
+def update_feature_stats(feature_values):
+    # feature_values: dict of {feature_name: value}
+    for fname, val in feature_values.items():
+        FEATURE_MEAN.labels(feature=fname).set(float(val))
+
+def observe_request(model='aegis-model', service='aegis-ml-canary', latency=0.0):
+    REQUEST_COUNT.labels(model=model, service=service).inc()
+    REQUEST_LATENCY.labels(model=model, service=service).observe(latency)
+
+def start_metrics_server(port=8000):
+    start_http_server(port)
+    while True:
+        time.sleep(60)
+
+if __name__ == "__main__":
+    t = threading.Thread(target=start_metrics_server, args=(8000,), daemon=True)
+    t.start()
+    # Example usage: integrate update_feature_stats and observe_request into your request handler
+    while True:
+        time.sleep(30)
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/sbom_trivy_block_and_issue.yml
+name: SBOM & Trivy Enforcement (block & create issue)
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+permissions:
+  contents: read
+
+jobs:
+  sbom-enforce:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build trainer image
+        run: |
+          IMAGE=ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:pr-${{ github.event.number }}
+          docker build -t $IMAGE -f mlops/docker/Dockerfile .
+          echo $IMAGE > image.txt
+      - name: Generate SBOM (Syft)
+        run: |
+          IMAGE=$(cat image.txt)
+          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+          syft $IMAGE -o json > sbom.json
+      - name: Trivy scan (block on HIGH/CRITICAL)
+        id: trivy
+        run: |
+          IMAGE=$(cat image.txt)
+          curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+          trivy image --exit-code 1 --severity HIGH,CRITICAL --format json --output trivy-report.json $IMAGE || true
+          jq '.Results[] | select(.Vulnerabilities != null) | .Vulnerabilities[] | select(.Severity=="CRITICAL" or .Severity=="HIGH")' trivy-report.json > criticals.json || true
+          if [ -s criticals.json ]; then
+             echo "Vulnerabilities found"
+             gh issue create --title "Security Vulnerabilities in trainer image (PR #${{ github.event.number }})" --body "Trivy found HIGH/CRITICAL vulnerabilities. See artifacts." || true
+             exit 1
+          fi
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom-trivy
+          path: sbom.json,trivy-report.json || true
+
*** End Patch
*** Begin Patch
*** Add File: mlops/multi_tenant/gatekeeper_namespace_label.yaml
+---
+# Gatekeeper ConstraintTemplate + Constraint to require 'tenant' label on Namespaces
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequirednamespacelabel
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequiredNamespaceLabel
+  targets:
+  - target: admission.k8s.gatekeeper.sh
+    rego: |
+      package k8srequirednamespacelabel
+      violation[{"msg": msg}] {
+        input.request.kind.kind == "Namespace"
+        not input.request.object.metadata.labels.tenant
+        msg := "Namespace must have label 'tenant'"
+      }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequiredNamespaceLabel
+metadata:
+  name: require-tenant-label
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Namespace"]
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/enforce_provenance_on_promotion.yml
+name: Enforce Provenance on Promotion PRs
+
+on:
+  pull_request:
+    types: [opened, reopened, labeled]
+
+permissions:
+  contents: read
+
+jobs:
+  check-provenance:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Check PR body for run_id and lakefs snapshot
+        run: |
+          BODY="${{ github.event.pull_request.body }}"
+          echo "PR Body: $BODY"
+          if ! echo "$BODY" | grep -E "run[ _-]?id[: ]|runs:/" >/dev/null; then
+            echo "Promotion PR missing MLflow run id in body. Please include run id and lakefs.commit."
+            gh pr comment ${{ github.event.pull_request.number }} --body "Promotion PR missing MLflow run id and provenance tags. Add 'run_id: <id>' and 'lakefs.commit: <commit>' to PR body." || true
+            exit 1
+          fi
+      - name: Require CODEOWNERS approval (reminder)
+        run: |
+          echo "Ensure CODEOWNERS reviews infra and security signoff before merging. Branch protection should enforce CODEOWNERS."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/dr/run_restore_and_measure.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# run_restore_and_measure.sh
+# Automate DR restore and measure restore time; produce artifacts for signoff.
+#
+OUTPUT_DIR="${OUTPUT_DIR:-/tmp/aegis-dr}"
+mkdir -p "$OUTPUT_DIR"
+
+if [ -z "${MLFLOW_BACKUP_BUCKET:-}" ]; then
+  echo "Set MLFLOW_BACKUP_BUCKET"
+  exit 2
+fi
+
+LATEST=$(aws s3 ls "s3://$MLFLOW_BACKUP_BUCKET/backups/mlflow-db/" --region "${AWS_REGION:-us-west-2}" | sort | tail -n1 | awk '{print $4}')
+if [ -z "$LATEST" ]; then
+  echo "No backups found"
+  exit 1
+fi
+S3PATH="s3://$MLFLOW_BACKUP_BUCKET/backups/mlflow-db/$LATEST"
+echo "Latest backup: $S3PATH"
+
+START_TS=$(date +%s)
+aws s3 cp "$S3PATH" "$OUTPUT_DIR/$LATEST" --region "${AWS_REGION:-us-west-2}"
+echo "Downloaded backup"
+
+echo "Restoring..."
+gunzip -c "$OUTPUT_DIR/$LATEST" | psql "$PGDATABASE" -h "$PGHOST" -U "$PGUSER"
+END_TS=$(date +%s)
+ELAPSED=$((END_TS - START_TS))
+echo "Restore completed in $ELAPSED seconds" | tee "$OUTPUT_DIR/restore_time.txt"
+
+psql -h "$PGHOST" -U "$PGUSER" -d "$PGDATABASE" -c "SELECT count(*) FROM experiments;" > "$OUTPUT_DIR/experiments_count.txt" || true
+
+tar -czf "$OUTPUT_DIR/aegis-dr-artifacts-$(date -u +%Y%m%dT%H%M%SZ).tgz" -C "$OUTPUT_DIR" .
+echo "DR artifacts: $OUTPUT_DIR"
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/chaos_model_serving.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple chaos exercise for KServe predictor to validate autoscaler and rollback behavior.
+#
+NAMESPACE="${1:-aegis-ml}"
+DEPLOYMENT="${2:-aegis-ml-canary-predictor-default}"
+DURATION="${3:-30}"  # seconds to keep scaled down
+
+echo "Starting chaos: scale $DEPLOYMENT in namespace $NAMESPACE to 0 for $DURATION seconds"
+kubectl -n "$NAMESPACE" scale deployment "$DEPLOYMENT" --replicas=0
+sleep "$DURATION"
+echo "Restoring replicas to 1"
+kubectl -n "$NAMESPACE" scale deployment "$DEPLOYMENT" --replicas=1
+echo "Chaos exercise complete. Monitor Prometheus and KServe for anomalies and recovery."
+
*** End Patch
*** Begin Patch
*** Add File: docs/prod_finalization_runbook.md
+Final Production Finalization Runbook (Aegis MLOps)
+===============================================
+
+This runbook summarizes the actions required to declare Aegis "fully AI" production-ready for metadata, auth, lineage, CI/CD, serving, observability, supply-chain, multi-tenant and DR.
+
+1) HA Postgres validation
+ - Confirm the Zalando Postgres CR is Healthy with 3 instances.
+ - Induce failover (delete primary pod) and record failover time.
+ - Run pgBackRest/PITR restore test and capture restore time via mlops/dr/run_restore_and_measure.sh.
+ - Signoff: SRE lead documents failover & restore times and approves.
+
+2) OAuth2 / RBAC verification
+ - Configure IdP client, ensure oauth2-proxy is deployed, and confirm login flows for admin/user groups.
+ - Verify MLflow admin vs user API access is enforced by proxy/gateway.
+ - Ensure oauth2-proxy + mlflow logs are collected by Fluent Bit and forwarded to SIEM; verify retention policy.
+
+3) LakeFS & Argo snapshot validation
+ - Install LakeFS (helm values provided).
+ - Run the Argo pipeline and confirm lakefs.commit exists in MLflow run tags.
+ - If snapshot step fails, Argo pipeline must stop; fix and re-run.
+
+4) CI/CD automation & provenance enforcement
+ - Run the Argo->CI workflow (argo_wait_fetch_promote_robust.yml) to wait for workflow completion, fetch artifacts, validate, and auto-create a promotion PR.
+ - Ensure promotion PR contains run_id, lakefs.commit and image.digest; enforce CODEOWNERS for final merge.
+
+5) Serving & canary tests
+ - Deploy canary using provided DestinationRule/VirtualService.
+ - Run canary load test; verify autoscaler behavior (KEDA/HPA).
+ - Execute canary_rollback.sh to verify rollback actions.
+
+6) Observability & drift
+ - Deploy instrumented inference container; verify Prometheus scrapes metrics and Grafana panels show accuracy/latency/drift.
+ - Simulate drift and verify alert routing to Slack/Pager.
+
+7) Supply-chain enforcement
+ - Ensure SBOM/Trivy workflow blocks PRs with HIGH/CRITICAL vulnerabilities and creates issues.
+ - Maintain pinned base images and a remediation process.
+
+8) Multi-tenant enforcement
+ - Gatekeeper must block namespace creation without tenant label.
+ - Roll out resource quotas and export billing metrics for chargeback.
+
+9) DR & Chaos
+ - Schedule monthly DR exercise; run restore and smoke validation and collect artifacts.
+ - Schedule quarterly chaos model serving tests and collect SLO metrics.
+
+Signoff template:
+ - SRE lead:
+ - Security lead:
+ - Date:
+ - Failover time:
+ - Restore time:
+ - Validation pass rate:
+
*** End Patch
*** End Patch
