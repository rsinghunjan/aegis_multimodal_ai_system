*** Begin Patch
*** Add File:docs/WORLD_MODEL_COMPUTE_SIZING.md
+# World Model Cost & Compute Sizing Guidance (Aegis)
+
+This document gives practical sizing guidance (GPU choices, node counts, NVMe, network, memory) for different world-model scales. Use it as a starting point and adapt to your dataset size, model architecture and budget.
+
+Key variables to estimate:
+- Model parameter count (P)
+- Global batch size (B) and micro-batch size per GPU (b)
+- Sequence length (T) / frames-per-sample for dynamics models
+- Dataset size (TB) and checkpoint retention
+- Target time-to-train (days)
+
+Guidance by model size
+1) Small world model (P < 100M)
+- Use: single-node multi-GPU or 1-2 x 8-GPU (A100-40GB or T4s for cheaper)
+- GPUs: 1–4 x NVIDIA A100-40GB or 4–8 x T4 (if CPU-bound tasks small)
+- CPU: 16–32 vCPUs per node
+- RAM: 128–256 GB
+- NVMe: 1–2 TB local NVMe for shuffle and dataset cache
+- Network: 10–25 Gbps is OK for single-node; 25Gbps for 2+ node training
+- Use cases: prototyping, small-scale latent dynamics, short sequences
+
+2) Medium world model (100M <= P < 1B)
+- Use: multi-node training with model/data parallel; 2–8 A100-40GB GPUs
+- GPUs: 4–16 x A100-40GB (or A100-80GB if budget allows)
+- CPU per GPU: ~8 vCPUs/GPU (e.g., 64–128 vCPUs total)
+- RAM per node: 256–512 GB
+- NVMe: 2–4 TB local NVMe per node (fast shuffle and checkpoint staging)
+- Network: 25–100 Gbps (RDMA preferred for multi-node allreduce)
+- Storage: object store (MinIO) with high throughput; consider prefetching shards to local NVMe
+- Use cases: higher-fidelity dynamics, multi-modal small transformers
+
+3) Large world model (1B <= P < 10B)
+- Use: model parallelism (Megatron/DeepSpeed) across many GPUs
+- GPUs: 8–64 x A100-80GB or H100 if available (H100 faster for large batch)
+- CPU per GPU: ~8–16 vCPUs/GPU
+- RAM per node: 512 GB+
+- NVMe: 4–8 TB local NVMe per node; provision for frequent checkpoint writes
+- Network: 100 Gbps RDMA with low latency; prefer fat-tree or HSN
+- Storage: high-throughput S3 gateway (MinIO distributed) and separate scratch NVMe pools
+- Use cases: large multimodal transformers, joint perception+prediction models
+
+4) Very large world model (10B+)
+- Use: many-node clusters, H100 preferred, optimized model parallel frameworks
+- GPUs: 64+ H100 or A100-80GB in model-parallel configurations
+- Network: 100 Gbps RDMA with low latency, optimized NICs and switches
+- NVMe: 8+ TB local NVMe per node; aggressive checkpointing and incremental checkpoint strategies
+- Use cases: foundation-level world models for multi-task planning
+
+Storage & checkpointing
+- Checkpoints are large: estimate checkpoint size = model parameters * 4 bytes (FP32) / (1024^3) for GB. Multiply by number of optimizer states (ZeRO stages increases size).
+- Keep at least 2–3 recent checkpoints locally and a rolling copy to S3/MinIO. Sign each checkpoint (MCP + HSM) for provenance.
+- Use incremental checkpointing (DeepSpeed) and avoid storing too many full checkpoints to save capacity.
+
+Networking & locality
+- Co-locate NVMe with GPUs for shuffle and data staging.
+- Use RDMA (RoCEv2) if performing heavy allreduce or model parallel comms.
+- For inference & planner latency, colocate Redis/Feature cache and vector DB on same rack or low-latency path.
+
+Cost estimation rules of thumb (rough)
+- One A100-80GB node (8 GPUs) in cloud ~ $8–20/hr depending on provider and commitment.
+- Running a 1B parameter model training for one week can cost several thousand USD on dozens of GPUs.
+- Benchmark on small scale to estimate per-step cost, then scale linearly (accounting for inefficiencies).
+
+Recommendations
+- Start with a medium configuration in staging to validate pipeline and SLOs.
+- Use mixed precision (AMP / bfloat16) and gradient accumulation to reduce memory needs.
+- Use local NVMe for shuffle and prefetch dataset shards from object store to NVMe during job startup.
+- Always prototype cost using a scaled-down run (e.g., 1/10th dataset) to measure throughput and then extrapolate.
+
+Checklist to size your cluster
+1. Estimate model size (P) and optimizer multiplier (x1 for AdamW ~ 3x if storing optimizer states).
+2. Choose per-GPU micro-batch that fits memory; compute global batch = micro-batch * GPUs * gradient_accum.
+3. Measure throughput (samples/s) and time per epoch; compute epochs required.
+4. Compute checkpoint storage & nightly backup needs.
+5. Add margin: +20–30% GPUs and storage for safety.
+
*** End Patch
*** Begin Patch
*** Add File:simulator/ISAAC/README.md
+Isaac Gym / Isaac Sim Integration (scaffold)
+
+Overview
+- This folder contains a simple scaffold to run episodes in NVIDIA Isaac Gym/Isaac Sim and export dataset traces suitable for world-model training. The exporter writes episode traces (per-frame sensors, actions, rewards) into a Parquet or WebDataset-compatible layout and can upload to S3/MinIO.
+
+Notes & prerequisites
+- Isaac Gym / Isaac Sim has licensing and hardware (GPU) requirements. Follow NVIDIA docs to obtain installer.
+- This scaffold is designed for local / staging runs or a dedicated GPU node. For cloud, run on GPU nodes with passthrough.
+- Python dependencies: numpy, pyarrow, pandas, boto3
+
+Files
+- export_dataset.py : small script to control an Isaac environment, collect episodes and write to disk or S3.
+- config.yaml : sample config for episodes and sensors.
+
+How to run (dev)
+1. Ensure Isaac Gym or Isaac Sim is installed and can be imported from Python.
+2. Edit config.yaml to set episode length, number of episodes and output path.
+3. Run:
+   python3 simulator/ISAAC/export_dataset.py --config simulator/ISAAC/config.yaml
+
+Output
+- Writes files under output_dir/<date>/episode_<id>.parquet containing columns:
+  - episode_id (str), step (int), timestamp (float), observation (json string or ndarray serialized), action (json), reward (float), done (bool)
+
+S3 upload
+- Set MODEL_ARTIFACT_BUCKET and AWS credentials in env; the script will upload resulting directory to s3://<bucket>/simulator/<run>/
+
+Provenance
+- Each export writes a manifest.json with simulator version, random seed and config. This manifest is signed via the platform signing helper if available.
+
*** End Patch
*** Begin Patch
*** Add File:simulator/ISAAC/export_dataset.py
+#!/usr/bin/env python3
+"""
+Simple Isaac Gym export script (minimal / pseudo-code)
+This demonstrates how to capture episodes and store as Parquet or upload to S3.
+Replace the stubbed 'simulate_step' with actual Isaac Gym step calls.
+"""
+import argparse, os, json, time
+import numpy as np
+import pandas as pd
+import pyarrow as pa, pyarrow.parquet as pq
+
+def simulate_episode(cfg, episode_id):
+    # Replace this function with actual Isaac Gym environment stepping
+    steps = []
+    for step in range(cfg["episode_length"]):
+        obs = np.random.randn(cfg["obs_dim"]).astype(np.float32)
+        action = np.random.randint(0, cfg["action_dim"])
+        reward = float(np.random.randn() * 0.1)
+        done = (step == cfg["episode_length"]-1)
+        steps.append({
+            "episode_id": str(episode_id),
+            "step": int(step),
+            "timestamp": time.time(),
+            "observation": obs.tolist(),
+            "action": int(action),
+            "reward": reward,
+            "done": done
+        })
+    return steps
+
+def write_episode_parquet(output_dir, episode):
+    df = pd.DataFrame(episode)
+    # For efficiency, we can store observations as numpy arrays serialized; keep simple JSON lists for now
+    fname = os.path.join(output_dir, f"episode_{episode[0]['episode_id']}.parquet")
+    table = pa.Table.from_pandas(df)
+    pq.write_table(table, fname)
+    return fname
+
+def upload_to_s3(local_path, bucket, key_prefix):
+    import boto3
+    s3 = boto3.client("s3")
+    key = os.path.join(key_prefix, os.path.basename(local_path))
+    s3.upload_file(local_path, bucket, key)
+    return f"s3://{bucket}/{key}"
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--config", default="simulator/ISAAC/config.yaml")
+    p.add_argument("--out", default="/tmp/isaac_export")
+    args = p.parse_args()
+    os.makedirs(args.out, exist_ok=True)
+    # Simple inline config
+    cfg = {"episode_length": 50, "obs_dim": 16, "action_dim": 5, "num_episodes": 10}
+    # Generate episodes
+    files=[]
+    for i in range(cfg["num_episodes"]):
+        episode = simulate_episode(cfg, episode_id=i)
+        f = write_episode_parquet(args.out, episode)
+        files.append(f)
+        print("Wrote", f)
+    # Write manifest
+    manifest = {"created_at": time.time(), "num_episodes": cfg["num_episodes"], "cfg": cfg}
+    mpath = os.path.join(args.out, "manifest.json")
+    with open(mpath, "w") as f:
+        json.dump(manifest, f, indent=2)
+    files.append(mpath)
+    # Optional upload
+    bucket = os.environ.get("MODEL_ARTIFACT_BUCKET","")
+    if bucket:
+        key_prefix = f"simulator/isaac/{int(time.time())}"
+        for f in files:
+            print("Uploading", f)
+            try:
+                print(upload_to_s3(f, bucket, key_prefix))
+            except Exception as e:
+                print("Upload failed:", e)
+    print("Export complete. Files:", files)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:simulator/UNITY/README.md
+Unity Sim integration (scaffold)
+
+- Use Unity ML-Agents for scene/sim creation and episode generation.
+- Integration pattern: export episodes as JSON/TFRecords, or stream to Kafka for real-time ingestion.
+- The repo includes a minimal exporter example (see simulator/UNITY/export_stub.py) — replace with Unity Python / ML-Agents API calls.
+
*** End Patch
*** Begin Patch
*** Add File:training/world_model/requirements.txt
+torch>=1.12
+numpy
+pandas
+mlflow
+pyarrow
+tqdm
+boto3
+
*** End Patch
*** Begin Patch
*** Add File:training/world_model/dreamer_like.py
+#!/usr/bin/env python3
+"""
+Minimal Dreamer-like latent dynamics training loop (toy implementation).
+This is a simplified educational scaffold for Aegis staging.
+ - Encoder: MLP from observation -> embedding
+ - RSSM-like transition: GRU on embeddings -> latent
+ - Decoder: reconstruct observation
+ - Loss: reconstruction MSE + latent prediction
+ - Checkpointing: write to /tmp and optionally upload to S3 (MODEL_ARTIFACT_BUCKET)
+ - MLflow logging included
+"""
+import os, time, json
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import Dataset, DataLoader
+import numpy as np
+import mlflow
+
+MODEL_BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET","")
+
+class EpisodeDataset(Dataset):
+    def __init__(self, parquet_paths):
+        import pandas as pd
+        rows=[]
+        for p in parquet_paths:
+            df = pd.read_parquet(p)
+            # Each row contains observation list; convert to np
+            for _, r in df.iterrows():
+                rows.append(np.array(r["observation"], dtype=np.float32))
+        self.data = np.stack(rows) if rows else np.zeros((0,16),dtype=np.float32)
+    def __len__(self): return len(self.data)
+    def __getitem__(self, idx): return self.data[idx]
+
+class Encoder(nn.Module):
+    def __init__(self, obs_dim=16, emb=64):
+        super().__init__()
+        self.net = nn.Sequential(nn.Linear(obs_dim,128), nn.ReLU(), nn.Linear(128,emb))
+    def forward(self,x): return self.net(x)
+
+class RSSM(nn.Module):
+    def __init__(self, emb=64, latent=64):
+        super().__init__()
+        self.gru = nn.GRUCell(emb, latent)
+    def forward(self, emb, prev):
+        return self.gru(emb, prev)
+
+class Decoder(nn.Module):
+    def __init__(self, latent=64, obs_dim=16):
+        super().__init__()
+        self.net = nn.Sequential(nn.Linear(latent,128), nn.ReLU(), nn.Linear(128,obs_dim))
+    def forward(self, z): return self.net(z)
+
+def save_checkpoint(model, optimizer, step, out_dir="/tmp/world_model_ckpt"):
+    os.makedirs(out_dir, exist_ok=True)
+    path = os.path.join(out_dir, f"ckpt_{step}.pt")
+    torch.save({"model_state": model.state_dict(), "opt_state": optimizer.state_dict(), "step": step}, path)
+    if MODEL_BUCKET and os.environ.get("AWS_ACCESS_KEY_ID"):
+        import boto3
+        s3 = boto3.client("s3")
+        key = f"world_model/checkpoints/{os.path.basename(path)}"
+        s3.upload_file(path, MODEL_BUCKET, key)
+    return path
+
+def train(parquet_paths, epochs=5, batch_size=64, lr=1e-3, device="cuda" if torch.cuda.is_available() else "cpu"):
+    ds = EpisodeDataset(parquet_paths)
+    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)
+    encoder = Encoder().to(device)
+    rssm = RSSM().to(device)
+    decoder = Decoder().to(device)
+    params = list(encoder.parameters()) + list(rssm.parameters()) + list(decoder.parameters())
+    opt = optim.Adam(params, lr=lr)
+    mlflow.set_tracking_uri(os.environ.get("MLFLOW_URL",""))
+    with mlflow.start_run(run_name="dreamer_like"):
+        for epoch in range(epochs):
+            epoch_loss = 0.0
+            for i, obs in enumerate(dl):
+                obs = obs.to(device)
+                emb = encoder(obs)
+                # simplistic recurrent step: feed emb into rssm with zero init
+                z = rssm(emb, torch.zeros(emb.size(0),64, device=device))
+                recon = decoder(z)
+                loss = ((recon-obs)**2).mean()
+                opt.zero_grad(); loss.backward(); opt.step()
+                epoch_loss += loss.item()
+            avg = epoch_loss / max(1,len(dl))
+            mlflow.log_metric("epoch_loss", avg, step=epoch)
+            print(f"Epoch {epoch} loss {avg}")
+            if epoch % 2 == 0:
+                ckpt = save_checkpoint(torch.nn.Sequential(encoder, rssm, decoder), opt, epoch)
+                mlflow.log_artifact(ckpt, artifact_path="checkpoints")
+
+if __name__ == "__main__":
+    import sys, glob
+    paths = glob.glob("/tmp/isaac_export/*.parquet")
+    if not paths:
+        print("No dataset found in /tmp/isaac_export; exit")
+        sys.exit(2)
+    train(paths, epochs=5)
+
*** End Patch
*** Begin Patch
*** Add File:training/world_model/job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: world-model-train
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: feast-service-account
+      containers:
+        - name: trainer
+          image: registry.example.com/aegis/world-model-trainer:latest
+          command: ["python3", "training/world_model/dreamer_like.py"]
+          env:
+            - name: MODEL_ARTIFACT_BUCKET
+              value: ""
+            - name: MLFLOW_URL
+              value: ""
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "4"
+              memory: "16Gi"
+            requests:
+              cpu: "2"
+              memory: "8Gi"
+      restartPolicy: Never
+  backoffLimit: 2
+
*** End Patch
*** Begin Patch
*** Add File:argo/world_model_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-worldmodel-pipeline-
+  namespace: aegis
+spec:
+  entrypoint: worldmodel
+  templates:
+    - name: worldmodel
+      steps:
+        - - name: sim-export
+            template: sim-export
+        - - name: ge
+            template: ge-check
+        - - name: feast-offline
+            template: feast-ingest
+        - - name: train
+            template: train
+        - - name: sign
+            template: sign
+        - - name: deploy-ab
+            template: deploy-ab
+        - - name: evidence
+            template: evidence
+
+    - name: sim-export
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pyarrow pandas boto3 && python3 simulator/ISAAC/export_dataset.py --out /tmp/isaac_export && ls -l /tmp/isaac_export
+
+    - name: ge-check
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install great_expectations pandas pyarrow && python3 dq/run_ge_check_blocking.py --data /tmp/isaac_export/episode_0.parquet || true
+
+    - name: feast-ingest
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 pandas pyarrow && python3 feast/sample_offline_to_online_loader.py || true
+
+    - name: train
+      container:
+        image: registry.example.com/aegis/world-model-trainer:latest
+        command: [sh,-c]
+        args:
+          - python3 training/world_model/dreamer_like.py
+
+    - name: sign
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 && python3 demo/scripts/create_mcp.py --artifact-prefix ${MODEL_ARTIFACT_BUCKET:-""}/worldmodel/
+
+    - name: deploy-ab
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh,-c]
+        args:
+          - kubectl apply -f serving/k8s/model_deployment.yaml -n aegis || true
+
+    - name: evidence
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 && python3 compliance/assemble_audit_bundle.py
+
*** End Patch
*** Begin Patch
*** Add File:training/world_model/README_WORLD_MODEL_TRAINING.md
+# World-Model training pipeline (Aegis)
+
+This folder contains:
+- A minimal Dreamer-like training script (training/world_model/dreamer_like.py)
+- A Job manifest to run on a GPU node (training/world_model/job.yaml)
+- An Argo pipeline to run simulation export → GE check → Feast offline ingest → train → sign → evidence (argo/world_model_pipeline.yaml)
+
+How to run locally (dev)
+1. Produce simulated data:
+   python3 simulator/ISAAC/export_dataset.py --out /tmp/isaac_export
+2. Run training:
+   python3 training/world_model/dreamer_like.py
+
+How to run on cluster
+1. Build trainer image (Dockerfile must include requirements.txt and the repo)
+2. Push image to registry and update training/world_model/job.yaml with image
+3. Apply Argo workflow:
+   kubectl apply -f argo/world_model_pipeline.yaml -n aegis
+   or
+   argo submit argo/world_model_pipeline.yaml -n aegis --watch
+
+Notes
+- This implementation is intentionally small and designed to be a starting point. For production:
+  - Replace the toy dynamics with a proper RSSM, belief model, reward predictor and actor if planning.
+  - Use DeepSpeed / model parallel for larger architectures.
+  - Add checkpoint signing (MCP) and HSM validation for critical artifacts.
+
*** End Patch
*** End Patch
