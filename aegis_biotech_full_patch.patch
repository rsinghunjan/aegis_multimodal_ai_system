*** Begin Patch
*** Add File:nextflow/genomics/main.nf
+#!/usr/bin/env nextflow
+/*
+ Simple Nextflow genomics pipeline (toy).
+ Stages: fastq -> align (bwa mem) -> sort/index -> variant call (bcftools) -> vcf -> parquet
+ This is a scaffold: replace container images and tool commands with your production images.
+*/
+params.reads = "$baseDir/data/reads/*.fastq.gz"
+params.outdir = "$baseDir/out"
+
+process ALIGN {
+  publishDir "${params.outdir}/bam", mode: 'copy'
+  input:
+    path fastq from file(params.reads)
+  output:
+    path "${fastq.baseName}.sorted.bam"
+  container "quay.io/biocontainers/bwa:0.7.17--hed695b0_7"
+  script:
+  """
+  # placeholder: align with bwa mem and sort
+  echo "Aligning $fastq"
+  # actual commands left to operator; create a dummy file for scaffold
+  mkdir -p ${params.outdir}/bam
+  touch ${params.outdir}/bam/${fastq.baseName}.sorted.bam
+  """
+}
+
+process CALL_VARS {
+  publishDir "${params.outdir}/vcf", mode: 'copy'
+  input:
+    path bam from ALIGN.out
+  output:
+    path "${bam.baseName}.vcf"
+  container "quay.io/biocontainers/bcftools:1.10.2--he6710b0_3"
+  script:
+  """
+  echo "Calling variants on $bam"
+  mkdir -p ${params.outdir}/vcf
+  touch ${params.outdir}/vcf/${bam.baseName}.vcf
+  """
+}
+
+process VCF_TO_PARQUET {
+  publishDir "${params.outdir}/parquet", mode: 'copy'
+  input:
+    path vcf from CALL_VARS.out
+  output:
+    path "features.parquet"
+  container "python:3.10-slim"
+  script:
+  """
+  pip install cyvcf2 pandas pyarrow >/dev/null 2>&1 || true
+  python3 <<PY
+import pandas as pd
+# minimal stub: create an example parquet
+df = pd.DataFrame([{'sample': 's1', 'variant_count': 10}])
+df.to_parquet('features.parquet', index=False)
+PY
+  """
+}
+
+workflow {
+  ALIGN | CALL_VARS | VCF_TO_PARQUET
+}
+
*** End Patch
*** Begin Patch
*** Add File:argo/genomics/genomics_nextflow_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: genomics-nextflow-
+  namespace: aegis
+spec:
+  entrypoint: genomics-pipeline
+  templates:
+    - name: genomics-pipeline
+      steps:
+        - - name: run-nextflow
+            template: run-nextflow
+        - - name: upload-features
+            template: upload-features
+
+    - name: run-nextflow
+      container:
+        image: nfcore/nextflow:23.04.0
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            echo "Running Nextflow genomics pipeline (scaffold)"
+            mkdir -p /workspace
+            cp -r /workspace-src/nextflow/genomics /workspace || true
+            cd /workspace/genomics || true
+            # run Nextflow; requires Nextflow and tools in image
+            nextflow run main.nf -work-dir /workspace/work -with-report report.html || true
+            ls -l /workspace/genomics/out || true
+      volumeMounts:
+        - name: workspace-src
+          mountPath: /workspace-src
+      outputs:
+        artifacts:
+          - name: features-parquet
+            path: /workspace/genomics/out/parquet/features.parquet
+
+    - name: upload-features
+      inputs:
+        artifacts:
+          - name: features-parquet
+            path: /tmp/features.parquet
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            pip install boto3 || true
+            if [ -f /tmp/features.parquet ]; then
+              echo "Uploading features to S3 if MODEL_ARTIFACT_BUCKET set"
+              if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+                aws s3 cp /tmp/features.parquet s3://${MODEL_ARTIFACT_BUCKET}/genomics/features_$(date -u +%s).parquet || true
+              fi
+            else
+              echo "No features produced"
+            fi
+      volumeMounts:
+        - name: workspace-src
+          mountPath: /workspace-src
+
+  volumes:
+    - name: workspace-src
+      persistentVolumeClaim:
+        claimName: workspace-src-pvc
+
*** End Patch
*** Begin Patch
*** Add File:dq/great_expectations/genomics_suite.yml
+{
+  "expectation_suite_name": "genomics_pipeline_suite",
+  "expectations": [
+    {
+      "expectation_type": "expect_table_columns_to_match_ordered_list",
+      "kwargs": {"column_list": ["sample","variant_count"]}
+    },
+    {
+      "expectation_type": "expect_column_values_to_not_be_null",
+      "kwargs": {"column": "variant_count"}
+    },
+    {
+      "expectation_type": "expect_column_values_to_be_between",
+      "kwargs": {"column": "variant_count", "min_value": 0, "max_value": 1000000}
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:dq/great_expectations/imaging_suite.yml
+{
+  "expectation_suite_name": "imaging_pipeline_suite",
+  "expectations": [
+    {
+      "expectation_type": "expect_table_row_count_to_be_between",
+      "kwargs": {"min_value": 1}
+    },
+    {
+      "expectation_type": "expect_column_to_exist",
+      "kwargs": {"column": "tile"}
+    },
+    {
+      "expectation_type": "expect_column_values_to_match_regex",
+      "kwargs": {"column": "tile", "regex": ".*_tile_\\d+.png"}
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:argo/federated/federated_hospital_training.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: federated-hospital-train-
+  namespace: aegis
+spec:
+  entrypoint: federated-train
+  templates:
+    - name: federated-train
+      steps:
+        - - name: launch-site-jobs
+            template: launch-site-job
+            arguments:
+              parameters:
+                - name: site
+                  value: "site-a"
+        - - name: launch-site-jobs-b
+            template: launch-site-job
+            arguments:
+              parameters:
+                - name: site
+                  value: "site-b"
+        - - name: run-secure-agg
+            template: secure-agg
+        - - name: dp-accountant
+            template: dp-accountant
+        - - name: assemble-evidence
+            template: assemble-evidence
+
+    - name: launch-site-job
+      inputs:
+        parameters:
+          - name: site
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - |
+            SITE="{{inputs.parameters.site}}"
+            echo "Templating and launching per-site training job for $SITE"
+            kubectl -n aegis apply -f federated/k8s/site_fed_job.yaml || true
+
+    - name: secure-agg
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install crypten torch boto3 || true
+            python3 federated/secure_agg_runner.py || true
+
+    - name: dp-accountant
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install opacus || true
+            python3 federation/secure_agg_integration/privacy/accountant_demo.py || true
+
+    - name: assemble-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 evidence/enhanced_assemble_audit_bundle.py || true
+
*** End Patch
*** Begin Patch
*** Add File:federated/k8s/site_fed_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: site-federated-trainer
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: site-train
+          image: registry.example.com/aegis/fed-site-trainer:latest
+          env:
+            - name: SITE_ID
+              valueFrom:
+                fieldRef:
+                  fieldPath: metadata.name
+          command: ["/bin/sh","-c"]
+          args:
+            - |
+              echo "Running per-site local training (stub)"
+              python3 /app/train_local.py --data /data/local --out /tmp/site_model.pt || true
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_agg_runner.py
+#!/usr/bin/env python3
+"""
+Simple orchestrator to demonstrate secure aggregation step.
+In production, this should call CrypTen multi-host scripts or a secure aggregator service.
+"""
+import time, boto3, os
+print("Starting secure aggregation runner (demo)")
+time.sleep(3)
+print("Fetching per-site model artifacts and performing secure aggregation (demo).")
+# stub: in production load per-site weights and perform CrypTen secure aggregation
+out="/tmp/aggregated_model.pt"
+open(out,"wb").write(b"dummy-aggregated-model")
+print("Wrote aggregated model to", out)
+# upload to artifact bucket if configured
+bucket=os.environ.get("MODEL_ARTIFACT_BUCKET","")
+if bucket:
+    s3=boto3.client("s3")
+    key=f"federated/aggregated_{int(time.time())}.pt"
+    try:
+        s3.upload_file(out,bucket,key)
+        print("Uploaded aggregated model to s3://%s/%s"%(bucket,key))
+    except Exception as e:
+        print("Upload failed",e)
+
*** End Patch
*** Begin Patch
*** Add File:argo/clinical/silent_mode_deploy.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: clinical-silent-deploy-
+  namespace: aegis
+spec:
+  entrypoint: silent-deploy
+  templates:
+    - name: silent-deploy
+      steps:
+        - - name: deploy-shadow
+            template: deploy-shadow
+        - - name: run-shadow-traffic
+            template: run-shadow-traffic
+        - - name: collect-audit
+            template: collect-audit
+        - - name: clinician-approval
+            template: clinician-approval
+
+    - name: deploy-shadow
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh,-c]
+        args:
+          - |
+            echo "Deploying model in shadow mode to triton (no traffic shifted)"
+            kubectl -n aegis apply -f triton/model_prep_job.yaml || true
+            kubectl -n aegis apply -f triton/deployment/triton_deployment.yaml || true
+
+    - name: run-shadow-traffic
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install requests boto3 || true
+            python3 scripts/clinical/shadow_inference_runner.py --duration 60 --out /tmp/shadow_preds.json || true
+            cat /tmp/shadow_preds.json
+
+    - name: collect-audit
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 || true
+            python3 scripts/clinical/upload_audit_predictions.py /tmp/shadow_preds.json || true
+
+    - name: clinician-approval
+      suspend:
+        message: "Await clinician approval to promote model to production. Add annotation 'clinical.approval=true' to Deployment to proceed."
+
*** End Patch
*** Begin Patch
*** Add File:governance/gatekeeper_clinical_constraint.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: requireclinicalapproval
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireClinicalApproval
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package requireclinicalapproval
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Deployment"
+          metadata := input.review.object.metadata
+          not metadata.annotations["clinical.approval"]
+          msg := "Deployment requires clinical.approval annotation before promotion to production"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireClinicalApproval
+metadata:
+  name: clinical-approval-required
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Deployment"]
+
*** End Patch
*** Begin Patch
*** Add File:scripts/clinical/shadow_inference_runner.py
+#!/usr/bin/env python3
+"""
+Run shadow inferences against the deployed Triton endpoint and collect predictions (no effect on live traffic).
+"""
+import argparse, json, time, random
+
+def run(duration, out):
+    end = time.time() + duration
+    preds=[]
+    while time.time() < end:
+        # generate stub prediction record
+        preds.append({"ts": int(time.time()), "patient_id": f"p{random.randint(1,100)}", "pred": random.random()})
+        time.sleep(0.5)
+    with open(out,"w") as f:
+        json.dump(preds,f)
+    print("Wrote", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--duration", type=int, default=30)
+    p.add_argument("--out", default="/tmp/shadow_preds.json")
+    args = p.parse_args()
+    run(args.duration, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:scripts/clinical/upload_audit_predictions.py
+#!/usr/bin/env python3
+import sys, json, os, boto3
+path = sys.argv[1]
+with open(path) as f:
+    preds = json.load(f)
+# attach metadata and upload
+out_key = f"clinical/audit/preds_{int(__import__('time').time())}.json"
+bucket = os.environ.get("EVIDENCE_BUCKET","")
+if bucket:
+    s3 = boto3.client("s3")
+    s3.put_object(Bucket=bucket, Key=out_key, Body=json.dumps(preds).encode())
+    print("Uploaded audit preds to s3://%s/%s"%(bucket,out_key))
+else:
+    print("EVIDENCE_BUCKET not set; writing to /tmp")
+    open("/tmp/"+os.path.basename(path),"w").write(json.dumps(preds))
+
*** End Patch
*** Begin Patch
*** Add File:ops/erasure/extended_discover_and_erase_all.py
+#!/usr/bin/env python3
+"""
+Extended discovery and erase script that includes common archive/backup sinks.
+Adds support for:
+ - S3 (regular)
+ - S3 Glacier (lists inventory via S3 API if available)
+ - Database snapshot references (passed via config / prefix list)
+ - Local and tape/backup vault listings (operator-provided)
+
+Produces a signed manifest using existing signing helper.
+"""
+import argparse, boto3, json, os, time
+
+def list_s3(s3, bucket, prefix):
+    out=[]
+    kwargs={"Bucket":bucket,"Prefix":prefix}
+    while True:
+        resp = s3.list_objects_v2(**kwargs)
+        for o in resp.get("Contents",[]):
+            out.append(o["Key"])
+        if not resp.get("IsTruncated"):
+            break
+        kwargs["ContinuationToken"]=resp.get("NextContinuationToken")
+    return out
+
+def list_glacier_inventory(s3, bucket):
+    # Glacier inventory listing is asynchronous; here we stub and attempt to list archived objects via list_objects_v2
+    try:
+        return list_s3(s3, bucket, "")
+    except Exception:
+        return []
+
+def sign_manifest(manifest):
+    try:
+        from production.policy.signing.sign_with_retry import sign_payload
+        sig, meta = sign_payload(json.dumps(manifest).encode(), None)
+        manifest['signature'] = sig
+        manifest['signed_by'] = meta
+    except Exception as e:
+        manifest['signature'] = ''
+        manifest['signing_error'] = str(e)
+    return manifest
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--prefixes", default="genomics/,images/,models/")
+    p.add_argument("--db-snapshots-json", default="")
+    p.add_argument("--dry-run", action="store_true")
+    p.add_argument("--out", default="/tmp/erasure_extended_manifest.json")
+    args = p.parse_args()
+
+    s3 = boto3.client("s3")
+    report={"time": time.time(), "s3_bucket": args.s3_bucket, "prefixes": {}, "db_snapshots": []}
+    for pref in args.prefixes.split(","):
+        pref = pref.strip()
+        if not pref:
+            continue
+        keys = list_s3(s3, args.s3_bucket, pref)
+        report['prefixes'][pref] = {"count": len(keys), "sample": keys[:50]}
+        if not args.dry_run and keys:
+            # delete keys (careful!)
+            for i in range(0, len(keys), 1000):
+                chunk = keys[i:i+1000]
+                objs = [{"Key":k} for k in chunk]
+                s3.delete_objects(Bucket=args.s3_bucket, Delete={"Objects": objs})
+            report['prefixes'][pref]['deleted'] = True
+
+    # Glacier inventory check (best-effort)
+    report['glacier_inventory'] = list_glacier_inventory(s3, args.s3_bucket)
+
+    # DB snapshot references (JSON list of S3 keys or DB snapshot ids)
+    if args.db_snapshots_json and os.path.exists(args.db_snapshots_json):
+        report['db_snapshots'] = json.load(open(args.db_snapshots_json))
+
+    signed = sign_manifest(report)
+    with open(args.out, "w") as f:
+        json.dump(signed, f, indent=2)
+    print("Wrote manifest to", args.out)
+    eb = os.environ.get("EVIDENCE_BUCKET","")
+    if eb:
+        s3.put_object(Bucket=eb, Key=f"erasure/manifest_{int(time.time())}.json", Body=json.dumps(signed).encode())
+        print("Uploaded signed manifest to", eb)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/erasure/README_EXTENDED.md
+# Extended Erasure & DSR Discovery
+
+This extension expands ops/erasure/discover_and_erase_all.py to include archive/glacier inventory and DB snapshot references.
+
+Usage:
+  python3 ops/erasure/extended_discover_and_erase_all.py --s3-bucket my-bucket --prefixes "genomics/,images/" --db-snapshots-json /tmp/db_snapshots.json --dry-run
+
+Notes:
+- Glacier inventory may require additional AWS IAM permissions and asynchronous inventory jobs; here we attempt a best-effort listing.
+- DB snapshot JSON should be a list of snapshot identifiers or S3 URIs for backups.
+- The script signs the manifest (if signing helper available) and uploads to EVIDENCE_BUCKET if set.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/clinical_promotion_checklist.md
+# Clinical Promotion Checklist
+
+Before promoting a clinical model to production, ensure:
+
+1. Evidence bundle assembled and uploaded to EVIDENCE_BUCKET (includes GE reports, training data manifests, HSM signatures, clinician validation).
+2. Prospective silent-mode run completed and audit logs uploaded.
+3. Clinician signoff recorded (GitHub issue or signed document) and Deployment annotated with clinical.approval=true.
+4. Gatekeeper constraint verified in staging to require clinical.approval.
+5. DP/Privacy accountant outputs included if federated or DP techniques used.
+6. DR/rollback image available and tested.
+
*** End Patch
*** End Patch
