*** Begin Patch
*** Add File:secrets/required-secrets.yaml.template
+apiVersion: v1
+kind: Secret
+metadata:
+  name: aegis-required-secrets
+  namespace: aegis
+type: Opaque
+stringData:
+  # Fill these values before applying (use sealed-secrets / Vault / GitOps to inject)
+  COSIGN_KMS_KEY_ARN: "arn:aws:kms:REPLACE/COSIGN_KEY"
+  EVIDENCE_BUCKET: "REPLACE-evidence-bucket"
+  REKOR_URL: "https://rekor.REPLACE.svc:3000"
+  VAULT_KMS_ARN: "arn:aws:kms:REPLACE/VAULT_KEY"
+  MONGO_URI: "mongodb://user:pass@mongo.aegis.svc:27017/aegis"
+  MODEL_ARTIFACT_BUCKET: "REPLACE-model-artifact-bucket"
+  # Optional: ARGO auth token if Argo server is secured
+  ARGO_AUTH_TOKEN: ""
+
+# Notes:
+# - Do NOT commit real secrets. Use sealed-secrets, SOPS, or Vault to populate these values in GitOps flows.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/gke/gpu_node_pool.tf
+/*
+GKE node pool example: create a GPU node pool.
+Fill project, cluster and zone variables before running terraform.
+This is an example; adapt to your cloud provider.
+*/
+variable "project" {}
+variable "region" { default = "us-west1" }
+variable "cluster_name" {}
+variable "node_pool_name" { default = "gpu-pool" }
+variable "machine_type" { default = "n1-standard-16" }
+variable "gpu_count" { default = 1 }
+
+provider "google" {
+  project = var.project
+  region  = var.region
+}
+
+resource "google_container_node_pool" "gpu_pool" {
+  name       = var.node_pool_name
+  cluster    = var.cluster_name
+  node_count = 1
+
+  node_config {
+    machine_type = var.machine_type
+    oauth_scopes = ["https://www.googleapis.com/auth/cloud-platform"]
+    taint {
+      key    = "workload-type"
+      value  = "gpu"
+      effect = "NO_SCHEDULE"
+    }
+    guest_accelerator {
+      count = var.gpu_count
+      type  = "nvidia-tesla-t4"
+    }
+  }
+  upgrade_settings {
+    max_surge       = 1
+    max_unavailable = 0
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/eks/gpu_node_group.tf
+/*
+EKS managed node group example with GPU instances (AWS).
+Fill variables: cluster_name, subnet_ids, node_role_arn, region.
+*/
+variable "cluster_name" {}
+variable "subnet_ids" { type = list(string) }
+variable "node_role_arn" {}
+variable "region" { default = "us-west-2" }
+
+provider "aws" {
+  region = var.region
+}
+
+resource "aws_eks_node_group" "gpu" {
+  cluster_name    = var.cluster_name
+  node_group_name = "aegis-gpu-ng"
+  node_role_arn   = var.node_role_arn
+  subnet_ids      = var.subnet_ids
+
+  scaling_config {
+    desired_size = 1
+    max_size     = 5
+    min_size     = 1
+  }
+
+  instance_types = ["g4dn.xlarge"]
+
+  labels = {
+    "workload-type" = "gpu"
+  }
+
+  taints = [
+    {
+      key    = "workload-type"
+      value  = "gpu"
+      effect = "NO_SCHEDULE"
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:scripts/provision_gpu_nodepool.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper to provision GPU node pool using Terraform (choose gke or eks)
+#
+CMD=${1:-}
+case "$CMD" in
+  gke)
+    pushd terraform/gke
+    terraform init
+    terraform apply -auto-approve
+    popd
+    ;;
+  eks)
+    pushd terraform/eks
+    terraform init
+    terraform apply -auto-approve
+    popd
+    ;;
+  *)
+    echo "Usage: $0 {gke|eks}"
+    exit 2
+    ;;
+esac
+
*** End Patch
*** Begin Patch
*** Add File:scripts/deploy_prod_services.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy production services: Velero, Seldon Operator, Triton PVC, Milvus statefulset.
+# Requires kubectl & helm and the required cloud creds for Velero S3 access.
+
+NAMESPACE=${NAMESPACE:-aegis}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-}
+VELERO_SECRET=${VELERO_SECRET:-velero-credentials}
+
+echo "1) Create namespaces"
+kubectl create ns velero || true
+kubectl create ns seldon-system || true
+kubectl create ns milvus || true
+kubectl create ns ${NAMESPACE} || true
+
+echo "2) Install Velero (example using helm + S3). Ensure velero credentials secret exists in velero namespace."
+helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts || true
+helm repo update || true
+helm upgrade --install velero vmware-tanzu/velero -n velero \
+  --set configuration.provider=aws \
+  --set configuration.backupStorageLocation.name=default \
+  --set configuration.backupStorageLocation.bucket=${EVIDENCE_BUCKET} \
+  --set configuration.backupStorageLocation.config.region=${AWS_REGION:-us-west-2} \
+  --set credentials.secretContents.cloud=credentials-velero || true
+
+echo "3) Install Seldon Core (for canary + inference orchestration)"
+helm repo add seldon-core https://storage.googleapis.com/seldon-charts || true
+helm repo update || true
+helm upgrade --install seldon-core seldon-core/seldon-core-operator --namespace seldon-system --set usageMetrics.enabled=true || true
+
+echo "4) Apply Triton PVC (ensure storage class exists)"
+kubectl apply -f infra/triton/tuned_deployment.yaml || true
+
+echo "5) Deploy production Milvus statefulset"
+kubectl apply -f infra/milvus/statefulset-prod.yaml || true
+
+echo "Deployed Velero, Seldon, Triton PVC and Milvus (check pod status and iterate)."
+
*** End Patch
*** Begin Patch
*** Add File:scripts/run_vault_bootstrap_and_check.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run the Vault auto-init/rotate script and verify audit file uploads to S3
+# Use from a bastion with AWS creds OR allow pod access to S3/SecretsManager
+
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${VAULT_ADDR:?VAULT_ADDR required}"
+: "${KMS_ARN:?KMS_ARN required}"
+: "${AWS_REGION:?AWS_REGION required}"
+
+echo "Running Vault bootstrap (auto_init_rotate_advanced.sh)"
+./deploy/vault/auto_init_rotate_advanced.sh
+
+echo "Waiting for audit log uploads..."
+# wait up to 5 minutes for at least one audit file in s3://EVIDENCE_BUCKET/vault-audit/
+COUNT=0
+for i in $(seq 1 30); do
+  if aws s3 ls "s3://${EVIDENCE_BUCKET}/vault-audit/" >/dev/null 2>&1; then
+    echo "Found audit uploads"
+    COUNT=1
+    break
+  fi
+  sleep 10
+done
+if [ ${COUNT} -eq 0 ]; then
+  echo "No audit uploads detected to s3://${EVIDENCE_BUCKET}/vault-audit/ - check CronJobUploader and PVC mount"
+  exit 2
+fi
+
+echo "Vault bootstrap and audit upload check complete."
+
*** End Patch
*** Begin Patch
*** Add File:terraform/rekor_rds/deploy.sh
+#!/usr/bin/env bash
+set -euo pipefail
+pushd terraform/rekor_rds
+terraform init
+terraform apply -auto-approve
+popd
+echo "Rekor RDS terraform applied. Update deploy/rekor/rekor-rds-secret-template.yaml with DB endpoint/creds and apply Kubernetes secret."
+
*** End Patch
*** Begin Patch
*** Add File:scripts/integrate_token_budget.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Patch the LangGraph deployment to add TOKEN_BUDGET_URL env and set team default; this assumes the langgraph deployment is named 'langgraph-runtime'
+# In production you'd bake the middleware into the container image; this is a stop-gap to inject env vars and a sidecar init container.
+
+NAMESPACE=${NAMESPACE:-langgraph}
+TOKEN_URL=${TOKEN_URL:-http://token-budget.aegis.svc:9200}
+
+echo "Patching deployment langgraph-runtime in namespace ${NAMESPACE} to include TOKEN_BUDGET_URL"
+kubectl -n ${NAMESPACE} patch deployment langgraph-runtime --type='json' -p="[
+  {\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/env/-\", \"value\": {\"name\":\"TOKEN_BUDGET_URL\",\"value\":\"${TOKEN_URL}\"}},
+  {\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/env/-\", \"value\": {\"name\":\"DEFAULT_TEAM\",\"value\":\"default\"}}
+]" || true
+
+echo "Note: For full enforcement, modify the LangGraph runtime image to call ops/token_budget/enforce_client.py at HF callsites (see langgraph/runtime/inference_middleware.py)."
+
*** End Patch
*** Begin Patch
*** Add File:scripts/run_full_e2e.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrates an end-to-end smoke run:
+# - ensures required k8s secrets exist (template provided)
+# - ensures GPU node pool (terraform) is provisioned (operator step)
+# - deploys production services (Velero, Seldon, Triton PVC, Milvus)
+# - bootstraps Vault and validates audit upload
+# - deploys Rekor RDS via terraform and runs test failover & restore
+# - patches LangGraph to use token-budget
+# - triggers Argo promotion workflow to produce signed compliance pack and checks EVIDENCE_BUCKET for pack
+
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${COSIGN_KMS_KEY_ARN:?COSIGN_KMS_KEY_ARN required}"
+
+echo "Step 0: Ensure required k8s secret template is populated and applied (secrets/required-secrets.yaml.template -> secrets/required-secrets.yaml)"
+echo "Please populate secrets/required-secrets.yaml.template and apply it (or use sealed-secrets)."
+kubectl apply -f secrets/required-secrets.yaml.template || true
+
+echo "Step 1: Provision GPU node pool - operator should run scripts/provision_gpu_nodepool.sh {gke|eks}"
+echo "Skipping automatic provisioning in this script. Ensure GPU nodes labeled 'workload-type=gpu' are ready."
+kubectl get nodes -l workload-type=gpu || echo "No GPU nodes found yet"
+
+echo "Step 2: Deploy production services (Velero, Seldon, Triton PVC, Milvus)"
+./scripts/deploy_prod_services.sh
+
+echo "Step 3: Run Vault bootstrap and check audit upload"
+./scripts/run_vault_bootstrap_and_check.sh
+
+echo "Step 4: Deploy Rekor RDS via terraform"
+./terraform/rekor_rds/deploy.sh
+
+echo "Step 5: Run Rekor failover & restore test"
+./deploy/rekor/test_failover_and_restore.sh || true
+
+echo "Step 6: Integrate token-budget wrapper into LangGraph runtime (inject env vars)"
+./scripts/integrate_token_budget.sh
+
+echo "Step 7: Trigger Argo promotion workflow for test manifest (ensure manifest is uploaded to S3)"
+MANIFEST_S3=${MANIFEST_S3:-}
+if [ -z "${MANIFEST_S3}" ]; then
+  echo "No MANIFEST_S3 provided; please upload a manifest to S3 and set MANIFEST_S3 env before running. Exiting."
+  exit 2
+fi
+
+echo "Submitting Argo promotion workflow (requires argo CLI or kubectl + CRD access)"
+argo submit -n aegis argo/workflows/promotion_and_compliance.yaml -p manifest-s3="${MANIFEST_S3}" || \
+  kubectl -n aegis create -f argo/workflows/promotion_and_compliance.yaml || true
+
+echo "Waiting for compliance pack to appear in s3://${EVIDENCE_BUCKET}/ (poll for 5 minutes)"
+FOUND=0
+for i in $(seq 1 30); do
+  if aws s3 ls "s3://${EVIDENCE_BUCKET}/" | grep -q "compliance"; then
+    FOUND=1
+    break
+  fi
+  sleep 10
+done
+if [ ${FOUND} -eq 1 ]; then
+  echo "Compliance pack found in EVIDENCE_BUCKET"
+else
+  echo "No compliance pack found; check Argo workflow logs and attach_and_sign scripts"
+  exit 2
+fi
+
+echo "Full E2E smoke run complete (subject to operator-supplied secrets & infra)."
+
*** End Patch
*** Begin Patch
*** Add File:langgraph/runtime/app_budget.py
+#!/usr/bin/env python3
+"""
+LangGraph runtime variant that uses token-budget enforcement and Presidio redaction via redaction client.
+This is an alternative runtime you can deploy while you productionize the main runtime image.
+It demonstrates integration points and records evidence via the existing inference evidence script.
+"""
+import os, json, time, requests
+from flask import Flask, request, jsonify
+from langgraph.runtime.redaction_client import redact_text
+from ops.token_budget.enforce_client import check_and_consume
+from langgraph.runtime.app import load_graph, call_hf  # reuse helpers; ensure import paths are correct
+
+app = Flask("langgraph-runtime-budget")
+
+DEFAULT_TEAM = os.environ.get("DEFAULT_TEAM","default")
+TOKEN_BUDGET_URL = os.environ.get("TOKEN_BUDGET_URL","http://token-budget.aegis.svc:9200")
+EVIDENCE_SCRIPT = os.environ.get("INFERENCE_EVIDENCE_SCRIPT","/app/scripts/evidence/inference_with_evidence.py")
+
+@app.route("/run/<graph_name>", methods=["POST"])
+def run_graph(graph_name):
+    payload = request.get_json() or {}
+    graph = load_graph(graph_name)
+    # simple: assume single LLM node and one prompt; estimate tokens as 10 per call
+    team = payload.get("team", DEFAULT_TEAM)
+    token_est = 10
+    if not check_and_consume(team, token_est):
+        return jsonify({"error":"token budget exceeded"}), 403
+    # redact input
+    input_obj = payload.get("input",{})
+    q = input_obj.get("question","")
+    q_redacted = redact_text(q)
+    # call HF via existing helper
+    try:
+        ans = call_hf({"model": graph["nodes"][0]["model"], "prompt": q_redacted})
+    except Exception as e:
+        return jsonify({"error":"llm call failed","detail":str(e)}), 500
+    # record evidence (calls existing script)
+    try:
+        import subprocess, tempfile
+        tmp = tempfile.mkdtemp()
+        reqfile = tmp + "/req.json"
+        resfile = tmp + "/res.json"
+        json.dump({"question":q_redacted}, open(reqfile,"w"))
+        json.dump({"answer":ans}, open(resfile,"w"))
+        subprocess.run(["python3", EVIDENCE_SCRIPT, graph_name, reqfile, resfile], check=False)
+    except Exception:
+        pass
+    return jsonify({"answer": ans})
+
+@app.route("/health")
+def health():
+    return jsonify({"status":"ok"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:docs/OPERATOR_CHECKLIST_FOR_E2E.md
+# Operator checklist to enable the final E2E runs
+
+1) Populate secrets
+ - Edit secrets/required-secrets.yaml.template or create a sealed-secret with values:
+   - COSIGN_KMS_KEY_ARN
+   - EVIDENCE_BUCKET
+   - REKOR_URL
+   - VAULT_KMS_ARN
+   - MONGO_URI
+   - MODEL_ARTIFACT_BUCKET
+ - Apply: kubectl -n aegis apply -f secrets/required-secrets.yaml (use sealed secrets)
+
+2) Provision GPUs
+ - Choose cloud provider and run scripts/provision_gpu_nodepool.sh {gke|eks}
+ - Wait for GPU nodes to become Ready and labeled with workload-type=gpu
+
+3) Deploy infra services
+ - Run scripts/deploy_prod_services.sh (Velero, Seldon operator, Triton PVC, Milvus statefulset)
+
+4) Bootstrap Vault
+ - From a secure bastion with AWS creds run:
+     VAULT_ADDR=https://vault.vault.svc:8200 EVIDENCE_BUCKET=<bucket> KMS_ARN=<kms> AWS_REGION=<region> ./deploy/vault/auto_init_rotate_advanced.sh
+ - Verify audit upload: aws s3 ls s3://<EVIDENCE_BUCKET>/vault-audit/
+
+5) Rekor RDS
+ - Run terraform/rekor_rds/deploy.sh (configure variables)
+ - Create rekor-db-secret in aegis namespace and apply deploy/rekor/rekor-rds-secret-template.yaml
+ - Run deploy/rekor/test_failover_and_restore.sh and verify backups in s3://${EVIDENCE_BUCKET}/rekor-backups/
+
+6) Integrate token budget
+ - Ensure token-budget service deployed and reachable
+ - Run scripts/integrate_token_budget.sh to patch LangGraph deployment (or bake into new image)
+
+7) Run full E2E smoke
+ - Export a test manifest to S3 (MODEL_ARTIFACT_BUCKET)
+ - Set MANIFEST_S3 env and run:
+     EVIDENCE_BUCKET=<bucket> COSIGN_KMS_KEY_ARN=<arn> ./scripts/run_full_e2e.sh
+ - Verify compliance pack in s3://${EVIDENCE_BUCKET}/compliance or restore-drills path
+
+8) Post-checks
+ - Inspect Argo workflow logs for promotion_and_compliance
+ - Inspect MongoDB registry for new artifact entry
+ - Verify Triton pods and Seldon deployments are healthy and autoscaling active
+
*** End Patch
*** End Patch
