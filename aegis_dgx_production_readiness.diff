diff --git a/docker/Dockerfile.deepspeed.h100 b/docker/Dockerfile.deepspeed.h100
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/docker/Dockerfile.deepspeed.h100
@@ -0,0 +1,120 @@
+# Dockerfile for DeepSpeed image tuned for NVIDIA H100 (example)
+# IMPORTANT: Replace the pinned versions below with the exact CUDA/cuDNN/NCCL/PyTorch/DeepSpeed
+# versions validated against your DGX Cloud cluster. This file is a template for production images.
+#
+# Suggested naming: <REGISTRY>/aegis-deepspeed:h100-cuda12.1-pytorch2.2-deepspeed0.9.2
+
+FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
+
+LABEL maintainer="platform-team@example.com"
+ENV DEBIAN_FRONTEND=noninteractive
+
+# System deps
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    git ca-certificates wget build-essential python3 python3-pip python3-venv \
+    libnuma-dev && rm -rf /var/lib/apt/lists/*
+
+# Python & pip
+RUN python3 -m pip install --upgrade pip setuptools wheel
+
+# ---- PINNED LIBRARIES (edit to match DGX validated matrix) ----
+ENV PYTORCH_VERSION=2.2.0
+ENV TORCH_CUDA_ARCH_LIST="80;90"   # adjust per GPU generation
+ENV DEEPSPEED_VERSION=0.9.2
+ENV NCCL_VERSION=2.18.0
+ENV TRANSFORMERS_VERSION=4.35.0
+# --------------------------------------------------------------
+
+# Install PyTorch (binary wheel from official index matching CUDA)
+RUN pip install --no-cache-dir "torch==${PYTORCH_VERSION}+cu121" -f https://download.pytorch.org/whl/torch_stable.html
+
+# Install DeepSpeed and common ML libs
+RUN pip install --no-cache-dir "deepspeed==${DEEPSPEED_VERSION}" \
+    transformers==${TRANSFORMERS_VERSION} datasets accelerate bitsandbytes
+
+# Optional: install NCCL-related tools if needed (usually part of driver/runtime)
+# Ensure NCCL ABI matching your cluster; do not overwrite system driver-provided libs.
+
+WORKDIR /workspace
+COPY . /workspace
+
+# Default entrypoint is a shell; users should run training command via k8s job or orchestrator.
+ENV PATH="/workspace/bin:${PATH}"
+CMD ["/bin/bash"]
+
+# SBOM hint: run `syft <image> -o json > /sbom/image-sbom.json` after build in CI.
+
diff --git a/docker/Dockerfile.deepspeed.a100 b/docker/Dockerfile.deepspeed.a100
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/docker/Dockerfile.deepspeed.a100
@@ -0,0 +1,118 @@
+# Dockerfile for DeepSpeed image tuned for NVIDIA A100 (example)
+# IMPORTANT: Replace pinned versions with the exact GPU/runtime versions validated for A100.
+#
+# Suggested naming: <REGISTRY>/aegis-deepspeed:a100-cuda11.8-pytorch2.1-deepspeed0.9.0
+
+FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04
+
+LABEL maintainer="platform-team@example.com"
+ENV DEBIAN_FRONTEND=noninteractive
+
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    git ca-certificates wget build-essential python3 python3-pip python3-venv \
+    libnuma-dev && rm -rf /var/lib/apt/lists/*
+
+RUN python3 -m pip install --upgrade pip setuptools wheel
+
+# ---- PINNED LIBRARIES (edit to match DGX validated matrix) ----
+ENV PYTORCH_VERSION=2.1.1
+ENV DEEPSPEED_VERSION=0.9.0
+ENV TRANSFORMERS_VERSION=4.30.0
+# --------------------------------------------------------------
+
+RUN pip install --no-cache-dir "torch==${PYTORCH_VERSION}+cu118" -f https://download.pytorch.org/whl/torch_stable.html
+RUN pip install --no-cache-dir "deepspeed==${DEEPSPEED_VERSION}" transformers==${TRANSFORMERS_VERSION} datasets
+
+WORKDIR /workspace
+COPY . /workspace
+CMD ["/bin/bash"]
+
diff --git a/scripts/dgx/publish_and_sign_images.sh b/scripts/dgx/publish_and_sign_images.sh
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/dgx/publish_and_sign_images.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Build, generate SBOM, scan, and sign DeepSpeed images for DGX (H100/A100).
+# Requires: docker (or podman), syft, trivy (or similar), cosign on PATH.
+#
+# Usage:
+#   REGISTRY=ghcr.io/org TARGET=h100 ./scripts/dgx/publish_and_sign_images.sh
+
+set -euo pipefail
+
+REGISTRY="${REGISTRY:-<REGISTRY>}"
+TARGET="${TARGET:-h100}"   # "h100" or "a100"
+IMAGE_TAG="${IMAGE_TAG:-aegis-deepspeed:${TARGET}-validated}"
+
+if [[ "$REGISTRY" == "<REGISTRY>" ]]; then
+  echo "Please set REGISTRY env var to your container registry (e.g., ghcr.io/org)"; exit 2
+fi
+
+if ! command -v docker >/dev/null 2>&1; then
+  echo "docker is required"; exit 2
+fi
+if ! command -v syft >/dev/null 2>&1; then
+  echo "syft (Sbom) is required. Install: https://github.com/anchore/syft"; exit 2
+fi
+if ! command -v trivy >/dev/null 2>&1; then
+  echo "trivy (image scanner) is required. Install: https://github.com/aquasecurity/trivy"; exit 2
+fi
+if ! command -v cosign >/dev/null 2>&1; then
+  echo "cosign (image signer) is required. Install: https://github.com/sigstore/cosign"; exit 2
+fi
+
+IMAGE_FULL="${REGISTRY}/${IMAGE_TAG}"
+
+case "$TARGET" in
+  h100) DOCKERFILE="docker/Dockerfile.deepspeed.h100";;
+  a100) DOCKERFILE="docker/Dockerfile.deepspeed.a100";;
+  *) echo "Unknown TARGET $TARGET (use h100 or a100)"; exit 3;;
+esac
+
+echo "Building image $IMAGE_FULL from $DOCKERFILE"
+docker build -t "$IMAGE_FULL" -f "$DOCKERFILE" .
+
+SBOM_FILE="./artifacts/${IMAGE_TAG//[:\/]/_}-sbom.json"
+mkdir -p ./artifacts
+echo "Generating SBOM -> $SBOM_FILE"
+syft "$IMAGE_FULL" -o json > "$SBOM_FILE"
+
+echo "Scanning image with Trivy (fail on HIGH/CRITICAL vulnerabilities)"
+trivy image --severity CRITICAL,HIGH --no-progress --exit-code 1 "$IMAGE_FULL" || { echo "Trivy found HIGH/CRITICAL issues"; exit 4; }
+
+echo "Pushing image to registry: $IMAGE_FULL"
+docker push "$IMAGE_FULL"
+
+# Sign image with cosign (assumes COSIGN_PASSWORD or COSIGN_KEY env / keyless flow)
+if [[ -n "${COSIGN_KEY:-}" ]]; then
+  echo "Signing with cosign key..."
+  cosign sign --key "$COSIGN_KEY" "$IMAGE_FULL"
+else
+  echo "Attempting keyless cosign sign (fulfills transparency via OIDC / keyless mode)."
+  cosign sign --oidc "$IMAGE_FULL" || echo "Keyless sign failed; set COSIGN_KEY to sign."
+fi
+
+echo "Uploaded & signed image: $IMAGE_FULL"
+echo "SBOM saved: $SBOM_FILE"
+echo "Upload SBOM and scan results to your artifact store for audit."
+exit 0
+
diff --git a/scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh b/scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Simple Slurm adapter to submit a DeepSpeed job to Slurm-managed DGX clusters.
+# This script is a scaffold: adapt SBATCH flags to your cluster's partition names and reservation policies.
+#
+# Usage:
+#   ./scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh --nodes 1 --gpus-per-node 8 --script examples/deepspeed/train.sh
+
+set -euo pipefail
+
+NODES=1
+GPUS_PER_NODE=8
+TIME="02:00:00"
+PARTITION="dgx"
+JOB_SCRIPT=""
+JOB_NAME="aegis-deepspeed-$(date +%s)"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --gpus-per-node) GPUS_PER_NODE="$2"; shift 2;;
+    --time) TIME="$2"; shift 2;;
+    --partition) PARTITION="$2"; shift 2;;
+    --script) JOB_SCRIPT="$2"; shift 2;;
+    --job-name) JOB_NAME="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$JOB_SCRIPT" ]]; then
+  echo "--script is required (path to the training wrapper to run under Slurm)"
+  exit 2
+fi
+
+cat > /tmp/${JOB_NAME}.sbatch <<SBATCH
+#!/bin/bash
+#SBATCH --job-name=${JOB_NAME}
+#SBATCH --partition=${PARTITION}
+#SBATCH --nodes=${NODES}
+#SBATCH --gres=gpu:${GPUS_PER_NODE}
+#SBATCH --time=${TIME}
+#SBATCH --output=slurm-%j.out
+#SBATCH --error=slurm-%j.err
+
+module load cuda
+module load nccl
+cd \$SLURM_SUBMIT_DIR
+
+# Ensure NCCL envs (tune as needed)
+export NCCL_DEBUG=INFO
+export NCCL_SOCKET_IFNAME=eth0
+export NCCL_IB_HCA=mlx5_0
+
+echo "Starting training wrapper: $JOB_SCRIPT"
+bash "$JOB_SCRIPT"
+SBATCH
+
+echo "Submitting SBATCH: /tmp/${JOB_NAME}.sbatch"
+JOBID=$(sbatch /tmp/${JOB_NAME}.sbatch | awk '{print $4}')
+echo "Submitted Slurm job: $JOBID"
+echo "Poll logs with: squeue -j $JOBID  and  tail -f slurm-${JOBID}.out"
+exit 0
+
diff --git a/.github/workflows/dgx_image_scan_and_publish.yml b/.github/workflows/dgx_image_scan_and_publish.yml
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/.github/workflows/dgx_image_scan_and_publish.yml
@@ -0,0 +1,220 @@
+name: DGX Image Build, SBOM, Scan & Sign
+
+on:
+  workflow_dispatch:
+    inputs:
+      target:
+        description: "Target image (h100|a100)"
+        required: true
+        default: "h100"
+
+jobs:
+  build-scan-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+
+      - name: Install tooling
+        run: |
+          sudo apt-get update && sudo apt-get install -y curl
+          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+          curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+          curl -sSfL https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64 -o /usr/local/bin/cosign
+          chmod +x /usr/local/bin/cosign
+
+      - name: Build, SBOM, Scan & Sign
+        env:
+          REGISTRY: ${{ secrets.DGX_IMAGE_REGISTRY }}
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }}
+        run: |
+          TARGET="${{ github.event.inputs.target }}"
+          ./scripts/dgx/publish_and_sign_images.sh REGISTRY="$REGISTRY" TARGET="$TARGET"
+
+      - name: Upload SBOM artifact
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-image-sbom
+          path: artifacts || .
+
diff --git a/.github/workflows/dgx_nightly_validation.yml b/.github/workflows/dgx_nightly_validation.yml
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/.github/workflows/dgx_nightly_validation.yml
@@ -0,0 +1,220 @@
+name: DGX Nightly Validation & Chaos (self-hosted)
+
+on:
+  schedule:
+    - cron: '0 3 * * *'  # daily 03:00 UTC
+  workflow_dispatch:
+    inputs:
+      profile:
+        description: "Validation profile"
+        required: false
+        default: "dgx"
+
+jobs:
+  nightly-dgx:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Restore KUBECONFIG
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          if [ -z "$KUBECONFIG_DGX" ]; then echo "KUBECONFIG_DGX secret missing"; exit 1; fi
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+
+      - name: Run NCCL tuning & smoke
+        run: |
+          chmod +x scripts/dgx/nccl_tuning.sh
+          ./scripts/dgx/nccl_tuning.sh --nodes 1 --gpus-per-node 8 --out ./artifacts/nccl || true
+          chmod +x scripts/dgx/dgx_smoke_test.sh
+          ./scripts/dgx/dgx_smoke_test.sh || true
+
+      - name: Run chaos matrix for DGX profile
+        env:
+          PROFILE: dgx
+        run: |
+          # Call your chaos matrix runner with DGX profile if available
+          if [ -x tools/chaos_matrix_runner.sh ]; then
+            ./tools/chaos_matrix_runner.sh --profile dgx --out artifacts/chaos || true
+          else
+            echo "No chaos_matrix_runner available; skipping"
+          fi
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-nightly-artifacts
+          path: artifacts || .
+
+      - name: Cleanup kubeconfig
+        if: always()
+        run: rm -f "$HOME/.kube/config_dgx" || true
+
diff --git a/k8s/manifests/dgx/gpu-operator-values-no-driver.yaml b/k8s/manifests/dgx/gpu-operator-values-no-driver.yaml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/k8s/manifests/dgx/gpu-operator-values-no-driver.yaml
@@ -0,0 +1,64 @@
+# GPU Operator values variant: do not auto-install drivers (for platform-managed drivers)
+driver:
+  enabled: true
+  autoInstall: false
+
+toolkit:
+  enabled: true
+  installMode: "daemonset"
+
+devicePlugin:
+  enabled: true
+
+dcgmExporter:
+  enabled: true
+
+nodeSelector:
+  "node.kubernetes.io/dgx": "true"
+
+rbac:
+  create: true
+
+securityContext:
+  enabled: true
+
+# Use: helm install --namespace gpu-operator gpu-operator nvidia/gpu-operator -f k8s/manifests/dgx/gpu-operator-values-no-driver.yaml
+
diff --git a/docs/dgx/IMAGE_MATRIX.md b/docs/dgx/IMAGE_MATRIX.md
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/docs/dgx/IMAGE_MATRIX.md
@@ -0,0 +1,240 @@
+# Validated DGX Image Matrix (example)
+
+This document records the validated image matrix for DGX images you publish. Keep this file updated when you publish new validated builds.
+
+Example table (fill with your validated values):
+
+- aegis-deepspeed:h100-cuda12.1-pytorch2.2-deepspeed0.9.2
+  - CUDA: 12.1.1
+  - cuDNN: 8.9.x
+  - NCCL: 2.18.0
+  - PyTorch: 2.2.0 (cu121 wheel)
+  - DeepSpeed: 0.9.2
+  - Validated on: 2025-12-01
+  - Notes: tested 1/2/4-node scaling, NCCL env NCCL_IB_HCA=mlx5_0, NCCL_SOCKET_IFNAME=eth0
+
+- aegis-deepspeed:a100-cuda11.8-pytorch2.1-deepspeed0.9.0
+  - CUDA: 11.8.0
+  - cuDNN: 8.x
+  - NCCL: 2.12.x
+  - PyTorch: 2.1.1
+  - DeepSpeed: 0.9.0
+  - Validated on: 2025-11-26
+  - Notes: tuned for A100 DGX nodes; used nvme-fast PVC for checkpoints
+
+How to add a new validated image:
+1. Build and push via scripts/dgx/publish_and_sign_images.sh (produces SBOM and signs image).  
+2. Run tools/benchmarks/deepspeed_dgx_benchmark.sh and scripts/dgx/nccl_tuning.sh to collect artifacts.  
+3. Add an entry to this file with exact pinned versions, date, and notes about NCCL env tuning.
+
+Keep this file under source control so auditors can verify which images were used in production training runs.
+
diff --git a/scripts/dgx/README_slurm_adapter.md b/scripts/dgx/README_slurm_adapter.md
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/scripts/dgx/README_slurm_adapter.md
@@ -0,0 +1,220 @@
+# Slurm Adapter (DGX) — README
+
+This folder contains a simple Slurm adapter scaffold to submit DeepSpeed jobs to DGX clusters that use Slurm (non-k8s).
+
+Files:
+- scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh — sbatch wrapper script
+
+How to use:
+1. Place your training wrapper script (e.g., examples/deepspeed/train.sh) in a location accessible to the compute nodes.
+2. Run:
+   ./scripts/dgx/slurm_adapter/submit_deepspeed_slurm.sh --nodes 1 --gpus-per-node 8 --script examples/deepspeed/train.sh
+
+Customize:
+- Adjust PARTITION, gres, time and module loads in the generated SBATCH script according to your cluster policy.
+- Extend the adapter to post back job status to Aegis control plane (e.g., via a REST call to an orchestrator endpoint) and write decision_log entries.
+
+Notes:
+- This is intended as a scaffold; production deployments should implement secure authentication and robust job lifecycle reporting.
+
diff --git a/.github/workflows/chaos_matrix_with_dgx.yml b/.github/workflows/chaos_matrix_with_dgx.yml
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/.github/workflows/chaos_matrix_with_dgx.yml
@@ -0,0 +1,220 @@
+name: Chaos Matrix (include DGX profile)
+
+on:
+  workflow_dispatch:
+    inputs:
+      profile:
+        description: "Chaos profile (dgx|default)"
+        required: true
+        default: "dgx"
+
+jobs:
+  chaos:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Restore KUBECONFIG
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+
+      - name: Run chaos matrix runner with DGX profile
+        run: |
+          if [ -x tools/chaos_matrix_runner.sh ]; then
+            ./tools/chaos_matrix_runner.sh --profile "${{ github.event.inputs.profile }}" --out artifacts/chaos || true
+          else
+            echo "No chaos_matrix_runner.sh present; skipping"
+          fi
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-chaos-artifacts
+          path: artifacts || .
+
+      - name: Cleanup
+        run: rm -f "$HOME/.kube/config_dgx" || true
+
diff --git a/docs/dgx/PRODUCTION_ENABLEMENT.md b/docs/dgx/PRODUCTION_ENABLEMENT.md
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/docs/dgx/PRODUCTION_ENABLEMENT.md
@@ -0,0 +1,240 @@
+# DGX Production Enablement Checklist
+
+Follow these steps to move DGX support from "functional" to "production-ready" in Aegis.
+
+1) Validate and publish base images
+   - Build images for each validated hardware profile (H100 / A100) using docker/Dockerfile.deepspeed.*
+   - Generate SBOM with syft and scan with Trivy. Sign images with cosign.
+   - Record validated images in docs/dgx/IMAGE_MATRIX.md.
+
+2) Decide driver handling
+   - If DGX Cloud provides certified drivers, use k8s/manifests/dgx/gpu-operator-values-no-driver.yaml (autoInstall=false).
+   - Otherwise let GPU Operator install drivers (default gpu-operator-values.yaml).
+
+3) Configure networking & NCCL tuning
+   - Run scripts/dgx/nccl_tuning.sh and generate suggested env via scripts/dgx/generate_nccl_env_suggest.sh.
+   - Update k8s job templates with tuned NCCL_SOCKET_IFNAME and NCCL_IB_HCA.
+
+4) Storage & checkpoints
+   - Use fast NVMe/Lustre-backed StorageClass for checkpoints (update k8s/manifests/dgx/checkpoints-pvc.yaml).
+   - If using S3, deploy an upload sidecar or sync job to persist checkpoints.
+
+5) CI & validation
+   - Add KUBECONFIG_DGX, DGX_IMAGE_REGISTRY, and COSIGN_KEY to GitHub secrets per docs/dgx/ci_secrets.md.
+   - Use .github/workflows/dgx_image_scan_and_publish.yml to produce signed images and SBOM.
+   - Nightly runs: .github/workflows/dgx_nightly_validation.yml runs smoke and chaos.
+   - Use .github/workflows/chaos_matrix_with_dgx.yml to include DGX profile in chaos CI.
+
+6) Optional Slurm support
+   - If DGX is not running Kubernetes, use scripts/dgx/slurm_adapter/* scaffolds to submit jobs to Slurm and report status back to Aegis orchestrator.
+
+7) Security & supply chain
+   - Enforce image signing and SBOM requirements for images used in production.
+   - Scan images for critical vulnerabilities and fail fast.
+
+8) Observability
+   - Ensure DCGM exporter is scraped by Prometheus and GPU dashboards exist for NVLink/NVSwitch metrics.
+   - Add DGX-specific alerts to notify SRE on ECC errors, NVLink errors, or large drop in GPU utilization.
+
+9) Run final validation
+   - Run the benchmark suite across 1/2/4 nodes and verify canary promote/rollback works with compensation controller.
+   - Ensure decision_log and artifact provenance show SBOM + signed image digest and training manifest commit id.
+
+End of checklist.
+
