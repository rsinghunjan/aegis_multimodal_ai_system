*** Begin Patch
*** Add File:k8s/storage/local-ssd-storageclass.yaml
+apiVersion: storage.k8s.io/v1
+kind: StorageClass
+metadata:
+  name: local-ssd
+provisioner: kubernetes.io/no-provisioner
+volumeBindingMode: WaitForFirstConsumer
+reclaimPolicy: Delete
+mountOptions:
+  - noatime
+  - nodiratime
+parameters:
+  type: "local-ssd"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/storage/local-pv-example.yaml
+apiVersion: v1
+kind: PersistentVolume
+metadata:
+  name: local-nvme-pv-1
+  labels:
+    storage-type: local-ssd
+spec:
+  capacity:
+    storage: 200Gi
+  volumeMode: Filesystem
+  accessModes:
+    - ReadWriteOnce
+  persistentVolumeReclaimPolicy: Delete
+  storageClassName: local-ssd
+  local:
+    path: /mnt/disks/nvme1n1/spark-local-1
+  nodeAffinity:
+    required:
+      nodeSelectorTerms:
+        - matchExpressions:
+            - key: kubernetes.io/hostname
+              operator: In
+              values:
+                - node-with-nvme-1
+
*** End Patch
*** Begin Patch
*** Add File:k8s/spark/spark-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: spark-config
+  namespace: aegis
+data:
+  spark-defaults.conf: |
+    spark.local.dir               /mnt/disks/ssd1/spark
+    spark.eventLog.enabled        true
+    spark.eventLog.dir            hdfs:///spark-event-log
+    spark.history.fs.logDirectory hdfs:///spark-event-log
+    spark.shuffle.service.enabled true
+    spark.dynamicAllocation.enabled true
+    spark.dynamicAllocation.minExecutors 1
+    spark.dynamicAllocation.maxExecutors 50
+
*** End Patch
*** Begin Patch
*** Add File:patroni/postgres-patroni-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: patroni-config
+  namespace: aegis
+data:
+  patroni.yml: |
+    scope: aegis-postgres
+    namespace: /service/
+    name: postgresql
+    restapi:
+      listen: 0.0.0.0:8008
+      connect_address: 0.0.0.0:8008
+    etcd:
+      host: 127.0.0.1:2379
+    bootstrap:
+      dcs:
+        ttl: 30
+        loop_wait: 10
+        retry_timeout: 10
+        maximum_lag_on_failover: 1048576
+      initdb:
+        - encoding: UTF8
+        - data-checksums
+      users:
+        postgres:
+          password: postgres
+          options:
+            - createrole
+    postgresql:
+      listen: 0.0.0.0:5432
+      connect_address: 0.0.0.0:5432
+      data_dir: /var/lib/postgresql/data
+      parameters:
+        max_connections: 200
+        shared_buffers: 256MB
+
*** End Patch
*** Begin Patch
*** Add File:patroni/postgres-patroni-statefulset.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: patroni
+  namespace: aegis
+spec:
+  serviceName: "patroni"
+  replicas: 3
+  selector:
+    matchLabels:
+      app: patroni
+  template:
+    metadata:
+      labels:
+        app: patroni
+    spec:
+      containers:
+        - name: patroni
+          image: patroni:2.1.1
+          env:
+            - name: PATRONI_KUBERNETES_NAMESPACE
+              valueFrom:
+                fieldRef:
+                  fieldPath: metadata.namespace
+          volumeMounts:
+            - name: data
+              mountPath: /var/lib/postgresql/data
+            - name: config
+              mountPath: /etc/patroni
+      volumes:
+        - name: config
+          configMap:
+            name: patroni-config
+  volumeClaimTemplates:
+    - metadata:
+        name: data
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 20Gi
+
*** End Patch
*** Begin Patch
*** Add File:minio/distributed-minio-deployment.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: minio
+  namespace: aegis
+spec:
+  serviceName: "minio"
+  replicas: 4
+  selector:
+    matchLabels:
+      app: minio
+  template:
+    metadata:
+      labels:
+        app: minio
+    spec:
+      containers:
+        - name: minio
+          image: minio/minio:RELEASE.2024-01-01T00-00-00Z
+          args: ["server", "/export"]
+          env:
+            - name: MINIO_ACCESS_KEY
+              valueFrom:
+                secretKeyRef:
+                  name: minio-creds
+                  key: accesskey
+            - name: MINIO_SECRET_KEY
+              valueFrom:
+                secretKeyRef:
+                  name: minio-creds
+                  key: secretkey
+          ports:
+            - containerPort: 9000
+          volumeMounts:
+            - name: export
+              mountPath: /export
+  volumeClaimTemplates:
+    - metadata:
+        name: export
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 500Gi
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: minio
+  namespace: aegis
+spec:
+  ports:
+    - port: 9000
+  selector:
+    app: minio
+
*** End Patch
*** Begin Patch
*** Add File:redis/redis-cluster-helm-values.yaml
+# Minimal values.yaml sample for a Redis Helm chart in staging (tune resources for prod)
+master:
+  persistence:
+    enabled: true
+    size: 10Gi
+replica:
+  persistence:
+    enabled: true
+    size: 10Gi
+resources:
+  master:
+    requests:
+      cpu: "200m"
+      memory: "512Mi"
+    limits:
+      cpu: "1000m"
+      memory: "1Gi"
+  replica:
+    requests:
+      cpu: "200m"
+      memory: "512Mi"
+    limits:
+      cpu: "1000m"
+      memory: "1Gi"
+metrics:
+  enabled: true
+  serviceMonitor:
+    enabled: true
+
*** End Patch
*** Begin Patch
*** Add File:redis/redis-exporter-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: redis-exporter
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: redis-exporter
+  template:
+    metadata:
+      labels:
+        app: redis-exporter
+    spec:
+      containers:
+        - name: redis-exporter
+          image: oliver006/redis_exporter:v1.46.0
+          args:
+            - --redis.addr=redis://redis.aegis.svc:6379
+          ports:
+            - containerPort: 9121
+
*** End Patch
*** Begin Patch
*** Add File:feast/sample_offline_to_online_loader.py
+#!/usr/bin/env python3
+"""
+Sample loader that reads latest offline Parquet files (feast/offline) in S3 and populates Redis online store.
+This is a simple one-off loader for staging demos; production ingestion uses Feast jobs.
+"""
+import os, boto3, pandas as pd, redis, pyarrow.parquet as pq
+from urllib.parse import urlparse
+import tempfile
+
+MODEL_ARTIFACT_BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET")
+REDIS_HOST = os.environ.get("REDIS_HOST","redis.aegis.svc")
+REDIS_PORT = int(os.environ.get("REDIS_PORT","6379"))
+
+def list_latest_parquet(prefix="feast/offline/"):
+    s3 = boto3.client("s3")
+    resp = s3.list_objects_v2(Bucket=MODEL_ARTIFACT_BUCKET, Prefix=prefix)
+    if "Contents" not in resp: return []
+    return sorted([o["Key"] for o in resp["Contents"]])[-5:]
+
+def load_to_redis(keys):
+    s = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
+    for key in keys:
+        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".parquet")
+        boto3.client("s3").download_file(MODEL_ARTIFACT_BUCKET, key, tmp.name)
+        df = pd.read_parquet(tmp.name)
+        for _, row in df.iterrows():
+            entity = int(row["entity_id"])
+            data = {"feature_1": float(row["feature_1"]), "feature_2": int(row["feature_2"])}
+            s.hset(f"feat:entity:{entity}", mapping={k: json.dumps(v) for k,v in data.items()})
+    print("Loaded keys:", keys)
+
+if __name__ == "__main__":
+    import json
+    keys = list_latest_parquet()
+    if keys:
+        load_to_redis(keys)
+    else:
+        print("No offline parquet files found")
+
*** End Patch
*** Begin Patch
*** Add File:benchmarks/redis_latency_bench.py
+#!/usr/bin/env python3
+"""
+Lightweight Redis P95 bench used by CI and Argo benchmark workflows.
+Measures GET latency and reports P50/P95/P99.
+"""
+import redis, time, statistics, argparse, json
+
+def run(host, port, duration=30, qps=100):
+    r = redis.StrictRedis(host=host, port=port, decode_responses=True)
+    lat = []
+    end = time.time() + duration
+    interval = 1.0 / qps
+    while time.time() < end:
+        t0 = time.time()
+        try:
+            r.get("nonexistent:key")
+        except Exception:
+            pass
+        lat.append((time.time() - t0) * 1000.0)
+        time.sleep(interval)
+    p50 = statistics.median(lat)
+    p95 = sorted(lat)[int(len(lat)*0.95)-1] if lat else None
+    p99 = sorted(lat)[int(len(lat)*0.99)-1] if lat else None
+    print(json.dumps({"p50_ms":p50, "p95_ms":p95, "p99_ms":p99, "samples":len(lat)}))
+    return p50, p95, p99
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--host", default="redis.aegis.svc")
+    p.add_argument("--port", type=int, default=6379)
+    p.add_argument("--duration", type=int, default=30)
+    p.add_argument("--qps", type=int, default=100)
+    args = p.parse_args()
+    run(args.host, args.port, args.duration, args.qps)
+
*** End Patch
*** Begin Patch
*** Add File:benchmarks/spark_shuffle_bench.py
+#!/usr/bin/env python3
+"""
+Spark shuffle benchmark script to be run inside a Spark job.
+It generates a large RDD, runs groupByKey/shuffle and computes throughput.
+This is a simple diagnostic; run via spark-submit on your SparkOperator.
+"""
+import time
+from pyspark.sql import SparkSession
+from pyspark.sql.functions import rand
+
+def main():
+    spark = SparkSession.builder.appName("shuffle-bench").getOrCreate()
+    # Generate a large dataset; size tuned by environment
+    n = int(spark.conf.get("spark.shuffle.bench.rows", "10000000"))
+    df = spark.range(0, n).withColumn("k", (rand()*1000).cast("int"))
+    t0 = time.time()
+    grouped = df.rdd.map(lambda r: (r['k'], 1)).groupByKey().mapValues(len).count()
+    t1 = time.time()
+    print("Grouped result count:", grouped)
+    elapsed = t1 - t0
+    print("Elapsed seconds:", elapsed)
+    # Report simple throughput metric: rows/sec
+    print("Throughput rows/sec:", n / max(1.0, elapsed))
+    spark.stop()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/benchmarks/workflow_benchmarks.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-benchmarks-
+  namespace: aegis
+spec:
+  entrypoint: run-benchmarks
+  templates:
+    - name: run-benchmarks
+      steps:
+        - - name: redis-bench
+            template: redis-bench
+        - - name: inference-bench
+            template: inference-bench
+        - - name: spark-shuffle-bench
+            template: spark-shuffle-bench
+
+    - name: redis-bench
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install redis && python3 benchmarks/redis_latency_bench.py --host redis.aegis.svc --port 6379 --duration 30 --qps 200
+
+    - name: inference-bench
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests && python3 serving/bench/load_test.py --target http://model-proxy.aegis.svc:80 --duration 30 --qps 50
+
+    - name: spark-shuffle-bench
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - kubectl apply -f spark/bench/spark-shuffle-job.yaml -n aegis && echo "Spark job submitted"
+
*** End Patch
*** Begin Patch
*** Add File:spark/bench/spark-shuffle-job.yaml
+apiVersion: "sparkoperator.k8s.io/v1beta2"
+kind: SparkApplication
+metadata:
+  name: aegis-shuffle-bench
+  namespace: aegis
+spec:
+  type: Python
+  mode: cluster
+  image: registry.example.com/aegis/pyspark-job:latest
+  mainApplicationFile: local:///app/benchmarks/spark_shuffle_bench.py
+  sparkVersion: "3.3.2"
+  driver:
+    cores: 1
+    memory: "2g"
+    labels:
+      bench: shuffle
+  executor:
+    cores: 2
+    memory: "4g"
+    instances: 4
+  sparkConf:
+    spark.shuffle.bench.rows: "2000000"
+
*** End Patch
*** Begin Patch
*** Add File:policy/gatekeeper/ct_image_signed.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: imagesigned
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireImageSigned
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package imagesigned
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Pod"
+          containers := input.review.object.spec.containers
+          not all_images_signed(containers)
+          msg := "One or more container images are not signed or lack cosign signature annotation"
+        }
+
+        all_images_signed(containers) {
+          all([is_signed(c) | c := containers[_]])
+        }
+
+        is_signed(c) {
+          # Check for an annotation on the pod that indicates cosign verification happened earlier,
+          # or check the image has a specific label/annotation in your image metadata pipeline.
+          ann := input.review.object.metadata.annotations
+          ann["aegis/image-signed"] == "true"
+        }
+
*** End Patch
*** Begin Patch
*** Add File:policy/gatekeeper/constraint_require_image_signed.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireImageSigned
+metadata:
+  name: require-image-signed
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["aegis", "aegis-prod"]
+
*** End Patch
*** Begin Patch
*** Add File:spire/linkerd_spire_quickstart.md
+# SPIRE + Linkerd Quickstart (staging guidance)
+#
+# Summary:
+#  - Install Linkerd to add mTLS and service identity.
+#  - Install SPIRE to issue SPIFFE IDs to workloads and integrate with Linkerd for attestation.
+#
+# Steps (high-level):
+# 1. Install Linkerd:
+#    curl -sL https://run.linkerd.io/install | sh
+#    linkerd install | kubectl apply -f -
+#    linkerd check
+# 2. Deploy SPIRE (use SPIRE k8s manifests):
+#    kubectl apply -f https://raw.githubusercontent.com/spiffe/spire/main/examples/k8s/manifests/spire-all.yaml
+# 3. Register workloads:
+#    Use spire-server CLI or API to create entries mapping ServiceAccount selectors -> SPIFFE IDs.
+#    Example helper script (register_workload.sh) below.
+# 4. Test workload can obtain SVID and Linkerd can use it for identity.
+#
+# Caveats:
+#  - Production: use HA SPIRE server, external DB, and proper CA rotation.
+#  - Integrate Linkerd identity with Gatekeeper/OPA policies for admission controls.
+
*** End Patch
*** Begin Patch
*** Add File:spire/register_workload.sh
+#!/usr/bin/env bash
+# Example script - run where spire-server CLI is available and configured
+# Registers a workload entry mapping a Kubernetes serviceAccount to a SPIFFE ID
+# Usage: ./register_workload.sh <spiffe_id> <namespace> <serviceaccount>
+set -euo pipefail
+SPIFFE_ID=${1:-"spiffe://example.org/ns/aegis/sa/aegis-agent"}
+NAMESPACE=${2:-aegis}
+SA=${3:-aegis-agent}
+kubectl -n spire exec deploy/spire-server -- /opt/spire/bin/spire-server entry create \
+  -spiffeID "${SPIFFE_ID}" \
+  -selector "k8s:ns:${NAMESPACE}" \
+  -selector "k8s:sa:${SA}" \
+  -parentID "spiffe://example.org/spire/server"
+echo "Registered SPIFFE ID ${SPIFFE_ID} for ${NAMESPACE}:${SA}"
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/pkcs11_latency_exporter.py
+#!/usr/bin/env python3
+"""
+PKCS#11 latency checker and Prometheus metrics exporter (simple HTTP server).
+- Measures sign operation latency (using sign_with_retry HSM path if available).
+- Exposes metrics: hsm_sign_latency_seconds_count, hsm_sign_latency_seconds_sum, hsm_sign_success_total, hsm_sign_failure_total
+"""
+from http.server import BaseHTTPRequestHandler, HTTPServer
+import time, threading, json
+from prometheus_client import CollectorRegistry, Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
+
+registry = CollectorRegistry()
+hsm_latency = Histogram('hsm_sign_latency_seconds', 'HSM sign latency seconds', registry=registry, buckets=(0.01,0.05,0.1,0.2,0.5,1,2,5))
+hsm_success = Counter('hsm_sign_success_total', 'Successful HSM sign ops', registry=registry)
+hsm_failure = Counter('hsm_sign_failure_total', 'Failed HSM sign ops', registry=registry)
+
+def measure_sign():
+    try:
+        # Use sign_with_retry with a small payload; fallback if module not present
+        payload = b"latency-test"
+        from production.policy.signing.sign_with_retry import sign_payload
+        t0 = time.time()
+        sig, meta = sign_payload(payload, None)
+        latency = time.time() - t0
+        if sig:
+            hsm_latency.observe(latency)
+            hsm_success.inc()
+        else:
+            hsm_failure.inc()
+    except Exception:
+        hsm_failure.inc()
+
+class Handler(BaseHTTPRequestHandler):
+    def do_GET(self):
+        if self.path == "/metrics":
+            data = generate_latest(registry)
+            self.send_response(200)
+            self.send_header("Content-Type", CONTENT_TYPE_LATEST)
+            self.end_headers()
+            self.wfile.write(data)
+        else:
+            self.send_response(404)
+            self.end_headers()
+
+def periodic_measure(interval=60):
+    while True:
+        measure_sign()
+        time.sleep(interval)
+
+if __name__ == "__main__":
+    t = threading.Thread(target=periodic_measure, args=(60,), daemon=True)
+    t.start()
+    server = HTTPServer(('0.0.0.0', 9102), Handler)
+    print("HSM latency exporter listening on :9102")
+    server.serve_forever()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/hsm/hsm-latency-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: hsm-latency-check
+  namespace: aegis
+spec:
+  schedule: "*/5 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-agent
+          containers:
+            - name: hsm-latency
+              image: python:3.10-slim
+              command: ["bash","-lc"]
+              args:
+                - pip install prometheus_client boto3 && python3 ops/hsm/pkcs11_latency_exporter.py
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:prometheus/scrape_config_example.md
+# Prometheus scrape config snippets (add to prometheus.yml or ServiceMonitor)
+#
+# Scrape Redis exporter:
+#  - job_name: 'redis-exporter'
+#    static_configs:
+#      - targets: ['redis-exporter.aegis.svc:9121']
+#
+# Scrape HSM latency exporter:
+#  - job_name: 'hsm-latency'
+#    static_configs:
+#      - targets: ['hsm-latency-job.aegis.svc:9102']
+#
+# For Helm-prometheus / kube-prometheus-stack use ServiceMonitors to automatically discover services
+
*** End Patch
*** Begin Patch
*** Add File:docs/DEPLOY_A_TO_D_PLAYBOOK.md
+# Deployment Playbook for A/B/C/D (Data-center focused)
+#
+# Overview:
+# - A: Provision StorageClass, local PV, Spark config that uses NVMe local disks
+# - B: Benchmarks: Redis lookup, inference load, and Spark shuffle throughput via Argo
+# - C: Linkerd + SPIRE quickstart and Gatekeeper constraints for image-sign and MCP enforcement
+# - D: HSM PKCS#11 latency exporter & CronJob; Prometheus scrape config
+#
+# Steps (staging):
+# 1. Create namespace: kubectl create ns aegis
+# 2. Create StorageClass and local PV(s):
+#    kubectl apply -f k8s/storage/local-ssd-storageclass.yaml
+#    kubectl apply -f k8s/storage/local-pv-example.yaml
+# 3. Apply Spark ConfigMap:
+#    kubectl apply -f k8s/spark/spark-configmap.yaml -n aegis
+# 4. Deploy Redis (helm or manifest) and exporter:
+#    kubectl apply -f redis/redis-cluster-helm-values.yaml -n aegis (if using helm)
+#    kubectl apply -f redis/redis-exporter-deployment.yaml -n aegis
+# 5. Deploy MinIO (manifest) if using local object store:
+#    kubectl apply -f minio/distributed-minio-deployment.yaml -n aegis
+# 6. Deploy Patroni Postgres statefulset:
+#    kubectl apply -f patroni/postgres-patroni-configmap.yaml -n aegis
+#    kubectl apply -f patroni/postgres-patroni-statefulset.yaml -n aegis
+# 7. Deploy Gatekeeper constraints:
+#    kubectl apply -f policy/gatekeeper/ct_image_signed.yaml
+#    kubectl apply -f policy/gatekeeper/constraint_require_image_signed.yaml
+# 8. (Optional) Install Linkerd and SPIRE following spire/linkerd_spire_quickstart.md
+# 9. Deploy HSM latency exporter CronJob:
+#    kubectl apply -f k8s/hsm/hsm-latency-cronjob.yaml -n aegis
+# 10. Submit benchmark workflow:
+#    argo submit argo/benchmarks/workflow_benchmarks.yaml -n aegis --watch
+#
+# Notes:
+# - Replace image names and bucket names with your environment-specific values.
+# - For production readiness: convert manifests to Helm charts, add proper RBAC, configure PV provisioning and secrets (Vault).
+#
+# Acceptance:
+# - Redis bench P95 within configured SLO
+# - Model inference benchmark P95 within SLO
+# - Spark shuffle bench completes and reports throughput
+# - HSM latency exporter reports metrics to Prometheus and shows stable latency
+# - Gatekeeper denies pods without required image-signed annotation
+
*** End Patch
