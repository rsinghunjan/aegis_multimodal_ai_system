*** Begin Patch
*** Add File: .github/workflows/pr_assistant.yml
+name: PR Assistant (AI)
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+    paths:
+      - 'infra/**'
+      - 'scripts/**'
+      - 'k8s/**'
+      - 'docs/**'
+      - '.github/workflows/**'
+
+jobs:
+  pr-assistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests
+
+      - name: Run PR Assistant
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+          LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }} # optional alternative
+          REPO: ${{ github.repository }}
+          PR_NUMBER: ${{ github.event.pull_request.number }}
+        run: |
+          python scripts/pr_assistant.py
+
*** End Patch
*** Add File: scripts/pr_assistant.py
+#!/usr/bin/env python3
+"""
+Lightweight PR Assistant: summarize infra PR changes and post LLM-based review suggestions.
+
+Requirements:
+  - Python requests
+  - In Actions: set OPENAI_API_KEY (or LLM_ENDPOINT/LLM_API_KEY) and GITHUB_TOKEN
+
+Behavior & safety:
+  - Does NOT send secrets to external LLMs (it strips files matching obvious secret patterns)
+  - Posts a single comment on the PR with suggestions and recommended next steps
+  - Uses repo context to create a RAG-like prompt; this is a scaffold — adapt prompts & redaction rules for your environment
+"""
+import os
+import re
+import json
+import requests
+from typing import List
+
+GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
+REPO = os.getenv("REPO")
+PR_NUMBER = os.getenv("PR_NUMBER")
+OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
+LLM_ENDPOINT = os.getenv("LLM_ENDPOINT")  # optional alternative endpoint
+
+API_BASE = "https://api.github.com"
+
+def gh_get(path):
+    url = f"{API_BASE}/repos/{REPO}{path}"
+    r = requests.get(url, headers={"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"})
+    r.raise_for_status()
+    return r.json()
+
+def gh_post(path, payload):
+    url = f"{API_BASE}/repos/{REPO}{path}"
+    r = requests.post(url, headers={"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}, json=payload)
+    r.raise_for_status()
+    return r.json()
+
+def redaction_filter(content: str) -> str:
+    # Very conservative redaction: remove lines that look like keys/credentials or 'password'
+    lines = []
+    for line in content.splitlines():
+        if re.search(r'(?i)(secret|password|api[_-]?key|access[_-]?key|private[_-]?key)', line):
+            lines.append("[REDACTED_SENSITIVE_LINE]")
+        else:
+            lines.append(line)
+    return "\n".join(lines)
+
+def fetch_changed_files(pr_number: str) -> List[dict]:
+    files = gh_get(f"/pulls/{pr_number}/files")
+    return files
+
+def build_prompt(changed_files: List[dict]) -> str:
+    header = (
+        "You are an AI assistant that helps review infrastructure and ops changes.\n"
+        "Summarize risks, missing tests, security considerations, and suggest mitigations or code snippets.\n"
+        "Be concise and include actionable next steps.\n\n"
+    )
+    body = []
+    for f in changed_files:
+        filename = f.get("filename")
+        patch = f.get("patch") or ""
+        # Keep small extracts only
+        excerpt = "\n".join(patch.splitlines()[:120])
+        safe_excerpt = redaction_filter(excerpt)
+        body.append(f"File: {filename}\nPatch excerpt:\n{safe_excerpt}\n---\n")
+    prompt = header + "\n".join(body)
+    return prompt
+
+def call_llm(prompt: str) -> str:
+    # Simple OpenAI ChatCompletion call if OPENAI_API_KEY present.
+    if OPENAI_API_KEY:
+        url = "https://api.openai.com/v1/chat/completions"
+        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
+        data = {
+            "model": "gpt-4o-mini" if False else "gpt-4o", # placeholder; adjust to your subscription
+            "messages": [{"role": "system", "content": "You are a concise infra/security reviewer."},
+                         {"role": "user", "content": prompt}],
+            "max_tokens": 800,
+            "temperature": 0.2
+        }
+        r = requests.post(url, headers=headers, json=data)
+        r.raise_for_status()
+        resp = r.json()
+        # extract text
+        return resp["choices"][0]["message"]["content"].strip()
+    elif LLM_ENDPOINT:
+        # Generic LLM endpoint: POST prompt and get `response` field
+        r = requests.post(LLM_ENDPOINT, json={"prompt": prompt})
+        r.raise_for_status()
+        return r.json().get("response", "")
+    else:
+        return "LLM not configured. Set OPENAI_API_KEY or LLM_ENDPOINT to enable PR assistant."
+
+def post_pr_comment(pr_number: str, body: str):
+    marker = "<!-- aegis-pr-assistant -->"
+    full = f"{marker}\n{body}"
+    # find existing bot comment
+    comments = gh_get(f"/issues/{pr_number}/comments")
+    existing = None
+    for c in comments:
+        if marker in (c.get("body") or ""):
+            existing = c
+            break
+    if existing:
+        # update
+        url = f"{API_BASE}/repos/{REPO}/issues/comments/{existing['id']}"
+        r = requests.patch(url, headers={"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}, json={"body": full})
+        r.raise_for_status()
+    else:
+        gh_post(f"/issues/{pr_number}/comments", {"body": full})
+
+def main():
+    if not (GITHUB_TOKEN and REPO and PR_NUMBER):
+        print("GITHUB_TOKEN, REPO, and PR_NUMBER must be set in the environment.")
+        return
+    files = fetch_changed_files(PR_NUMBER)
+    prompt = build_prompt(files)
+    llm_resp = call_llm(prompt)
+    summary = (
+        "**Aegis PR Assistant** — automated suggestions\n\n"
+        "Summary of suggestions (concise):\n\n"
+        f"{llm_resp}\n\n"
+        "_This comment was generated by an automated assistant. Treat suggestions as advisory; perform human review._"
+    )
+    post_pr_comment(PR_NUMBER, summary)
+    print("Posted PR assistant comment.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Add File: .github/workflows/anomaly_detector.yml
+name: Anomaly Detector (Prometheus -> Slack/Issue)
+
+on:
+  schedule:
+    - cron: '0 */4 * * *'  # every 4 hours
+  workflow_dispatch: {}
+
+jobs:
+  detect:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Run anomaly detector
+        run: |
+          chmod +x scripts/anomaly_detector.sh
+          ./scripts/anomaly_detector.sh
+        env:
+          PROM_URL: ${{ secrets.PROM_URL }}
+          PROM_QUERY: ${{ secrets.PROM_QUERY }}
+          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
+          GITHUB_REPOSITORY: ${{ github.repository }}
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          ALLOW_AUTO_ISSUE: ${{ secrets.ALLOW_AUTO_ISSUE }}
+
*** End Patch
*** Add File: scripts/anomaly_detector.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple anomaly detector scaffold:
+# - Queries PROM_URL with PROM_QUERY (both set via secrets)
+# - If matches/anomalies found, posts a concise message to Slack and optionally opens a GitHub Issue
+#
+# Configure via repo/organization secrets:
+#  PROM_URL (http://prometheus:9090), PROM_QUERY (PromQL), SLACK_WEBHOOK_URL, GITHUB_TOKEN
+PROM_URL="${PROM_URL:-}"
+PROM_QUERY="${PROM_QUERY:-'increase(container_cpu_usage_seconds_total[5m]) > 0'}"
+SLACK_WEBHOOK_URL="${SLACK_WEBHOOK_URL:-}"
+GITHUB_REPOSITORY="${GITHUB_REPOSITORY:-}"
+GITHUB_TOKEN="${GITHUB_TOKEN:-}"
+ALLOW_AUTO_ISSUE="${ALLOW_AUTO_ISSUE:-false}"
+
+if [ -z "$PROM_URL" ]; then
+  echo "PROM_URL not set; skipping anomaly detection."
+  exit 0
+fi
+
+query_prom() {
+  url="${PROM_URL}/api/v1/query"
+  resp=$(curl -sS --get --data-urlencode "query=${PROM_QUERY}" "$url")
+  echo "$resp"
+}
+
+parse_and_notify() {
+  resp="$1"
+  status=$(echo "$resp" | jq -r '.status')
+  if [ "$status" != "success" ]; then
+    echo "Prometheus query failed or returned non-success"
+    return 1
+  fi
+  result_count=$(echo "$resp" | jq '.data.result | length')
+  if [ "$result_count" -gt 0 ]; then
+    title="Aegis Anomaly Detector: ${result_count} matches for PROM_QUERY"
+    body="Detected ${result_count} result(s) for configured PROM_QUERY.\n\nQuery: ${PROM_QUERY}\n\nSample result (JSON):\n\`\`\`json\n$(echo "$resp" | jq '.data.result[0]')\n\`\`\`\n"
+
+    # Post to Slack (best-effort)
+    if [ -n "$SLACK_WEBHOOK_URL" ]; then
+      payload=$(jq -n --arg t "$title" --arg b "$body" '{text: ($t + "\n\n" + $b)}')
+      curl -sS -X POST -H 'Content-type: application/json' --data "$payload" "$SLACK_WEBHOOK_URL" || echo "Slack post failed"
+    else
+      echo "No SLACK_WEBHOOK_URL configured; skipping Slack notification."
+    fi
+
+    # Optionally open GitHub issue for the anomaly
+    if [ "$ALLOW_AUTO_ISSUE" = "true" ] && [ -n "$GITHUB_TOKEN" ] && [ -n "$GITHUB_REPOSITORY" ]; then
+      gh_api="https://api.github.com/repos/${GITHUB_REPOSITORY}/issues"
+      issue_body=$(jq -n --arg t "$title" --arg b "$body" '{title:$t, body:$b, labels:["aegis-anomaly", "automated"]}')
+      curl -sS -X POST -H "Authorization: token ${GITHUB_TOKEN}" -H "Accept: application/vnd.github+json" -d "$issue_body" "$gh_api" >/dev/null || echo "Failed to create issue"
+    else
+      echo "Auto-issue disabled or missing GITHUB_TOKEN/GITHUB_REPOSITORY; skipping issue creation."
+    fi
+  else
+    echo "No anomalies detected (result_count=0)."
+  fi
+}
+
+resp=$(query_prom)
+parse_and_notify "$resp"
+
*** End Patch
*** Add File: docs/AI_ROADMAP.md
+## Aegis AI Platform Roadmap (scaffold)
+
+Goal: Make Aegis an AI-driven platform that reasons about infra and ML workloads and assists/automates ops, security, and developer workflows.
+
+This repo patch adds two safe scaffolds:
+- PR Assistant (LLM-driven) — runs on infra PRs, summarizes risks, suggests mitigations, posts PR comments.
+- Anomaly Detector (Prometheus -> Slack/Issue) — scheduled run that notifies on metric anomalies.
+
+Next recommended steps:
+1. Securely configure secrets: OPENAI_API_KEY (or LLM_ENDPOINT + key), PROM_URL, PROM_QUERY, SLACK_WEBHOOK_URL.
+2. Tune PR Assistant prompts and add redaction rules and an internal vector store for RAG.
+3. Add human-in-the-loop approval flows for any automated actions.
+4. Instrument audit logging for all AI decisions and actions (immutable store).
+5. Expand to: model registry, decision engine operator (CRD), predictive autoscaler, cost optimizer, and policy auto-fixes.
+
+Security notes:
+- Never send secrets to external LLMs. Use redaction and RAG with local docs.
+- Use short-lived credentials and scoped service accounts for actioning.
+- Keep recommendation mode separate from action mode until confidence & safety are proven.
+
*** End Patch
*** End Patch
