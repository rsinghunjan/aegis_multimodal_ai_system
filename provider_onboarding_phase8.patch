*** Begin Patch
*** Add File: automation/run_full_e2e_and_adapt.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrator: run provider E2E on a secure runner (K8s Job), collect results,
+# analyze failures and produce adapter/controller remediation suggestions.
+#
+# Dependencies:
+#  - kubectl configured (or KUBECONFIG env)
+#  - jq, grep, sed, awk
+#  - scripts/trigger_provider_job.sh (uses k8s/jobs/provider-endtoend-job.yaml)
+#  - scripts/provider_stress_test.py
+#
+# Usage:
+#  ./automation/run_full_e2e_and_adapt.sh --provider ibm --namespace aegis --out ./e2e_results --wait 600
+
+PROVIDER="ibm"
+NAMESPACE="aegis"
+OUT_DIR="./e2e_results"
+WAIT=600
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --provider) PROVIDER="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    --wait) WAIT="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 [--provider ibm|braket] [--namespace aegis] [--out ./e2e_results] [--wait 600]"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "Triggering provider end-to-end job for provider=$PROVIDER in namespace=$NAMESPACE ..."
+./scripts/trigger_provider_job.sh "$PROVIDER" "$NAMESPACE" "$WAIT" "$OUT_DIR" || echo "trigger job returned non-zero"
+
+echo "Running provider stress test (count=20, concurrency=4) ..."
+python3 scripts/provider_stress_test.py --provider "$PROVIDER" --count 20 --concurrency 4 > "$OUT_DIR/provider_stress_test.log" 2>&1 || true
+
+echo "Collecting logs and summaries..."
+if [ -d "$OUT_DIR/results" ]; then
+  cp -r "$OUT_DIR/results" "$OUT_DIR/provider_job_results" 2>/dev/null || true
+fi
+if [ -d "provider_results" ]; then
+  cp -r provider_results "$OUT_DIR/" 2>/dev/null || true
+fi
+
+# find summary.json
+SUMMARY=""
+if [ -f "$OUT_DIR/provider_job_results/summary.json" ]; then
+  SUMMARY="$OUT_DIR/provider_job_results/summary.json"
+elif [ -f "$OUT_DIR/results/summary.json" ]; then
+  SUMMARY="$OUT_DIR/results/summary.json"
+fi
+
+echo "Analyzing results..."
+ISSUES=()
+if [ -n "$SUMMARY" ] && jq -e '.ok == true' "$SUMMARY" >/dev/null 2>&1; then
+  echo "Provider E2E indicated success (summary.ok == true)."
+else
+  echo "Provider E2E failed or summary missing. Inspecting logs for common provider error patterns..."
+  # search logs under OUT_DIR for error patterns
+  PATTERNS=("429" "RateLimit" "throttl" "503" "ServiceUnavailable" "quota" "Timeout" "connection refused" "401" "permission denied" "permissiondenied" "Insufficient")
+  MATCHED=()
+  while IFS= read -r -d '' f; do
+    for p in "${PATTERNS[@]}"; do
+      if grep -i -m1 -H -- "$p" "$f" >/dev/null 2>&1; then
+        MATCHED+=("$p:$f")
+      fi
+    done
+  done < <(find "$OUT_DIR" -type f -name "*.log" -print0 2>/dev/null || true)
+
+  if [ ${#MATCHED[@]} -gt 0 ]; then
+    echo "Detected the following issue signatures in logs:"
+    printf '%s\n' "${MATCHED[@]}" | sed 's/^/ - /'
+    ISSUES+=("${MATCHED[@]}")
+  else
+    echo "No provider-specific error signatures found in logs; checking pod logs for k8s issues..."
+    # collect common k8s pod issues
+    kubectl get pods -n "$NAMESPACE" -o wide > "$OUT_DIR/pods_after_e2e.txt" 2>/dev/null || true
+    if grep -E "ImagePullBackOff|CrashLoopBackOff|ErrImagePull|FailedMount" "$OUT_DIR/pods_after_e2e.txt" >/dev/null 2>&1; then
+      ISSUES+=("k8s_pod_state_problems")
+    fi
+  fi
+fi
+
+echo "Generating remediation suggestions..."
+REMED_FILE="$OUT_DIR/remediation_suggestions.md"
+cat > "$REMED_FILE" <<EOF
+# Remediation Suggestions (generated)
+
+Provider: $PROVIDER
+Namespace: $NAMESPACE
+Date: $(date -u +"%Y-%m-%dT%H:%M:%SZ")
+
+Summary:
+- Provider E2E results and stress test logs are in: $OUT_DIR
+- Issues detected: $( [ ${#ISSUES[@]} -gt 0 ] && printf '%s; ' "${ISSUES[@]}" || echo "none" )
+
+Suggested next steps:
+EOF
+
+if printf '%s\n' "${ISSUES[@]}" | grep -qi "429\|RateLimit\|throttl"; then
+  cat >> "$REMED_FILE" <<EOF
+- Throttling / Rate-limit detected:
+  - Implement exponential backoff with jitter in provider adapters and controller retry logic.
+  - Suggested code-level change (pseudocode):
+
+    retry(attempts=5, backoff=exp_jitter(base=1.0, cap=60.0)) {
+      submit_to_provider(...)
+    }
+
+  - Consider adding provider-side rate-limit headers parsing and graceful queueing.
+EOF
+fi
+
+if printf '%s\n' "${ISSUES[@]}" | grep -qi "503\|ServiceUnavailable"; then
+  cat >> "$REMED_FILE" <<EOF
+- Transient service errors (503):
+  - Treat as transient, implement retries and longer backoff windows.
+  - Ensure controller marks job as 'PendingRetry' and does not discard results.
+EOF
+fi
+
+if printf '%s\n' "${ISSUES[@]}" | grep -qi "quota"; then
+  cat >> "$REMED_FILE" <<EOF
+- Quota-related failures:
+  - Add billing/cost guardrails to prevent exceeding provider quotas.
+  - Ensure admin approval flows for high-cost jobs.
+EOF
+fi
+
+if printf '%s\n' "${ISSUES[@]}" | grep -qi "k8s_pod_state_problems"; then
+  cat >> "$REMED_FILE" <<EOF
+- Kubernetes pod state problems (ImagePullBackOff / CrashLoopBackOff / FailedMount):
+  - Inspect 'kubectl describe pod <pod>' and pod events for image pull errors or PVC mount failures.
+  - Ensure correct imagePullSecrets, PVCs created and storage class available.
+  - Increase resources if OOMKill seen (check pod events/logs).
+EOF
+fi
+
+cat >> "$REMED_FILE" <<EOF
+Manual actions and followups:
+- Provide provider sandbox credentials or ensure secure runner is available and try E2E again.
+- If errors persist, capture full logs and open an issue with the provider. Use the remediation_suggestions.md as context.
+- Optionally, create a branch with adapter retry/backoff patch and run CI to validate.
+
+EOF
+
+echo "Remediation suggestions written to $REMED_FILE"
+echo "E2E orchestration complete. Collected artifacts under $OUT_DIR"
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: automation/operator_bootstrap_and_rotator.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper: Run the one-click operator bootstrap (consume terraform outputs),
+# deploy rotator CronJob, validate secret and token, and report status.
+#
+# Usage:
+#  ./automation/operator_bootstrap_and_rotator.sh --tf-json tf_out.json --vault-addr https://vault.example.com --deploy-rotator
+
+TF_JSON=""
+VAULT_ADDR=""
+NAMESPACE="aegis"
+DEPLOY_ROTATOR="true"
+WAIT=120
+OUT_DIR="./bootstrap_results"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --tf-json) TF_JSON="$2"; shift 2;;
+    --vault-addr) VAULT_ADDR="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --no-rotator) DEPLOY_ROTATOR="false"; shift;;
+    --wait) WAIT="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --tf-json tf_out.json --vault-addr https://vault.example.com [--namespace aegis] [--no-rotator] [--wait 120]"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [ -z "$TF_JSON" ] || [ -z "$VAULT_ADDR" ]; then
+  echo "tf-json and vault-addr are required" >&2
+  exit 2
+fi
+
+mkdir -p "$OUT_DIR"
+
+echo "Bootstrapping k8s secret from Terraform outputs..."
+./operator/bootstrap_from_tf_outputs.sh --tf-json "$TF_JSON" --vault-addr "$VAULT_ADDR" --deploy-rotator
+echo "Bootstrap step completed; checking vault-credentials secret..."
+
+if kubectl get secret vault-credentials -n "$NAMESPACE" -o jsonpath='{.data.role_id}' >/dev/null 2>&1; then
+  echo "vault-credentials secret exists in namespace $NAMESPACE"
+  kubectl get secret vault-credentials -n "$NAMESPACE" -o yaml > "$OUT_DIR/vault-credentials.yaml"
+else
+  echo "vault-credentials secret not found; aborting." >&2
+  exit 3
+fi
+
+if [ "$DEPLOY_ROTATOR" = "true" ]; then
+  echo "Deploying vault-token-rotator CronJob..."
+  kubectl apply -f k8s/cronjobs/vault-token-rotator-cronjob.yaml
+  echo "Waiting up to $WAIT seconds for rotator to populate runtime token in vault-credentials..."
+  SECONDS_WAIT=0
+  while [ $SECONDS_WAIT -lt $WAIT ]; do
+    if kubectl get secret vault-credentials -n "$NAMESPACE" -o jsonpath='{.data.token}' >/dev/null 2>&1; then
+      echo "vault-credentials contains runtime token (base64). Rotator active."
+      kubectl get secret vault-credentials -n "$NAMESPACE" -o yaml > "$OUT_DIR/vault-credentials-post-rotator.yaml"
+      break
+    fi
+    sleep 5
+    SECONDS_WAIT=$((SECONDS_WAIT+5))
+  done
+  if [ $SECONDS_WAIT -ge $WAIT ]; then
+    echo "Timed out waiting for rotator to populate token; check CronJob logs" >&2
+    kubectl get jobs -n "$NAMESPACE" -l job-name -o wide > "$OUT_DIR/rotator_jobs.txt" || true
+    exit 4
+  fi
+fi
+
+echo "Operator bootstrap and rotator deployment finished. Artifacts in $OUT_DIR"
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: automation/argocd_sync_collect_and_remediate.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Drive ArgoCD sync (if available), run staging tests, collect logs and generate remediation steps.
+#
+# Usage:
+#  ./automation/argocd_sync_collect_and_remediate.sh --results ./staging_results --wait 300
+
+RESULTS_DIR="./staging_results"
+WAIT=300
+NAMESPACES=("aegis" "monitoring" "observability" "kube-system")
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --results) RESULTS_DIR="$2"; shift 2;;
+    --wait) WAIT="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 [--results ./staging_results] [--wait 300]"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$RESULTS_DIR"
+
+echo "Syncing Argo apps (argo/apps/*.yaml) via argocd CLI if available..."
+for f in argo/apps/*.yaml; do
+  appname="$(basename "$f" .yaml)"
+  if command -v argocd >/dev/null 2>&1; then
+    echo "Syncing $appname ..."
+    argocd app sync "$appname" || echo "argocd sync failed for $appname"
+    argocd app wait "$appname" --health --timeout 300s || echo "wait timed out for $appname"
+  else
+    echo "argocd CLI not found, please ensure ArgoCD auto-sync is enabled or use ArgoCD UI to sync $appname"
+  fi
+done
+
+echo "Waiting ${WAIT}s for resources to settle..."
+sleep "$WAIT"
+
+echo "Running staging tests (Redis failover, provider stress tests)..."
+python3 scripts/redis_failover_test.py > "$RESULTS_DIR/redis_failover.log" 2>&1 || echo "redis failover test produced errors"
+python3 scripts/provider_stress_test.py --provider ibm --count 30 --concurrency 6 > "$RESULTS_DIR/provider_stress.log" 2>&1 || echo "provider stress test produced errors"
+
+echo "Collecting pod lists and logs for namespaces: ${NAMESPACES[*]}"
+for ns in "${NAMESPACES[@]}"; do
+  mkdir -p "$RESULTS_DIR/$ns"
+  kubectl get pods -n "$ns" -o wide > "$RESULTS_DIR/$ns/pods.txt" 2>/dev/null || true
+  while IFS= read -r pod; do
+    name=$(echo "$pod" | awk '{print $1}')
+    if [ "$name" = "NAME" ] || [ -z "$name" ]; then
+      continue
+    fi
+    echo "Collecting logs for pod $name in $ns"
+    kubectl logs -n "$ns" "$name" --all-containers --tail=500 > "$RESULTS_DIR/$ns/${name}.log" 2>&1 || echo "Could not collect logs for $name"
+  done < <(kubectl get pods -n "$ns" -o custom-columns=NAME:.metadata.name --no-headers 2>/dev/null || true)
+done
+
+echo "Analyzing collected logs for common failure patterns..."
+ANALYSIS="$RESULTS_DIR/analysis.txt"
+> "$ANALYSIS"
+
+check_patterns() {
+  local pattern="$1"
+  grep -R --line-number -i "$pattern" "$RESULTS_DIR" || true
+}
+
+echo "Kubernetes common issues:" >> "$ANALYSIS"
+check_patterns "ImagePullBackOff" >> "$ANALYSIS" || true
+check_patterns "ErrImagePull" >> "$ANALYSIS" || true
+check_patterns "CrashLoopBackOff" >> "$ANALYSIS" || true
+check_patterns "FailedMount" >> "$ANALYSIS" || true
+check_patterns "Back-off pulling image" >> "$ANALYSIS" || true
+
+echo -e "\nProvider & Vault issues:" >> "$ANALYSIS"
+check_patterns "RateLimit" >> "$ANALYSIS" || true
+check_patterns "429" >> "$ANALYSIS" || true
+check_patterns "503" >> "$ANALYSIS" || true
+check_patterns "permission denied" >> "$ANALYSIS" || true
+check_patterns "401" >> "$ANALYSIS" || true
+check_patterns "Vault" >> "$ANALYSIS" || true
+
+echo -e "\nGenerating remediation report (docs/remediation_report.md) ..." 
+REPORT="$RESULTS_DIR/remediation_report.md"
+cat > "$REPORT" <<EOF
+# Staging Test Remediation Report
+
+Generated: $(date -u +"%Y-%m-%dT%H:%M:%SZ")
+
+Summary:
+- Redis failover logs: $RESULTS_DIR/redis_failover.log
+- Provider stress logs: $RESULTS_DIR/provider_stress.log
+- Pod lists & logs: $RESULTS_DIR
+
+Detected issues (from analysis):
+EOF
+sed -n '1,200p' "$ANALYSIS" >> "$REPORT"
+
+cat >> "$REPORT" <<EOF
+
+Suggested remediations (automated suggestions):
+
+1) Image pull / CrashLoopBackOff:
+ - Ensure imagePullSecrets are configured for private registries.
+ - Verify the image tag exists and that nodes can reach the registry.
+ - Increase resources (cpu/memory) in Deployment values if OOMKilled.
+
+2) FailedMount / PVC problems:
+ - Verify storage class exists and PVCs bound.
+ - Increase PVC size if pods fail due to size constraints.
+
+3) Vault / permission issues:
+ - Ensure vault-credentials secret exists and contains a token or approle values.
+ - If missing, ask Vault admin to run terraform/vault/ and deliver tf_out.json; operator then runs create_k8s_secret_from_tf_outputs.sh
+
+4) Provider rate limits / 429/503:
+ - Implement retries with exponential backoff + jitter in adapters and controller.
+ - Respect provider-supplied rate-limit headers where available.
+
+5) RBAC errors:
+ - Ensure ServiceAccounts have roles bound (see k8s/rbac/* manifests).
+
+Operational next steps:
+- Fix the highest-severity items in the report, redeploy, and re-run this script.
+- If provider failures persist, collect full provider logs and consider contacting provider support with request id and timestamps.
+
+EOF
+
+echo "Remediation report written to $REPORT"
+echo "All staging artifacts and logs are in $RESULTS_DIR"
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: docs/remediation_example_templates.md
+Examples of remediation patches & PR templates (manual)
+----------------------------------------------------
+
+If the E2E or staging tests detect rate limiting or retry problems, consider the following quick patch template to add retry+backoff helper to adapters:
+
+Patch idea (pseudocode):
+
+1) Add utils/retry_backoff.py
+--------------------------------
+def exp_backoff_jitter(attempt, base=1.0, cap=60.0):
+    import random, math
+    backoff = min(cap, base * (2 ** attempt))
+    return backoff * (0.5 + random.random() * 0.5)
+
+2) Use in providers/qiskit_runtime_adapter_real.py and providers/braket_adapter.py:
+--------------------------------
+from utils.retry_backoff import exp_backoff_jitter
+for attempt in range(max_attempts):
+    try:
+        submit()
+        break
+    except TransientProviderError as e:
+        sleep(exp_backoff_jitter(attempt))
+        continue
+
+Create a PR with the above and add unit tests that simulate 429/503 responses.
+
+For k8s infra issues like ImagePullBackOff:
+- Update Deployment/Helm values to include imagePullSecrets and correct image tags.
+- Ensure storage classes exist and PVCs are bound: kubectl get pvc -n aegis
+
+For Vault issues:
+- Ensure approle role_id/secret_id are created via terraform and secrets applied to k8s via create_k8s_secret_from_tf_outputs.sh
+- Ensure rotator CronJob is running and that vault-credentials secret gets a .data.token field
+
*** End Patch
*** End Patch
