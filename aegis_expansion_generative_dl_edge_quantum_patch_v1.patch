*** Begin Patch
*** Add File: llm/gateway.py
+#!/usr/bin/env python3
+"""
+Lightweight LLM gateway for routing requests to hosted LLM providers and enforcing
+rate / token usage / cost controls. Exposes:
+ - POST /v1/generate {model, prompt, max_tokens, tenant}
+ - GET  /metrics (Prometheus)
+
+Usage: run in Kubernetes as a Deployment behind ingress. Secrets (API keys) must be
+retrieved via Vault / SecretProvider, not stored in repo.
+"""
+import os, time, asyncio
+from fastapi import FastAPI, HTTPException, Request
+from pydantic import BaseModel
+import httpx
+from prometheus_client import start_http_server, Counter, Histogram, Gauge
+
+API_KEYS = {}  # resolved by Vault in production via init container or CSI
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+TOKEN_COUNTER = Counter("llm_tokens_total", "Total LLM tokens used", ["tenant", "model"])
+REQ_COUNTER = Counter("llm_requests_total", "Total LLM requests", ["tenant", "model"])
+LATENCY = Histogram("llm_request_latency_seconds", "LLM latency", ["model"])
+CURRENT_INFLIGHT = Gauge("llm_inflight_requests", "Inflight LLM requests")
+
+app = FastAPI(title="Aegis LLM Gateway")
+
+class GenRequest(BaseModel):
+    model: str
+    prompt: str
+    max_tokens: int = 256
+    tenant: str
+
+def provider_call_openai(payload, api_key):
+    # example for hosted OpenAI-compatible provider
+    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
+    r = httpx.post("https://api.openai.com/v1/completions", json=payload, headers=headers, timeout=30.0)
+    r.raise_for_status()
+    return r.json()
+
+@app.post("/v1/generate")
+async def generate(req: GenRequest):
+    # simple tenant-level rate limit (placeholder). In real infra use Redis quota.
+    tenant = req.tenant
+    model = req.model
+    REQ_COUNTER.labels(tenant=tenant, model=model).inc()
+    CURRENT_INFLIGHT.inc()
+    start = time.time()
+    try:
+        # Choose provider API key from Vault-resolved map (simplified)
+        api_key = API_KEYS.get(model) or os.environ.get("OPENAI_API_KEY")
+        if not api_key:
+            raise HTTPException(status_code=503, detail="No API key configured for model")
+        payload = {"model": model, "prompt": req.prompt, "max_tokens": req.max_tokens, "temperature": 0.2}
+        with LATENCY.labels(model=model).time():
+            loop = asyncio.get_event_loop()
+            resp = await loop.run_in_executor(None, provider_call_openai, payload, api_key)
+        # record token usage if provider returns usage info
+        usage = resp.get("usage") or {}
+        tokens = usage.get("total_tokens") or usage.get("token_count") or req.max_tokens
+        TOKEN_COUNTER.labels(tenant=tenant, model=model).inc(int(tokens))
+        # quick hallucination detector hook: check for "I don't know" vs confident factual claims (simple)
+        text = resp.get("choices",[{}])[0].get("text","")
+        if "I don't know" in text or "I am not sure" in text:
+            # emit an operator alert (best effort)
+            if OPERATOR_WEBHOOK:
+                try:
+                    httpx.post(OPERATOR_WEBHOOK, json={"alert":"llm_low_confidence","tenant":tenant,"model":model,"snippet":text[:200]}, timeout=2.0)
+                except Exception:
+                    pass
+        return {"result": text, "usage": usage, "raw": resp}
+    finally:
+        CURRENT_INFLIGHT.dec()
+
+@app.get("/health")
+def health():
+    return {"status":"ok"}
+
+if __name__=="__main__":
+    # start metrics server for Prometheus on sidecar port 9105
+    start_http_server(int(os.environ.get("METRICS_PORT","9105")))
+    import uvicorn
+    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", "8085")))
+
*** End Patch
*** Begin Patch
*** Add File: llm/prompt_registry.py
+#!/usr/bin/env python3
+"""
+Simple prompt registry to store prompts, kits and versions for reproducibility.
+Operators should store registry in a secure DB; this is a simple SQLite-backed example.
+"""
+import os, sqlite3, json
+DB = os.environ.get("PROMPT_DB", "/data/prompt_registry.db")
+
+def ensure_db():
+    os.makedirs(os.path.dirname(DB), exist_ok=True)
+    c = sqlite3.connect(DB)
+    cur = c.cursor()
+    cur.execute("""CREATE TABLE IF NOT EXISTS prompts (
+        id INTEGER PRIMARY KEY AUTOINCREMENT,
+        tenant TEXT,
+        name TEXT,
+        version TEXT,
+        prompt TEXT,
+        metadata TEXT,
+        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
+    )""")
+    c.commit(); c.close()
+
+def register_prompt(tenant, name, version, prompt, metadata=None):
+    ensure_db()
+    c = sqlite3.connect(DB)
+    cur = c.cursor()
+    cur.execute("INSERT INTO prompts (tenant,name,version,prompt,metadata) VALUES (?, ?, ?, ?, ?)",
+                (tenant,name,version,prompt,json.dumps(metadata or {})))
+    c.commit()
+    c.close()
+
+def get_prompt(tenant, name, version=None):
+    ensure_db()
+    c = sqlite3.connect(DB)
+    cur = c.cursor()
+    if version:
+        cur.execute("SELECT prompt, metadata FROM prompts WHERE tenant=? AND name=? AND version=? ORDER BY created_at DESC LIMIT 1",
+                    (tenant,name,version))
+    else:
+        cur.execute("SELECT prompt, metadata FROM prompts WHERE tenant=? AND name=? ORDER BY created_at DESC LIMIT 1",
+                    (tenant,name))
+    row = cur.fetchone()
+    c.close()
+    if not row:
+        return None
+    return {"prompt": row[0], "metadata": json.loads(row[1] or "{}")}
+
+if __name__=="__main__":
+    register_prompt("demo","summarize","v1","Summarize the following text in one paragraph.", {"example_input":"..."})
+    print(get_prompt("demo","summarize"))
+
*** End Patch
*** Begin Patch
*** Add File: agents/langchain_adapter.py
+"""
+Agent framework skeleton integrating LangChain-style agents with tool adapters.
+This adapter wires an agent to tools: scheduler, ledger, rekor, hsm.
+Safety gates: pre-execution approval hook and sandboxing (dry-run).
+"""
+from typing import Dict, Any
+import os, requests
+
+# placeholder: in real deployment install langchain and actual LLM connector
+
+class ToolAdapter:
+    def call(self, input: Dict[str,Any]) -> Dict[str,Any]:
+        raise NotImplementedError
+
+class SchedulerAdapter(ToolAdapter):
+    def __init__(self, url):
+        self.url = url
+    def call(self, input):
+        r = requests.post(self.url+"/submit-job", json=input, timeout=5)
+        return {"status": r.status_code, "body": r.json() if r.ok else r.text}
+
+class LedgerAdapter(ToolAdapter):
+    def __init__(self, url):
+        self.url = url
+    def call(self, input):
+        r = requests.post(self.url+"/ledger/event", json=input, timeout=5)
+        return {"status": r.status_code}
+
+class RekorAdapter(ToolAdapter):
+    def __init__(self, rekor_url):
+        self.rekor_url = rekor_url
+    def call(self, artifact):
+        # minimal Rekor lookup
+        try:
+            r = requests.get(f"{self.rekor_url}/api/v1/log/entries", timeout=5)
+            return {"ok": r.ok}
+        except Exception as e:
+            return {"error": str(e)}
+
+class HSMAdapter(ToolAdapter):
+    def __init__(self, sign_service_url):
+        self.url = sign_service_url
+    def call(self, payload):
+        # route sign requests to operator-managed sign service (requires approval)
+        r = requests.post(self.url+"/sign", json=payload, timeout=10)
+        return {"status": r.status_code, "text": r.text}
+
+class Agent:
+    def __init__(self, tools: Dict[str, ToolAdapter], approval_hook=None):
+        self.tools = tools
+        self.approval_hook = approval_hook
+
+    def act(self, plan):
+        # plan is list of actions: [{"tool":"scheduler","input":{...}}, ...]
+        # enforce safety: approval hook
+        if self.approval_hook and not self.approval_hook(plan):
+            return {"error":"approval_required"}
+        results = []
+        for step in plan:
+            tool = self.tools.get(step.get("tool"))
+            if not tool:
+                results.append({"error":"tool_missing", "tool": step.get("tool")})
+                continue
+            res = tool.call(step.get("input"))
+            results.append(res)
+        return {"results": results}
+
+def example():
+    tools = {
+        "scheduler": SchedulerAdapter("http://scheduler-service.aegis.svc:8201"),
+        "ledger": LedgerAdapter("http://ledger.aegis.svc:8300"),
+        "rekor": RekorAdapter("https://rekor.example"),
+        "hsm": HSMAdapter("http://hsm-sign.aegis.svc:8110")
+    }
+    def approve(plan):
+        # simple policy: require operator for any sign action
+        for s in plan:
+            if s.get("tool") == "hsm":
+                return False
+        return True
+    agent = Agent(tools, approval_hook=approve)
+    plan = [{"tool":"scheduler","input":{"job_id":"job-1","tenant":"alpha","estimated_kgco2e":1.2}}]
+    print(agent.act(plan))
+
+if __name__=="__main__":
+    example()
+
*** End Patch
*** Begin Patch
*** Add File: observability/llm_metrics.py
+"""
+Extra observability helpers for LLM/agent usage:
+ - export Prometheus metrics for token usage and hallucination counts
+ - simple hallucination detector stub using keyword heuristics
+"""
+from prometheus_client import start_http_server, Counter, Gauge
+from typing import Optional
+import re
+
+TOKENS = Counter("aegis_llm_tokens_total", "Total tokens used by tenant", ["tenant"])
+HALLU_COUNT = Counter("aegis_llm_hallucinations_total", "Detected hallucination events", ["tenant"])
+INFLIGHT = Gauge("aegis_llm_inflight", "LLM inflight requests")
+
+HALLU_PATTERNS = [re.compile(r"\bi don't know\b", re.I), re.compile(r"\bnot sure\b", re.I)]
+
+def detect_hallucination(text: str) -> bool:
+    for p in HALLU_PATTERNS:
+        if p.search(text):
+            return True
+    return False
+
+def record_usage(tenant: str, tokens: int, text: Optional[str]):
+    TOKENS.labels(tenant=tenant).inc(tokens)
+    if text and detect_hallucination(text):
+        HALLU_COUNT.labels(tenant=tenant).inc()
+
+if __name__=="__main__":
+    start_http_server(9111)
+    print("LLM metrics server running")
+    import time
+    while True:
+        time.sleep(10)
+
*** End Patch
*** Begin Patch
*** Add File: dl/gpu_cluster_k8s.yaml
+apiVersion: cluster.x-k8s.io/v1alpha4
+kind: MachineDeployment
+metadata:
+  name: aegis-gpu-workers
+  namespace: kube-system
+spec:
+  replicas: 2
+  template:
+    spec:
+      bootstrap:
+        configRef:
+          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha4
+      clusterName: aegis
+      version: v1.26.0
+  # This is a placeholder manifest illustrating a GPU nodepool; actual provider-specific node pool needed
+---
+# Example Pod with GPU resource request (training job template)
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-train-example
+  namespace: aegis-ml
+spec:
+  template:
+    spec:
+      containers:
+      - name: trainer
+        image: aegis/trainer:gpu-latest
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: "32Gi"
+            cpu: "8"
+        env:
+          - name: S3_BUCKET
+            value: "s3://aegis-ml-data"
+        command: ["python", "train.py", "--config", "/etc/config/train.yaml"]
+      restartPolicy: Never
+  backoffLimit: 2
+
*** End Patch
*** Begin Patch
*** Add File: dl/mlflow_config.md
+# MLflow integration (operator guidance)
+
+- Deploy mlflow server + backend DB + artifact store (S3/MinIO).
+- Configure training jobs to log experiments to MLflow (tracking URI in env).
+- Example env for training job:
+  - MLFLOW_TRACKING_URI=http://mlflow.aegis.svc:5000
+  - AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY for artifact store
+
+- Use MLflow model registry for DL artifacts and integrate with model_registry API for promotions.
+
*** End Patch
*** Begin Patch
*** Add File: dl/model_registry_checksum.py
+#!/usr/bin/env python3
+"""
+Utility to compute checksums for model artifacts and store metadata in model registry.
+Ensures artifact integrity for DL models.
+"""
+import hashlib, json, os
+from forecast.model_registry import register_model
+
+def sha256_file(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def publish_model(path, region, mae=None):
+    checksum = sha256_file(path)
+    # upload artifact to S3 first (operator must implement)
+    s3_path = f"s3://models/{os.path.basename(path)}"
+    model_id = register_model(s3_path, region, mae)
+    # persist checksum in registry metadata (simplified)
+    print("Registered model", model_id, "checksum", checksum)
+
+if __name__=="__main__":
+    import sys
+    publish_model(sys.argv[1], "US")
+
*** End Patch
*** Begin Patch
*** Add File: inference/kserve_inference.yaml
+apiVersion: "serving.kubeflow.org/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-onnx-inference"
+  namespace: "aegis-ml"
+spec:
+  predictor:
+    onnx:
+      storageUri: "s3://models/onnx/my_model"
+      resources:
+        limits:
+          cpu: "4"
+          memory: "8Gi"
+        requests:
+          cpu: "2"
+          memory: "4Gi"
+
+# Canary rollout / autoscaling should be configured via Knative settings and KServe traffic routing
+
*** End Patch
*** Begin Patch
*** Add File: edge/packager.py
+#!/usr/bin/env python3
+"""
+Simple model packager to convert a PyTorch model to ONNX and produce a quantized TF-Lite artifact (if applicable).
+This is a local tool example; integrate into CI for production edge packaging.
+"""
+import torch, os, subprocess
+
+def to_onnx(model_path, out_path, input_shape=(1,3,224,224)):
+    model = torch.load(model_path, map_location="cpu")
+    model.eval()
+    dummy = torch.randn(*input_shape)
+    torch.onnx.export(model, dummy, out_path, opset_version=11)
+    print("Wrote ONNX", out_path)
+
+if __name__=="__main__":
+    import argparse
+    p=argparse.ArgumentParser()
+    p.add_argument("--model")
+    p.add_argument("--out")
+    args=p.parse_args()
+    to_onnx(args.model, args.out)
+
*** End Patch
*** Begin Patch
*** Add File: edge/ota_service.py
+#!/usr/bin/env python3
+"""
+Minimal OTA service to register packaged models and provide download URLs to device agents.
+Records versions and signatures for safe rollout.
+"""
+import os, json, sqlite3
+DB = "/data/ota.db"
+os.makedirs("/data", exist_ok=True)
+conn = sqlite3.connect(DB)
+cur = conn.cursor()
+cur.execute("CREATE TABLE IF NOT EXISTS artifacts (id INTEGER PRIMARY KEY, name TEXT, version TEXT, s3_path TEXT, signature TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)")
+conn.commit()
+
+def register(name, version, s3_path, signature):
+    cur.execute("INSERT INTO artifacts (name,version,s3_path,signature) VALUES (?,?,?,?)", (name,version,s3_path,signature))
+    conn.commit()
+
+def list_artifacts():
+    cur.execute("SELECT id,name,version,s3_path,signature,created_at FROM artifacts ORDER BY created_at DESC")
+    return cur.fetchall()
+
+if __name__=="__main__":
+    register("onnx-sample","v1","s3://models/onnx/onnx-sample","sig-abc123")
+    print(list_artifacts())
+
*** End Patch
*** Begin Patch
*** Add File: edge/device_agent.py
+#!/usr/bin/env python3
+"""
+Lightweight device agent that polls OTA service, downloads artifacts, verifies signature and applies models.
+Intended for edge devices with constrained resources.
+"""
+import requests, time, os, subprocess
+OTA_URL = os.environ.get("OTA_URL", "http://ota-service.aegis.svc")
+CHECK_INTERVAL = int(os.environ.get("OTA_CHECK_SEC","300"))
+
+def fetch_and_install(artifact):
+    s3 = artifact.get("s3_path")
+    # operator must provision credentials; here we simulate download by wget
+    out = "/tmp/artifact.bin"
+    try:
+        # placeholder for actual S3 download
+        subprocess.check_call(["/bin/true"])
+        # verify signature (placeholder)
+        print("Installed artifact", artifact)
+    except Exception as e:
+        print("install failed", e)
+
+def main_loop():
+    while True:
+        try:
+            r = requests.get(OTA_URL + "/artifacts")
+            if r.ok:
+                arts = r.json()
+                for a in arts:
+                    fetch_and_install(a)
+        except Exception as e:
+            print("ota error", e)
+        time.sleep(CHECK_INTERVAL)
+
+if __name__=="__main__":
+    main_loop()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/qiskit_demo.py
+#!/usr/bin/env python3
+"""
+Small demo that runs a simple circuit on a local Qiskit Aer simulator.
+Intended as a research integration POC; real quantum work requires cloud providers.
+"""
+from qiskit import QuantumCircuit, Aer, execute
+
+def run_simple():
+    qc = QuantumCircuit(2,2)
+    qc.h(0)
+    qc.cx(0,1)
+    qc.measure([0,1],[0,1])
+    backend = Aer.get_backend('qasm_simulator')
+    job = execute(qc, backend, shots=1024)
+    res = job.result().get_counts()
+    print("Counts:", res)
+
+if __name__=="__main__":
+    run_simple()
+
*** End Patch
*** Begin Patch
*** Add File: security/vault_k8s_example.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: vault-credentials
+  namespace: aegis
+type: Opaque
+data:
+  # Example placeholder; operator must use Vault Agent Injector or CSI driver
+  VAULT_TOKEN: ""
+
+---
+# SecretProviderClass (example) already provided elsewhere; use Vault Agent or CSI provider to mount secrets
+
*** End Patch
*** Begin Patch
*** Add File: security/oidc_ingress_example.yaml
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: aegis-ui-oidc
+  namespace: aegis
+  annotations:
+    nginx.ingress.kubernetes.io/auth-url: "https://oauth2-proxy.aegis.svc/oauth2/auth"
+    nginx.ingress.kubernetes.io/auth-signin: "https://oauth2-proxy.aegis.svc/oauth2/start?rd=$request_uri"
+spec:
+  rules:
+    - host: aegis-ui.example.com
+      http:
+        paths:
+          - path: /
+            pathType: Prefix
+            backend:
+              service:
+                name: tenant-ui
+                port:
+                  number: 80
+
*** End Patch
*** Begin Patch
*** Add File: compliance/pre_promotion_llm_sop.md
+# Pre-promotion SOP for LLM & Generative Model Promotion
+
+Before promoting a new LLM prompt kit or LLM model to production:
+1. Run prompt registry validation: ensure prompt version and tenant mappings are recorded.
+2. Run CI PI validation for region(s) (forecast/region_pi_validation.py) if model is forecasting.
+3. Run hallucination tests on held-out prompts; ensure hallucination rate below configured threshold.
+4. Ensure token usage and cost estimate is acceptable and recorded.
+5. If model performs actions (agents), run sandboxed agent test with approval hook enabled.
+6. Require operator signoff and archival of test artifacts to COMPLIANCE_BUCKET.
+
*** End Patch
*** Begin Patch
*** Add File: ci/llm_pre_promotion_gate.yml
+name: LLM Pre-Promotion Gate
+on:
+  workflow_dispatch:
+    inputs:
+      tenant:
+        required: true
+      prompt_name:
+        required: true
+      prompt_version:
+        required: true
+
+jobs:
+  run-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install deps
+        run: pip install requests
+      - name: Run prompt & small hallucination test
+        env:
+          PROMPT_DB: /tmp/prompt_registry.db
+        run: |
+          python llm/prompt_registry.py  # ensures DB exists
+          # placeholder for actual tests: operator must implement test harness
+          echo "Run hallucination suite (operator implementation required)"
+      - name: Fail if pen-test findings open
+        run: python compliance/check_pen_findings.py
+
*** End Patch
*** End Patch
