*** Begin Patch
*** Add File:runbooks/telemetry_calibration_retrain_RUNBOOK.md
+# Telemetry → Calibration → Retrain Automated Runbook
+
+Goal
+- Automate the full pipeline: collect telemetry → run calibration → train runtime predictor → publish artifacts (ConfigMap) → run validation checks.
+
+Prereqs
+- kubectl configured for cluster with aegis namespace
+- AWS CLI creds with access to evidence S3 and EKS/Redshift as needed
+- GitHub repo has CI runner with kubectl and aws CLI (or a runner with kubeconfig)
+- cert-manager, Redis, Prometheus already deployed in cluster
+- Images built to run calibration and training:
+  - ghcr.io/yourorg/aegis-calibration:latest (contains calibration/calibration_pipeline.py)
+  - ghcr.io/yourorg/aegis-runtime-train:latest (contains runtime_predictor/train_model.py)
+
+High-level steps (automated)
+1. CI kick-off (manual or scheduled) triggers GitHub Action which:
+   a. Applies K8s calibration Job (ci/jobs/calibration_job.yaml) that mounts the telemetry PVC and runs calibration_pipeline.py
+   b. Waits for job completion, then creates/updates ConfigMap `aegis-power-profiles` in `aegis` namespace
+   c. Applies K8s training Job (ci/jobs/runtime_train_job.yaml) that consumes runtime telemetry PVC and writes model artifact to a shared PVC
+   d. After training, CI collects the model file from the training Job pod (or from PVC) and publishes ConfigMap `runtime-model-cm`
+   e. Runs Great Expectations validations via a K8s Job and collects results into S3 evidence bucket
+2. CI publishes final signed evidence (cosign) and Rekor entry for audit
+3. Operators review GE results and reconciliation metrics; human approval is required before applying power profile changes to production.
+
+Operator controls & safety
+- Jobs run in `aegis` namespace and use restricted service accounts (see ci/jobs/*)
+- Auto-apply to production ConfigMaps only after passing validations and (optionally) manual approval step.
+- All generated artifacts are uploaded to evidence S3 and signed with cosign KMS key.
+
+How to run manually (one-off)
+- kubectl apply -f ci/jobs/calibration_job.yaml
+- kubectl -n aegis wait --for=condition=complete job/aegis-calibration-manual --timeout=1800s
+- kubectl apply -f ci/jobs/runtime_train_job.yaml
+- After job completes, fetch model: kubectl -n aegis cp <pod>:/tmp/runtime_model.pkl ./runtime_model.pkl
+- Create ConfigMap: kubectl -n aegis create configmap runtime-model-cm --from-file=runtime_model.pkl=./runtime_model.pkl -o yaml --dry-run=client | kubectl apply -f -
+
+Rollback & recovery
+- CI creates a snapshot of previous ConfigMaps (s3://${EVIDENCE_BUCKET}/calibration/backups/) before applying new ones.
+- If new profiles cause regression, restore previous ConfigMap and re-run GE validations.
+
+Contacts & escalation
+- SRE: oncall@sre.example.com
+- ML platform owner: ml-platform@example.com
+
*** End Patch
*** Begin Patch
*** Add File:ci/jobs/calibration_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-calibration-manual
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-ci-sa
+      containers:
+        - name: calibrate
+          image: ghcr.io/yourorg/aegis-calibration:latest
+          command: ["/bin/sh","-c"]
+          args:
+            - python3 /opt/calibration/calibration_pipeline.py --telem-dir /data/telemetry --out /tmp/power_profiles.yaml && \
+              kubectl -n aegis create configmap aegis-power-profiles --from-file=power_profiles.yaml=/tmp/power_profiles.yaml --dry-run=client -o yaml | kubectl apply -f - && \
+              aws s3 cp /tmp/power_profiles.yaml s3://${EVIDENCE_BUCKET:-aegis-evidence}/calibration/power_profiles_$(date -u +%Y%m%dT%H%M%SZ).yaml || true
+          volumeMounts:
+            - name: telemetry
+              mountPath: /data/telemetry
+            - name: kube
+              mountPath: /root/.kube
+      restartPolicy: Never
+      volumes:
+        - name: telemetry
+          persistentVolumeClaim:
+            claimName: telemetry-pvc
+        - name: kube
+          hostPath:
+            path: /root/.kube
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:ci/jobs/runtime_train_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-runtime-train
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-ci-sa
+      containers:
+        - name: train
+          image: ghcr.io/yourorg/aegis-runtime-train:latest
+          command: ["/bin/sh","-c"]
+          args:
+            - python3 /opt/runtime_predictor/train_model.py --input-dir /data/runtime --out /tmp/runtime_model.pkl && \
+              cp /tmp/runtime_model.pkl /shared/runtime_model.pkl || true
+          volumeMounts:
+            - name: runtime
+              mountPath: /data/runtime
+            - name: shared
+              mountPath: /shared
+      restartPolicy: Never
+      volumes:
+        - name: runtime
+          persistentVolumeClaim:
+            claimName: runtime-telemetry-pvc
+        - name: shared
+          persistentVolumeClaim:
+            claimName: shared-artifacts-pvc
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/telemetry_calibration.yml
+name: telemetry-calibration
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 6 * * 1' # weekly Monday 06:00 UTC
+
+env:
+  KUBECONFIG: ${{ secrets.KUBECONFIG }}
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+
+jobs:
+  run-pipeline:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.28.0'
+
+      - name: Configure kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+
+      - name: Apply calibration job
+        run: |
+          kubectl apply -f ci/jobs/calibration_job.yaml
+          kubectl -n aegis wait --for=condition=complete job/aegis-calibration-manual --timeout=1800s
+
+      - name: Apply runtime train job
+        run: |
+          kubectl apply -f ci/jobs/runtime_train_job.yaml
+          kubectl -n aegis wait --for=condition=complete job/aegis-runtime-train --timeout=3600s
+
+      - name: Extract runtime model from shared PVC (example using a helper pod)
+        run: |
+          # Copy runtime model from a temporary pod that mounts the shared PVC
+          POD=$(kubectl -n aegis run tmpcopy --restart=Never --rm -i --image=alpine -- sh -c "sleep 3600" & sleep 2; kubectl get pod -n aegis -l run=tmpcopy -o name | head -n1)
+          kubectl -n aegis cp ${POD#pod/}:/shared/runtime_model.pkl ./runtime_model.pkl || true
+          kubectl -n aegis delete pod -l run=tmpcopy || true
+          if [ -f ./runtime_model.pkl ]; then
+            kubectl -n aegis create configmap runtime-model-cm --from-file=runtime_model.pkl=./runtime_model.pkl --dry-run=client -o yaml | kubectl apply -f -
+          fi
+
+      - name: Run GE validations (optional)
+        run: |
+          kubectl -n aegis create job ge-run --image=ghcr.io/yourorg/aegis-ge-runner:latest -- curl -sS || true
+
+      - name: Upload evidence & sign (placeholder)
+        run: |
+          echo "Signing artifacts (placeholder)"
+          # example: cosign sign --key awskms://${{ secrets.COSIGN_KMS_KEY_ARN }} /tmp/power_profiles.yaml || true
+
*** End Patch
*** Begin Patch
*** Add File:extender/kube_scheduler_extender.py
+#!/usr/bin/env python3
+"""
+Kubernetes Scheduler Extender scaffold (HTTP service)
+- Implements /filter and /prioritize endpoints.
+- For pods/workflows annotated with aegis.carbon.hint, apply filtering/prioritization:
+  * Filter out nodes that don't match nodeSelector hints (e.g., lifecycle=spot required)
+  * Prioritize nodes with label carbon_preference=low
+
+Note: To enable, operator must configure kube-scheduler policy to call this extender.
+"""
+from flask import Flask, request, jsonify
+import os, json
+
+app = Flask("aegis-scheduler-extender")
+
+def node_matches_selector(node, selector):
+    for k,v in selector.items():
+        if node.get("labels", {}).get(k) != v:
+            return False
+    return True
+
+@app.route("/filter", methods=["POST"])
+def filter_nodes():
+    body = request.get_json(force=True)
+    pod = body.get("pod", {})
+    nodes = body.get("nodes", {}).get("items", [])
+    annotations = pod.get("metadata", {}).get("annotations", {}) or {}
+    hint_json = annotations.get("aegis.carbon.hint")
+    if not hint_json:
+        # allow all nodes
+        return jsonify({"nodes": {"items": nodes}})
+    hint = json.loads(hint_json)
+    selector = {}
+    if hint.get("prefer_spot"):
+        selector["lifecycle"] = "spot"
+    filtered = [n for n in nodes if node_matches_selector(n, selector)] if selector else nodes
+    return jsonify({"nodes": {"items": filtered}})
+
+@app.route("/prioritize", methods=["POST"])
+def prioritize():
+    body = request.get_json(force=True)
+    nodes = body.get("nodes", {}).get("items", [])
+    scores = []
+    for idx, node in enumerate(nodes):
+        # higher score for carbon_preference=low
+        score = 0
+        if node.get("labels", {}).get("carbon_preference") == "low":
+            score += 100
+        # lower score for spot (we may prefer spot, so increase)
+        if node.get("labels", {}).get("lifecycle") == "spot":
+            score += 50
+        scores.append({"host": node.get("name"), "score": score})
+    return jsonify({"hostPriorityList": scores})
+
+@app.route("/healthz")
+def healthz():
+    return "ok"
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
+
*** End Patch
*** Begin Patch
*** Add File:extender/deploy_extender.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-scheduler-extender
+  namespace: kube-system
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-scheduler-extender
+  template:
+    metadata:
+      labels:
+        app: aegis-scheduler-extender
+    spec:
+      containers:
+        - name: extender
+          image: ghcr.io/yourorg/aegis-scheduler-extender:latest
+          ports:
+            - containerPort: 8000
+          env:
+            - name: PORT
+              value: "8000"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-scheduler-extender
+  namespace: kube-system
+spec:
+  selector:
+    app: aegis-scheduler-extender
+  ports:
+    - port: 80
+      targetPort: 8000
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/eks_spot_nodegroup/README.md
+# EKS Spot Nodegroup Terraform Module
+
+This module provisions an EKS managed nodegroup with capacity_type = "SPOT".
+
+Inputs (example)
+- cluster_name
+- nodegroup_name
+- node_role_arn
+- subnet_ids
+- instance_types
+- min_size / max_size / desired_size
+
+Outputs
+- node_group_arn
+- node_group_name
+
+Usage
+- Call from your infra repo where AWS provider and EKS cluster exist.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/eks_spot_nodegroup/main.tf
+variable "cluster_name" {}
+variable "nodegroup_name" {}
+variable "subnet_ids" { type = list(string) }
+variable "node_role_arn" {}
+variable "instance_types" { type = list(string) }
+variable "min_size" { default = 1 }
+variable "max_size" { default = 4 }
+variable "desired_size" { default = 1 }
+
+resource "aws_eks_node_group" "spot" {
+  cluster_name    = var.cluster_name
+  node_group_name = var.nodegroup_name
+  node_role_arn   = var.node_role_arn
+  subnet_ids      = var.subnet_ids
+  scaling_config {
+    desired_size = var.desired_size
+    min_size     = var.min_size
+    max_size     = var.max_size
+  }
+  instance_types = var.instance_types
+  capacity_type  = "SPOT"
+  tags = {
+    "lifecycle" = "spot"
+  }
+}
+
+output "node_group_arn" {
+  value = aws_eks_node_group.spot.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/redshift/README.md
+# Redshift module (skeleton)
+Provision Redshift cluster, IAM role and optionally create external schema/stage for S3 parquet ingestion.
+
+Operators must configure networking, subnet groups, and secure access.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/redshift/main.tf
+variable "cluster_identifier" {}
+variable "node_type" { default = "ra3.xlplus" }
+variable "master_username" {}
+variable "master_password" {}
+variable "cluster_type" { default = "single-node" }
+
+resource "aws_redshift_cluster" "aegis" {
+  cluster_identifier = var.cluster_identifier
+  node_type          = var.node_type
+  database_name      = "dev"
+  master_username    = var.master_username
+  master_password    = var.master_password
+  cluster_type       = var.cluster_type
+}
+
+output "endpoint" {
+  value = aws_redshift_cluster.aegis.endpoint
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/snowflake/README.md
+# Snowflake module skeleton
+This contains a sample for creating a stage that points to S3 and a simple role.
+Operator should adapt to Snowflake provider and account settings.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/modules/snowflake/main.tf
+# Placeholder Snowflake resources - operator to adapt to their Snowflake provider and account
+resource "null_resource" "snowflake_placeholder" {
+  provisioner "local-exec" {
+    command = "echo 'Please replace with Snowflake proper resources'"
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:dashboards/superset/finance_sustainability_dashboard.json
+{
+  "dashboard": {
+    "dashboard_title": "Finance & Sustainability - Aegis",
+    "slices": [
+      {
+        "slice_name": "Daily Team Carbon & Cost",
+        "viz_type": "table",
+        "params": {
+          "groupby": ["team","day"],
+          "metrics": ["total_estimated_kg","total_measured_kg","allocated_cost_usd"]
+        }
+      },
+      {
+        "slice_name": "Top Models by Cost",
+        "viz_type": "table",
+        "params": {
+          "groupby": ["model_name"],
+          "metrics": ["allocated_cost_usd"],
+          "order_desc": true
+        }
+      }
+    ],
+    "position_json": "{}"
+  }
+}
*** End Patch
*** Begin Patch
*** Add File:schedules/superset_report_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: superset-daily-report
+  namespace: aegis
+spec:
+  schedule: "0 8 * * *" # daily 08:00 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-report-sa
+          containers:
+            - name: exporter
+              image: ghcr.io/yourorg/aegis-superset-exporter:latest
+              env:
+                - name: DASHBOARD_ID
+                  value: "1"
+                - name: EVIDENCE_BUCKET
+                  value: "${EVIDENCE_BUCKET:-aegis-evidence}"
+                - name: COSIGN_KMS_KEY_ARN
+                  value: "${COSIGN_KMS_KEY_ARN:-}"
+              command: ["/bin/sh","-c"]
+              args:
+                - /opt/scripts/superset_schedule_report.sh ${DASHBOARD_ID}
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/EXTENDER_AND_SCHEDULER_SETUP.md
+# Scheduler Extender / Argo Placement Integration Runbook
+
+Overview
+- We provide a kube-scheduler extender scaffold that implements /filter and /prioritize so the K8s scheduler can consult Aegis policies and hints during scheduling.
+
+Operator steps
+1. Deploy extender service into `kube-system` (apply extender/deploy_extender.yaml).
+2. Label node pools you want available:
+   - Low-carbon nodes: kubectl label nodes <node> carbon_preference=low
+   - Spot nodes: set lifecycle=spot
+3. Update kube-scheduler configuration to include extender:
+   - Add extenders:
+       - urlPrefix: http://aegis-scheduler-extender.kube-system.svc
+         filterVerb: filter
+         prioritizeVerb: prioritize
+         weight: 1
+   - Restart kube-scheduler (depends on k8s distro).
+4. For Argo Workflows, ensure nodeSelector annotations or podSpec selectors are honored by runner (controller-enforcer also patches workflows).
+
+Testing
+- Submit a pod annotated with aegis.carbon.hint preferring spot: { "action": "run", "prefer_spot": true }
+- Verify scheduler uses extender and places pod on spot-labeled node.
+
+Security
+- Run extender in kube-system and restrict access via NetworkPolicy and RBAC if necessary.
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/BI_final_delivery_RUNBOOK.md
+# Final BI Delivery Runbook (summary)
+
+This runbook ties together:
+- Automated telemetry→calibration→retrain CI job
+- Scheduler extender scaffold for placement enforcement
+- Terraform modules for Redshift/Snowflake and EKS spot nodegroups
+- Superset dashboards + scheduled signed report exports
+
+Deploy sequence (recommended)
+1. Deploy Extender and label node pools
+2. Deploy CI jobs and run a single manual workflow to exercise calibration and retrain
+3. Deploy parquet pipeline and bulk loaders; run end-to-end ingestion to DW
+4. Deploy dbt and run models; import Superset dashboards
+5. Enable GE validations and DataHub lineage ingestion
+6. Configure RLS and Superset SSO before opening dashboards to wider audience
+
+KPIs to validate
+- Median estimator error ≤ 10%
+- ETL daily success rate ≥ 99%
+- Scheduler enforcement coverage ≥ 95%
+- Daily reports generated/signed and uploaded to evidence bucket
+
*** End Patch
*** End Patch
