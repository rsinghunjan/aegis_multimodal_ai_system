*** Begin Patch
*** Add File:feature_store/feast/feature_repo.yaml
+project: aegis_feature_repo
+registry: registry.db
+provider: local
+online_store:
+  type: redis
+  connection_string: "localhost:6379"
+
+entities:
+  - name: user
+    value_type: STRING
+
+feature_services:
+  - name: default_service
+    features:
+      - user: email_domain
+
+sources:
+  - name: user_events
+    type: file
+    path: ../data/user_events.parquet
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/ingest/ingest_example.py
+#!/usr/bin/env python3
+"""
+Simple Feast ingestion example to create a feature and push to offline/online stores.
+Run: python3 ingest_example.py
+"""
+from feast import FeatureStore, Entity, FeatureView, FileSource, ValueType, Feature
+import pandas as pd
+import os
+
+repo_path = os.path.join(os.path.dirname(__file__), "..", "feast")
+store = FeatureStore(repo_path=repo_path)
+
+def create_example_source():
+    df = pd.DataFrame({
+        "user_id": ["u1","u2","u3"],
+        "email_domain": ["example.com","acme.org","example.com"],
+        "event_timestamp": pd.Timestamp.now()
+    })
+    path = os.path.join(os.path.dirname(__file__), "user_events.parquet")
+    df.to_parquet(path, index=False)
+    return path
+
+def apply_feature_repo():
+    # This assumes feature_repo.yaml exists and Feast CLI / SDK is configured
+    store.apply(repo_path=os.path.join(repo_path, "feature_repo.yaml"))
+
+if __name__ == "__main__":
+    src = create_example_source()
+    print("Wrote sample source:", src)
+    print("Run Feast apply and ingestion steps manually per your Feast environment.")
+
*** End Patch
*** Begin Patch
*** Add File:serving/seldon/values.yaml
+replicaCount: 1
+image:
+  repository: seldonio/seldon-core
+  tag: "1.18.0"
+  pullPolicy: IfNotPresent
+resources:
+  limits:
+    cpu: 500m
+    memory: 512Mi
+  requests:
+    cpu: 200m
+    memory: 256Mi
+service:
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File:serving/bentoml/Dockerfile
+FROM python:3.10-slim
+WORKDIR /opt/model
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+COPY model_server.py .
+EXPOSE 3000
+CMD ["python3","model_server.py"]
+
*** End Patch
*** Begin Patch
*** Add File:serving/bentoml/requirements.txt
+bentoml
+fastapi
+uvicorn[standard]
+
*** End Patch
*** Begin Patch
*** Add File:serving/bentoml/model_server.py
+from fastapi import FastAPI
+import bentoml
+
+app = FastAPI()
+model = None
+
+@app.on_event("startup")
+def load_model():
+    global model
+    # Placeholder: load a BentoML saved model by tag
+    try:
+        model = bentoml.import_model("aegis-demo:latest")
+    except Exception:
+        model = None
+
+@app.post("/predict")
+def predict(payload: dict):
+    if not model:
+        return {"error": "model not loaded"}
+    return model.predict(payload)
+
*** End Patch
*** Begin Patch
*** Add File:agent/orchestrator/planner.py
+"""
+Simple agent planner that takes a high-level task and emits steps (jobs) for workers.
+This is a minimal skeleton for agentic orchestration.
+"""
+import uuid, json, time
+
+def plan_experiment(exp_name, tasks):
+    plan_id = str(uuid.uuid4())
+    plan = {"plan_id": plan_id, "name": exp_name, "created_at": time.time(), "tasks": []}
+    for t in tasks:
+        plan["tasks"].append({"task_id": str(uuid.uuid4()), "action": t, "state": "pending"})
+    # In production, persist to broker DB or Argo workflow
+    with open(f"/tmp/agent_plan_{plan_id}.json", "w") as f:
+        json.dump(plan, f, indent=2)
+    return plan
+
+if __name__ == "__main__":
+    print(plan_experiment("sample", ["run-braket-pilot", "verify-rekor", "notify-compliance"]))
+
*** End Patch
*** Begin Patch
*** Add File:agent/orchestrator/worker.py
+"""
+Worker that polls plan files and executes simple steps.
+This is a minimal prototype; replace with queue-backed worker (Redis/RabbitMQ) for production.
+"""
+import time, json, os, subprocess
+
+def poll_and_execute(poll_dir="/tmp", interval=5):
+    while True:
+        for fname in os.listdir(poll_dir):
+            if fname.startswith("agent_plan_") and fname.endswith(".json"):
+                path = os.path.join(poll_dir, fname)
+                with open(path) as f:
+                    plan = json.load(f)
+                for task in plan.get("tasks", []):
+                    if task["state"] == "pending":
+                        print("Executing", task["action"])
+                        # map action to command
+                        if task["action"] == "run-braket-pilot":
+                            subprocess.Popen(["python3","../../providers/pilot/orchestrator.py","--provider","braket","--vault-path","secret/data/quantum/providers","--program","/tmp/demo.qasm","--s3-bucket","aegis-staging-bucket"])
+                        elif task["action"] == "verify-rekor":
+                            subprocess.run(["python3","../../quantum/rekor/check_mlflow_rekor.py","--mlflow-url","http://mlflow:5000"])
+                        task["state"] = "done"
+                with open(path, "w") as f:
+                    json.dump(plan, f, indent=2)
+        time.sleep(interval)
+
+if __name__ == "__main__":
+    poll_and_execute()
+
*** End Patch
*** Begin Patch
*** Add File:explainability/shap/run_shap.py
+#!/usr/bin/env python3
+"""
+Compute SHAP explanations for a given model and dataset and log artifacts to MLflow.
+Requires installed shap, scikit-learn and mlflow.
+"""
+import shap, mlflow, joblib, pandas as pd, sys
+
+def run(model_path, data_csv, out_artifact="/tmp/shap_explainer.pkl"):
+    model = joblib.load(model_path)
+    data = pd.read_csv(data_csv)
+    explainer = shap.Explainer(model.predict, data)
+    shap_values = explainer(data)
+    joblib.dump(explainer, out_artifact)
+    mlflow.log_artifact(out_artifact, artifact_path="explainability")
+    print("Saved explainer to", out_artifact)
+
+if __name__ == "__main__":
+    if len(sys.argv) < 3:
+        print("Usage: run_shap.py <model_path> <data_csv>")
+    else:
+        run(sys.argv[1], sys.argv[2])
+
*** End Patch
*** Begin Patch
*** Add File:edge/packaging/Dockerfile.edge
+FROM python:3.10-slim
+RUN pip install --no-cache-dir uvicorn fastapi requests
+WORKDIR /opt/aegis-edge
+COPY edge_agent.py .
+EXPOSE 8080
+CMD ["python3","edge_agent.py"]
+
*** End Patch
*** Begin Patch
*** Add File:edge/agent/edge_agent.py
+from fastapi import FastAPI
+import requests, os
+app = FastAPI()
+
+@app.get("/health")
+def health():
+    return {"ok": True}
+
+@app.post("/update")
+def update(payload: dict):
+    # Placeholder: receive OTA update instruction and apply
+    return {"status": "update-accepted", "payload": payload}
+
*** End Patch
*** Begin Patch
*** Add File:edge/ota/ota_manifest.yaml
+name: aegis-edge-update
+version: "1.0"
+targets:
+  - device_group: lab-raspberries
+    image: <registry>/aegis-edge:latest
+    rollout_strategy:
+      max_unavailable: 1
+      batch_size: 5
+schedule: immediate
+
*** End Patch
*** Begin Patch
*** Add File:sdk/cli/setup.py
+from setuptools import setup, find_packages
+setup(
+    name="aegis-cli",
+    version="0.0.1",
+    packages=find_packages(),
+    install_requires=["click","hvac","requests"],
+    entry_points={
+        "console_scripts": [
+            "aegis=aegis_cli.commands:cli"
+        ]
+    }
+)
+
*** End Patch
*** Begin Patch
*** Add File:sdk/cli/aegis_cli/commands.py
+import click, subprocess, os
+
+@click.group()
+def cli():
+    """Aegis operator CLI"""
+    pass
+
+@cli.command()
+@click.option("--hsm-tfvars", default="", help="Path to HSM terraform tfvars")
+def provision_hsm(hsm_tfvars):
+    """Provision HSM using terraform wrapper"""
+    cmd = ["bash","cloud/hsm/provision_cloudhsm.sh","--tf-vars", hsm_tfvars]
+    subprocess.run(cmd)
+
+@cli.command()
+def run_pilots():
+    """Run QPU pilots via orchestrator"""
+    subprocess.run(["bash","providers/pilot/run_all_pilots.sh"])
+
+if __name__ == "__main__":
+    cli()
+
*** End Patch
*** Begin Patch
*** Add File:ui/backend/app.py
+from flask import Flask, jsonify, request
+import os, subprocess
+
+app = Flask(__name__)
+
+@app.route("/api/health")
+def health():
+    return jsonify({"ok": True})
+
+@app.route("/api/plans", methods=["POST"])
+def create_plan():
+    data = request.json
+    # call planner
+    subprocess.run(["python3","agent/orchestrator/planner.py"])
+    return jsonify({"status":"planned","input":data})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8081)))
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/run_now_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-runnow-
+spec:
+  entrypoint: aegis-all
+  templates:
+  - name: aegis-all
+    steps:
+      - - name: provision-hsm
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "cloud/hsm/provision_cloudhsm.sh --tf-vars {{workflow.parameters.hsm_tfvars}}"
+      - - name: validate-hsm
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "quantum/hsm/validate_hsm_end_to_end.sh --artifact /tmp/aegis_hsm_test.bin --pkcs11-lib {{workflow.parameters.pkcs11_lib}} --pkcs11-slot {{workflow.parameters.pkcs11_slot}} --pkcs11-pin {{workflow.parameters.pkcs11_pin}} --pkcs11-keylabel {{workflow.parameters.pkcs11_keylabel}} --s3-bucket {{workflow.parameters.hsm_audit_bucket}}"
+      - - name: run-pilots
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "providers/pilot/orchestrator.py --provider braket --vault-path {{workflow.parameters.vault_path}} --program {{workflow.parameters.program}} --s3-bucket {{workflow.parameters.s3_bucket}}"
+      - - name: deploy-broker
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "broker/scripts/deploy_broker_prod.sh --tf-vars {{workflow.parameters.rds_tfvars}} --namespace aegis --helm-release aegis-quantum-broker"
+
+  - name: run-script
+    inputs:
+      parameters:
+        - name: script
+    container:
+      image: alpine:3.17
+      command: [sh, -c]
+      args: ["{{inputs.parameters.script}}"]
+
+  arguments:
+    parameters:
+      - name: hsm_tfvars
+        value: ""
+      - name: pkcs11_lib
+        value: "/opt/vendor/lib/pkcs11.so"
+      - name: pkcs11_slot
+        value: "0"
+      - name: pkcs11_pin
+        value: "1234"
+      - name: pkcs11_keylabel
+        value: "pqkey"
+      - name: hsm_audit_bucket
+        value: ""
+      - name: vault_path
+        value: "secret/data/quantum/providers"
+      - name: program
+        value: "/tmp/demo.qasm"
+      - name: s3_bucket
+        value: "aegis-staging-bucket"
+      - name: rds_tfvars
+        value: ""
+
*** End Patch
*** Begin Patch
*** Add File:ansible/playbooks/run_now.yml
+- hosts: localhost
+  connection: local
+  gather_facts: false
+  tasks:
+    - name: Run operator run-now script
+      shell: |
+        bash operator/run_now_full_pipeline.sh --skip-confirm
+      args:
+        chdir: "{{ playbook_dir }}/../../"
+      register: runnow
+    - debug:
+        var: runnow.stdout_lines
+
*** End Patch
*** Begin Patch
*** Add File:docs/architecture/overview.md
+# Aegis Platform â€” Architecture Overview
+
+This document summarizes the full Aegis platform scaffold delivered:
+
+- Feature Store: Feast skeleton and example ingestion (feature_store/).
+- Serving: Seldon and BentoML example stubs for model serving (serving/).
+- Agentic Orchestration: planner + worker prototype for agent-driven experiments (agent/).
+- Explainability: SHAP run script and MLflow artifact logging (explainability/).
+- Edge & OTA: edge agent Dockerfile and OTA manifest for rollout (edge/).
+- SDK & CLI: aegis-cli stub for operator commands (sdk/cli/).
+- UI: Backend Flask skeleton for a minimal experiments UI (ui/).
+- Automation: Argo Workflow for orchestrating end-to-end run-now and Ansible playbook to invoke it (argo/, ansible/).
+
+Next steps:
+- Wire real infrastructure components (Feast core, Redis, Seldon Core, BentoML model builds).
+- Harden agent orchestration with a queue and auth.
+- Integrate explainability runs into training CI.
+- Implement OTA secure rollout with device authentication.
+
*** End Patch
*** End Patch
