*** Begin Patch
*** Add File:explainability/service/requirements.txt
fastapi
uvicorn[standard]
boto3
mlflow
requests
python-multipart
pydantic
prometheus-client
shap
sklearn
nltk
torch
*** End Patch
*** Begin Patch
*** Add File:explainability/service/explain_service.py
#!/usr/bin/env python3
"""
Explanation & Rationale Service (FastAPI)

Provides:
 - /explain : return provenance, chain-of-thought (if available), confidence, attributions, and counterfactual suggestions for a given model run or request.
 - /model_card/{model_id} : fetch model card JSON from MLflow/S3
 - Pushes simple metrics to Prometheus (hallucination, pii_detected, explanations_served)
This is a lightweight, auditable explanation API intended to be called by UI or by support workflows.
"""
import os
import json
import tempfile
import subprocess
from typing import Optional
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
import boto3
import mlflow
import requests
from prometheus_client import Counter, start_http_server

EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
S3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))

if MLFLOW_TRACKING_URI:
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

app = FastAPI(title="Aegis Explanation Service")

# Prometheus metrics
EXPLANATIONS_TOTAL = Counter("aegis_explanations_total", "Total explanation requests served")
HALLUCINATION_FLAGS = Counter("aegis_hallucination_flags_total", "Total hallucination flags from explain service")
PII_DETECTIONS = Counter("aegis_pii_detections_total", "PII detections surfaced by explain service")
start_http_server(int(os.environ.get("EXPLAIN_METRICS_PORT", "9222")))

class ExplainReq(BaseModel):
    model_id: Optional[str] = None
    run_id: Optional[str] = None
    input_text: Optional[str] = None
    top_k: int = 3

def fetch_provenance_for_run(run_id: str):
    # Attempt MLflow tags first
    try:
        client = mlflow.tracking.MlflowClient()
        run = client.get_run(run_id)
        tags = {t.key: t.value for t in run.data.tags}
        metrics = {m.key: m.value for m in run.data.metrics}
        return {"tags": tags, "metrics": metrics}
    except Exception:
        return {}

def fetch_model_card(model_id: str):
    # Prefer MLflow model metadata; fallback to S3 key
    try:
        client = mlflow.tracking.MlflowClient()
        models = client.search_registered_models(filter_string=f"name='{model_id}'")
        if models:
            # take latest version
            rm = models[0]
            return {"model": rm.to_json()}
    except Exception:
        pass
    # fallback to S3 path
    key = f"model_cards/{model_id}.json"
    try:
        obj = S3.get_object(Bucket=EVIDENCE_BUCKET, Key=key)
        return json.loads(obj["Body"].read())
    except Exception:
        raise HTTPException(status_code=404, detail="model card not found")

def basic_shap_attribution(model_id: str, input_text: str, top_k: int = 3):
    """
    Placeholder attribution: for tabular models we'd call SHAP, for LLMs we return token salience
    Here we return a simple token-frequency proxy as token 'importance'
    """
    import re
    tokens = re.findall(r"\w+", input_text.lower())
    freqs = {}
    for t in tokens:
        freqs[t] = freqs.get(t, 0) + 1
    # return top_k tokens with counts as proxy importance
    top = sorted(freqs.items(), key=lambda x: -x[1])[:top_k]
    return [{"feature": t, "score": s} for t, s in top]

def generate_counterfactuals(model_id: str, input_text: str, top_k: int = 3):
    """
    Naive counterfactual generator: perturb high-salience tokens and re-query the model (placeholder).
    Operators should replace with semantic-preserving paraphrase / causal method.
    """
    # simple token swap heuristic
    parts = input_text.split()
    if not parts:
        return []
    cands = []
    for i in range(min(3, len(parts))):
        p = parts.copy()
        p[i] = "<mask>"
        cands.append(" ".join(p))
    return [{"counterfactual_input": c, "note": "mask token placeholder"} for c in cands[:top_k]]

@app.post("/explain")
def explain(req: ExplainReq):
    EXPLANATIONS_TOTAL.inc()
    # Fetch provenance if run_id provided
    prov = {}
    if req.run_id:
        prov = fetch_provenance_for_run(req.run_id)
    # Chain-of-thought: if stored in provenance tags or S3 audit logs, return it
    cot = None
    if prov.get("tags") and prov["tags"].get("chain_of_thought"):
        cot = prov["tags"]["chain_of_thought"]
    else:
        # try S3 audit
        if req.run_id:
            audit_key = f"audit/{req.run_id}/cot.json"
            try:
                obj = S3.get_object(Bucket=EVIDENCE_BUCKET, Key=audit_key)
                cot = obj["Body"].read().decode()
            except Exception:
                cot = None
    # Confidence: try to read from provenance or return heuristic
    confidence = None
    if prov.get("metrics") and prov["metrics"].get("confidence"):
        confidence = float(prov["metrics"]["confidence"])
    else:
        confidence = 0.5  # placeholder
    # Attributions
    attributions = []
    if req.input_text:
        attributions = basic_shap_attribution(req.model_id or "unknown", req.input_text, req.top_k)
    # Counterfactuals
    cfs = generate_counterfactuals(req.model_id or "unknown", req.input_text or "", req.top_k)
    out = {
        "provenance": prov,
        "chain_of_thought": cot,
        "confidence": confidence,
        "attributions": attributions,
        "counterfactuals": cfs
    }
    return out

@app.get("/model_card/{model_id}")
def model_card(model_id: str):
    return fetch_model_card(model_id)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("explain_service:app", host="0.0.0.0", port=int(os.environ.get("EXPLAIN_PORT", "8085")))
*** End Patch
*** Begin Patch
*** Add File:explainability/ui/README.md
Simple static UI for explanations.

Instructions:
1. Host explainability/ui/index.html behind your dashboard server or serve via static host.
2. Set EXPLAIN_API_URL in the JS to point to your explain service.
3. The page queries /explain?run_id=... and shows provenance, CoT, confidence, attributions and counterfactuals.

This is a minimal support UI to be extended by your frontend team.
*** End Patch
*** Begin Patch
*** Add File:explainability/ui/index.html
<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Aegis Explainability UI (Demo)</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 24px; }
    pre { background:#f3f3f3; padding:10px; }
    .box { border:1px solid #ddd; padding:12px; margin-bottom:12px; }
  </style>
</head>
<body>
  <h1>Aegis Explainability (Demo)</h1>
  <div>
    <label>Run ID: <input id="runid" value=""/></label>
    <button onclick="load()">Explain</button>
  </div>
  <div id="out"></div>
  <script>
    const API = window.EXPLAIN_API_URL || "http://localhost:8085";
    async function load(){
      const run = document.getElementById('runid').value;
      const res = await fetch(`${API}/explain`, {
        method: 'POST',
        headers: {'Content-Type':'application/json'},
        body: JSON.stringify({run_id: run})
      });
      const j = await res.json();
      const out = document.getElementById('out');
      out.innerHTML = '';
      out.appendChild(renderSection('Provenance', JSON.stringify(j.provenance, null, 2)));
      out.appendChild(renderSection('Chain of Thought', j.chain_of_thought || 'N/A'));
      out.appendChild(renderSection('Confidence', j.confidence));
      out.appendChild(renderSection('Attributions', JSON.stringify(j.attributions, null, 2)));
      out.appendChild(renderSection('Counterfactuals', JSON.stringify(j.counterfactuals, null, 2)));
    }
    function renderSection(title, content){
      const d = document.createElement('div'); d.className='box';
      const h = document.createElement('h3'); h.textContent = title; d.appendChild(h);
      const p = document.createElement('pre'); p.textContent = (typeof content === 'string') ? content : JSON.stringify(content, null, 2);
      d.appendChild(p);
      return d;
    }
  </script>
</body>
</html>
*** End Patch
*** Begin Patch
*** Add File:explainability/serving/cot_capture.py
#!/usr/bin/env python3
"""
Chain-of-Thought capture middleware (example).

Integrate this into your inference pipeline:
 - When generating a response, if model returns chain_of_thought, record it to audit S3 and MLflow run tags.
 - This example shows how to push a CoT blob to S3 and tag MLflow run.
"""
import os
import json
import boto3
import mlflow

EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
S3 = boto3.client("s3", region_name=AWS_REGION)

def capture_cot(run_id: str, cot_text: str):
    key = f"audit/{run_id}/cot.json"
    S3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=cot_text.encode())
    try:
        mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI",""))
        mlflow.start_run(run_id=run_id)
        mlflow.set_tag("chain_of_thought", cot_text[:4000])
        mlflow.end_run()
    except Exception:
        pass
    return key
*** End Patch
*** Begin Patch
*** Add File:interpret/attribution.py
#!/usr/bin/env python3
"""
Attribution helpers (SHAP / LIME wrappers and LLM token salience proxies).

Note: SHAP requires model object; for large LLMs use token-attention probing in model serving to collect salience.
This module provides convenient entry points to produce attribution payloads that the explain service can consume.
"""
from typing import List, Dict
import numpy as np

def tabular_shap_explain(model, X, top_k=10):
    try:
        import shap
        explainer = shap.Explainer(model.predict, X)
        shap_vals = explainer(X)
        # produce top_k features per row
        out = []
        for row_vals in shap_vals.values:
            feature_idx = np.argsort(-np.abs(row_vals))[:top_k]
            out.append([{"feature_index": int(i), "value": float(row_vals[i])} for i in feature_idx])
        return out
    except Exception as e:
        # fallback proxy
        return [{"note":"shap unavailable","error": str(e)}]

def llm_token_salience(tokens: List[str], attn_scores: List[float], top_k=10):
    # tokens: list of tokens, attn_scores: per-token salience proxy (same length)
    pairs = list(zip(tokens, attn_scores))
    pairs.sort(key=lambda x: -x[1])
    return [{"token": t, "score": float(s)} for t,s in pairs[:top_k]]
*** End Patch
*** Begin Patch
*** Add File:interpret/counterfactuals.py
#!/usr/bin/env python3
"""
Simple counterfactuals generator (placeholder).
 - Operators should replace with a semantic paraphrase generator or causal counterfactual method.
 - This script produces a small set of perturbed inputs to test model robustness and differences in outputs.
"""
import random
import nltk
nltk.download('wordnet', quiet=True)
from nltk.corpus import wordnet

def synonyms(word):
    syns = wordnet.synsets(word)
    lemmas = set()
    for s in syns:
        for l in s.lemmas():
            lemmas.add(l.name().replace('_',' '))
    return list(lemmas)[:5]

def generate_counterfactuals(text, n=3):
    words = text.split()
    if not words:
        return []
    out = []
    for _ in range(n):
        idx = random.randrange(len(words))
        syns = synonyms(words[idx])
        if syns:
            w2 = syns[0]
            cand = words.copy()
            cand[idx] = w2
            out.append(" ".join(cand))
        else:
            # simple deletion
            cand = words.copy()
            cand.pop(idx)
            out.append(" ".join(cand))
    return out
*** End Patch
*** Begin Patch
*** Add File:fairness/fairness_eval.py
#!/usr/bin/env python3
"""
Fairness evaluation harness.

Input: labelled test CSV with columns: features..., label, sensitive_attr
Outputs:
 - Demographic parity / TPR parity per group
 - Calibration by group
 - Logs results to MLflow and uploads JSON report to S3
"""
import os
import json
import pandas as pd
import numpy as np
import boto3
import mlflow
from sklearn.metrics import confusion_matrix, accuracy_score

EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
S3 = boto3.client("s3", region_name=AWS_REGION)

if MLFLOW_TRACKING_URI:
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

def group_metrics(y_true, y_pred, groups):
    results = {}
    for g in np.unique(groups):
        mask = (groups == g)
        yt = y_true[mask]
        yp = y_pred[mask]
        acc = float(accuracy_score(yt, yp))
        tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0,1]).ravel()
        tpr = tp / (tp + fn) if (tp + fn)>0 else None
        results[str(g)] = {"accuracy": acc, "tpr": tpr, "support": int(len(yt))}
    return results

def run_fairness(csv_path, sensitive_col, label_col, pred_col, run_id="fairness-run"):
    df = pd.read_csv(csv_path)
    groups = df[sensitive_col].values
    y_true = df[label_col].values
    y_pred = df[pred_col].values
    res = group_metrics(y_true, y_pred, groups)
    report = {"run_id": run_id, "metrics": res}
    # log to MLflow
    try:
        mlflow.start_run(run_name=run_id)
        mlflow.log_params({"sensitive_col": sensitive_col, "label_col": label_col})
        mlflow.log_metric("groups_count", len(res))
        mlflow.log_text(json.dumps(report, indent=2), "fairness_report.json")
        mlflow.end_run()
    except Exception:
        pass
    # upload to S3
    key = f"eval/fairness/{run_id}.json"
    S3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(report).encode())
    return report

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--csv", required=True)
    p.add_argument("--sensitive", required=True)
    p.add_argument("--label", required=True)
    p.add_argument("--pred", required=True)
    p.add_argument("--run-id", default="fairness-run")
    args = p.parse_args()
    print(run_fairness(args.csv, args.sensitive, args.label, args.pred, args.run_id))
*** End Patch
*** Begin Patch
*** Add File:privacy/dp_training.py
#!/usr/bin/env python3
"""
Differential Privacy wrapper (PyTorch + Opacus) example.

This provides a simple train loop that can optionally enable DP-SGD if opacus is installed.
Operators should tune noise_multiplier, max_grad_norm and epochs for their privacy budget.
"""
import os
import torch
from torch.utils.data import DataLoader
from typing import Callable

def train_with_dp(model, dataset, loss_fn, optimizer, epochs=1, batch_size=32, use_dp=False, noise_multiplier=1.0, max_grad_norm=1.0):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    if use_dp:
        try:
            from opacus import PrivacyEngine
            privacy_engine = PrivacyEngine(
                model,
                sample_rate=batch_size / len(dataset),
                noise_multiplier=noise_multiplier,
                max_grad_norm=max_grad_norm,
            )
            privacy_engine.attach(optimizer)
        except Exception as e:
            print("Opacus not available, falling back to non-DP:", e)
            use_dp = False
    for epoch in range(epochs):
        model.train()
        for xb, yb in loader:
            xb = xb.to(device); yb = yb.to(device)
            pred = model(xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    if use_dp:
        eps, best_alpha = privacy_engine.get_privacy_spent(delta=1e-5)
        return {"eps": eps, "alpha": best_alpha}
    return {}
*** End Patch
*** Begin Patch
*** Add File:privacy/membership_inference_monitor.py
#!/usr/bin/env python3
"""
Membership inference monitoring harness (periodic job).

Given a model endpoint and a holdout dataset (members and non-members), estimate a leakage score.
Logs to MLflow and emits a Prometheus metric (membership_inference_score).
"""
import os
import requests
import json
import time
from prometheus_client import Gauge, start_http_server
import mlflow
import boto3

EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
METRICS_PORT = int(os.environ.get("MI_METRICS_PORT", "9233"))
start_http_server(METRICS_PORT)
mi_gauge = Gauge("aegis_membership_inference_score", "Membership inference leakage score")

if MLFLOW_TRACKING_URI:
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

def score_leakage(endpoint, samples):
    # naive: send samples and compute avg confidence difference between members and non-members
    members = [s for s in samples if s.get("member")]
    nonmembers = [s for s in samples if not s.get("member")]
    def avg_conf(list_samples):
        cs = []
        for s in list_samples:
            r = requests.post(endpoint, json={"prompt": s["text"]}, timeout=10)
            if r.status_code==200:
                cs.append(r.json().get("confidence",0.0))
        return sum(cs)/len(cs) if cs else 0.0
    return avg_conf(members) - avg_conf(nonmembers)

def run_monitor(endpoint, sample_key_s3):
    # sample_key_s3: s3 key to JSON list of {"text":..., "member":true/false}
    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=EVIDENCE_BUCKET, Key=sample_key_s3)
    samples = json.loads(obj["Body"].read())
    score = score_leakage(endpoint, samples)
    mi_gauge.set(score)
    # log to mlflow
    try:
        mlflow.start_run(run_name="membership-inference-monitor")
        mlflow.log_metric("mi_score", score)
        mlflow.end_run()
    except Exception:
        pass
    return score

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--endpoint", required=True)
    p.add_argument("--s3-key", required=True)
    args = p.parse_args()
    print(run_monitor(args.endpoint, args.s3_key))
*** End Patch
*** Begin Patch
*** Add File:docs/model_card/model_card_template.json
{
  "model_name": "MODEL_NAME",
  "version": "v0.0.1",
  "created_at": "2025-01-01T00:00:00Z",
  "authors": ["team@example.com"],
  "description": "Short description of model purpose and limitations.",
  "training_data": {
    "datasets": ["s3://bucket/dataset1", "s3://bucket/dataset2"],
    "sampling": "random/sample-100k",
    "sensitive_attributes": ["gender", "race"]
  },
  "evaluation": {
    "benchmarks": {
      "gsm8k": {"accuracy": 0.0},
      "mmlu": {"accuracy": 0.0}
    },
    "fairness": "s3://bucket/eval/fairness/v0.json"
  },
  "privacy": {
    "dp": false,
    "membership_inference_score": 0.0
  },
  "explainability": {
    "explanation_endpoint": "https://explain.example/models/MODEL_NAME",
    "explanation_types": ["provenance","cot","attribution","counterfactual"]
  },
  "signing": {
    "cosign_signed": false,
    "rekor_entry": ""
  }
}
*** End Patch
*** Begin Patch
*** Add File:docs/model_card/generator.py
#!/usr/bin/env python3
"""
Model Card generator

Generates a model_card.json from MLflow run metadata and provided template, uploads to S3 and optionally registers model_card key in MLflow.
"""
import os
import json
import boto3
import mlflow
from datetime import datetime

EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
S3 = boto3.client("s3", region_name=AWS_REGION)

if MLFLOW_TRACKING_URI:
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

def generate_model_card(model_name, version, run_id, template_path="docs/model_card/model_card_template.json"):
    tmpl = json.load(open(template_path))
    tmpl["model_name"] = model_name
    tmpl["version"] = version
    tmpl["created_at"] = datetime.utcnow().isoformat() + "Z"
    # try fetch mlflow run summary to attach params/metrics
    try:
        client = mlflow.tracking.MlflowClient()
        run = client.get_run(run_id)
        tmpl["training_run"] = {"run_id": run_id, "params": run.data.params, "metrics": run.data.metrics}
    except Exception:
        pass
    key = f"model_cards/{model_name}-{version}.json"
    S3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(tmpl).encode())
    try:
        client = mlflow.tracking.MlflowClient()
        client.set_tag(run_id, "model_card_s3", key)
    except Exception:
        pass
    print("Uploaded model_card to s3://{}/{}".format(EVIDENCE_BUCKET, key))
    return key

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--model", required=True)
    p.add_argument("--version", default="v0.0.1")
    p.add_argument("--run-id", required=True)
    args = p.parse_args()
    print(generate_model_card(args.model, args.version, args.run_id))
*** End Patch
*** Begin Patch
*** Add File:devsecops/gatekeeper/constraint_require_modelcard.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: requiremodelcard
spec:
  crd:
    spec:
      names:
        kind: RequireModelCard
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package requiremodelcard
        violation[{"msg": msg}] {
          input.review.object.kind == "Deployment"
          not input.review.object.metadata.annotations["aegis.model_card_s3"]
          msg := "Deployment missing aegis.model_card_s3 annotation linking to model_card in S3"
        }
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/explainability_ci.yml
name: Explainability & Fairness CI gates

on:
  pull_request:

jobs:
  fairness-and-explain:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install deps
        run: |
          pip install -r fairness/requirements.txt || true
      - name: Run fairness tests (example)
        run: |
          # This expects a test CSV to be present at fairness/test_data/test_preds.csv in the PR or generated artifacts
          if [ -f fairness/test_data/test_preds.csv ]; then
            python fairness/fairness_eval.py --csv fairness/test_data/test_preds.csv --sensitive sensitive_attr --label label --pred pred --run-id ci-fairness
          else
            echo "No fairness test data found; skipping"
          fi
      - name: Run hallucination & PII eval (best-effort)
        run: |
          if [ -f safety/hallucination/pairs.json ]; then
            python safety/hallucination_factuality/hallucination_eval.py --run-id ci-halluc --model-url http://localhost:8088/predict --pairs safety/hallucination/pairs.json
          else
            echo "No hallucination pairs found; skipping"
          fi
      - name: Fail on fairness report if violations detected (placeholder)
        run: |
          # Operators should parse the generated S3/MLflow report and fail on thresholds
          echo "CI fairness gate executed (operator must enforce thresholds in production)"
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana/ethics_dashboard.json
{
  "dashboard": {
    "title": "Aegis Ethics & Explainability",
    "panels": [
      {
        "title": "Hallucination Rate",
        "type": "graph",
        "targets": [{"expr":"rate(aegis_hallucination_flags_total[5m])"}]
      },
      {
        "title": "PII Detections",
        "type": "graph",
        "targets": [{"expr":"increase(aegis_pii_detections_total[5m])"}]
      },
      {
        "title": "Membership Inference Score",
        "type": "graph",
        "targets": [{"expr":"aegis_membership_inference_score"}]
      },
      {
        "title": "Fairness Violations (per run)",
        "type": "table",
        "targets": [{"expr":"aegis_fairness_violations_total"}]
      }
    ]
  }
}
*** End Patch
*** Begin Patch
*** Add File:metrics/ethics_metrics.py
#!/usr/bin/env python3
"""
Small metrics exporter to expose ethics KPI gauges incremented by eval jobs.
Other scripts (fairness_eval, membership monitor, hallucination harness) should push to Prometheus Pushgateway or call these functions.
This file provides helper functions to be imported by evaluation code.
"""
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

PUSHGATEWAY = None  # set to "gateway:9091" via env when used

def push_metric(name, value, job="aegis-ethics"):
    registry = CollectorRegistry()
    g = Gauge(name, "Aegis ethics metric", registry=registry)
    g.set(value)
    if PUSHGATEWAY:
        push_to_gateway(PUSHGATEWAY, job=job, registry=registry)
*** End Patch
*** Begin Patch
*** Add File:runbooks/explainability_ethics_runbook.md
# Explainability & Ethical AI Runbook (Aegis)

This runbook describes how to run the explainability, fairness, privacy and CI pipelines added to Aegis.

Key steps:
1. Deploy explain service:
   - Build image from explainability/service and deploy to aegis namespace.
   - Configure environment: EVIDENCE_BUCKET, MLFLOW_TRACKING_URI, AWS_REGION.
2. Deploy UI:
   - Host explainability/ui/index.html behind your internal dashboard; set EXPLAIN_API_URL to the explain service.
3. Enable CoT capture:
   - Integrate explainability/serving/cot_capture.py into model-serving pipeline so Chain-of-Thought is stored to S3 and MLflow tags.
4. Run fairness tests:
   - Populate fairness/test_data with labelled CSV.
   - Run: python fairness/fairness_eval.py --csv fairness/test_data/test_preds.csv --sensitive sensitive_attr --label label --pred pred --run-id manual-fairness
5. Run membership inference monitor:
   - Provide a labeled sample set at s3://<EVIDENCE_BUCKET>/mi_samples/ci_samples.json
   - Run privacy/membership_inference_monitor.py to compute leakage score.
6. CI gating:
   - Enable .github/workflows/explainability_ci.yml to run fairness/hallucination checks and fail PRs if configured thresholds fail.
7. Gatekeeper:
   - Apply devsecops/gatekeeper/constraint_require_modelcard.yaml to require model_card annotations on Deployments.
8. Dashboards:
   - Import monitoring/grafana/ethics_dashboard.json into Grafana and tune panels/alerts.

Acceptance criteria:
- Explanation endpoint returns provenance, CoT (if available), attribution and counterfactuals for a sample run.
- All production models have model_card_s3 annotation linking to a JSON in S3.
- Fairness tests run in CI for new model artifacts; violations produce alerts and require remediation.
- Privacy checks (membership inference) run periodically and report to MLflow + dashboard.

Notes:
- Many explainability features (attribution, counterfactuals) are researchâ€‘grade; validate methods and present uncertainties in UI.
- Do not store raw PII in UI; show redacted evidence for user-facing pages.
*** End Patch
*** End Patch
