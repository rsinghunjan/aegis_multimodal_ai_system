*** Begin Patch
*** Add File:devsecops/README_DEVSECOPS.md
+DevSecOps hardening for Aegis
+================================
+
+Purpose
+- Provide policy-as-code, continuous checks, secret rotation hooks, and RBAC remediation tooling so Aegis can move from "DevOps-ready" to "DevSecOps-hardened".
+
+What this folder includes
+- OPA/Gatekeeper Rego policy to deny overly-broad ClusterRoleBindings.
+- Gatekeeper ConstraintTemplate + Constraint (example) to enforce the policy in-cluster.
+- GitHub Actions workflows that run conftest (OPA) and static security scans (kube-bench / kubeaudit / trufflehog) on manifests and commits.
+- Scripts to validate KMS key policies and to rotate production secrets stored in AWS Secrets Manager (and sync to cluster via ExternalSecrets).
+- RBAC continuous audit script that produces an S3 audit report for operators and triggers remediation suggestions.
+
+Usage notes
+- Replace placeholders (bucket names, GitHub token secrets, AWS ARNs, Vault endpoints) with your environment values before use.
+- Gatekeeper must be installed in the cluster to enforce constraints.
+- ExternalSecrets (or similar) is required to mirror cloud secrets into Kubernetes.
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/policy/rego/no_cluster_admin.rego
+package aegis.admission
+
+deny[msg] {
+  input.request.kind.kind == "ClusterRoleBinding"
+  some i
+  subject := input.request.object.subjects[i]
+  # deny if roleRef is cluster-admin or admin
+  input.request.object.roleRef.name == "cluster-admin"
+  msg = sprintf("ClusterRoleBinding to cluster-admin is not allowed for subject: %v", [subject])
+}
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/gatekeeper/constrainttemplate_clusterrolebinding.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8sdisallowclusteradmin
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sDisallowClusterAdmin
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8sdisallowclusteradmin
+
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "ClusterRoleBinding"
+          input.review.object.roleRef.name == "cluster-admin"
+          subj := input.review.object.subjects[_]
+          msg = sprintf("ClusterRoleBinding to cluster-admin is not allowed for subject: %v", [subj])
+        }
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/gatekeeper/constraint_clusterrolebinding.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sDisallowClusterAdmin
+metadata:
+  name: disallow-cluster-admin-bindings
+spec:
+  match:
+    kinds:
+      - apiGroups: ["rbac.authorization.k8s.io"]
+        kinds: ["ClusterRoleBinding"]
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/ci/conftest_policy_check.yml
+name: Conftest (OPA) policy checks for K8s manifests
+
+on:
+  pull_request:
+  push:
+    branches:
+      - main
+
+jobs:
+  conftest:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install conftest
+        run: |
+          curl -sSLf https://github.com/open-policy-agent/conftest/releases/download/v0.29.0/conftest_0.29.0_Linux_x86_64.tar.gz | tar xz -C /tmp
+          sudo mv /tmp/conftest /usr/local/bin/
+      - name: Run conftest with Aegis policy
+        run: |
+          # Run conftest against manifests in k8s/ and devsecops/policy
+          conftest test k8s/ devsecops/policy --policy devsecops/policy
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/ci/devsecops_scan.yml
+name: DevSecOps Scans (kube-bench, kubeaudit, trufflehog)
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 2 * * 1' # weekly
+
+jobs:
+  scans:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: kube-bench (CIS check)
+        run: |
+          docker run --rm --net host aquasec/kube-bench:latest --version 1.6.0
+      - name: kubeaudit (best-effort)
+        run: |
+          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock shopify/kubeaudit all || true
+      - name: secret scanning (trufflehog)
+        run: |
+          pip install truffleHog
+          trufflehog filesystem --depth 3 --include-subpaths . || true
+      - name: Upload scan artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: devsec-scan-artifacts
+          path: |
+            . || true
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/security/externalsecret_aws_kms_cosign.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: SecretStore
+metadata:
+  name: aws-secrets-manager
+  namespace: aegis
+spec:
+  provider:
+    aws:
+      service: SecretsManager
+      region: REPLACE_WITH_AWS_REGION
+      auth:
+        jwt:
+          serviceAccountRef:
+            name: external-secrets-sa
+
+---
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: cosign-kms-secret
+  namespace: aegis
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: aws-secrets-manager
+    kind: SecretStore
+  target:
+    name: cosign-kms
+    creationPolicy: Owner
+  data:
+    - secretKey: COSIGN_KMS_KEY_ARN
+      remoteRef:
+        key: aegis/cosign/kms_key_arn
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/security/rotate_secret_aws.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate a secret in AWS Secrets Manager and wait for ExternalSecrets to sync it into k8s
+#
+if [ $# -lt 2 ]; then
+  echo "Usage: $0 <secret-name-in-sm> <new-value-file>"
+  exit 2
+fi
+SM_NAME=$1
+NEW_FILE=$2
+REGION=${AWS_REGION:-us-west-2}
+
+NEW_VAL=$(cat "$NEW_FILE")
+aws secretsmanager put-secret-value --secret-id "$SM_NAME" --secret-string "$NEW_VAL" --region "$REGION"
+echo "Secret rotated in AWS Secrets Manager: $SM_NAME"
+echo "Waiting 60s for ExternalSecrets to refresh..."
+sleep 60
+kubectl -n aegis get secret cosign-kms -o yaml
+echo "Rotation complete (verify application config/CI secrets updated as needed)"
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/tools/kms_policy_validator.py
+#!/usr/bin/env python3
+"""
+Validate that a KMS key policy only grants Sign/GetPublicKey/Describe to allowed principals.
+Usage: export AWS_REGION and run with key-id or key-arn.
+"""
+import boto3, sys, json, os
+
+ALLOWED_ACTIONS = set(["kms:Sign","kms:GetPublicKey","kms:DescribeKey"])
+
+def main():
+    if len(sys.argv) < 2:
+        print("usage: kms_policy_validator.py <kms-key-id-or-arn>")
+        sys.exit(2)
+    key = sys.argv[1]
+    client = boto3.client("kms", region_name=os.environ.get("AWS_REGION","us-west-2"))
+    policy = client.get_key_policy(KeyId=key, PolicyName='default')['Policy']
+    p = json.loads(policy)
+    bad = []
+    for stmt in p.get("Statement",[]):
+        actions = stmt.get("Action", [])
+        if isinstance(actions, str):
+            actions = [actions]
+        for a in actions:
+            if a not in ALLOWED_ACTIONS and not a.endswith("*"):
+                bad.append(a)
+    if bad:
+        print("Found disallowed actions in policy:", bad)
+        sys.exit(1)
+    print("KMS policy actions appear restricted to allowed operations.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/security/rbac_continuous_audit.py
+#!/usr/bin/env python3
+"""
+Periodically scan RBAC and produce an S3 audit report; fail CI if cluster-admin bindings exist.
+"""
+import subprocess, json, boto3, os, time
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+def kubectl_json(args):
+    out = subprocess.check_output(["kubectl"] + args, text=True)
+    return json.loads(out)
+
+def main():
+    crbs = kubectl_json(["get","clusterrolebinding","-o","json"])
+    findings = []
+    for cr in crbs.get("items",[]):
+        role = cr.get("roleRef",{}).get("name")
+        if role == "cluster-admin":
+            findings.append({
+                "binding": cr["metadata"]["name"],
+                "subjects": cr.get("subjects", [])
+            })
+    report = {"ts": int(time.time()), "findings": findings}
+    out = json.dumps(report, indent=2)
+    print(out)
+    if EVIDENCE_BUCKET:
+        s3 = boto3.client("s3", region_name=AWS_REGION)
+        key = f"security/rbac_audit_{int(time.time())}.json"
+        s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=out.encode())
+        print("Uploaded RBAC audit to s3://%s/%s" % (EVIDENCE_BUCKET, key))
+    if findings:
+        print("Cluster-admin bindings found - failing check")
+        raise SystemExit(2)
+    print("No cluster-admin ClusterRoleBindings found")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/ci/devsecops_ci_checks.yml
+name: DevSecOps CI Checks (conftest, RBAC audit, KMS policy)
+
+on:
+  pull_request:
+    branches:
+      - main
+
+jobs:
+  conftest:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run conftest
+        run: |
+          curl -sSLf https://github.com/open-policy-agent/conftest/releases/download/v0.29.0/conftest_0.29.0_Linux_x86_64.tar.gz | tar xz -C /tmp
+          sudo mv /tmp/conftest /usr/local/bin/
+          conftest test k8s/ devsecops/policy
+
+  rbac-audit:
+    runs-on: ubuntu-latest
+    needs: conftest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup kubectl (requires KUBECONFIG secret)
+        if: ${{ secrets.KUBECONFIG }}
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+      - name: Run RBAC continuous audit (best-effort)
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          python3 devsecops/security/rbac_continuous_audit.py || true
+
+  kms-validate:
+    runs-on: ubuntu-latest
+    needs: conftest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install boto3
+        run: pip install boto3
+      - name: KMS policy validation
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          # Best-effort: operator should provide KMS key as input; skip if not present
+          if [ -n "${{ secrets.COSIGN_KMS_KEY_ARN }}" ]; then
+            python3 devsecops/tools/kms_policy_validator.py "${{ secrets.COSIGN_KMS_KEY_ARN }}" || exit 1
+          else
+            echo "COSIGN_KMS_KEY_ARN not provided; skipping KMS policy validation"
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/README_HOW_TO_ENFORCE.md
+How to enforce DevSecOps checks in Aegis
+---------------------------------------
+1) Install Gatekeeper in your cluster and apply the ConstraintTemplate and Constraint in devsecops/gatekeeper/.
+2) Install ExternalSecrets (or Kubernetes External Secrets) and apply the SecretStore + ExternalSecret manifest to sync COSIGN KMS ARN from AWS Secrets Manager.
+3) Add the DevSecOps CI workflows to your GitHub repo to run conftest, kube-bench, kubeaudit and the RBAC audit on PRs and on a schedule.
+4) Add a cron job or schedule a GitHub Action that runs devsecops/security/rbac_continuous_audit.py nightly and uploads results to S3 for compliance tracking.
+5) Use devsecops/security/rotate_secret_aws.sh to rotate secrets in AWS Secrets Manager and verify ExternalSecrets sync to k8s.
+6) Run devsecops/tools/kms_policy_validator.py after provisioning KMS keys to ensure policies are restrictive.
+
*** End Patch
*** End Patch
