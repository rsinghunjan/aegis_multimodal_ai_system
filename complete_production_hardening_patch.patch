*** Begin Patch
*** Add File: .github/workflows/ci_sign_kms_rekor_multi_cloud.yml
+name: CI: Build, SBOM, SCA, Sign with KMS (AWS / GCP) & Rekor verify
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+permissions:
+  contents: read
+  id-token: write
+  packages: write
+
+jobs:
+  build-and-sign:
+    runs-on: ubuntu-latest
+    env:
+      IMAGE: ${{ secrets.REGISTRY_HOST }}/aegis-model:${{ github.sha }}
+      REKOR_URL: ${{ secrets.REKOR_URL }}
+      COSIGN_EXPERIMENTAL: "1"
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Buildx
+        uses: docker/setup-buildx-action@v2
+
+      - name: Build image
+        run: docker build -t "$IMAGE" -f services/serving/Dockerfile .
+
+      - name: Generate SBOM (syft)
+        uses: anchore/syft-action@v1
+        with:
+          image: ${{ env.IMAGE }}
+          output: sbom.json
+
+      - name: Scan (Trivy)
+        uses: aquasecurity/trivy-action@v1
+        with:
+          image-ref: ${{ env.IMAGE }}
+          format: json
+          output: trivy.json
+
+      - name: Fail on critical CVEs
+        run: |
+          if jq -e '.[] | .Vulnerabilities?[]? | select(.Severity=="CRITICAL")' trivy.json >/dev/null 2>&1; then
+            echo "Critical vulnerability found"; jq . trivy.json; exit 1
+          fi
+
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.REGISTRY_HOST }}
+          username: ${{ secrets.REGISTRY_USER }}
+          password: ${{ secrets.REGISTRY_TOKEN }}
+
+      - name: Push image
+        run: docker push "$IMAGE"
+
+      - name: Get image digest
+        id: digest
+        run: |
+          DIG=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE" | awk -F@ '{print $2}')
+          echo "digest=$DIG" >> $GITHUB_OUTPUT
+
+      - name: Sign image using cloud KMS
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          if [ "${{ secrets.CLOUD_PROVIDER }}" = "aws" ]; then
+            # AWS KMS: ensure OIDC role has kms:Sign on AWS_KMS_KEY_ARN
+            cosign sign --key "awskms://${{ secrets.AWS_KMS_KEY_ARN }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+          elif [ "${{ secrets.CLOUD_PROVIDER }}" = "gcp" ]; then
+            # GCP KMS: ensure service account / IAM permissions configured
+            cosign sign --key "gcpkms://${{ secrets.GCP_KMS_KEY_REF }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+          else
+            echo "Unsupported CLOUD_PROVIDER (set secrets.CLOUD_PROVIDER=aws|gcp)"; exit 2
+          fi
+
+      - name: Verify cosign + Rekor
+        run: |
+          cosign verify --rekor-url "${REKOR_URL}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+
+      - name: Upload artifacts (optional)
+        if: ${{ secrets.COMPLIANCE_BUCKET != '' }}
+        run: |
+          aws s3 cp sbom.json "${{ secrets.COMPLIANCE_BUCKET }}/artifacts/${GITHUB_RUN_ID}/sbom.json" || true
+          aws s3 cp trivy.json "${{ secrets.COMPLIANCE_BUCKET }}/artifacts/${GITHUB_RUN_ID}/trivy.json" || true
+
*** End Patch
*** Begin Patch
*** Add File: terraform/aws/kms_oidc_full.tf
+// Terraform to provision AWS KMS key and GitHub OIDC IAM role allowing cosign to use the key.
+// Customize variables: aws_region, github_org, github_repo, oidc_provider_arn.
+provider "aws" {
+  region = var.aws_region
+}
+
+data "aws_iam_policy_document" "github_oidc_assume" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(var.oidc_provider_url, "https://", "")}:sub"
+      values   = ["repo:${var.github_org}/${var.github_repo}:ref:refs/heads/main"]
+    }
+  }
+}
+
+resource "aws_iam_role" "github_oidc_role" {
+  name               = "aegis-github-oidc-role"
+  assume_role_policy = data.aws_iam_policy_document.github_oidc_assume.json
+}
+
+resource "aws_kms_key" "cosign_key" {
+  description             = "KMS key for cosign signing"
+  deletion_window_in_days = 30
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign"
+  target_key_id = aws_kms_key.cosign_key.key_id
+}
+
+resource "aws_iam_role_policy" "allow_kms_sign" {
+  name = "allow-kms-sign"
+  role = aws_iam_role.github_oidc_role.id
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Action = [
+          "kms:Sign",
+          "kms:GetPublicKey",
+          "kms:DescribeKey"
+        ]
+        Effect   = "Allow"
+        Resource = aws_kms_key.cosign_key.arn
+      }
+    ]
+  })
+}
+
+output "kms_key_arn" {
+  value = aws_kms_key.cosign_key.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File: terraform/gcp/kms_oidc_full.tf
+// Terraform for GCP: create KMS key ring/key and IAM binding for GitHub OIDC service account
+provider "google" {
+  project = var.gcp_project
+  region  = var.gcp_region
+}
+
+resource "google_kms_key_ring" "cosign_ring" {
+  name     = "aegis-cosign-ring"
+  location = var.gcp_region
+}
+
+resource "google_kms_crypto_key" "cosign_key" {
+  name            = "aegis-cosign-key"
+  key_ring        = google_kms_key_ring.cosign_ring.id
+  rotation_period = "7776000s" # 90 days
+}
+
+resource "google_service_account" "github_actions" {
+  account_id   = "aegis-github-actions"
+  display_name = "Aegis GitHub Actions"
+}
+
+resource "google_kms_crypto_key_iam_member" "binding" {
+  crypto_key_id = google_kms_crypto_key.cosign_key.id
+  role          = "roles/cloudkms.signerVerifier"
+  member        = "serviceAccount:${google_service_account.github_actions.email}"
+}
+
+output "gcp_kms_key_ref" {
+  value = "gcpkms://projects/${var.gcp_project}/locations/${var.gcp_region}/keyRings/${google_kms_key_ring.cosign_ring.name}/cryptoKeys/${google_kms_crypto_key.cosign_key.name}"
+}
+
*** End Patch
*** Begin Patch
*** Add File: helm/vault/values-prod.yaml
+server:
+  ha:
+    enabled: true
+  extraEnvironmentVars:
+    VAULT_LOCAL_CONFIG: |
+      ui = true
+      telemetry {
+        prometheus_retention_time = "168h"
+      }
+      listener "tcp" {
+        address = "0.0.0.0:8200"
+        tls_disable = 1
+      }
+  dataStorage:
+    enabled: true
+injector:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/vault_bootstrap_and_secrets.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Bootstraps Vault (assumes vault CLI and kubectl configured)
+# Usage: ./scripts/vault_bootstrap_and_secrets.sh <vault_addr> <bootstrap_token>
+VAULT_ADDR=${1:-http://vault.ops.svc.cluster.local:8200}
+BOOTSTRAP_TOKEN=${2:-}
+if [ -z "$BOOTSTRAP_TOKEN" ]; then
+  echo "Usage: $0 <VAULT_ADDR> <BOOTSTRAP_TOKEN>"
+  exit 2
+fi
+export VAULT_ADDR
+vault login "$BOOTSTRAP_TOKEN"
+vault secrets enable -path=secret kv-v2 || true
+vault policy write aegis-agent - <<EOF
+path "secret/data/agent/*" {
+  capabilities = ["read", "list"]
+}
+EOF
+echo "Policy created: aegis-agent"
+echo "Create example secret for agent DB connection"
+vault kv put secret/agent/db url="postgresql://aegis:password@postgres.ops.svc.cluster.local:5432/agentdb"
+echo "Store complete. Configure apps to use Vault Agent or K8s auth."
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/postgres-bitnami-values.yaml
+primary:
+  persistence:
+    enabled: true
+    size: 100Gi
+  resources:
+    requests:
+      cpu: 500m
+      memory: 1Gi
+replica:
+  replicaCount: 2
+  resources:
+    requests:
+      cpu: 250m
+      memory: 512Mi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/redis-cluster-values.yaml
+cluster:
+  enabled: true
+  slaveCount: 2
+resources:
+  requests:
+    cpu: 200m
+    memory: 512Mi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/milvus-values-ha.yaml
+replicaCount: 3
+persistence:
+  enabled: true
+  size: 500Gi
+minIO:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: k8s/attestation/create_certmanager_issuer.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: aegis-selfsigned-issuer
+spec:
+  selfSigned: {}
+
*** End Patch
*** Begin Patch
*** Add File: k8s/attestation/attestation-cert.yaml
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: attestation-server-cert
+  namespace: ops
+spec:
+  secretName: attestation-tls
+  duration: 2160h
+  renewBefore: 360h
+  commonName: attestation.ops.svc.cluster.local
+  dnsNames:
+    - attestation
+    - attestation.ops
+    - attestation.ops.svc.cluster.local
+  issuerRef:
+    name: aegis-selfsigned-issuer
+    kind: ClusterIssuer
+
*** End Patch
*** Begin Patch
*** Add File: k8s/attestation/attestation-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: attestation
+  namespace: ops
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: attestation
+  template:
+    metadata:
+      labels:
+        app: attestation
+    spec:
+      containers:
+      - name: attestation
+        image: ghcr.io/yourorg/attestation-service:latest
+        ports:
+        - containerPort: 8443
+        env:
+          - name: COSIGN_BIN
+            value: "cosign"
+          - name: REKOR_URL
+            value: "https://rekor.sigstore.dev"
+        volumeMounts:
+          - name: tls
+            mountPath: /tls
+            readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: attestation-tls
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: attestation
+  namespace: ops
+spec:
+  selector:
+    app: attestation
+  ports:
+    - port: 8443
+      targetPort: 8443
+
*** End Patch
*** Begin Patch
*** Add File: k8s/webhook/validatingwebhook.yaml
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  name: aegis-attestation-webhook
+webhooks:
+  - name: attestation.aegis.local
+    sideEffects: None
+    admissionReviewVersions: ["v1"]
+    clientConfig:
+      service:
+        name: attestation
+        namespace: ops
+        path: /verify
+      caBundle: "" # replace with base64 of cert-manager CA or self-signed CA
+    rules:
+      - operations: ["CREATE", "UPDATE"]
+        apiGroups: ["apps",""]
+        apiVersions: ["v1"]
+        resources: ["deployments","pods"]
+    failurePolicy: Fail
+    timeoutSeconds: 10
+    matchPolicy: Equivalent
+
*** End Patch
*** Begin Patch
*** Add File: scripts/inject_ca_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+WEBHOOK_NAME=${1:-aegis-attestation-webhook}
+CA_B64=${2:-}
+if [ -z "$CA_B64" ]; then
+  echo "Usage: $0 <webhook-name> <ca-bundle-base64>"
+  exit 2
+fi
+kubectl get validatingwebhookconfiguration "$WEBHOOK_NAME" -o yaml > /tmp/webhook.yaml
+python3 - <<PY
+import yaml,sys
+f="/tmp/webhook.yaml"
+data=yaml.safe_load(open(f))
+for w in data.get("webhooks",[]):
+  w["clientConfig"]["caBundle"] = "$CA_B64"
+open(f,"w").write(yaml.safe_dump(data))
+print("Wrote /tmp/webhook.yaml")
+PY
+kubectl apply -f /tmp/webhook.yaml
+echo "Patched $WEBHOOK_NAME"
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constrainttemplate_attestation_advanced.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequireattestedadvanced
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequireAttestedAdvanced
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8srequireattestedadvanced
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Deployment"
+          annotations := input.review.object.metadata.annotations
+          not annotations["aegis.attested"]
+          msg := "Deployment missing aegis.attested annotation"
+        }
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_require_attested_advanced.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequireAttestedAdvanced
+metadata:
+  name: require-attested-advanced
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["staging","ops"]
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File: services/attestation/attestation_service.py
+"""
+Enhanced attestation service:
+ - Verifies cosign signature (KMS support)
+ - Confirms Rekor entries programmatically (via Rekor REST API)
+ - Optionally validates SBOM via Trivy
+ - Returns JSON result to ValidatingWebhook
+"""
+import os, json, subprocess, tempfile, requests
+from flask import Flask, request, jsonify
+
+app = Flask("attestation")
+COSIGN = os.environ.get("COSIGN_BIN", "cosign")
+REKOR = os.environ.get("REKOR_URL", "https://rekor.sigstore.dev")
+TRIVY = os.environ.get("TRIVY_BIN", "trivy")
+
+def run(cmd, timeout=60):
+    try:
+        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, timeout=timeout)
+        return True, out.decode()
+    except subprocess.CalledProcessError as e:
+        return False, e.output.decode()
+    except Exception as e:
+        return False, str(e)
+
+def cosign_verify(image):
+    ok, out = run([COSIGN, "verify", "--output", "json", "--rekor-url", REKOR, image], timeout=90)
+    if not ok:
+        return False, {"error": out}
+    try:
+        return True, json.loads(out)
+    except Exception:
+        return True, {"raw": out}
+
+def rekor_has_entry(digest):
+    try:
+        r = requests.get(f"{REKOR}/api/v1/log/entries?hash={digest}", timeout=10)
+        if r.status_code == 200 and r.json():
+            return True, r.json()
+        return False, {}
+    except Exception as e:
+        return False, {"error": str(e)}
+
+def check_sbom(sbom_uri):
+    tf = tempfile.NamedTemporaryFile(delete=False)
+    try:
+        if sbom_uri.startswith("s3://"):
+            rc = subprocess.run(["aws","s3","cp", sbom_uri, tf.name], capture_output=True, text=True)
+            if rc.returncode != 0:
+                return False, {"error": rc.stderr}
+        elif sbom_uri.startswith("http"):
+            rc = subprocess.run(["curl","-sS", sbom_uri, "-o", tf.name], capture_output=True, text=True)
+            if rc.returncode != 0:
+                return False, {"error": rc.stderr}
+        else:
+            # local path
+            tf.name = sbom_uri
+        ok, out = run([TRIVY, "sbom", "--format", "json", tf.name], timeout=120)
+        if not ok:
+            return False, {"error": out}
+        data = json.loads(out)
+        for r in data.get("Results", []):
+            for v in r.get("Vulnerabilities", []) or []:
+                if v.get("Severity") == "CRITICAL":
+                    return False, {"critical": v}
+        return True, {}
+    finally:
+        try:
+            tf.close()
+        except Exception:
+            pass
+
+@app.route("/verify", methods=["POST"])
+def verify():
+    body = request.get_json() or {}
+    image = body.get("image")
+    sbom = body.get("sbom")
+    if not image:
+        return jsonify({"ok": False, "reason": "no_image"}), 400
+
+    ok, cosign_info = cosign_verify(image)
+    if not ok:
+        return jsonify({"ok": False, "reason": "cosign_failed", "details": cosign_info}), 403
+
+    digest = None
+    if "@" in image:
+        digest = image.split("@",1)[1]
+    rekor_ok, rekor_info = (False, {})
+    if digest:
+        rekor_ok, rekor_info = rekor_has_entry(digest)
+        if not rekor_ok:
+            return jsonify({"ok": False, "reason": "rekor_missing", "details": rekor_info}), 403
+
+    if sbom:
+        sbom_ok, sbom_details = check_sbom(sbom)
+        if not sbom_ok:
+            return jsonify({"ok": False, "reason": "sbom_bad", "details": sbom_details}), 403
+
+    return jsonify({"ok": True, "cosign": cosign_info, "rekor": rekor_info})
+
+if __name__ == "__main__":
+    # if TLS secret mounted, use it; fallback to HTTP for local testing
+    tls_dir = "/tls"
+    cert = os.path.join(tls_dir, "tls.crt")
+    key = os.path.join(tls_dir, "tls.key")
+    if os.path.exists(cert) and os.path.exists(key):
+        app.run(host="0.0.0.0", port=8443, ssl_context=(cert, key))
+    else:
+        app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/network/agent-exec-networkpolicy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: agent-exec-restrict-egress
+  namespace: agent-exec
+spec:
+  podSelector: {}
+  policyTypes:
+  - Egress
+  egress:
+    # Allow DNS and API server and internal ops namespace, and S3 endpoints by CIDR (adjust)
+    - to:
+      - namespaceSelector:
+          matchLabels:
+            name: ops
+    - to:
+      - ipBlock:
+          cidr: 10.0.0.0/8
+    - to:
+      - ipBlock:
+          cidr: 0.0.0.0/0
+      ports:
+      - protocol: TCP
+        port: 443
+    # Note: above broad 0.0.0.0/0:443 is placeholder for S3/registry endpoints; tighten for prod.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/policies/podsecurity_restrictive.yaml
+apiVersion: policy/v1beta1
+kind: PodSecurityPolicy
+metadata:
+  name: aegis-restrictive
+spec:
+  privileged: false
+  allowPrivilegeEscalation: false
+  requiredDropCapabilities:
+    - ALL
+  volumes:
+    - 'configMap'
+    - 'emptyDir'
+    - 'persistentVolumeClaim'
+  hostNetwork: false
+  hostIPC: false
+  hostPID: false
+  readOnlyRootFilesystem: true
+  runAsUser:
+    rule: 'MustRunAsNonRoot'
+  seLinux:
+    rule: RunAsAny
+  fsGroup:
+    rule: RunAsAny
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/leader_election.py
+"""
+Lease-based leader election helper (improved).
+Used by agent controller so only leader runs planners.
+"""
+from kubernetes import client, config
+import socket, uuid, time, os
+
+LEASE_NAME = os.environ.get("AGENT_LEASE", "agent-controller-lease")
+NAMESPACE = os.environ.get("LEADER_NS", "staging")
+IDENT = f"{socket.gethostname()}-{uuid.uuid4().hex[:6]}"
+
+def load_config():
+    try:
+        config.load_incluster_config()
+    except:
+        config.load_kube_config()
+
+def acquire_lease(ttl=30):
+    load_config()
+    coord = client.CoordinationV1Api()
+    body = client.V1Lease(
+        metadata=client.V1ObjectMeta(name=LEASE_NAME, namespace=NAMESPACE),
+        spec=client.V1LeaseSpec(holder_identity=IDENT)
+    )
+    try:
+        coord.patch_namespaced_lease(LEASE_NAME, NAMESPACE, body)
+        return True
+    except Exception:
+        try:
+            coord.create_namespaced_lease(NAMESPACE, body)
+            return True
+        except Exception:
+            return False
+
+def leader_loop(on_leader, on_standby, poll=10):
+    while True:
+        ok = acquire_lease()
+        if ok:
+            on_leader()
+        else:
+            on_standby()
+        time.sleep(poll)
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/app_prod.py
+"""
+Agent controller entrypoint for production:
+ - Integrates leader election, Postgres persistence, Redis queues.
+ - Only leader runs planner loops; standby serves API requests.
+"""
+import os, threading, time
+from services.agent_controller.leader_election import leader_loop
+from services.agent_controller.app_postgres_leader_integration import init_db, start_http_server_app
+
+def on_leader():
+    print("Becoming leader: start planners")
+    start_http_server_app(start_planners=True)
+
+def on_standby():
+    print("Standby: serve requests only")
+    start_http_server_app(start_planners=False)
+
+if __name__ == "__main__":
+    init_db()
+    threading.Thread(target=lambda: leader_loop(on_leader, on_standby), daemon=True).start()
+    # keep process alive
+    while True:
+        time.sleep(60)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/rollouts/model-serving-rollout-with-analysis.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: model-serving-rollout
+  namespace: staging
+spec:
+  replicas: 4
+  selector:
+    matchLabels:
+      app: model-serving
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+          pause:
+            duration: 60
+            analysis:
+              templates:
+                - templateName: loadtest-analysis
+        - setWeight: 50
+          pause:
+            duration: 120
+            analysis:
+              templates:
+                - templateName: loadtest-analysis
+  template:
+    metadata:
+      labels:
+        app: model-serving
+    spec:
+      containers:
+      - name: serving
+        image: ghcr.io/yourorg/model-serving:stable@sha256:REPLACE
+        ports:
+        - containerPort: 8085
+
*** End Patch
*** Begin Patch
*** Add File: argo/analysis/loadtest-analysis-template.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: AnalysisTemplate
+metadata:
+  name: loadtest-analysis
+  namespace: staging
+spec:
+  metrics:
+  - name: p95-latency
+    interval: 30s
+    count: 3
+    successCondition: result[0] < 1.0
+    failureCondition: result[0] > 2.0
+    provider:
+      prometheus:
+        address: http://prometheus.monitoring.svc.cluster.local
+        query: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le))
+
*** End Patch
*** Begin Patch
*** Add File: services/loadtest/locustfile.py
+from locust import HttpUser, task, between
+
+class InferenceUser(HttpUser):
+    wait_time = between(0.2, 1)
+
+    @task(8)
+    def predict(self):
+        payload = {"inputs": ["test"]*8}
+        self.client.post("/predict", json=payload, timeout=30)
+
+    @task(1)
+    def health(self):
+        self.client.get("/health")
+
*** End Patch
*** Begin Patch
*** Add File: scripts/milvus_scale_test.py
+#!/usr/bin/env python3
+"""
+Milvus scale insert & search benchmark (configurable)
+"""
+from pymilvus import connections, utility, Collection
+import numpy as np, time, os
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST","milvus")
+MILVUS_PORT = os.environ.get("MILVUS_PORT","19530")
+COL = os.environ.get("MILVUS_COLLECTION","scale_test")
+N = int(os.environ.get("MILVUS_INSERT_N", "50000"))
+DIM = int(os.environ.get("MILVUS_DIM", "128"))
+
+def setup():
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    if utility.has_collection(COL):
+        c = Collection(COL)
+    else:
+        from pymilvus import FieldSchema, CollectionSchema, DataType
+        fields = [
+            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
+            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM)
+        ]
+        schema = CollectionSchema(fields)
+        c = Collection(COL, schema)
+        c.create_index("vector", {"index_type":"IVF_FLAT","metric_type":"L2","params":{"nlist":1024}})
+        c.load()
+    return c
+
+def insert(col):
+    ids = list(range(1, N+1))
+    vecs = np.random.random((N, DIM)).astype("float32").tolist()
+    start = time.time()
+    col.insert([ids, vecs])
+    col.flush()
+    dt = time.time() - start
+    print(f"Inserted {N} vectors in {dt:.2f}s -> {N/dt:.2f} inserts/s")
+
+def search(col):
+    import numpy as np
+    q = np.random.random((1,DIM)).astype("float32").tolist()[0]
+    start = time.time()
+    res = col.search([q], "vector", param={"metric_type":"L2","params":{"nprobe":10}}, limit=10)
+    print("Search latency:", time.time()-start)
+
+if __name__ == "__main__":
+    c = setup()
+    insert(c)
+    for _ in range(5):
+        search(c)
+
*** End Patch
*** Begin Patch
*** Add File: services/edge_fleet/ota_controller.py
+"""
+Edge OTA controller with canary rollout + verification + rollback.
+This is a controller that:
+ - Accepts artifact URI and signature info
+ - Rolls out to a canary subset, monitors health callbacks, requests human approval, then promotes or rolls back
+ - Emits audit events to compliance bucket
+"""
+import os, time, requests, json
+
+ENROLL_URL = os.environ.get("ENROLL_URL", "http://edge-enroll.ops.svc.cluster.local:8086")
+APPROVAL_URL = os.environ.get("APPROVAL_URL", "http://approval:8096")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def get_devices():
+    r = requests.get(f"{ENROLL_URL}/devices", timeout=20)
+    r.raise_for_status()
+    return list(r.json().keys())
+
+def apply_to_devices(devices, artifact_uri):
+    # in production use Ansible or device agent; here we call enrollment service placeholder
+    results = {}
+    for d in devices:
+        results[d] = {"applied": True, "ts": int(time.time())}
+    return results
+
+def start_rollout(artifact_uri, canary_count=2, wait_seconds=60):
+    devices = get_devices()
+    if not devices:
+        return {"error":"no_devices"}
+    canaries = devices[:canary_count]
+    rest = devices[canary_count:]
+    res_can = apply_to_devices(canaries, artifact_uri)
+    # wait and perform health checks (placeholder)
+    time.sleep(wait_seconds)
+    # collect health: for demo assume canaries healthy
+    canaries_ok = True
+    if not canaries_ok:
+        # rollback canaries
+        return {"status":"rollback","reason":"canary_failed"}
+    # request human approval
+    apr = requests.post(f"{APPROVAL_URL}/request", json={"action": f"ota_promote:{artifact_uri}", "required":2}).json()
+    req_id = apr.get("id")
+    # poll approval
+    for _ in range(60):
+        st = requests.get(f"{APPROVAL_URL}/status/{req_id}").json()
+        if st.get("status") == "approved":
+            apply_to_devices(rest, artifact_uri)
+            return {"status":"promoted"}
+        time.sleep(10)
+    # timeout -> rollback
+    return {"status":"rollback","reason":"approval_timeout"}
+
+if __name__ == "__main__":
+    print(start_rollout(os.environ.get("ARTIFACT_URI","s3://aegis-edge/art.tflite"), canary_count=2))
+
*** End Patch
*** Begin Patch
*** Add File: tests/pen_tests/attestation_and_sandbox_suite.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "1) Attestation: attempt unsigned deploy (expect reject)"
+if kubectl apply -f tests/unsigned_deploy.yaml; then
+  echo "ERROR: unsigned deploy accepted"; kubectl delete -f tests/unsigned_deploy.yaml || true; exit 1
+else
+  echo "OK: unsigned deploy rejected"
+fi
+echo "2) Attestation: signed deploy (CI-signed image expected)"
+# user should ensure CI produced signed image name in env: SIGNED_IMAGE
+if [ -z "${SIGNED_IMAGE:-}" ]; then
+  echo "Please set SIGNED_IMAGE env var to the CI-built signed image"; exit 2
+fi
+sed "s|REPLACE_WITH_SIGNED_IMAGE|${SIGNED_IMAGE}|g" tests/signed_deploy.yaml > /tmp/signed_dep.yaml
+kubectl apply -f /tmp/signed_dep.yaml
+kubectl rollout status deploy/signed-test -n staging --timeout=60s || (kubectl describe deploy signed-test -n staging; kubectl logs deploy/signed-test -n staging --all-containers; exit 1)
+kubectl delete -f /tmp/signed_dep.yaml --ignore-not-found
+echo "3) Agent sandbox pen test"
+bash tests/agent_sandbox_pen_test.sh
+echo "4) Milvus scale test"
+python3 scripts/milvus_scale_test.py
+echo "Pen tests completed; collect logs and file remediation tickets for failures."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/production_slos_and_alerts.yaml
+groups:
+- name: aegis.production
+  rules:
+  - alert: ModelServingP95High
+    expr: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le)) > 1
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Model serving p95 > 1s"
+
+  - alert: CheckpointUploadMissing
+    expr: increase(aegis_training_steps_total[1h]) > 0 and absent(aegis_checkpoint_uploaded_total)
+    for: 30m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Checkpoints not uploaded during training"
+
+  - alert: AgentEgressBlocked
+    expr: increase(aegis_agent_egress_blocked_total[5m]) > 0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Agent egress attempts blocked"
+
+  - alert: BudgetBurnHigh
+    expr: increase(cloud_billing_cost_total[1h]) > 0 and (increase(cloud_billing_cost_total[1h]) / increase(cloud_billing_budget_total[1h]) ) * 100 > 90
+    for: 10m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Budget usage > 90%"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/full_production_hardening_runbook.md
+# Full Production Hardening Runbook (summary)
+
+Scope: complete the remaining operational, security, scale and observability validations to declare Aegis production-ready.
+
+Step A: KMS & CI signing
+- Provision KMS key (AWS/GCP) using Terraform snippets in terraform/.
+- Update GitHub Actions secrets:
+  - CLOUD_PROVIDER=aws|gcp
+  - AWS_KMS_KEY_ARN or GCP_KMS_KEY_REF
+  - REKOR_URL
+  - REGISTRY_HOST, REGISTRY_USER, REGISTRY_TOKEN
+- Trigger push to main and verify actions/ci pipeline signs image and Rekor entry exists (check Rekor UI or API).
+
+Step B: Vault & secrets
+- Deploy Vault with Helm using helm/vault/values-prod.yaml.
+- Run scripts/vault_bootstrap_and_secrets.sh to create policies and example secrets.
+- Modify deployments to use Vault Agent sidecar or K8s auth; remove static secrets from manifests.
+
+Step C: Attestation webhook & Gatekeeper
+- Install cert-manager (kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml).
+- kubectl apply -f k8s/attestation/create_certmanager_issuer.yaml
+- kubectl apply -f k8s/attestation/attestation-cert.yaml
+- kubectl apply -f k8s/attestation/attestation-deployment.yaml
+- Generate CA bundle: kubectl get secret attestation-tls -n ops -o jsonpath='{.data.tls\.crt}' | base64 -d | base64 -w0
+- Patch webhook: edit k8s/webhook/validatingwebhook.yaml caBundle field then apply.
+- Apply Gatekeeper templates: governance/gatekeeper/constrainttemplate_attestation_advanced.yaml and governance/gatekeeper/constraint_require_attested_advanced.yaml
+- Run tests: scripts/attestation_smoke_test.sh and tests/pen_tests/attestation_and_sandbox_suite.sh
+
+Step D: HA infra
+- Deploy Postgres primary+replica via Helm using k8s/ha/postgres-bitnami-values.yaml or use managed RDS/Cloud SQL.
+- Deploy Redis cluster via helm using k8s/ha/redis-cluster-values.yaml.
+- Deploy Milvus with HA values.
+- Configure backups: k8s/ha/pg_backup_cronjob.yaml
+- Validate failover & restore using instructions in scripts/backup_restore_instructions.md
+
+Step E: Canary + SLO automation & load tests
+- Deploy Argo Rollouts + AnalysisTemplate (argo/analysis/loadtest-analysis-template.yaml).
+- Ensure Prometheus queries return valid metrics and deploy monitoring/prometheus/production_slos_and_alerts.yaml
+- Run Locust load tests (services/loadtest/locustfile.py) or trigger Argo analysis that launches load job.
+- Verify that Rollout analysis fails when SLO degraded and automatic rollback occurs.
+
+Step F: Agent hardening & pen tests
+- Deploy agent controller with leader election (k8s/manifests/agent-controller-deployment-with-lease.yaml).
+- Ensure NetworkPolicy (k8s/network/agent-exec-networkpolicy.yaml) applied and PodSecurityPolicy restrictive (k8s/policies/podsecurity_restrictive.yaml).
+- Run tests/pen_tests/* and tests/agent_sandbox_pen_test.sh to verify sandbox protections.
+
+Step G: Edge OTA canary & attestation
+- Deploy edge enrollment service and OTA controller (services/edge_fleet/*).
+- Simulate devices with services/edge_fleet/ota_canary_test.py
+- Validate canary rollback by simulating canary failure.
+
+Step H: Observability & runbooks
+- Ensure all services export Prometheus metrics; import dashboards to Grafana.
+- Validate alerts escalate to on-call and runbooks in runbooks/alert_runbooks.md
+
+Acceptance checklist
+- CI-produced images signed with KMS and Rekor entries exist.
+- Validating webhook + Gatekeeper reject unsigned artifacts.
+- Vault in use, no static secrets in manifests.
+- DBs & Milvus HA deployed and backups validated.
+- Agent leader election & sandbox validated via pen tests.
+- Canary analysis auto-rolls back on SLO breach.
+- Edge OTA canary tested and rollback verified.
+- Pen tests done and remediation tracked.
+
*** End Patch
*** End Patch
