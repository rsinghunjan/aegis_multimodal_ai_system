*** Begin Patch
*** Add File:tenant-finalization/aws/complete_aws_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# complete_aws_tenant.sh
+# Finalize AWS tenant wiring for Aegis:
+#  - ensure GitHub OIDC provider exists (guide),
+#  - create least-privilege IAM role for GitHub Actions OIDC with restrictive policy,
+#  - create (optional) asymmetric KMS key for signing and enable key rotation,
+#  - print GitHub secrets to set and optionally emit gh CLI commands.
+#
+usage(){ cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --region <aws-region> --s3-bucket <bucket> [--create-kms]
+
+Outputs (print these to GitHub secrets):
+  AWS_ROLE_ARN, AWS_REGION, KMS_KEY_ARN, S3_BUCKET
+EOF
+exit 1; }
+
+CREATE_KMS=false
+GH_REPO=""
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; GH_REPO="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --create-kms) CREATE_KMS=true; shift;;
+    -h|--help) usage;;
+    *) echo "Unknown arg $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${REGION:-}" ] || usage
+[ -n "${S3_BUCKET:-}" ] || usage
+
+ACCOUNT_ID=$(aws sts get-caller-identity --query Account -o text)
+ROLE_NAME="aegis-github-oidc-${ORG}-${REPO}"
+
+echo "1) Check for OIDC provider (token.actions.githubusercontent.com). If missing, follow AWS console steps to add it."
+aws iam list-open-id-connect-providers --output text || true
+
+TRUST_POLICY=$(cat <<EOF
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Principal": { "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/token.actions.githubusercontent.com" },
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Condition": {
+        "StringLike": {
+          "token.actions.githubusercontent.com:sub": "repo:${ORG}/${REPO}:ref:refs/heads/main"
+        }
+      }
+    }
+  ]
+}
+EOF
+)
+echo "$TRUST_POLICY" >/tmp/aegis-aws-trust.json
+
+if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
+  echo "Role exists: $ROLE_NAME"
+else
+  aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file:///tmp/aegis-aws-trust.json
+  echo "Created role $ROLE_NAME"
+fi
+
+POLICY_JSON=$(cat <<EOF
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {
+      "Effect":"Allow",
+      "Action":[
+        "kms:Sign",
+        "kms:GetPublicKey",
+        "kms:DescribeKey"
+      ],
+      "Resource":"*"
+    },
+    {
+      "Effect":"Allow",
+      "Action":[
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:ListBucket"
+      ],
+      "Resource":[
+        "arn:aws:s3:::${S3_BUCKET}",
+        "arn:aws:s3:::${S3_BUCKET}/*"
+      ]
+    }
+  ]
+}
+EOF
+)
+echo "$POLICY_JSON" >/tmp/aegis-aws-inline-policy.json
+aws iam put-role-policy --role-name "$ROLE_NAME" --policy-name "AegisOIDCPolicy" --policy-document file:///tmp/aegis-aws-inline-policy.json
+
+KMS_KEY_ARN="<REPLACE_WITH_KMS_ARN>"
+if $CREATE_KMS; then
+  echo "Creating asymmetric KMS key for signing..."
+  KMS_KEY_ID=$(aws kms create-key --customer-master-key-spec ECC_NIST_P384 --key-usage SIGN_VERIFY --description "Aegis signing key" --query KeyMetadata.KeyId -o text)
+  aws kms enable-key-rotation --key-id "$KMS_KEY_ID"
+  KMS_KEY_ARN=$(aws kms describe-key --key-id "$KMS_KEY_ID" --query KeyMetadata.Arn -o text)
+  echo "Created KMS Key: $KMS_KEY_ARN"
+fi
+
+ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query Role.Arn -o text)
+echo
+echo "---- GitHub secrets to set (copy these into repository secrets) ----"
+echo "AWS_ROLE_ARN=$ROLE_ARN"
+echo "AWS_REGION=$REGION"
+echo "KMS_KEY_ARN=$KMS_KEY_ARN"
+echo "S3_BUCKET=$S3_BUCKET"
+echo "-----------------------------------------------------------------"
+
+if [ -n "$GH_REPO" ]; then
+  echo
+  echo "To create secrets via gh CLI (run locally):"
+  echo "gh secret set AWS_ROLE_ARN --repo $GH_REPO --body \"$ROLE_ARN\""
+  echo "gh secret set AWS_REGION --repo $GH_REPO --body \"$REGION\""
+  echo "gh secret set KMS_KEY_ARN --repo $GH_REPO --body \"$KMS_KEY_ARN\""
+  echo "gh secret set S3_BUCKET --repo $GH_REPO --body \"$S3_BUCKET\""
+fi
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/aws/elasticache/outputs.tf
+output "redis_primary_endpoint" {
+  value = aws_elasticache_replication_group.redis.primary_endpoint_address
+}
+
+output "redis_reader_endpoint" {
+  value = aws_elasticache_replication_group.redis.reader_endpoint_address
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/aws/elasticache/README.md
+# ElastiCache Terraform module for Aegis
+
+This module provisions an ElastiCache Redis replication group suitable for Feast online store.
+Ensure you populate variables for VPC subnet IDs and security group IDs.
+
+Notes:
+- For production, prefer cluster mode disabled vs enabled depending on Feast version.
+- Review parameter group and engine version pinning before apply.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/s3/hardened_lakefs.tf
+resource "aws_s3_bucket" "lakefs" {
+  bucket = var.bucket_name
+  acl    = "private"
+  versioning { enabled = true }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm = "aws:kms"
+        kms_master_key_id = var.kms_key_id
+      }
+    }
+  }
+  lifecycle_rule {
+    enabled = true
+    noncurrent_version_expiration { days = 30 }
+  }
+  tags = merge(var.common_tags, { "aegis-managed" = "true" })
+}
+
+output "lakefs_bucket" { value = aws_s3_bucket.lakefs.bucket }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/s3/variables.tf
+variable "bucket_name" { type = string }
+variable "region" { type = string default = "us-east-1" }
+variable "kms_key_id" { type = string }
+variable "stage" { type = string default = "staging" }
+variable "common_tags" { type = map(string) default = {} }
+
*** End Patch
*** Begin Patch
*** Add File:prod/feature-store/migrate_to_managed.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# migrate_to_managed.sh
+# Steps to reconfigure Feast + lakeFS to use managed backends.
+#
+FEAST_NAMESPACE=${FEAST_NAMESPACE:-feast}
+LAKEFS_BUCKET=${LAKEFS_BUCKET:-aegis-lakefs-staging}
+REDIS_ENDPOINT=${REDIS_ENDPOINT:-redis-cluster.some.aws:6379}
+
+echo "Updating Feast online store config to use ${REDIS_ENDPOINT}"
+# operator: patch feast configmaps or redeploy with helm values pointing to Redis endpoint
+kubectl -n $FEAST_NAMESPACE set env deployment/feast-online REDIS_HOST="$REDIS_ENDPOINT" || true
+
+echo "Updating lakeFS config to use s3 bucket ${LAKEFS_BUCKET}"
+# operator: update lakeFS helm values or config map
+kubectl -n $FEAST_NAMESPACE patch configmap lakefs-config --patch "{\"data\":{\"STORAGE_TYPE\":\"s3\",\"S3_BUCKET\":\"${LAKEFS_BUCKET}\"}}" || true
+
+echo "Restarting pods to pick up config changes"
+kubectl -n $FEAST_NAMESPACE rollout restart deployment --all || true
+
+echo "Run validation: ensure Feast can read/write online features and lakeFS lists bucket"
+# Basic checks (operator must adapt)
+kubectl -n $FEAST_NAMESPACE get pods || true
+aws s3 ls "s3://${LAKEFS_BUCKET}" || true
+
*** End Patch
*** Begin Patch
*** Add File:prod/vault/auto_unseal/kms_auto_unseal_example.tf
+# Example Terraform to create IAM role and policy for Vault Auto-unseal with AWS KMS
+resource "aws_kms_key" "vault_unseal" {
+  description = "Vault auto-unseal key for Aegis"
+  deletion_window_in_days = 30
+}
+
+resource "aws_iam_role" "vault_autounseal_role" {
+  name = "aegis-vault-autounseal-role"
+  assume_role_policy = data.aws_iam_policy_document.vault_assume.json
+}
+
+data "aws_iam_policy_document" "vault_assume" {
+  statement {
+    actions = ["sts:AssumeRole"]
+    principals {
+      type = "Service"
+      identifiers = ["ec2.amazonaws.com"]
+    }
+  }
+}
+
+resource "aws_iam_role_policy" "vault_kms_access" {
+  name = "vault-kms-access"
+  role = aws_iam_role.vault_autounseal_role.id
+  policy = <<POL
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {
+      "Effect":"Allow",
+      "Action":["kms:Decrypt","kms:GenerateDataKey","kms:DescribeKey"],
+      "Resource":["${aws_kms_key.vault_unseal.arn}"]
+    }
+  ]
+}
+POL
+}
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/pentest/prepare_pentest_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUTDIR=${OUTDIR:-/tmp/aegis-pentest-bundle}
+mkdir -p "$OUTDIR"
+
+echo "1) Collect container image list"
+kubectl get pods --all-namespaces -o jsonpath='{..image}' | tr -s '[[:space:]]' '\n' | sort -u > "$OUTDIR/images.txt"
+
+echo "2) Run trivy scans (if available)"
+if command -v trivy >/dev/null 2>&1; then
+  while read -r img; do
+    trivy image --severity HIGH,CRITICAL -o "$OUTDIR/$(echo $img | tr '/:' '_' ).json" "$img" || true
+  done < "$OUTDIR/images.txt"
+fi
+
+echo "3) Cluster manifests & helm list"
+kubectl get all --all-namespaces -o yaml > "$OUTDIR/k8s_all.yaml"
+helm list -A -o yaml > "$OUTDIR/helm_list.yaml"
+
+echo "4) Vault policies & audit devices"
+if command -v vault >/dev/null 2>&1; then
+  vault audit list -format=json > "$OUTDIR/vault_audit.json" || true
+  vault policy list -format=json > "$OUTDIR/vault_policies.json" || true
+fi
+
+echo "5) Package bundle"
+tar czf "${OUTDIR}.tar.gz" -C "$OUTDIR" .
+echo "Pentest bundle: ${OUTDIR}.tar.gz"
+
*** End Patch
*** Begin Patch
*** Add File:prod/vault/audit_forward/helm_values_example.yaml
+fluentd:
+  enabled: true
+  config:
+    forward_host: "syslog.example.com"
+    forward_port: 24224
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/mtls/README.md
+# mTLS for control plane
+
+This folder contains scripts and examples to generate a CA, issue certs and install secrets to Kubernetes for mTLS.
+
+Usage:
+- Generate CA & certs: bash generate_ca_and_certs.sh
+- Load into Kubernetes secrets: kubectl apply -f mtls-secrets.yaml
+- Configure webhooks/controllers to use the TLS secrets and validate client certs using CA.
+
*** End Patch
*** Begin Patch
*** Add File:prod/reliability/cron_vault_snapshot_example.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-vault-snapshot
+  namespace: ops
+spec:
+  schedule: "0 2 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+          - name: snapshot
+            image: hashicorp/vault:1.14.3
+            env:
+            - name: VAULT_ADDR
+              value: "https://vault.ops.svc.cluster.local:8200"
+            - name: VAULT_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: vault-root
+                  key: root-token
+            command: ["/bin/sh","-c"]
+            args:
+            - |
+              vault operator raft snapshot save /backup/vault-$(date -u +%Y%m%dT%H%M%SZ).snap
+            volumeMounts:
+            - name: backup
+              mountPath: /backup
+          restartPolicy: OnFailure
+          volumes:
+          - name: backup
+            persistentVolumeClaim:
+              claimName: vault-backup-pvc
+
*** End Patch
*** Begin Patch
*** Add File:prod/reliability/drill/drill_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# drill_runner.sh --dry-run defaults to true to avoid destructive restores
+DRY_RUN=${DRY_RUN:-1}
+BACKUP_PREFIX="aegis-drill-$(date -u +%Y%m%dT%H%M%SZ)"
+BACKUP_BUCKET=${BACKUP_BUCKET:-aegis-lakefs-backups}
+
+echo "Starting DR Drill (DRY_RUN=$DRY_RUN) - backup prefix: $BACKUP_PREFIX"
+
+echo "Snapshotting Vault..."
+SNAP="/tmp/vault-$(date -u +%s).snap"
+vault operator raft snapshot save "$SNAP" || { echo "Vault snapshot failed"; exit 1; }
+echo "Snapshot saved: $SNAP"
+
+echo "Backing up lakeFS to s3://$BACKUP_BUCKET/$BACKUP_PREFIX ..."
+aws s3 sync s3://aegis-lakefs s3://$BACKUP_BUCKET/$BACKUP_PREFIX || true
+
+if [ "$DRY_RUN" -eq 0 ]; then
+  echo "Performing restore (operator must confirm cluster safety)..."
+  vault operator raft snapshot restore "$SNAP"
+  echo "Restore initiated. Follow Vault recovery procedures."
+else
+  echo "Dry-run mode: skipping actual restore."
+fi
+
+echo "DR Drill complete. Backup artifacts at s3://$BACKUP_BUCKET/$BACKUP_PREFIX"
+
*** End Patch
*** Begin Patch
*** Add File:prod/serving/perf/sweep_and_tune.sh
+#!/usr/bin/env bash
+set -euo pipefail
+HOST=${1:-}
+MODEL=${2:-aegis-triton-model}
+OUTDIR=${3:-/tmp/aegis-perf}
+mkdir -p "$OUTDIR"
+
+if [ -z "$HOST" ]; then
+  echo "Usage: $0 <inference-host> [model] [outdir]"
+  exit 1
+fi
+
+echo "Running perf sweep for model=$MODEL host=$HOST"
+for B in 1 2 4 8 16 32; do
+  for C in 1 2 4 8; do
+    FN="$OUTDIR/perf_b${B}_c${C}.txt"
+    if command -v perf_analyzer >/dev/null 2>&1; then
+      perf_analyzer -m "$MODEL" -u "$HOST" -b "$B" -n 200 --concurrency-range $C:$C > "$FN" 2>&1 || true
+    else
+      echo "perf_analyzer not found; skipping; you can run k6-based tests as fallback" > "$FN"
+    fi
+    echo "Wrote $FN"
+  done
+done
+
+python3 - <<PY
+import os,sys,glob,re,statistics
+files=glob.glob("$OUTDIR/perf_b*_c*.txt")
+lat=[]
+for f in files:
+  with open(f) as fh:
+    data=fh.read()
+    m=re.search(r'90th.*?([\d\.]+)ms', data)
+    if m:
+      lat.append(float(m.group(1)))
+print("Collected p90 latencies:", lat)
+if lat:
+  print("median p90:", statistics.median(lat))
+else:
+  print("No latency numbers parsed")
+PY
+
+echo "Run suggest_thresholds.py against outdir to get KEDA heuristics"
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/keda/suggest_thresholds.py
+#!/usr/bin/env python3
+import sys,os,glob,re,statistics
+def main(outdir):
+    lat=[]
+    for fn in glob.glob(os.path.join(outdir,"perf_*.txt")):
+        s=open(fn).read()
+        m=re.search(r'90th.*?([\d\.]+)ms', s)
+        if m:
+            lat.append(float(m.group(1)))
+    if not lat:
+        print("No latencies found; default threshold 60")
+        return
+    p90 = sorted(lat)[int(0.9*len(lat))-1]
+    # heuristic: map p90 to GPU util threshold
+    thresh = max(30, min(90, int(70 - (p90/10))))
+    print("suggested_keda_threshold_percent =", thresh)
+
+if __name__ == "__main__":
+    if len(sys.argv)<2:
+        print("Usage: suggest_thresholds.py <perf_results_dir>")
+    else:
+        main(sys.argv[1])
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/volcano/resubmit_on_preempt.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple resubmit helper for preempted jobs: finds failed jobs with annotation 'aegis/checkpoint'
+# and re-creates them with a resume flag.
+NS=${NS:-ml}
+for job in $(kubectl get jobs -n $NS -o jsonpath='{.items[*].metadata.name}'); do
+  failed=$(kubectl get job $job -n $NS -o jsonpath='{.status.failed}' || echo 0)
+  if [ "$failed" != "0" ]; then
+    chk=$(kubectl get job $job -n $NS -o jsonpath='{.metadata.annotations.aegis/checkpoint}' || echo "")
+    if [ -n "$chk" ]; then
+      echo "Resubmitting job $job with checkpoint $chk"
+      # Operator: implement resubmit logic according to your Argo/Volcano job spec
+    fi
+  fi
+done
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/tpm/attest_server_example.py
+#!/usr/bin/env python3
+"""
+Simple TPM attestation validation stub for demo purposes.
+In production this must validate quotes properly, check PCRs, and integrate with CA/HSM.
+"""
+from flask import Flask, request, jsonify
+app = Flask(__name__)
+
+@app.post("/enroll")
+def enroll():
+    payload = request.get_json(force=True)
+    device_id = payload.get("device_id")
+    quote = payload.get("quote")
+    # For demo: accept any quote and return a signed cert placeholder
+    return jsonify({"device_id": device_id, "cert": "CERT_PEM_PLACEHOLDER"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/ota/sign_firmware.sh
+#!/usr/bin/env bash
+set -euo pipefail
+FIRMWARE=${1:-}
+if [ -z "$FIRMWARE" ]; then
+  echo "Usage: $0 /path/to/firmware.bin"
+  exit 1
+fi
+if ! command -v cosign >/dev/null 2>&1; then
+  echo "Install cosign to sign firmware"
+  exit 2
+fi
+
+# Sign firmware with cosign (example using local key; operator may use vault:// KMS)
+SIG_OUT="${FIRMWARE}.sig"
+cosign sign-blob --key cosign.key --output-signature "$SIG_OUT" "$FIRMWARE"
+echo "Signed firmware: $SIG_OUT"
+
+# Create offline Rekor bundle (tar.gz of artifact + signature)
+tar czf "${FIRMWARE}.bundle.tar.gz" "$FIRMWARE" "$SIG_OUT"
+echo "Offline Rekor bundle created: ${FIRMWARE}.bundle.tar.gz"
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/auth_complete.py
+"""
+Enhanced auth helpers for aegis-sdk/aegisctl:
+- OIDC device flow + browser flow skeleton
+- refresh token logic + local credential store (~/.aegis/credentials.json)
+"""
+import os,json,time,webbrowser,requests
+CREDS=os.path.expanduser("~/.aegis/credentials.json")
+
+def save(data):
+    os.makedirs(os.path.dirname(CREDS), exist_ok=True)
+    with open(CREDS,"w") as f:
+        json.dump(data,f)
+
+def load():
+    if not os.path.exists(CREDS):
+        return {}
+    return json.load(open(CREDS))
+
+def refresh_if_needed():
+    tok = load()
+    if not tok:
+        return False
+    if tok.get("expires_at",0) - time.time() < 300 and tok.get("refresh_token"):
+        resp = requests.post(tok["token_url"], data={
+            "grant_type":"refresh_token",
+            "refresh_token":tok["refresh_token"],
+            "client_id":tok["client_id"]
+        })
+        if resp.status_code==200:
+            new = resp.json()
+            new["expires_at"]=time.time()+new.get("expires_in",3600)
+            save(new)
+            return True
+    return False
+
+def device_flow_start(client_id, device_authorization_endpoint):
+    r = requests.post(device_authorization_endpoint, data={"client_id":client_id, "scope":"openid offline_access"})
+    return r.json()
+
*** End Patch
*** Begin Patch
*** Add File:prod/ui/dashboard_stub/README.md
+# Dashboard stub for non-technical users
+
+This folder contains a simple Flask app that surfaces high-level model status, SLOs and audit summary for non-technical stakeholders.
+It reads from Prometheus (for SLOs) and from Rekor (for provenance) to show:
+- Models deployed and who approved them
+- SLO status (good/warning/critical)
+- Recent incidents and retrain jobs
+
+Operator: implement a production UI or connect existing BI tools to Prometheus/Rekor.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/production_finalize_runbook.md
+# Production Finalization Runbook (High level)
+
+1) Tenant & IAM
+ - Run tenant-finalization scripts for targeted clouds (tenant-finalization/*)
+ - Create GitHub secrets using gh CLI helper (do not paste secrets in chat)
+ - Validate least-privilege via cloud policy simulator
+
+2) Managed HA backends
+ - Run terraform modules in terraform/managed/*
+ - Migrate Feast & lakeFS using prod/feature-store/migrate_to_managed.sh
+ - Validate failover by simulating node outage and monitoring Feast responses
+
+3) Security & pentest
+ - Run prod/security/pentest/prepare_pentest_bundle.sh and provide bundle to vendor
+ - Deploy Fluentd forwarder and validate SIEM ingestion
+ - Apply mTLS, network policies and RBAC hardening
+
+4) SRE & DR
+ - Deploy cron snapshot job and schedule weekly checks
+ - Run DR drill (dry-run) using prod/reliability/drill/drill_runner.sh
+ - Document RTO/RPO and runbook updates
+
+5) Perf & Autoscale
+ - Run perf sweep scripts (prod/serving/perf/sweep_and_tune.sh)
+ - Tune KEDA thresholds using suggest_thresholds.py
+ - Deploy Volcano resubmit/ checkpoint hooks for training jobs
+
+6) Edge & OTA
+ - Replace demo enroll/attest with TPM-backed flow and OTA signing
+ - Use cosign and offline bundling for devices with intermittent connectivity
+
+7) UX
+ - Complete aegisctl auth flows and run CI tests (prod/devtools/aegis_sdk)
+ - Deploy dashboard skeleton and iterate with product team
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/prod_finalize_ci.yml
+name: Prod Finalize CI
+on:
+  workflow_dispatch:
+jobs:
+  lint-terraform:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Terraform
+        uses: hashicorp/setup-terraform@v2
+      - name: Terraform fmt & validate
+        run: |
+          find terraform -name '*.tf' -print0 | xargs -0 -n1 dirname | sort -u | while read d; do
+            (cd $d && terraform fmt -check) || true
+            (cd $d && terraform validate) || true
+          done
+  run-pentest-scan:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run trivy (local container images)
+        run: |
+          if command -v trivy >/dev/null 2>&1; then
+            ./prod/security/pentest/prepare_pentest_bundle.sh
+          else
+            echo "trivy not installed in runner"
+          fi
+
*** End Patch
*** End Patch
