*** Begin Patch
*** Add File:argo/train_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: ml-train-
+  namespace: aegis
+spec:
+  entrypoint: train-and-validate
+  arguments:
+    parameters:
+      - name: image
+        value: "REPLACE_GHCR_ORG/train:REPLACE_IMAGE_TAG"
+      - name: dataset-s3
+        value: "s3://REPLACE_EVIDENCE_BUCKET/datasets/example"
+      - name: mlflow-tracking-uri
+        value: "REPLACE_MLFLOW_TRACKING_URI"
+      - name: experiment-name
+        value: "aegis-training"
+      - name: model-name
+        value: "aegis-model"
+      - name: validation-threshold
+        value: "0.80"
+      - name: output-s3
+        value: "s3://REPLACE_EVIDENCE_BUCKET/models/{{workflow.uid}}"
+
+  templates:
+    - name: train-and-validate
+      steps:
+        - - name: fetch-data
+            template: s3-copy
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{workflow.parameters.dataset-s3}}"
+                - name: dst
+                  value: "/tmp/dataset"
+        - - name: train
+            template: ml-train
+            arguments:
+              parameters:
+                - name: image
+                  value: "{{workflow.parameters.image}}"
+                - name: mlflow-tracking-uri
+                  value: "{{workflow.parameters.mlflow-tracking-uri}}"
+                - name: experiment-name
+                  value: "{{workflow.parameters.experiment-name}}"
+                - name: output-s3
+                  value: "{{workflow.parameters.output-s3}}"
+        - - name: validate
+            template: validate-model
+            arguments:
+              parameters:
+                - name: model-s3
+                  value: "{{workflow.parameters.output-s3}}/model.tar.gz"
+                - name: threshold
+                  value: "{{workflow.parameters.validation-threshold}}"
+                - name: mlflow-tracking-uri
+                  value: "{{workflow.parameters.mlflow-tracking-uri}}"
+        - - name: register
+            template: register-model
+            when: "{{steps.validate.outputs.parameters.passed}} == true"
+            arguments:
+              parameters:
+                - name: model-s3
+                  value: "{{workflow.parameters.output-s3}}/model.tar.gz"
+                - name: mlflow-tracking-uri
+                  value: "{{workflow.parameters.mlflow-tracking-uri}}"
+                - name: model-name
+                  value: "{{workflow.parameters.model-name}}"
+
+    - name: s3-copy
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: amazon/aws-cli:2.9.0
+        command: ["/bin/sh","-c"]
+        args:
+          - aws s3 cp --recursive "{{inputs.parameters.src}}" "{{inputs.parameters.dst}}"
+
+    - name: ml-train
+      inputs:
+        parameters:
+          - name: image
+          - name: mlflow-tracking-uri
+          - name: experiment-name
+          - name: output-s3
+      container:
+        image: "{{inputs.parameters.image}}"
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            python /opt/ops/train.py \
+              --data /tmp/dataset \
+              --mlflow-tracking-uri "{{inputs.parameters.mlflow-tracking-uri}}" \
+              --experiment "{{inputs.parameters.experiment-name}}" \
+              --output /tmp/output
+            tar czf /tmp/model.tar.gz -C /tmp/output model/
+            aws s3 cp /tmp/model.tar.gz "{{inputs.parameters.output-s3}}/model.tar.gz"
+
+    - name: validate-model
+      inputs:
+        parameters:
+          - name: model-s3
+          - name: threshold
+          - name: mlflow-tracking-uri
+      outputs:
+        parameters:
+          - name: passed
+            valueFrom:
+              path: /tmp/passed.txt
+      container:
+        image: python:3.10-slim
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            python /opt/ops/validate_model.py --model-s3 "{{inputs.parameters.model-s3}}" --threshold "{{inputs.parameters.threshold}}" --mlflow-tracking-uri "{{inputs.parameters.mlflow-tracking-uri}}"
+            echo "true" > /tmp/passed.txt
+
+    - name: register-model
+      inputs:
+        parameters:
+          - name: model-s3
+          - name: mlflow-tracking-uri
+          - name: model-name
+      container:
+        image: python:3.10-slim
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            python - <<PY
+import subprocess, sys
+ms3="{{inputs.parameters.model-s3}}"
+mlflow_uri="{{inputs.parameters.mlflow-tracking-uri}}"
+model_name="{{inputs.parameters.model-name}}"
+subprocess.check_call(["aws","s3","cp", ms3, "/tmp/model.tar.gz"])
+subprocess.check_call(["tar","xzf","/tmp/model.tar.gz","-C","/tmp"])
+subprocess.check_call(["pip","install","mlflow"])
+import mlflow
+mlflow.set_tracking_uri(mlflow_uri)
+client = mlflow.tracking.MlflowClient()
+run = mlflow.start_run()
+mlflow.log_artifact("/tmp/model", artifact_path="model")
+result = mlflow.register_model(f"runs:/{run.info.run_id}/model", model_name)
+mlflow.end_run()
+print("Registered model:", result.name)
+PY
+
*** End Patch
*** Begin Patch
*** Add File:ops/train.py
+#!/usr/bin/env python3
+"""
+Minimal training script used by Argo workflow.
+It trains a tiny model (placeholder) and logs to MLflow. Replace with your real training code.
+"""
+import argparse, os, json
+import mlflow
+import numpy as np
+from sklearn.linear_model import LogisticRegression
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+import joblib
+
+def main(args):
+    mlflow.set_tracking_uri(args.mlflow_tracking_uri)
+    mlflow.set_experiment(args.experiment)
+    with mlflow.start_run() as run:
+        X,y = make_classification(n_samples=200, n_features=10, random_state=42)
+        Xtrain,Xval,ytrain,yval = train_test_split(X,y,test_size=0.2,random_state=42)
+        clf = LogisticRegression(max_iter=200)
+        clf.fit(Xtrain,ytrain)
+        acc = clf.score(Xval,yval)
+        mlflow.log_metric("val_accuracy", float(acc))
+        os.makedirs(args.output, exist_ok=True)
+        joblib.dump(clf, os.path.join(args.output, "model.pkl"))
+        # Save a minimal model folder
+        model_dir = os.path.join(args.output, "model")
+        os.makedirs(model_dir, exist_ok=True)
+        joblib.dump(clf, os.path.join(model_dir, "model.pkl"))
+        # record run id
+        with open(os.path.join(args.output, "run_meta.json"), "w") as f:
+            json.dump({"run_id": run.info.run_id, "val_accuracy": acc}, f)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--data", required=True)
+    p.add_argument("--mlflow-tracking-uri", required=True)
+    p.add_argument("--experiment", required=True)
+    p.add_argument("--output", required=True)
+    args = p.parse_args()
+    main(args)
+
*** End Patch
*** Begin Patch
*** Add File:ops/validate_model.py
+#!/usr/bin/env python3
+"""
+Model validation script: downloads model bundle from S3, runs a simple validation and fails if threshold not met.
+"""
+import argparse, subprocess, json, os, sys
+import tempfile, joblib
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-s3", required=True)
+    p.add_argument("--threshold", type=float, required=True)
+    p.add_argument("--mlflow-tracking-uri", required=True)
+    args = p.parse_args()
+    tmp = tempfile.mkdtemp()
+    local = os.path.join(tmp, "model.tar.gz")
+    subprocess.check_call(["aws","s3","cp", args.model_s3, local])
+    subprocess.check_call(["tar","xzf", local, "-C", tmp])
+    model_path = os.path.join(tmp, "model", "model.pkl")
+    if not os.path.exists(model_path):
+        print("Model artifact missing", model_path)
+        sys.exit(2)
+    clf = joblib.load(model_path)
+    # small validation dataset
+    X,y = make_classification(n_samples=100, n_features=10, random_state=1)
+    Xtr,Xv,ytr,yv = train_test_split(X,y,test_size=0.2, random_state=1)
+    acc = clf.score(Xv,yv)
+    print("Validation accuracy:", acc)
+    if acc < args.threshold:
+        print("Validation failed: below threshold")
+        sys.exit(3)
+    print("Validation passed")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:kserve/inference_inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-model"
+  namespace: "aegis"
+spec:
+  predictor:
+    serviceAccountName: "aegis-inference-sa"
+    sklearn:
+      storageUri: "s3://REPLACE_EVIDENCE_BUCKET/models/latest"
+      resources:
+        limits:
+          cpu: "1000m"
+          memory: "2Gi"
+    container:
+      env:
+        - name: MODEL_NAME
+          value: "aegis-model"
+  # Autoscaling via KServe annotations (Knative autoscaler)
+  # Adjust concurrency/replicas according to SLOs
+  minReplicas: 1
+  maxReplicas: 5
+  annotations:
+    autoscaling.knative.dev/minScale: "1"
+    autoscaling.knative.dev/maxScale: "5"
+
+---
+# Example traffic split via InferenceService can be done by creating two inference services
+# and using Knative Route or Seldon for gradual rollout. This is a placeholder for canary.
+# Replace with your traffic split policy tooling (Kubernetes Service Mesh / Istio / Seldon).
+
*** End Patch
*** Begin Patch
*** Add File:featurestore/feast_sample/feature_repo.yaml
+# Minimal Feast feature repo config (example). Replace with your cluster/infra values.
+project: aegis-feast
+registry: s3://REPLACE_EVIDENCE_BUCKET/feast/registry.db
+provider: local
+online_store:
+  type: redis
+  connection_string: "redis://REPLACE_REDIS_HOST:6379"
+
*** End Patch
*** Begin Patch
*** Add File:featurestore/feast_sample/feature_view.py
+from feast import Entity, Feature, FeatureView, ValueType, FileSource
+from datetime import timedelta
+
+driver = FileSource(
+    path="data/driver_stats.parquet",
+    event_timestamp_column="event_ts",
+)
+
+driver_entity = Entity(name="driver_id", value_type=ValueType.INT64, description="driver id")
+
+driver_fv = FeatureView(
+    name="driver_features",
+    entities=["driver_id"],
+    ttl=timedelta(days=1),
+    features=[
+        Feature(name="avg_daily_trips", dtype=ValueType.FLOAT),
+        Feature(name="avg_rating", dtype=ValueType.FLOAT),
+    ],
+    online=True,
+    source=driver,
+)
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/ci_train_and_promote.yml
+name: CI — train (small) and promote
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  unit-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install -r requirements.txt
+      - name: Run unit tests
+        run: pytest -q
+
+  train-and-validate:
+    needs: unit-tests
+    runs-on: ubuntu-latest
+    env:
+      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install -r requirements.txt mlflow boto3 scikit-learn joblib
+      - name: Run quick training (small)
+        run: |
+          python3 ops/train.py --data tests/fixtures/tiny_dataset --mlflow-tracking-uri "${MLFLOW_TRACKING_URI}" --experiment "ci-test" --output /tmp/ci_output
+      - name: Validate model
+        run: |
+          python3 ops/validate_model.py --model-s3 "s3://REPLACE_EVIDENCE_BUCKET/ci_model/model.tar.gz" --threshold 0.5 --mlflow-tracking-uri "${MLFLOW_TRACKING_URI}" || exit 1
+      - name: Promote model to registry (manual approval recommended)
+        if: success()
+        run: |
+          echo "Model validated in CI. Please run promotion pipeline to staging/production manually or via protected workflow."
+
*** End Patch
*** Begin Patch
*** Add File:docs/operator_end_to_end_runbook.md
+# Operator End-to-End Runbook — Train → Validate → Serve
+
+This document walks an operator through running a full E2E: training in Argo, validating, registering, and serving via KServe. Replace REPLACE_* tokens locally; do NOT commit secrets.
+
+Prerequisites
+- kubectl configured to target cluster with Argo + KServe installed.
+- AWS CLI configured with permissions to read/write EVIDENCE_BUCKET.
+- MLflow server available and MLFLOW_TRACKING_URI set.
+- GitHub secrets set: MLFLOW_TRACKING_URI, COSIGN_KMS_ARN, REKOR_URL, EVIDENCE_BUCKET.
+- IRSA roles created and ServiceAccounts annotated.
+
+Steps
+1. Build and push training image (operator workstation):
+   ./ops/build_and_cosign.sh REPLACE_GHCR_ORG/train REPLACE_IMAGE_TAG
+
+2. Apply Argo training workflow (already in repo):
+   kubectl apply -f argo/train_workflow.yaml
+   argo submit -n aegis argo/train_workflow.yaml -p image=REPLACE_GHCR_ORG/train:REPLACE_IMAGE_TAG -p dataset-s3=s3://REPLACE_EVIDENCE_BUCKET/datasets/example -p mlflow-tracking-uri=REPLACE_MLFLOW_TRACKING_URI
+
+3. Monitor workflow:
+   argo get @latest -n aegis
+   argo logs <wf-name> -n aegis
+
+4. After success, validate artifacts exist:
+   aws s3 ls s3://REPLACE_EVIDENCE_BUCKET/models/<workflow-uid>/
+
+5. Deploy InferenceService (KServe):
+   kubectl apply -f kserve/inference_inferenceservice.yaml
+   # Wait for readiness:
+   kubectl -n aegis get inferenceservice aegis-model -o yaml
+
+6. Test inference:
+   # Use port-forward or ingress hostname
+   kubectl -n aegis port-forward svc/istio-ingressgateway 8080:80
+   curl -v -H "Content-Type: application/json" -d '{"instances":[[0.1,0.2,...]]}' http://localhost:8080/v1/models/aegis-model:predict
+
+7. Audit artifacts & sign-off:
+   Ensure mlflow run recorded; signed model artifacts exist in S3 and (if configured) Rekor entries were created.
+
+Troubleshooting
+- If training fails due to AWS permissions, verify IRSA role ARN annotation on ServiceAccount.
+- If KServe pod fails, check events: kubectl -n aegis describe inferenceservice aegis-model
+
*** End Patch
*** End Patch
