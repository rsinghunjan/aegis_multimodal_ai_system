*** Begin Patch
*** Add File: infra/iam/aws/managed_policies/s3_backup_policy_prod_strict.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowListBucket",
+      "Effect": "Allow",
+      "Action": [
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::YOUR_MILVUS_BACKUP_BUCKET"
+      ],
+      "Condition": {
+        "StringLike": {
+          "s3:prefix": [
+            "backups/*",
+            ""
+          ]
+        }
+      }
+    },
+    {
+      "Sid": "AllowObjectOpsForBackups",
+      "Effect": "Allow",
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject"
+      ],
+      "Resource": [
+        "arn:aws:s3:::YOUR_MILVUS_BACKUP_BUCKET/backups/*"
+      ]
+    },
+    {
+      "Sid": "AllowKmsUsageForBackup",
+      "Effect": "Allow",
+      "Action": [
+        "kms:Encrypt",
+        "kms:Decrypt",
+        "kms:ReEncrypt*",
+        "kms:GenerateDataKey*",
+        "kms:DescribeKey"
+      ],
+      "Resource": "arn:aws:kms:REGION:ACCOUNT_ID:key/REPLACE_ME_KEY_ID"
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: infra/iam/aws/managed_policies/kms_policy_prod_strict.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowLimitedKMSUse",
+      "Effect": "Allow",
+      "Action": [
+        "kms:Encrypt",
+        "kms:Decrypt",
+        "kms:GenerateDataKey*",
+        "kms:ReEncrypt*",
+        "kms:DescribeKey"
+      ],
+      "Resource": "arn:aws:kms:REGION:ACCOUNT_ID:key/REPLACE_ME_KEY_ID"
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: scripts/apply_aws_policies_and_attach.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# apply_aws_policies_and_attach.sh
+# Creates AWS managed policies (from infra/iam/aws/managed_policies) for the backup bucket/KMS,
+# and attaches them to the provided IAM role (IRSA role) used by the backup ServiceAccount.
+#
+# Usage:
+#   ./scripts/apply_aws_policies_and_attach.sh --bucket my-bucket --kms-arn arn:aws:kms:... --role-name my-irsa-role
+#
+BUCKET=""
+KMS_ARN=""
+ROLE_NAME=""
+OUT_DIR="infra/iam/aws/generated"
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --bucket) BUCKET="$2"; shift 2;;
+    --kms-arn) KMS_ARN="$2"; shift 2;;
+    --role-name) ROLE_NAME="$2"; shift 2;;
+    --out-dir) OUT_DIR="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --bucket BUCKET --kms-arn KMS_ARN --role-name ROLE_NAME"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$BUCKET" ] || [ -z "$KMS_ARN" ] || [ -z "$ROLE_NAME" ]; then
+  echo "Missing args"
+  exit 2
+fi
+
+mkdir -p "$OUT_DIR"
+
+S3_POLICY_SRC="infra/iam/aws/managed_policies/s3_backup_policy_prod_strict.json"
+KMS_POLICY_SRC="infra/iam/aws/managed_policies/kms_policy_prod_strict.json"
+S3_POLICY_TMP="$OUT_DIR/s3_backup_policy_${BUCKET}.json"
+KMS_POLICY_TMP="$OUT_DIR/kms_policy_prod.json"
+
+# Substitute placeholders
+sed "s|YOUR_MILVUS_BACKUP_BUCKET|${BUCKET}|g; s|arn:aws:kms:REGION:ACCOUNT_ID:key/REPLACE_ME_KEY_ID|${KMS_ARN}|g" "$S3_POLICY_SRC" > "$S3_POLICY_TMP"
+sed "s|arn:aws:kms:REGION:ACCOUNT_ID:key/REPLACE_ME_KEY_ID|${KMS_ARN}|g" "$KMS_POLICY_SRC" > "$KMS_POLICY_TMP"
+
+echo "Creating managed policy for S3 backup"
+S3_POLICY_ARN=$(aws iam create-policy --policy-name "AegisS3BackupPolicy-${BUCKET}" --policy-document file://"${S3_POLICY_TMP}" --query Policy.Arn --output text)
+echo "S3 policy created: $S3_POLICY_ARN"
+
+echo "Creating managed policy for KMS usage (if not existing)"
+KMS_POLICY_ARN=$(aws iam create-policy --policy-name "AegisKMSBackupPolicy-${ROLE_NAME}" --policy-document file://"${KMS_POLICY_TMP}" --query Policy.Arn --output text)
+echo "KMS policy created: $KMS_POLICY_ARN"
+
+echo "Attaching policies to role ${ROLE_NAME}"
+aws iam attach-role-policy --role-name "${ROLE_NAME}" --policy-arn "${S3_POLICY_ARN}"
+aws iam attach-role-policy --role-name "${ROLE_NAME}" --policy-arn "${KMS_POLICY_ARN}"
+
+echo "Policies attached. You may want to review and test in staging."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/aws_backup_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# aws_backup_restore_drill.sh
+# Orchestrates a staging backup -> restore drill for Milvus on AWS:
+# 1) Trigger a Milvus backup job (Helm job or Kubernetes job)
+# 2) Wait and confirm S3 objects written
+# 3) Download latest backup to a staging restore location
+# 4) (Optional) Run Milvus restore command or provide artifacts for manual restore
+#
+# Usage:
+#  ./scripts/aws_backup_restore_drill.sh --namespace aegis --backup-job milvus-backup-job --s3-bucket my-bucket --restore-dir /tmp/milvus-restore
+#
+NAMESPACE="aegis"
+BACKUP_JOB="aegis-milvus-backup"
+S3_BUCKET=""
+RESTORE_DIR="/tmp/milvus-restore-$(date -u +%Y%m%dT%H%M%SZ)"
+TIMEOUT=900
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --backup-job) BACKUP_JOB="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --restore-dir) RESTORE_DIR="$2"; shift 2;;
+    --timeout) TIMEOUT="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --namespace N --backup-job JOB --s3-bucket BUCKET [--restore-dir DIR]"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$S3_BUCKET" ]; then
+  echo "Missing --s3-bucket"
+  exit 2
+fi
+
+echo "Triggering backup job in namespace ${NAMESPACE}"
+# Try to create a job from an existing CronJob name (safe approach)
+JOB_NAME="${BACKUP_JOB}-manual-$(date +%s)"
+kubectl -n "${NAMESPACE}" create job --from=cronjob/"${BACKUP_JOB}" "${JOB_NAME}" || echo "Could not create job from cronjob; ensure backup job exists"
+
+echo "Waiting for job completion (timeout ${TIMEOUT}s)"
+kubectl -n "${NAMESPACE}" wait --for=condition=complete job/"${JOB_NAME}" --timeout=${TIMEOUT}s || echo "Job did not complete in time"
+
+echo "Listing S3 backup prefixes for bucket ${S3_BUCKET}"
+aws s3 ls "s3://${S3_BUCKET}/backups/" --recursive | tee /tmp/s3_backups_list.txt
+LATEST_PREFIX=$(aws s3 ls "s3://${S3_BUCKET}/backups/" --recursive | sort | tail -n 20 | tail -n 1 | awk '{print $4}' || true)
+
+if [ -z "$LATEST_PREFIX" ]; then
+  echo "No backups found; aborting drill"
+  exit 1
+fi
+
+echo "Latest backup object key sample: ${LATEST_PREFIX}"
+mkdir -p "${RESTORE_DIR}"
+echo "Downloading backup objects from s3://${S3_BUCKET}/${LATEST_PREFIX} -> ${RESTORE_DIR}"
+aws s3 cp --recursive "s3://${S3_BUCKET}/${LATEST_PREFIX}" "${RESTORE_DIR}/"
+
+echo "Downloaded backup artifacts to ${RESTORE_DIR}"
+echo "Next: run your Milvus restore job using the downloaded artifacts (not performed automatically by this script)."
+echo "Collect logs, verify data integrity and run any application-level checks."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/aws_staging_drill.yml
+name: AWS Staging Backup→Restore Drill
+
+on:
+  workflow_dispatch:
+    inputs:
+      namespace:
+        description: "K8s namespace containing backup CronJob"
+        required: true
+        default: aegis
+      backup_job:
+        description: "Backup CronJob name"
+        required: true
+        default: aegis-milvus-backup
+      s3_bucket:
+        description: "S3 bucket name"
+        required: true
+
+permissions:
+  contents: read
+
+jobs:
+  staging-drill:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Configure AWS credentials
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          aws-region: ${{ secrets.AWS_REGION }}
+
+      - name: Decode kubeconfig
+        run: |
+          if [ -n "${{ secrets.KUBE_CONFIG_DATA }}" ]; then
+            echo "${{ secrets.KUBE_CONFIG_DATA }}" | base64 --decode > /tmp/kubeconfig
+            mkdir -p $HOME/.kube
+            mv /tmp/kubeconfig $HOME/.kube/config
+          fi
+
+      - name: Run staging drill
+        run: |
+          chmod +x scripts/aws_backup_restore_drill.sh
+          ./scripts/aws_backup_restore_drill.sh --namespace "${{ github.event.inputs.namespace }}" --backup-job "${{ github.event.inputs.backup_job }}" --s3-bucket "${{ github.event.inputs.s3_bucket }}"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/tagging_enforce_aws.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# tagging_enforce_aws.sh
+# Ensure test/staging EC2/EBS/S3 resources created by drills are tagged with required tags for chargeback and cleanup.
+#
+# Usage:
+#   ./scripts/tagging_enforce_aws.sh --resource-arns arn1,arn2 --tag key=value
+#
+RESOURCE_ARNS=""
+TAG=""
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --resource-arns) RESOURCE_ARNS="$2"; shift 2;;
+    --tag) TAG="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --resource-arns arn1,arn2 --tag key=value"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$RESOURCE_ARNS" ] || [ -z "$TAG" ]; then
+  echo "Missing args"
+  exit 2
+fi
+
+IFS=',' read -r -a ARNS <<< "$RESOURCE_ARNS"
+for arn in "${ARNS[@]}"; do
+  echo "Tagging $arn with $TAG"
+  aws resourcegroupstaggingapi tag-resources --resource-arn-list "$arn" --tags "${TAG%=*}=${TAG#*=}" || echo "Failed to tag $arn"
+done
+
+echo "Tagging enforcement attempted for provided ARNs"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/check_pod_identity_crds_detailed.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# check_pod_identity_crds_detailed.sh
+# Detect whether aad-pod-identity v1 CRDs or pod-identity v2 CRDs exist and print recommended CR names & examples.
+#
+echo "Checking CRDs for pod identity solutions..."
+CRDS=$(kubectl get crds -o name || true)
+echo "$CRDS" | grep -E 'azureassignedidentities|azureidentitybindings|azureidentities' >/dev/null 2>&1 && echo "Detected aad-pod-identity v1 CRDs (AzureIdentity / AzureIdentityBinding / AzureAssignedIdentity)"
+echo "$CRDS" | grep -i 'podidentity' >/dev/null 2>&1 && echo "Detected pod-identity v2-style CRDs (names may vary by implementation)"
+
+echo
+echo "Suggested next steps:"
+echo "- If using aad-pod-identity v1: use AzureIdentity / AzureIdentityBinding CRs. Example files are provided in scripts/ or infra/docs."
+echo "- If using pod-identity v2 (Azure AD Pod Identity v2), consult your operator docs and adapt CR names."
+echo
+echo "Example to check for AzureIdentity CRs:"
+kubectl get AzureIdentity --all-namespaces || true
+echo
+echo "Example to check for AzureAssignedIdentity:"
+kubectl get AzureAssignedIdentity --all-namespaces || true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/assign_keyvault_permissions_for_backup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# assign_keyvault_permissions_for_backup.sh
+# Grant Key Vault key permissions to a backup service principal / UAMI so backups can use CMK.
+#
+# Usage:
+#  ./scripts/assign_keyvault_permissions_for_backup.sh --vault VAULT_NAME --key-name KEY_NAME --object-id PRINCIPAL_OBJECT_ID
+#
+VAULT=""
+KEY_NAME=""
+OBJECT_ID=""
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --vault) VAULT="$2"; shift 2;;
+    --key-name) KEY_NAME="$2"; shift 2;;
+    --object-id) OBJECT_ID="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --vault VAULT --key-name KEY --object-id PRINCIPAL_ID"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$VAULT" ] || [ -z "$KEY_NAME" ] || [ -z "$OBJECT_ID" ]; then
+  echo "Missing args"
+  exit 2
+fi
+
+echo "Granting key permissions wrapKey/unwrapKey/get to principal $OBJECT_ID on key $KEY_NAME in vault $VAULT"
+az keyvault set-policy --name "$VAULT" --object-id "$OBJECT_ID" --key-permissions wrapKey unwrapKey get || {
+  echo "Failed to set Key Vault policy via az. Verify you have permissions."
+  exit 1
+}
+
+echo "Permission assignment complete. Verify with:"
+echo "az keyvault show --name $VAULT --query properties" 
+
*** End Patch
*** Begin Patch
*** Add File: scripts/azure_backup_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# azure_backup_restore_drill.sh
+# Orchestrates a staging backup -> restore drill for Milvus on AKS using Azure Blob + Key Vault CMK.
+#
+# Usage:
+#  ./scripts/azure_backup_restore_drill.sh --namespace aegis --backup-cronjob aegis-milvus-backup --storage-account myacct --container mycontainer --restore-dir /tmp/milvus-restore
+#
+NAMESPACE="aegis"
+BACKUP_CRONJOB="aegis-milvus-backup"
+STORAGE_ACCOUNT=""
+CONTAINER=""
+RESTORE_DIR="/tmp/milvus-restore-$(date -u +%Y%m%dT%H%M%SZ)"
+TIMEOUT=900
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --backup-cronjob) BACKUP_CRONJOB="$2"; shift 2;;
+    --storage-account) STORAGE_ACCOUNT="$2"; shift 2;;
+    --container) CONTAINER="$2"; shift 2;;
+    --restore-dir) RESTORE_DIR="$2"; shift 2;;
+    --timeout) TIMEOUT="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --namespace N --backup-cronjob JOB --storage-account ACCT --container CONTAINER [--restore-dir DIR]"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$STORAGE_ACCOUNT" ] || [ -z "$CONTAINER" ]; then
+  echo "Missing storage account or container"
+  exit 2
+fi
+
+echo "Triggering backup job in namespace ${NAMESPACE}"
+JOB_NAME="${BACKUP_CRONJOB}-manual-$(date +%s)"
+kubectl -n "${NAMESPACE}" create job --from=cronjob/"${BACKUP_CRONJOB}" "${JOB_NAME}" || echo "Could not create job from cronjob; ensure backup job exists"
+
+echo "Waiting for job completion (timeout ${TIMEOUT}s)"
+kubectl -n "${NAMESPACE}" wait --for=condition=complete job/"${JOB_NAME}" --timeout=${TIMEOUT}s || echo "Job did not complete in time"
+
+echo "Listing blobs in container ${CONTAINER}"
+az storage blob list --account-name "${STORAGE_ACCOUNT}" --container-name "${CONTAINER}" --output table
+
+LATEST_BLOB=$(az storage blob list --account-name "${STORAGE_ACCOUNT}" --container-name "${CONTAINER}" --query "[-1].name" -o tsv || true)
+if [ -z "$LATEST_BLOB" ]; then
+  echo "No blobs found; aborting drill"
+  exit 1
+fi
+
+mkdir -p "${RESTORE_DIR}"
+echo "Downloading blobs to ${RESTORE_DIR}"
+az storage blob download-batch -s "${CONTAINER}" -d "${RESTORE_DIR}" --account-name "${STORAGE_ACCOUNT}"
+
+echo "Downloaded backup artifacts to ${RESTORE_DIR}"
+echo "Next: run your Milvus restore job using the downloaded artifacts (not performed automatically by this script)."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/staging_backup_restore_drill.yml
+name: Staging Backup→Restore Drill (AWS + Azure)
+
+on:
+  workflow_dispatch:
+    inputs:
+      run_aws:
+        required: false
+        default: 'true'
+      run_azure:
+        required: false
+        default: 'true'
+
+permissions:
+  contents: read
+
+jobs:
+  aws-drill:
+    if: ${{ github.event.inputs.run_aws == 'true' }}
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Configure AWS credentials
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          aws-region: ${{ secrets.AWS_REGION }}
+
+      - name: Decode kubeconfig
+        run: |
+          if [ -n "${{ secrets.KUBE_CONFIG_DATA }}" ]; then
+            echo "${{ secrets.KUBE_CONFIG_DATA }}" | base64 --decode > /tmp/kubeconfig
+            mkdir -p $HOME/.kube
+            mv /tmp/kubeconfig $HOME/.kube/config
+          fi
+
+      - name: Run AWS drill
+        run: |
+          chmod +x scripts/aws_backup_restore_drill.sh
+          ./scripts/aws_backup_restore_drill.sh --namespace "${{ secrets.STAGING_NAMESPACE }}" --backup-job "${{ secrets.AWS_BACKUP_CRONJOB }}" --s3-bucket "${{ secrets.AWS_BACKUP_BUCKET }}" --restore-dir /tmp/aegis-aws-restore
+
+  azure-drill:
+    if: ${{ github.event.inputs.run_azure == 'true' }}
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Configure Azure CLI
+        uses: azure/login@v1
+        with:
+          creds: ${{ secrets.AZURE_CREDENTIALS }}
+
+      - name: Decode kubeconfig
+        run: |
+          if [ -n "${{ secrets.KUBE_CONFIG_DATA }}" ]; then
+            echo "${{ secrets.KUBE_CONFIG_DATA }}" | base64 --decode > /tmp/kubeconfig
+            mkdir -p $HOME/.kube
+            mv /tmp/kubeconfig $HOME/.kube/config
+          fi
+
+      - name: Run Azure drill
+        run: |
+          chmod +x scripts/azure_backup_restore_drill.sh
+          ./scripts/azure_backup_restore_drill.sh --namespace "${{ secrets.STAGING_NAMESPACE }}" --backup-cronjob "${{ secrets.AZURE_BACKUP_CRONJOB }}" --storage-account "${{ secrets.AZURE_STORAGE_ACCOUNT }}" --container "${{ secrets.AZURE_BACKUP_CONTAINER }}" --restore-dir /tmp/aegis-azure-restore
+
*** End Patch
*** Begin Patch
*** Add File: docs/staging_backup_restore_runbook.md
+Staging Backup→Restore Drill Runbook (AWS + Azure)
+===============================================
+
+Purpose
+-------
+This runbook explains how to run a staging backup→restore drill and what to validate for AWS (S3+KMS) and Azure (Blob+KeyVault).
+
+Preconditions
+-------------
+- Ensure staging Kubernetes cluster is reachable (KUBE_CONFIG_DATA).
+- Ensure secrets for cloud access are configured for GitHub Actions (see workflows).
+- Ensure backup CronJob names and bucket/container names are set in repository secrets.
+
+Steps (AWS)
+----------
+1. Trigger the GitHub Actions workflow: "Staging Backup→Restore Drill" with run_aws=true.
+2. The workflow will:
+   - Create a manual job from the backup CronJob.
+   - Wait for job completion.
+   - List S3 objects under backups/ and download the latest backup to the restore dir.
+3. Validate:
+   - Objects exist in s3://BUCKET/backups/...
+   - Objects show SSE-KMS metadata, if applicable.
+   - Start an isolated Milvus restore using the downloaded artifacts and run application-level checks.
+
+Steps (Azure)
+------------
+1. Trigger the GitHub Actions workflow: "Staging Backup→Restore Drill" with run_azure=true.
+2. The workflow will:
+   - Create a manual job from the backup CronJob.
+   - Wait for job completion.
+   - List blobs and download them to the restore dir.
+3. Validate:
+   - Blobs exist and are accessible.
+   - Key Vault access was successful during backup (no permission errors).
+   - Restore Milvus using downloaded artifacts and run application-level checks.
+
+Post-checks
+-----------
+- Collect logs from backup job pods, storage listings, and any restore job logs.
+- Tag created cloud resources with cost-center=test or similar using tagging_enforce_aws.sh or equivalent for Azure.
+- If tests pass, schedule KMS rotation test (create new key, update values in staging, run one backup).
+
+Failure handling
+----------------
+- If backup job fails: collect pod logs and events; do not proceed to restore until root cause is identified.
+- If object upload succeeds but decryption fails on restore: verify KMS grants/policies and the service identity key permissions.
+
*** End Patch
