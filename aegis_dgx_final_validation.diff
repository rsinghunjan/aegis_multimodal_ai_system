diff --git a/scripts/dgx/ensure_image_provenance.sh b/scripts/dgx/ensure_image_provenance.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/dgx/ensure_image_provenance.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Ensure image provenance: verify cosign signature, generate SBOM if missing, and append a digest-pinned entry
+# to docs/dgx/IMAGE_MATRIX.md. Writes artifacts to artifacts/dgx_provenance.
+#
+# Usage:
+#   REGISTRY=ghcr.io/org IMAGE_TAG=aegis-deepspeed:h100... \
+#     ./scripts/dgx/ensure_image_provenance.sh --sbom-dir ./artifacts/sbom --image-matrix docs/dgx/IMAGE_MATRIX.md
+
+set -euo pipefail
+
+IMAGE_FULL="${REGISTRY}/${IMAGE_TAG:-}"
+SBOM_DIR="${SBOM_DIR:-./artifacts/dgx_provenance/sbom}"
+IMAGE_MATRIX_FILE="${IMAGE_MATRIX_FILE:-docs/dgx/IMAGE_MATRIX.md}"
+OUT_DIR="${OUT_DIR:-./artifacts/dgx_provenance}"
+
+if [[ -z "${REGISTRY:-}" || -z "${IMAGE_TAG:-}" ]]; then
+  echo "REGISTRY and IMAGE_TAG environment variables are required"
+  exit 2
+fi
+
+mkdir -p "$SBOM_DIR" "$OUT_DIR"
+
+echo "Verifying cosign signature for image: $IMAGE_FULL"
+if command -v cosign >/dev/null 2>&1; then
+  if cosign verify --verbose "$IMAGE_FULL" > "$OUT_DIR/cosign_verify.txt" 2>&1; then
+    echo "cosign verification succeeded"
+    COSIGN_OK=true
+  else
+    echo "cosign verification FAILED; see $OUT_DIR/cosign_verify.txt"
+    COSIGN_OK=false
+  fi
+else
+  echo "cosign not installed; cannot verify signature"
+  COSIGN_OK=false
+fi
+
+echo "Generating SBOM with syft (if available)"
+SBOM_FILE="$SBOM_DIR/$(echo $IMAGE_TAG | tr '/:' '__')-sbom.json"
+if command -v syft >/dev/null 2>&1; then
+  syft "$IMAGE_FULL" -o json > "$SBOM_FILE" 2> "$OUT_DIR/syft.log" || true
+  echo "SBOM written to $SBOM_FILE"
+else
+  echo "syft not found; skipping SBOM generation"
+fi
+
+echo "Resolving image digest"
+DIGEST="unknown"
+if command -v skopeo >/dev/null 2>&1; then
+  skopeo inspect "docker://${IMAGE_FULL}" > "$OUT_DIR/skopeo_inspect.json" 2>/dev/null || true
+  DIGEST=$(jq -r '.Digest // empty' "$OUT_DIR/skopeo_inspect.json" 2>/dev/null || true)
+fi
+if [[ -z "$DIGEST" ]]; then
+  if command -v docker >/dev/null 2>&1; then
+    docker pull "$IMAGE_FULL" > "$OUT_DIR/docker_pull.txt" 2>&1 || true
+    DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE_FULL" 2>/dev/null || true)
+  fi
+fi
+if [[ -z "$DIGEST" ]]; then
+  DIGEST="unknown"
+fi
+echo "Image digest: $DIGEST" > "$OUT_DIR/image_digest.txt"
+
+echo "Appending/confirming IMAGE_MATRIX entry"
+timestamp=$(date -u +"%Y-%m-%d")
+entry="- image: ${IMAGE_FULL}
+  digest: ${DIGEST}
+  validated_on: ${timestamp}
+  sbom: ${SBOM_FILE}
+  cosign_verified: ${COSIGN_OK}
+"
+
+# Avoid duplicate entries for same image+digest
+if grep -qF "${IMAGE_FULL}" "$IMAGE_MATRIX_FILE" 2>/dev/null; then
+  echo "IMAGE_MATRIX already contains an entry for ${IMAGE_FULL}; skipping append (verify manually if needed)"
+else
+  echo "Appending entry to $IMAGE_MATRIX_FILE"
+  printf "%s\n" "$entry" >> "$IMAGE_MATRIX_FILE"
+fi
+
+echo "Provenance artifacts available in $OUT_DIR"
+exit 0
+
diff --git a/scripts/dgx/validate_gpu_operator_and_drivers.sh b/scripts/dgx/validate_gpu_operator_and_drivers.sh
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/scripts/dgx/validate_gpu_operator_and_drivers.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Validate GPU Operator driver strategy and check node-level driver/CUDA presence.
+# Uses helm (if available) to query GPU Operator values and the driver-check DaemonSet logs.
+#
+# Usage:
+#   ./scripts/dgx/validate_gpu_operator_and_drivers.sh --expected-autoInstall false --expected-cuda 12.1 --expected-driver 535
+
+set -euo pipefail
+
+EXPECTED_AUTOINSTALL=""
+EXPECTED_CUDA=""
+EXPECTED_DRIVER=""
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+OUT_DIR="${OUT_DIR:-./artifacts/dgx_driver_validation}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --expected-autoInstall) EXPECTED_AUTOINSTALL="$2"; shift 2;;
+    --expected-cuda) EXPECTED_CUDA="$2"; shift 2;;
+    --expected-driver) EXPECTED_DRIVER="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "Checking GPU Operator values (if helm present)"
+if command -v helm >/dev/null 2>&1; then
+  helm get values -n gpu-operator gpu-operator -o yaml > "$OUT_DIR/gpu_operator_values.yaml" 2>/dev/null || true
+  if [[ -n "$EXPECTED_AUTOINSTALL" ]]; then
+    if command -v yq >/dev/null 2>&1; then
+      current=$(yq e '.driver.autoInstall // "unknown"' "$OUT_DIR/gpu_operator_values.yaml")
+      echo "GPU Operator driver.autoInstall = $current (expected $EXPECTED_AUTOINSTALL)" | tee -a "$OUT_DIR/auto_install_check.txt"
+      if [[ "$current" != "$EXPECTED_AUTOINSTALL" ]]; then
+        echo "Mismatch in autoInstall setting" | tee -a "$OUT_DIR/auto_install_check.txt"
+      fi
+    else
+      echo "yq not available; inspect $OUT_DIR/gpu_operator_values.yaml manually"
+    fi
+  fi
+else
+  echo "helm not present; cannot query GPU Operator values"
+fi
+
+echo "Running driver compatibility check (daemonset) and collecting logs"
+kubectl apply -f k8s/manifests/dgx/driver-check-daemonset.yaml -n "$NAMESPACE" || true
+sleep 5
+kubectl -n "$NAMESPACE" rollout status daemonset/dgx-driver-check --timeout=60s || true
+pods=$(kubectl -n "$NAMESPACE" get pods -l app=dgx-driver-check -o jsonpath='{.items[*].metadata.name}')
+for p in $pods; do
+  kubectl -n "$NAMESPACE" logs "$p" > "$OUT_DIR/${p}.log" 2>&1 || true
+done
+
+echo "Inspecting collected logs for CUDA/Driver versions"
+grep -E "Driver Version|CUDA Version" -n "$OUT_DIR"/*.log || true
+
+if [[ -n "$EXPECTED_CUDA" ]]; then
+  if grep -R -q "$EXPECTED_CUDA" "$OUT_DIR"/*.log; then
+    echo "Expected CUDA ($EXPECTED_CUDA) observed in node logs" | tee -a "$OUT_DIR/check_results.txt"
+  else
+    echo "Expected CUDA ($EXPECTED_CUDA) NOT observed; review logs" | tee -a "$OUT_DIR/check_results.txt"
+  fi
+fi
+if [[ -n "$EXPECTED_DRIVER" ]]; then
+  if grep -R -q "$EXPECTED_DRIVER" "$OUT_DIR"/*.log; then
+    echo "Expected driver ($EXPECTED_DRIVER) observed" | tee -a "$OUT_DIR/check_results.txt"
+  else
+    echo "Expected driver ($EXPECTED_DRIVER) NOT observed; review logs" | tee -a "$OUT_DIR/check_results.txt"
+  fi
+fi
+
+echo "Driver validation completed. See $OUT_DIR for details."
+exit 0
+
diff --git a/scripts/dgx/apply_nccl_and_run_multinode.sh b/scripts/dgx/apply_nccl_and_run_multinode.sh
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/dgx/apply_nccl_and_run_multinode.sh
@@ -0,0 +1,300 @@
+#!/usr/bin/env bash
+#
+# Run NCCL tuning, apply suggested envs to dgx-nccl-config, update job manifest, run a multi-node DeepSpeed scaling test,
+# collect artifacts and run basic verification for NCCL errors.
+#
+# Usage:
+#   IMAGE=<REGISTRY>/aegis-deepspeed:... ./scripts/dgx/apply_nccl_and_run_multinode.sh --nodes 2 --gpus-per-node 8
+
+set -euo pipefail
+
+NODES=1
+GPUS_PER_NODE=8
+OUT_DIR="${OUT_DIR:-./artifacts/dgx_multinode}"
+JOB_MANIFEST="${JOB_MANIFEST:-k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml}"
+IMAGE="${IMAGE:-}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --gpus-per-node) GPUS_PER_NODE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    --job-manifest) JOB_MANIFEST="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$IMAGE" ]]; then
+  echo "IMAGE env var must be set to the image to use for the multi-node test"
+  exit 2
+fi
+
+mkdir -p "$OUT_DIR"
+
+echo "1) Run NCCL tuning harness"
+if [[ -x "./scripts/dgx/nccl_tuning.sh" ]]; then
+  ./scripts/dgx/nccl_tuning.sh --nodes "$NODES" --gpus-per-node "$GPUS_PER_NODE" --out "$OUT_DIR/nccl" || true
+else
+  echo "nccl_tuning.sh not found; aborting"
+  exit 3
+fi
+
+echo "2) Generate NCCL env suggestions"
+if [[ -x "./scripts/dgx/generate_nccl_env_suggest.sh" ]]; then
+  ./scripts/dgx/generate_nccl_env_suggest.sh --nccl-output "$OUT_DIR/nccl/all_reduce_perf.txt" --out "$OUT_DIR/nccl/env_suggest.sh" || true
+else
+  echo "generate_nccl_env_suggest.sh not found; continuing"
+fi
+
+echo "3) Apply NCCL envs to ConfigMap"
+if [[ -x "./scripts/dgx/apply_nccl_config.sh" && -f "$OUT_DIR/nccl/env_suggest.sh" ]]; then
+  ./scripts/dgx/apply_nccl_config.sh --env-file "$OUT_DIR/nccl/env_suggest.sh" --namespace aegis-ml || true
+else
+  echo "apply_nccl_config.sh or env_suggest not available; skipping ConfigMap apply"
+fi
+
+echo "4) Prepare and patch job manifest to set image to $IMAGE"
+TMP_JOB="$OUT_DIR/job_manifest.yaml"
+cp "$JOB_MANIFEST" "$TMP_JOB"
+sed -i "s|__IMAGE_FULL__|$IMAGE|g" "$TMP_JOB" || true
+
+echo "5) Submit multi-node job to validate scaling"
+kubectl create ns aegis-ml --dry-run=client -o yaml | kubectl apply -f - || true
+kubectl apply -f "$TMP_JOB"
+
+echo "Waiting for job pod(s) to be ready (timeout 600s)"
+kubectl -n aegis-ml wait --for=condition=ready pod -l app=deepspeed-dgx-prod-validate --timeout=600s || true
+POD=$(kubectl -n aegis-ml get pod -l app=deepspeed-dgx-prod-validate -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+if [[ -n "$POD" ]]; then
+  echo "Collecting logs from pod $POD to $OUT_DIR/${POD}.log"
+  kubectl -n aegis-ml logs -f "$POD" > "$OUT_DIR/${POD}.log" 2>&1 || true
+else
+  echo "No pod found for job; capture job status"
+  kubectl -n aegis-ml get job -l app=deepspeed-dgx-prod-validate -o wide > "$OUT_DIR/job_status.txt" 2>&1 || true
+fi
+
+echo "6) Verify multi-node logs for NCCL errors"
+if [[ -x "./scripts/dgx/verify_multi_node_scaling_results.sh" ]]; then
+  ./scripts/dgx/verify_multi_node_scaling_results.sh --artifact-dir "$OUT_DIR" || true
+else
+  grep -R -n -E "NCCL|NCCL ERROR|NCCL fatal|MPI_ABORT" "$OUT_DIR" || true
+fi
+
+echo "Multi-node validation complete. Artifacts in $OUT_DIR"
+exit 0
+
diff --git a/scripts/dgx/test_checkpoint_offload_and_pvc.sh b/scripts/dgx/test_checkpoint_offload_and_pvc.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/test_checkpoint_offload_and_pvc.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Check that checkpoint PVC exists & is bound, trigger an immediate offload job (create a Job from the CronJob template),
+# and verify that offloaded objects exist in S3 (optional).
+#
+# Usage:
+#   ./scripts/dgx/test_checkpoint_offload_and_pvc.sh --namespace aegis-ml --s3-bucket my-bucket
+
+set -euo pipefail
+
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+PVC_NAME="${PVC_NAME:-dgx-checkpoints-pvc}"
+CRONJOB_NAME="${CRONJOB_NAME:-dgx-checkpoint-offloader}"
+S3_BUCKET=""
+OUT_DIR="${OUT_DIR:-./artifacts/dgx_offload}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --pvc) PVC_NAME="$2"; shift 2;;
+    --cronjob) CRONJOB_NAME="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "1) Verify PVC $PVC_NAME"
+kubectl -n "$NAMESPACE" get pvc "$PVC_NAME" -o yaml > "$OUT_DIR/pvc.yaml" || { echo "PVC not bound or not found"; exit 3; }
+
+echo "2) Create a one-off Job from CronJob to run offloader immediately"
+JOB_MANIFEST="/tmp/dgx-offloader-job.yaml"
+kubectl -n "$NAMESPACE" get cronjob "$CRONJOB_NAME" -o yaml > "$OUT_DIR/cronjob.yaml" 2>/dev/null || { echo "CronJob $CRONJOB_NAME not found"; exit 4; }
+
+# Extract jobTemplate spec from cronjob and create a Job
+kubectl -n "$NAMESPACE" get cronjob "$CRONJOB_NAME" -o jsonpath='{.spec.jobTemplate.spec}' | jq -r '.' > /tmp/_jobtemplate.json || true
+cat > "$JOB_MANIFEST" <<EOF
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: dgx-offloader-immediate-$(date +%s)
+  namespace: ${NAMESPACE}
+spec:
+$(jq -c '.' /tmp/_jobtemplate.json)
+EOF
+
+kubectl apply -f "$JOB_MANIFEST"
+echo "Waiting for offloader job to start (timeout 300s)"
+kubectl -n "$NAMESPACE" wait --for=condition=complete job -l job-name --timeout=300s || true
+kubectl -n "$NAMESPACE" get jobs -o wide > "$OUT_DIR/jobs.txt" 2>&1 || true
+kubectl -n "$NAMESPACE" get pods -o wide > "$OUT_DIR/pods.txt" 2>&1 || true
+
+echo "Collecting logs for recent offloader pods"
+kubectl -n "$NAMESPACE" logs -l job-name -f > "$OUT_DIR/offloader_logs.txt" 2>&1 || true
+
+if [[ -n "$S3_BUCKET" ]]; then
+  if command -v aws >/dev/null 2>&1; then
+    echo "Checking S3 bucket for offloaded objects"
+    aws s3 ls "s3://${S3_BUCKET}/dgx-checkpoints/" > "$OUT_DIR/s3_list.txt" 2>&1 || echo "No objects found or AWS creds not configured"
+  else
+    echo "AWS CLI not installed; cannot check S3"
+  fi
+else
+  echo "S3 bucket not provided; offload job logs collected to $OUT_DIR/offloader_logs.txt"
+fi
+
+echo "Checkpoint offload test complete. Artifacts in $OUT_DIR"
+exit 0
+
diff --git a/scripts/dgx/exercise_sre_playbooks.sh b/scripts/dgx/exercise_sre_playbooks.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/dgx/exercise_sre_playbooks.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Exercise SRE playbooks safely: cordon a node, drain and then uncordon; run a test alert to verify routing.
+#
+# WARNING: This script performs disruptive actions. Use only in a controlled environment and with approval.
+#
+# Usage:
+#   ./scripts/dgx/exercise_sre_playbooks.sh --node dgx-node-1 --alertmanager-url http://alertmanager:9093
+
+set -euo pipefail
+
+NODE=""
+ALERTMANAGER_URL=""
+GRACE=60
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --node) NODE="$2"; shift 2;;
+    --alertmanager-url) ALERTMANAGER_URL="$2"; shift 2;;
+    --grace) GRACE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$NODE" ]]; then
+  echo "--node is required"; exit 2
+fi
+
+echo "Cordon node $NODE"
+kubectl cordon "$NODE"
+
+echo "Drain node $NODE (grace period ${GRACE}s)"
+kubectl drain "$NODE" --ignore-daemonsets --delete-local-data --grace-period="$GRACE" || echo "Drain reported issues; check pods"
+
+echo "Sleeping 10s to simulate operator steps"
+sleep 10
+
+echo "Uncordon node $NODE"
+kubectl uncordon "$NODE" || true
+
+if [[ -n "$ALERTMANAGER_URL" ]]; then
+  echo "Sending test alert to Alertmanager at $ALERTMANAGER_URL"
+  ALERTMANAGER_URL="$ALERTMANAGER_URL" ./scripts/dgx/send_test_alert.sh --summary "SRE exercise: node ${NODE}" --instance "exercise-${NODE}" || true
+else
+  echo "No Alertmanager URL provided; skipping alert send"
+fi
+
+echo "SRE playbook exercise complete. Verify on-call received notifications and that node recovered."
+exit 0
+
diff --git a/.github/workflows/dgx_full_validation_and_archive.yml b/.github/workflows/dgx_full_validation_and_archive.yml
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/.github/workflows/dgx_full_validation_and_archive.yml
@@ -0,0 +1,240 @@
+name: DGX Full Validation & Archive
+
+on:
+  workflow_dispatch:
+    inputs:
+      image:
+        description: "Image to validate (registry/image:tag)"
+        required: true
+        default: "aegis-deepspeed:h100-cuda12.1-pytorch2.2"
+      s3_bucket:
+        description: "Optional S3 bucket for offload checks"
+        required: false
+
+jobs:
+  dgx-full-validate:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Restore kubeconfig
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          if [ -z "$KUBECONFIG_DGX" ]; then echo "KUBECONFIG_DGX missing"; exit 1; fi
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+
+      - name: Ensure image provenance (SBOM + cosign + IMAGE_MATRIX)
+        env:
+          REGISTRY: ${{ secrets.DGX_IMAGE_REGISTRY }}
+          IMAGE_TAG: ${{ github.event.inputs.image }}
+        run: |
+          mkdir -p artifacts
+          chmod +x scripts/dgx/ensure_image_provenance.sh
+          REGISTRY="$REGISTRY" IMAGE_TAG="${{ github.event.inputs.image }}" ./scripts/dgx/ensure_image_provenance.sh --sbom-dir ./artifacts/sbom --image-matrix docs/dgx/IMAGE_MATRIX.md || true
+
+      - name: Validate GPU operator and drivers
+        run: |
+          chmod +x scripts/dgx/validate_gpu_operator_and_drivers.sh
+          ./scripts/dgx/validate_gpu_operator_and_drivers.sh --expected-autoInstall false --expected-cuda "12.1" --expected-driver "535" --out ./artifacts/driver_check || true
+
+      - name: Apply NCCL tune and run multi-node scaling test
+        env:
+          IMAGE: ${{ secrets.DGX_IMAGE_REGISTRY }}/${{ github.event.inputs.image }}
+        run: |
+          chmod +x scripts/dgx/apply_nccl_and_run_multinode.sh
+          IMAGE="${IMAGE}" ./scripts/dgx/apply_nccl_and_run_multinode.sh --nodes 2 --gpus-per-node 8 --out ./artifacts/multinode || true
+
+      - name: Check checkpoint PVC & offload
+        env:
+          S3_BUCKET: ${{ github.event.inputs.s3_bucket }}
+        run: |
+          chmod +x scripts/dgx/test_checkpoint_offload_and_pvc.sh
+          ./scripts/dgx/test_checkpoint_offload_and_pvc.sh --namespace aegis-ml --s3-bucket "$S3_BUCKET" --out ./artifacts/offload || true
+
+      - name: Exercise SRE playbooks (optional, manual approval recommended)
+        if: ${{ always() }}
+        env:
+          ALERTMANAGER_URL: ${{ secrets.ALERTMANAGER_URL }}
+        run: |
+          echo "This step exercises cordon/drain/uncordon and sends a test alert. Use only in test/staging."
+          chmod +x scripts/dgx/exercise_sre_playbooks.sh
+          # The node param should be provided by secure input in real usage. Here it's a no-op if NODE not set.
+          ./scripts/dgx/exercise_sre_playbooks.sh --node "${{ secrets.DGX_TEST_NODE_NAME }}" --alertmanager-url "$ALERTMANAGER_URL" || true
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-full-validation-artifacts
+          path: artifacts || .
+
+      - name: Enforce success criteria
+        run: |
+          # Basic enforcement: require presence of provenance.json or image digest, driver check logs, multi-node logs and offloader logs
+          if [ ! -f artifacts/dgx_provenance/image_digest.txt ] && ! grep -q "${{ github.event.inputs.image }}" docs/dgx/IMAGE_MATRIX.md; then
+            echo "Provenance (digest/SBOM) missing or IMAGE_MATRIX not updated; failing" ; exit 1
+          fi
+          if [ ! -d artifacts/driver_check ] || [ -z "$(ls -A artifacts/driver_check 2>/dev/null)" ]; then
+            echo "Driver check artifacts missing; failing" ; exit 2
+          fi
+          if [ ! -d artifacts/multinode ] || [ -z "$(ls -A artifacts/multinode 2>/dev/null)" ]; then
+            echo "Multi-node test artifacts missing; failing" ; exit 3
+          fi
+          echo "Basic validation artifacts present. Please review logs for NCCL/IB errors and offload success."
+
diff --git a/docs/dgx/PRODUCTION_ACCEPTANCE.md b/docs/dgx/PRODUCTION_ACCEPTANCE.md
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/docs/dgx/PRODUCTION_ACCEPTANCE.md
@@ -0,0 +1,240 @@
+# DGX Production Acceptance Criteria (Aegis)
+
+This file documents the concrete checks and artifacts required before declaring DGX support "production ready" for Aegis.
+
+1) Image provenance
+ - A digest-pinned image entry exists in docs/dgx/IMAGE_MATRIX.md.
+ - SBOM (syft JSON) present in CI artifacts and referenced in IMAGE_MATRIX.
+ - cosign verification succeeded (key-based or keyless) and recorded in artifacts.
+
+2) Driver compatibility
+ - GPU Operator installed with intended driver strategy (autoInstall true|false).
+ - Node-level checks (nvidia-smi) confirm driver version and CUDA runtime match the validated matrix.
+
+3) NCCL & multi-node scaling
+ - NCCL tuning harness run on DGX nodes and suggested envs applied to dgx-nccl-config ConfigMap.
+ - Multi-node DeepSpeed scaling test ran and no NCCL/IB fatal errors observed in logs.
+
+4) Checkpoint storage & offload
+ - Fast PVC (NVMe/Lustre) bound and accessible by jobs (dgx-checkpoints-pvc).
+ - Checkpoint offloader CronJob executed a one-off job and artifacts are seen in S3 (or validated store).
+
+5) Nightly validation & chaos
+ - Nightly validation workflow runs on self-hosted 'dgx' runners and archives artifacts.
+ - Chaos matrix for DGX profile executed and passes tolerable thresholds; failures are triaged.
+
+6) SRE & alerting exercised
+ - Alertmanager routes DGX alerts to on-call (PagerDuty/Slack) and test alerts were received.
+ - SRE playbooks (cordon/evacuate/offload) executed at least once in staging and documented outcomes.
+
+7) Promotion gate
+ - Promotion gating workflow enforces cosign verification + IMAGE_MATRIX entry + validation artifacts before allowing production promotion.
+
+How to use
+- Run .github/workflows/dgx_full_validation_and_archive.yml (on self-hosted 'dgx' runner) and inspect artifacts.  
+- If all checks pass, use scripts/dgx/promote_image_if_validated.sh to perform final promotion steps.
+
+End of acceptance criteria.
+
