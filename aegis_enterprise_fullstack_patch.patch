*** Begin Patch
*** Add File:charts/aegis-enterprise/Chart.yaml
+apiVersion: v2
+name: aegis-enterprise
+description: "Aegis Enterprise Helm chart (operator, admission, agent, federated, carbon collector, policy)"
+type: application
+version: 0.1.0
+appVersion: "0.1.0"
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/values.yaml
+replicaCount: 2
+
+image:
+  registry: registry.example.com/aegis
+  tag: latest
+
+serviceAccountName: aegis-agent
+
+agent:
+  enabled: true
+  image:
+    repository: registry.example.com/aegis/agent
+    tag: latest
+  resources:
+    requests:
+      cpu: "200m"
+      memory: "256Mi"
+    limits:
+      cpu: "1000m"
+      memory: "1Gi"
+
+operator:
+  enabled: true
+  image:
+    repository: registry.example.com/aegis/modelcontrol-operator
+    tag: latest
+
+admission:
+  enabled: true
+  image:
+    repository: registry.example.com/aegis/aegis-admission
+    tag: latest
+
+federated:
+  enabled: true
+  serverImage:
+    repository: registry.example.com/aegis/fed-coord
+    tag: latest
+
+carbon:
+  enabled: true
+  collectorImage:
+    repository: registry.example.com/aegis/carbon-collector
+    tag: latest
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/templates/deployment-agent.yaml
+{{- if .Values.agent.enabled }}
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-agent
+  labels:
+    app: aegis-agent
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-agent
+  template:
+    metadata:
+      labels:
+        app: aegis-agent
+    spec:
+      serviceAccountName: {{ .Values.serviceAccountName }}
+      containers:
+        - name: agent
+          image: "{{ .Values.agent.image.repository }}:{{ .Values.agent.image.tag }}"
+          resources:
+{{ toYaml .Values.agent.resources | indent 12 }}
+          env:
+            - name: MLFLOW_URL
+              value: ""
+            - name: REDIS_URL
+              value: ""
+          livenessProbe:
+            httpGet:
+              path: /healthz
+              port: 8080
+            initialDelaySeconds: 30
+            periodSeconds: 15
+{{- end }}
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/templates/deployment-operator.yaml
+{{- if .Values.operator.enabled }}
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: modelcontrol-operator
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: modelcontrol-operator
+  template:
+    metadata:
+      labels:
+        app: modelcontrol-operator
+    spec:
+      serviceAccountName: modelcontrol-operator
+      containers:
+        - name: operator
+          image: "{{ .Values.operator.image.repository }}:{{ .Values.operator.image.tag }}"
+          env:
+            - name: ARGO_CMD
+              value: "argo"
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+{{- end }}
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/templates/deployment-admission.yaml
+{{- if .Values.admission.enabled }}
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-admission
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-admission
+  template:
+    metadata:
+      labels:
+        app: aegis-admission
+    spec:
+      serviceAccountName: {{ .Values.serviceAccountName }}
+      containers:
+        - name: admission
+          image: "{{ .Values.admission.image.repository }}:{{ .Values.admission.image.tag }}"
+          ports:
+            - containerPort: 8443
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: aegis-admission-tls
+{{- end }}
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/templates/deployment-federated.yaml
+{{- if .Values.federated.enabled }}
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: fed-coordinator
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: fed-coord
+  template:
+    metadata:
+      labels:
+        app: fed-coord
+    spec:
+      serviceAccountName: {{ .Values.serviceAccountName }}
+      containers:
+        - name: fed-coord
+          image: "{{ .Values.federated.serverImage.repository }}:{{ .Values.federated.serverImage.tag }}"
+          ports:
+            - containerPort: 7000
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+            limits:
+              cpu: "500m"
+              memory: "512Mi"
+{{- end }}
+
*** End Patch
*** Begin Patch
*** Add File:charts/aegis-enterprise/templates/cron-carbon.yaml
+{{- if .Values.carbon.enabled }}
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: carbon-collector
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: carbon
+              image: "{{ .Values.carbon.collectorImage.repository }}:{{ .Values.carbon.collectorImage.tag }}"
+              command: ["python3","/app/carbon/collector.py"]
+          restartPolicy: OnFailure
+{{- end }}
+
*** End Patch
*** Begin Patch
*** Add File:agent/multimodal_agent.py
+#!/usr/bin/env python3
+"""
+Multimodal Agent runtime:
+ - Accepts inputs: text, image path, audio path
+ - Performs simple preprocessing for each modality
+ - Planning: constructs a plan that may call tools (retrieval, search, classifier)
+ - Execution: runs tools and returns structured output
+ - Logs traces for MCP/evidence collection
+"""
+import os, json, time
+from PIL import Image
+import numpy as np
+# librosa is optional for audio; placeholder functions used
+
+from agent.tools.retrieval import simple_search
+from agent.tools.vision import dummy_vision_features
+from agent.tools.audio import dummy_audio_features
+
+TRACE_DIR = os.environ.get("AGENT_TRACE_DIR", "/tmp/agent_traces")
+os.makedirs(TRACE_DIR, exist_ok=True)
+
+class MultimodalAgent:
+    def __init__(self, name="aegis-multimodal"):
+        self.name = name
+        self.tools = {"search": simple_search}
+
+    def preprocess(self, input):
+        features = {}
+        if "text" in input and input["text"]:
+            features["text"] = input["text"]
+        if "image" in input and input["image"]:
+            features["image"] = dummy_vision_features(input["image"])
+        if "audio" in input and input["audio"]:
+            features["audio"] = dummy_audio_features(input["audio"])
+        return features
+
+    def plan(self, features):
+        # very simple logic: if text contains "find" -> search; if image present -> visual-inspect tool
+        text = features.get("text","").lower() if "text" in features else ""
+        if "find" in text:
+            return {"tool": "search", "args": {"query": text}}
+        if "image" in features:
+            return {"tool": "vision_inspect", "args": {"img_features": features["image"]}}
+        return {"tool": "echo", "args": {"text": text}}
+
+    def execute(self, plan):
+        tool = plan.get("tool")
+        if tool == "search":
+            return {"result": self.tools["search"](plan["args"]["query"])}
+        if tool == "vision_inspect":
+            # placeholder: return a mock classification
+            return {"result": {"label": "object", "confidence": 0.87}}
+        if tool == "echo":
+            return {"result": plan["args"]["text"]}
+        return {"error": "unknown tool"}
+
+    def run(self, input):
+        t0 = time.time()
+        features = self.preprocess(input)
+        plan = self.plan(features)
+        result = self.execute(plan)
+        trace = {"input": input, "features": features, "plan": plan, "result": result, "duration": time.time()-t0}
+        fname = os.path.join(TRACE_DIR, f"trace_{int(t0)}.json")
+        with open(fname,"w") as f:
+            json.dump(trace, f, indent=2)
+        return trace
+
+if __name__ == "__main__":
+    a = MultimodalAgent()
+    demo = a.run({"text":"Find recent models for churn", "image": "", "audio": ""})
+    print(json.dumps(demo, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:agent/tools/vision.py
+from PIL import Image
+import numpy as np
+
+def dummy_vision_features(path):
+    # small image summary features as placeholder
+    try:
+        img = Image.open(path).convert("RGB").resize((64,64))
+        arr = np.array(img).mean(axis=(0,1)).tolist()
+        return {"avg_rgb": arr}
+    except Exception:
+        return {"avg_rgb": [0,0,0]}
+
*** End Patch
*** Begin Patch
*** Add File:agent/tools/audio.py
+def dummy_audio_features(path):
+    # placeholder: return a mock audio fingerprint / duration
+    return {"duration_s": 1.23, "fingerprint": "abc123"}
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_aggregation_poc.py
+#!/usr/bin/env python3
+"""
+Federated secure aggregation PoC (simple additive masking approach)
+This demo runs N simulated clients locally; clients mask their updates with random masks,
+send masked updates to coordinator; coordinator sums masked updates and removes sum of masks to recover aggregate.
+This is a simplified PoC for education — replace with vetted libraries for production.
+"""
+import numpy as np
+import requests
+import json
+import threading
+import time
+from http.server import BaseHTTPRequestHandler, HTTPServer
+
+COORD_PORT = 7100
+ROUNDS = {}
+
+class CoordHandler(BaseHTTPRequestHandler):
+    def do_POST(self):
+        length = int(self.headers.get('content-length'))
+        body = json.loads(self.rfile.read(length))
+        path = self.path
+        if path == "/submit":
+            r = body["round"]
+            cid = body["client_id"]
+            share = np.array(body["masked_update"])
+            ROUNDS.setdefault(r,{"masked":[], "masks":[]})
+            ROUNDS[r]["masked"].append(share.tolist())
+            self.send_response(200)
+            self.end_headers()
+            self.wfile.write(b"ok")
+        elif path == "/submit_mask":
+            r = body["round"]
+            mask = np.array(body["mask"])
+            ROUNDS.setdefault(r,{"masked":[], "masks":[]})
+            ROUNDS[r]["masks"].append(mask.tolist())
+            self.send_response(200)
+            self.end_headers()
+            self.wfile.write(b"ok")
+        else:
+            self.send_response(404)
+            self.end_headers()
+
+def run_coord():
+    server = HTTPServer(('0.0.0.0', COORD_PORT), CoordHandler)
+    print("Coordinator running on", COORD_PORT)
+    server.serve_forever()
+
+def client_run(vec, client_id, round_id="r1"):
+    vec = np.array(vec)
+    mask = np.random.normal(size=vec.shape)
+    masked = (vec + mask).tolist()
+    # send masked update
+    requests.post(f"http://localhost:{COORD_PORT}/submit", json={"round": round_id, "client_id": client_id, "masked_update": masked})
+    # send mask separately (this would normally go through secure channel or via key-splitting)
+    requests.post(f"http://localhost:{COORD_PORT}/submit_mask", json={"round": round_id, "client_id": client_id, "mask": mask.tolist()})
+    print("Client submitted", client_id)
+
+def aggregate_round(round_id="r1"):
+    rd = ROUNDS.get(round_id)
+    if not rd:
+        print("No data")
+        return None
+    masked_sum = np.sum(np.array(rd["masked"]), axis=0)
+    mask_sum = np.sum(np.array(rd["masks"]), axis=0)
+    agg = masked_sum - mask_sum
+    return agg.tolist()
+
+if __name__ == "__main__":
+    # start coord
+    t = threading.Thread(target=run_coord, daemon=True)
+    t.start()
+    time.sleep(1)
+    # start clients
+    clients = 5
+    vec = [1.0,2.0,3.0]
+    for i in range(clients):
+        client_run(vec, f"c{i}", round_id="r1")
+    time.sleep(1)
+    agg = aggregate_round("r1")
+    print("Aggregated:", agg)
+
*** End Patch
*** Begin Patch
*** Add File:privacy/dp_opacus_adapter.py
+"""
+Opacus integration example for DP-SGD training (PyTorch).
+This is a helper to illustrate how to enable DP on a PyTorch training loop.
+Install: pip install torch torchvision opacus
+"""
+try:
+    import torch
+    from opacus import PrivacyEngine
+except Exception:
+    torch = None
+
+def enable_dp(model, optimizer, sample_rate, noise_multiplier, max_grad_norm):
+    if torch is None:
+        raise RuntimeError("PyTorch/Opacus not installed")
+    privacy_engine = PrivacyEngine(
+        model,
+        sample_rate=sample_rate,
+        noise_multiplier=noise_multiplier,
+        max_grad_norm=max_grad_norm,
+    )
+    privacy_engine.attach(optimizer)
+    return privacy_engine
+
*** End Patch
*** Begin Patch
*** Add File:secure_inference/secure_wrapper_notes.md
+Secure inference adapters (notes)
+
+- This folder contains placeholders for secure inference adapters (HE, MPC, TFE). For production:
+  - Evaluate TF Encrypted, CrypTen, Microsoft SEAL, or other HE libs.
+  - Ensure hardware and latency budgets are acceptable (HE is compute heavy).
+  - Design API surface for a secure runtime: client sends encrypted input; secure backend computes and returns encrypted result.
+  - Integrate attestation and access control (SPIRE identity + mTLS).
+
*** End Patch
*** Begin Patch
*** Add File:policy/opa/enterprise_catalog/01_mcp.rego
+package aegis.rego
+
+deny[msg] {
+  input.kind == "Deployment"
+  not input.metadata.annotations["aegis/mcp-uri"]
+  msg = "Missing MCP annotation"
+}
+
+deny[msg] {
+  input.metadata.annotations["aegis/export-control"] == "restricted"
+  not input.metadata.annotations["aegis/export-control-approved"]
+  msg = "Export-control restricted and not approved"
+}
+
*** End Patch
*** Begin Patch
*** Add File:auth/keycloak_oidc_config_example.md
+# Keycloak / OIDC example config (samples)
+
+Set these env vars in deployments:
+- KEYCLOAK_URL=https://auth.example.com
+- KEYCLOAK_REALM=aegis
+- KEYCLOAK_AUDIENCE=aegis-api
+
+Use the auth/keycloak_jwt.py helper to validate tokens in APIs and admission servers.
+
*** End Patch
*** Begin Patch
*** Add File:zero_trust/linkerd_install.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Installing Linkerd (staging)"
+curl -sL https://run.linkerd.io/install | sh
+linkerd check --pre
+linkerd install | kubectl apply -f -
+linkerd check
+echo "Linkerd installed. Inject Linkerd into namespaces: kubectl label ns aegis linkerd.io/inject=enabled"
+
*** End Patch
*** Begin Patch
*** Add File:zero_trust/spire_quickstart.md
+# SPIRE Quickstart (staging)
+
+1. Follow SPIRE k8s example manifests: https://github.com/spiffe/spire
+2. Deploy SPIRE server and agents in your cluster; bind agents to K8s nodes.
+3. Register workload selectors to map ServiceAccount->SPIFFE ID for your pods.
+4. Use SPIFFE IDs with Linkerd or Istio for workload attestation.
+
+Production: deploy SPIRE with HA storage and external CA.
+
*** End Patch
*** Begin Patch
*** Add File:carbon/integrate_example.md
+# Carbon integration notes
+
+- The PoC collector (carbon/collector.py) returns synthetic intensity scores.
+- Replace with real APIs:
+  - ElectricityMap (https://api.electricitymap.org/)
+  - Google Cloud Carbon Footprint (GCP)
+  - AWS Customer Carbon Footprint APIs (where available)
+- Store recent intensity in a cache (Redis or S3) and query during placement decisions.
+
*** End Patch
*** Begin Patch
*** Add File:docs/DEPLOY_ENTERPRISE.md
+# Aegis Enterprise — Fullstack deploy guide (staging defaults)
+
+Overview
+- This guide walks through deploying the Aegis enterprise features (multimodal agent, federated PoC, operator, admission webhook, carbon collector, OPA policies, Linkerd quickstart).
+
+Prereqs
+- kubectl + cluster access
+- helm
+- argo CLI (optional)
+- Build and push images to a registry (registry.example.com/aegis) or adjust deployments
+
+Steps (high level)
+1. Create namespace:
+   kubectl create ns aegis
+2. (Optional) Install Linkerd:
+   ./zero_trust/linkerd_install.sh
+3. Apply CRDs (if not present):
+   kubectl apply -f k8s/crd/modelcontrolplane_crd.yaml
+4. Deploy modelcontrol operator:
+   kubectl apply -f controllers/modelcontrol-operator/k8s/operator_rbac.yaml
+   kubectl apply -f controllers/modelcontrol-operator/k8s/deployment.yaml
+5. Deploy admission webhook:
+   kubectl apply -f webhook/deployment.yaml
+   kubectl apply -f webhook/validatingwebhook.yaml
+   ./webhook/bootstrap_certs.sh aegis aegis-admission aegis-mcp-validator
+6. Deploy federated coordinator:
+   kubectl apply -f charts/aegis-enterprise/templates/deployment-federated.yaml
+7. Deploy agent (Helm) or run Argo demo:
+   helm upgrade --install aegis-enterprise charts/aegis-enterprise
+8. Start carbon collector CronJob:
+   kubectl apply -f charts/aegis-enterprise/templates/cron-carbon.yaml
+
+Validation
+- Submit Test ModelControlPlane CR that references MCP URI and watch operator status.
+- Submit a Deployment with aegis/mcp-uri annotation in a namespace with label aegis/enforce-mcp and expect webhook to allow.
+- Run agent demo: argo submit qa/argo/agent_multimodal.yaml --watch
+- Run federated PoC: python3 federated/secure_aggregation_poc.py
+
*** End Patch
*** Begin Patch
*** Add File:Makefile
+# Build and deploy helpers (staging defaults)
+REGISTRY ?= registry.example.com/aegis
+TAG ?= latest
+
+.PHONY: build-agent build-operator build-admission build-fed build-carbon
+build-agent:
+	docker build -t $(REGISTRY)/agent:$(TAG) -f agent/Dockerfile .
+	docker push $(REGISTRY)/agent:$(TAG)
+
+build-operator:
+	docker build -t $(REGISTRY)/modelcontrol-operator:$(TAG) -f controllers/modelcontrol-operator/Dockerfile controllers/modelcontrol-operator/
+	docker push $(REGISTRY)/modelcontrol-operator:$(TAG)
+
+build-admission:
+	docker build -t $(REGISTRY)/aegis-admission:$(TAG) -f webhook/Dockerfile webhook/
+	docker push $(REGISTRY)/aegis-admission:$(TAG)
+
+build-fed:
+	docker build -t $(REGISTRY)/fed-coord:$(TAG) -f federated/Dockerfile federated/
+	docker push $(REGISTRY)/fed-coord:$(TAG)
+
+build-carbon:
+	docker build -t $(REGISTRY)/carbon-collector:$(TAG) -f carbon/Dockerfile carbon/
+	docker push $(REGISTRY)/carbon-collector:$(TAG)
+
+.PHONY: deploy-helm
+deploy-helm:
+	helm upgrade --install aegis-enterprise charts/aegis-enterprise --wait
+
*** End Patch
*** Begin Patch
*** Add File:webhook/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY admission_server.py /app/
+COPY controllers/admission_server.py /app/controllers/
+COPY auth /app/auth
+RUN pip install flask requests python-jose cachetools
+CMD ["python3","/app/admission_server.py"]
+
*** End Patch
*** Begin Patch
*** Add File:controllers/admission_server.py
+#!/usr/bin/env python3
+"""
+Production-minded admission server that:
+ - validates JWTs with Keycloak (if configured)
+ - consults OPA (optional) for policy decisions via REST
+ - enforces MCP annotation, export-control and data-residency rules
+
+This file is used by webhook/app.py (or run directly).
+"""
+from flask import Flask, request, jsonify
+import os, requests, json
+from auth.keycloak_jwt import validate_and_extract_user
+
+app = Flask(__name__)
+
+OPA_URL = os.environ.get("OPA_URL","")
+CLUSTER_REGION = os.environ.get("CLUSTER_REGION","us-west-2")
+
+def opa_query(admission_request):
+    if not OPA_URL:
+        return {"allow": True}
+    r = requests.post(f"{OPA_URL}/v1/data/aegis/rego/allow", json={"input": admission_request}, timeout=5)
+    if r.status_code == 200:
+        return r.json().get("result", {"allow": True})
+    return {"allow": True}
+
+def deny(uid, msg):
+    return {"response": {"uid": uid, "allowed": False, "status": {"message": msg}}}
+
+def allow(uid):
+    return {"response": {"uid": uid, "allowed": True}}
+
+@app.route("/validate", methods=["POST"])
+def validate():
+    req = request.json
+    uid = req["request"]["uid"]
+    obj = req["request"]["object"]
+    metadata = obj.get("metadata",{})
+    annotations = metadata.get("annotations",{})
+    try:
+        auth = req["request"].get("userInfo",{})
+    except Exception:
+        auth = {}
+    # check JWT if header provided in original request (best-effort)
+    token = req["request"].get("object",{}).get("spec",{}).get("token","")
+    if token and os.environ.get("KEYCLOAK_URL"):
+        try:
+            user = validate_and_extract_user(token)
+        except Exception:
+            return jsonify(deny(uid, "Invalid authentication token"))
+    # consult OPA if configured
+    admission_input = {"kind": req["request"]["kind"], "metadata": metadata, "cluster_region": CLUSTER_REGION}
+    opa_res = opa_query(admission_input)
+    if isinstance(opa_res, dict):
+        if not opa_res.get("allow", True):
+            return jsonify(deny(uid, opa_res.get("message","OPA denied")))
+    # enforce MCP annotation presence
+    if not annotations.get("aegis/mcp-uri"):
+        return jsonify(deny(uid, "Missing aegis/mcp-uri annotation"))
+    # export-control check
+    if annotations.get("aegis/export-control") == "restricted" and annotations.get("aegis/export-control-approved","false") != "true":
+        return jsonify(deny(uid, "Export control flag requires approval"))
+    # data residency
+    resid = annotations.get("aegis/data-residency")
+    if resid and CLUSTER_REGION not in resid.split(","):
+        return jsonify(deny(uid, f"Cluster region {CLUSTER_REGION} not allowed by data-residency"))
+    return jsonify(allow(uid))
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8443")), ssl_context=("/tls/tls.crt","/tls/tls.key"))
+
*** End Patch
*** Begin Patch
*** Add File:carbon/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY carbon/collector.py /app/carbon/collector.py
+RUN pip install requests
+CMD ["python3","/app/carbon/collector.py"]
+
*** End Patch
*** Begin Patch
*** Add File:federated/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY federated/secure_aggregation_poc.py /app/
+RUN pip install requests numpy
+CMD ["python3","/app/secure_aggregation_poc.py"]
+
*** End Patch
*** Begin Patch
*** Add File:agent/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY agent /app/agent
+RUN pip install pillow numpy requests
+CMD ["python3","/app/agent/multimodal_agent.py"]
+
*** End Patch
*** Begin Patch
*** Add File:tools/mcp_integration/attach_repro_metadata.sh
+#!/usr/bin/env bash
+# Attach reproducible metadata to an MLflow run and to MCP (example)
+RUN_ID="$1"
+IMAGE="$2"
+SEED="${3:-42}"
+DATASET_SNAPSHOT="${4:-}"
+python3 reproducible/notebook_metadata.py --run-id "$RUN_ID" --image "$IMAGE" --seed "$SEED" --dataset-snapshot "$DATASET_SNAPSHOT"
+
*** End Patch
*** Begin Patch
*** Add File:security/terraform/kms_s3_prod.tf
+provider "aws" {
+  region = var.region
+}
+
+resource "aws_kms_key" "aegis" {
+  description = "Aegis production KMS key (do not edit)"
+  enable_key_rotation = true
+  deletion_window_in_days = 30
+}
+
+resource "aws_s3_bucket" "aegis" {
+  bucket = var.bucket_name
+  force_destroy = false
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        kms_master_key_id = aws_kms_key.aegis.arn
+        sse_algorithm     = "aws:kms"
+      }
+    }
+  }
+  versioning {
+    enabled = true
+  }
+  lifecycle_rule {
+    id      = "expire-logs"
+    enabled = true
+    expiration {
+      days = 3650
+    }
+  }
+}
+
+resource "aws_s3_bucket_public_access_block" "block" {
+  bucket = aws_s3_bucket.aegis.id
+  block_public_acls       = true
+  block_public_policy     = true
+  ignore_public_acls      = true
+  restrict_public_buckets = true
+}
+
+data "aws_iam_policy_document" "deny_insecure" {
+  statement {
+    sid    = "DenyInsecureTransport"
+    effect = "Deny"
+    principals {
+      type        = "AWS"
+      identifiers = ["*"]
+    }
+    actions = ["s3:*"]
+    resources = [
+      aws_s3_bucket.aegis.arn,
+      "${aws_s3_bucket.aegis.arn}/*"
+    ]
+    condition {
+      test     = "Bool"
+      variable = "aws:SecureTransport"
+      values   = ["false"]
+    }
+  }
+}
+
+resource "aws_s3_bucket_policy" "policy" {
+  bucket = aws_s3_bucket.aegis.id
+  policy = data.aws_iam_policy_document.deny_insecure.json
+}
+
+output "s3_bucket" {
+  value = aws_s3_bucket.aegis.id
+}
+output "kms_arn" {
+  value = aws_kms_key.aegis.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:docs/ENTERPRISE_READINESS_CHECKLIST.md
+# Enterprise Readiness Checklist (practical)
+
+1) Security & Zero-Trust
+ - Deploy Linkerd and SPIRE for mTLS and workload attestation.
+ - Ensure Vault dynamic secrets and HSM integration in staging.
+ - Validate admission webhook and OPA policies in enforcement namespaces.
+
+2) Compliance & Audit Evidence
+ - Run evidence_collector_enhanced.py weekly and attach to SOC2 pack.
+ - Demonstrate DSR export & erasure in staging using delete_user_data_prod.py with FORCE_ERASE enabled.
+
+3) Federated & Privacy
+ - Run federated PoC with secure aggregation; review privacy property with privacy team.
+ - Integrate Opacus DP in one training flow and produce privacy accounting.
+
+4) Multimodal & Agent
+ - Execute multimodal agent Argo workflow; capture agent traces into MCP evidence store.
+ - Add tool adapters for retrieval, knowledge base and external tool calls.
+
+5) Carbon & Cost
+ - Run carbon collector and record placement decisions enriched with carbon scores in MCP.
+ - Test scheduler with carbon-aware logic on a small job set.
+
+6) Production Hardening
+ - HA for controllers and admission server, TLS via cert-manager.
+ - Backups and restore runbook for MLflow, DB, S3 artifacts and KMS key rotation.
+
*** End Patch
*** End Patch
