*** Begin Patch
*** Add File:infra/terraform/README.md
+Infra Terraform helpers (examples)
+
+This directory contains example Terraform to provision AWS KMS for cosign, SecretsManager entries, and an S3 lifecycle policy for audit artifacts. It's intentionally minimal and must be reviewed and adapted by operators.
+
+Notes:
+- Do NOT run without validating configs and providing provider credentials.
+- Replace placeholder values (region, prefix, bucket) before applying.
+
*** End Patch
*** Begin Patch
*** Add File:infra/terraform/kms_secrets.tf
+terraform {
+  required_version = ">= 1.0.0"
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+variable "aws_region" {
+  type    = string
+  default = "us-west-2"
+}
+
+variable "secret_prefix" {
+  type    = string
+  default = "aegis"
+}
+
+variable "evidence_bucket" {
+  type    = string
+  default = "aegis-evidence-bucket"
+}
+
+resource "aws_kms_key" "cosign" {
+  description             = "KMS key for cosign signing in Aegis"
+  deletion_window_in_days = 30
+  enable_key_rotation     = true
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/${var.secret_prefix}-cosign"
+  target_key_id = aws_kms_key.cosign.key_id
+}
+
+resource "aws_ssm_parameter" "cosign_kms_arn" {
+  name  = "/${var.secret_prefix}/cosign/kms_key_arn"
+  type  = "String"
+  value = aws_kms_key.cosign.arn
+}
+
+resource "aws_secretsmanager_secret" "rekor" {
+  name = "${var.secret_prefix}/rekor"
+}
+
+resource "aws_secretsmanager_secret_version" "rekor_value" {
+  secret_id     = aws_secretsmanager_secret.rekor.id
+  secret_string = jsonencode({ "rekor_url" = "https://rekor.example.local:3000" })
+}
+
+resource "aws_s3_bucket" "evidence" {
+  bucket = var.evidence_bucket
+  acl    = "private"
+  versioning {
+    enabled = true
+  }
+  lifecycle_rule {
+    id      = "audit-archive"
+    enabled = true
+    prefix  = "audit/"
+    transition {
+      days          = 30
+      storage_class = "STANDARD_IA"
+    }
+    expiration {
+      days = 365
+    }
+  }
+}
+
+output "cosign_kms_arn" {
+  value = aws_kms_key.cosign.arn
+}
+
+output "evidence_bucket" {
+  value = aws_s3_bucket.evidence.bucket
+}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/build_sign_rekor_strict.yml
+name: Build, Sign Images, Publish Rekor & Deploy (strict)
+
+on:
+  workflow_dispatch:
+
+env:
+  REGISTRY: ghcr.io/${{ github.repository_owner }}
+
+jobs:
+  build-sign-deploy:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build images
+        run: |
+          IMAGE_TAG=${{ github.sha }}
+          docker build -t $REGISTRY/aegis-mcp:$IMAGE_TAG mcp/api
+          docker build -t $REGISTRY/aegis-mcp-operator:$IMAGE_TAG mcp/operator
+          docker build -t $REGISTRY/aegis-cosign-admission:$IMAGE_TAG security/cosign-admission
+      - name: Push images
+        run: |
+          IMAGE_TAG=${{ github.sha }}
+          docker push $REGISTRY/aegis-mcp:$IMAGE_TAG
+          docker push $REGISTRY/aegis-mcp-operator:$IMAGE_TAG
+          docker push $REGISTRY/aegis-cosign-admission:$IMAGE_TAG
+      - name: Install cosign & rekor-cli
+        run: |
+          curl -sSL https://github.com/sigstore/cosign/releases/download/v2.0.0/cosign-linux-amd64 -o /usr/local/bin/cosign
+          chmod +x /usr/local/bin/cosign
+          curl -sSL https://github.com/sigstore/rekor/releases/download/v0.12.0/rekor-cli_linux_amd64 -o /usr/local/bin/rekor-cli
+          chmod +x /usr/local/bin/rekor-cli
+      - name: Sign & verify images with cosign (KMS)
+        env:
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }} # awskms://...
+        run: |
+          set -euo pipefail
+          IMAGE_TAG=${{ github.sha }}
+          IMAGES=(
+            "$REGISTRY/aegis-mcp:$IMAGE_TAG"
+            "$REGISTRY/aegis-mcp-operator:$IMAGE_TAG"
+            "$REGISTRY/aegis-cosign-admission:$IMAGE_TAG"
+          )
+          for IMG in "${IMAGES[@]}"; do
+            cosign sign --key "$COSIGN_KEY" "$IMG"
+            cosign verify --key "$COSIGN_KEY" "$IMG"
+          done
+      - name: Ensure Rekor contains entries
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          # best-effort check that rekor accessible; cosign should have recorded entries if configured
+          if [ -z "${REKOR_URL:-}" ]; then
+            echo "REKOR_URL not configured; failing build"
+            exit 1
+          fi
+          echo "Rekor server configured: $REKOR_URL"
+      - name: Annotate images in GitHub (outputs)
+        run: |
+          echo "MCP_IMAGE=$REGISTRY/aegis-mcp:${{ github.sha }}" >> $GITHUB_OUTPUT
+          echo "OP_IMAGE=$REGISTRY/aegis-mcp-operator:${{ github.sha }}" >> $GITHUB_OUTPUT
+          echo "ADMISSION_IMAGE=$REGISTRY/aegis-cosign-admission:${{ github.sha }}" >> $GITHUB_OUTPUT
+
*** End Patch
*** Begin Patch
*** Add File:security/validatingwebhook/create_validatingwebhook.py
+#!/usr/bin/env python3
+"""
+Create/Update a ValidatingWebhookConfiguration with caBundle taken from cert-manager-issued secret.
+
+Usage:
+  python create_validatingwebhook.py --namespace security --secret cosign-admission-tls --service cosign-admission
+
+This script fetches the TLS secret created by cert-manager, extracts ca.crt and base64-encodes it into the caBundle field of a ValidatingWebhookConfiguration.
+It requires kubectl and kubeconfig context to be set (or in-cluster).
+"""
+import argparse
+import base64
+import subprocess
+import json
+import sys
+
+def kubectl(cmd):
+    out = subprocess.check_output(["kubectl"] + cmd, text=True)
+    return out
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--namespace", required=True)
+    p.add_argument("--secret", required=True)
+    p.add_argument("--service", required=True)
+    args = p.parse_args()
+    try:
+        secret = kubectl(["get","secret",args.secret,"-n",args.namespace,"-o","json"])
+        sec = json.loads(secret)
+        ca = sec["data"].get("ca.crt") or sec["data"].get("ca\\.crt")
+        if not ca:
+            print("ca.crt not found in secret")
+            sys.exit(2)
+        # create ValidatingWebhookConfiguration manifest (example)
+        cabundle = ca
+        webhook = {
+            "apiVersion": "admissionregistration.k8s.io/v1",
+            "kind": "ValidatingWebhookConfiguration",
+            "metadata": {"name": "aegis-cosign-admission"},
+            "webhooks": [
+                {
+                    "name": "cosign.admission.aegis",
+                    "clientConfig": {
+                        "service": {"name": args.service, "namespace": args.namespace, "path": "/validate"},
+                        "caBundle": cabundle
+                    },
+                    "rules": [
+                        {"apiGroups":["apps"], "apiVersions":["v1"], "operations":["CREATE","UPDATE"], "resources":["deployments"]},
+                        {"apiGroups":[""], "apiVersions":["v1"], "operations":["CREATE"], "resources":["pods"]}
+                    ],
+                    "failurePolicy": "Fail",
+                    "sideEffects": "None",
+                    "admissionReviewVersions": ["v1"]
+                }
+            ]
+        }
+        print(json.dumps(webhook, indent=2))
+        # apply
+        subprocess.run(["kubectl","apply","-f","-"], input=json.dumps(webhook), text=True, check=True)
+        print("ValidatingWebhookConfiguration applied/updated")
+    except subprocess.CalledProcessError as e:
+        print("kubectl error:", e.output)
+        sys.exit(1)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:registry/postgres/README.md
+This directory contains Helm values and manifest examples to migrate the Model Card Registry and Model Context Registry from SQLite -> Postgres (managed or chart).
+
+Operators: choose a managed Postgres or deploy bitnami/postgresql via Helm with provided values.yaml.
+
*** End Patch
*** Begin Patch
*** Add File:registry/postgres/values.yaml
+global:
+  postgresql:
+    postgresqlDatabase: aegis_registry
+    postgresqlUsername: aegis
+    postgresqlPassword: "REPLACE_POSTGRES_PASSWORD"
+persistence:
+  enabled: true
+  size: 10Gi
+
*** End Patch
*** Begin Patch
*** Add File:registry/postgres/modelcard-deployment-postgres.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: model-card-registry
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: model-card-registry
+  template:
+    metadata:
+      labels:
+        app: model-card-registry
+    spec:
+      containers:
+        - name: modelcard-registry
+          image: ghcr.io/yourorg/aegis-modelcard-registry:latest
+          env:
+            - name: DATABASE_URL
+              valueFrom:
+                secretKeyRef:
+                  name: registry-db-secret
+                  key: DATABASE_URL
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-app-secrets
+                  key: EVIDENCE_BUCKET
+          ports:
+            - containerPort: 8095
+
*** End Patch
*** Begin Patch
*** Add File:registry/migrate/migrate_sqlite_to_postgres.py
+#!/usr/bin/env python3
+"""
+Migrate registry SQLite DB entries to Postgres.
+Usage:
+  Set DATABASE_URL env var for destination Postgres, and run:
+    python migrate_sqlite_to_postgres.py --sqlite /path/to/model_cards.db
+"""
+import argparse
+import sqlite3
+import os
+import psycopg2
+import json
+
+def migrate(sqlite_path, pg_url):
+    sconn = sqlite3.connect(sqlite_path)
+    scur = sconn.cursor()
+    rows = scur.execute("SELECT id,model_name,version,s3_key,created_at,signed,cosign_key,rekor_entry,metadata FROM model_cards").fetchall()
+    sconn.close()
+    pconn = psycopg2.connect(pg_url)
+    pcur = pconn.cursor()
+    pcur.execute("""
+    CREATE TABLE IF NOT EXISTS model_cards (
+      id SERIAL PRIMARY KEY,
+      model_name TEXT,
+      version TEXT,
+      s3_key TEXT,
+      created_at TIMESTAMP,
+      signed BOOLEAN,
+      cosign_key TEXT,
+      rekor_entry TEXT,
+      metadata JSONB
+    )
+    """)
+    pconn.commit()
+    for r in rows:
+        pcur.execute("INSERT INTO model_cards (model_name,version,s3_key,created_at,signed,cosign_key,rekor_entry,metadata) VALUES(%s,%s,%s,%s,%s,%s,%s,%s)",
+                     (r[1], r[2], r[3], r[4], bool(r[5]), r[6], r[7], json.loads(r[8] or "{}")))
+    pconn.commit()
+    pcur.close()
+    pconn.close()
+    print("Migration complete, inserted", len(rows), "rows")
+
+if __name__ == "__main__":
+    import os
+    p = argparse.ArgumentParser()
+    p.add_argument("--sqlite", required=True)
+    args = p.parse_args()
+    pg = os.environ.get("DATABASE_URL")
+    if not pg:
+        print("DATABASE_URL not set")
+        exit(2)
+    migrate(args.sqlite, pg)
+
*** End Patch
*** Begin Patch
*** Add File:mcp_context/middleware/redaction_tests.py
+import unittest
+from mcp_context.middleware.redaction import redact_text
+
+class RedactionTests(unittest.TestCase):
+    def test_email_redaction(self):
+        txt = "contact admin@example.com for info"
+        r = redact_text(txt)
+        self.assertNotIn("admin@example.com", r)
+        self.assertIn("[REDACTED]", r)
+
+    def test_phone_redaction(self):
+        txt = "call +1 (415) 555-1234 now"
+        r = redact_text(txt)
+        self.assertNotIn("415", r)
+        self.assertIn("[REDACTED]", r)
+
+if __name__ == '__main__':
+    unittest.main()
+
*** End Patch
*** Begin Patch
*** Add File:mcp/verifier/nli_verifier.py
+#!/usr/bin/env python3
+"""
+Simple NLI-based verifier using Hugging Face transformers.
+This provides stronger verification than naive token overlap for promotion gates.
+
+Note: Operators must ensure the selected model is appropriate and sized for environment.
+"""
+from transformers import pipeline
+import os
+
+MODEL = os.environ.get("NLI_MODEL", "roberta-large-mnli")
+
+class NLIVerifier:
+    def __init__(self, model_name=MODEL):
+        # Using "roberta-large-mnli" for entailment (supports labels: entailment/neutral/contradiction)
+        self.classifier = pipeline("text-classification", model=model_name, return_all_scores=True)
+
+    def support_score(self, claim, doc_text):
+        # Compose hypothesis as claim and premise as doc_text
+        # For MNLI, label order varies; we interpret highest score for "ENTAILMENT" as support
+        inputs = claim
+        res = self.classifier(f"{doc_text} </s> {claim}")
+        # res is list of label-score entries; find entailment
+        # This is a pragmatic mapping — adapt per model output
+        best = 0.0
+        for r in res[0]:
+            label = r["label"].lower()
+            score = r["score"]
+            if "entailment" in label:
+                best = max(best, score)
+        return best
+
+    def supported(self, claim, docs, threshold=0.6):
+        scores = []
+        for d in docs:
+            scores.append(self.support_score(claim, d))
+        best = max(scores) if scores else 0.0
+        return best >= threshold, best
+
+if __name__ == "__main__":
+    v = NLIVerifier()
+    ok, score = v.supported("The sky is blue", ["The sky appears blue due to Rayleigh scattering."])
+    print("supported:", ok, "score:", score)
+
*** End Patch
*** Begin Patch
*** Add File:infra/monitoring/alerts_prometheus.yaml
+groups:
+- name: aegis-slo.rules
+  rules:
+  - alert: AegisServiceDown
+    expr: up{job=~"aegis-.*"} == 0
+    for: 2m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Aegis service down"
+      description: "One or more Aegis services are down ({{ $labels.job }})"
+
+  - alert: AegisHighErrorRate
+    expr: increase(http_requests_total{job=~"aegis-.*",status=~"5.."}[5m]) / increase(http_requests_total{job=~"aegis-.*"}[5m]) > 0.05
+    for: 5m
+    labels:
+      severity: warning
+    annotations:
+      summary: "High 5xx error rate"
+      description: "High rate of server errors in Aegis services."
+
+  - alert: CarbonBudgetExceeded
+    expr: aegis_carbon_reserved_kg > aegis_carbon_budget_kg
+    for: 1m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Carbon budget exceeded"
+      description: "Tenant carbon budget exceeded; admissions may be blocked."
+
*** End Patch
*** Begin Patch
*** Add File:infra/kms/rotate_cosign_kms.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate the cosign KMS key: create new key, update secret in SecretsManager, and output instructions to update CI.
+#
+ALIAS=${1:-"alias/aegis-cosign-rotate-$(date +%s)"}
+REGION=${2:-"us-west-2"}
+
+echo "Creating new KMS key with alias ${ALIAS} in ${REGION}..."
+KEY_ID=$(aws kms create-key --region $REGION --query KeyMetadata.KeyId --output text)
+aws kms create-alias --region $REGION --alias-name "${ALIAS}" --target-key-id "${KEY_ID}"
+KEY_ARN=$(aws kms describe-key --key-id "${KEY_ID}" --region $REGION --query KeyMetadata.Arn --output text)
+echo "New KMS ARN: ${KEY_ARN}"
+echo "Operator: update SecretsManager entry at path REPLACE_SECRET_PREFIX/cosign/kms_key_arn with ${KEY_ARN}"
+echo "Then update CI secret COSIGN_KEY to use awskms://${KEY_ARN}"
+echo "After rotation, sign a small test artifact and verify Rekor entries for continuity."
+
*** End Patch
*** Begin Patch
*** Add File:dsar/dsar_tool.py
+#!/usr/bin/env python3
+"""
+DSAR helper: locate run contexts referencing a subject hash and redact or delete objects in S3 & registry.
+
+Usage:
+  python dsar_tool.py --subject-hash <hash> --registry-url http://... --s3-bucket <bucket> --action redact|delete
+"""
+import argparse
+import requests
+import boto3
+import json
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--subject-hash", required=True)
+    p.add_argument("--registry-url", required=True)
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--action", choices=["redact","delete"], default="redact")
+    args = p.parse_args()
+    s3 = boto3.client("s3")
+    # search contexts for metadata containing subject hash (requires registry to expose metadata)
+    res = requests.get(f"{args.registry_url}/search?limit=1000")
+    rows = res.json()
+    affected = []
+    for r in rows:
+        rec = requests.get(f"{args.registry_url}/contexts/{r['run_id']}").json()
+        ctx_key = rec["s3_key"].replace("s3://","").split("/",1)[1]
+        # download and inspect
+        obj = s3.get_object(Bucket=args.s3_bucket, Key=ctx_key)
+        ctx = json.loads(obj["Body"].read())
+        if args.subject_hash in json.dumps(ctx):
+            affected.append((r["run_id"], ctx_key))
+            if args.action == "redact":
+                # redact by replacing context with redacted marker and retain minimal metadata
+                ctx["redacted_for_dsar"] = True
+                ctx["redaction_reason"] = "DSAR requested"
+                tmp = "/tmp/redacted.json"
+                with open(tmp,"w") as f:
+                    json.dump(ctx, f)
+                s3.upload_file(tmp, args.s3_bucket, ctx_key)
+            else:
+                s3.delete_object(Bucket=args.s3_bucket, Key=ctx_key)
+    print("Affected contexts:", affected)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:infra/scale/adversarial_tests.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Running adversarial test harness skeleton..."
+echo "Operator: replace with concrete adversarial prompt sets and evaluation harness."
+PYTHON=$(which python || echo "")
+if [ -z "$PYTHON" ]; then
+  echo "python required for adversarial tests"
+  exit 0
+fi
+python - <<PY
+print("Adversarial harness placeholder — test a set of prompts against the verifier and log results.")
+PY
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/governance/process_and_roles.md
+# Governance: Roles, Approval SLAs & Incident Response (Aegis)
+
+Define roles:
+- Model Owner: responsible for model training, card, and remediation.
+- Data Steward: responsible for dataset provenance, PII handling and DSAR responses.
+- Security Lead: responsible for keys, Gatekeeper policies and admission webhooks.
+- Approver: authorized person(s) to approve promotions for high-risk models.
+- Auditor: verifies evidence and signs off on audit packages.
+
+Approval SLAs:
+- Routine models: approval within 24 hours.
+- High-risk models (contains PII or high-impact): approval within 4 hours.
+
+Incident response:
+- On high-severity: page Security Lead and Model Owner, gather run_context S3 pointer, model_card, Rekor entries, and produce signed audit bundle.
+- For DSAR: Data Steward executes dsar_tool to redact or delete contexts within SLA (30 days per policy).
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_end_to_end_smoke.py
+import os
+import requests
+
+def test_smoke_mcp_promote():
+    # Basic smoke test to ensure MCP promote endpoint is reachable in staging (adjust URL)
+    url = os.environ.get("MCP_URL", "http://localhost:8089")
+    r = requests.get(f"{url}/models")
+    assert r.status_code in (200,404)
+
*** End Patch
*** Begin Patch
*** Update File:mcp/api/mcp_service.py
@@
 import mlflow
 import requests
 import kubernetes
 from kubernetes import client, config
 import os
+from urllib.parse import urljoin
@@
 def verify_signature(artifact_uri: str):
@@
-    try:
-        cmd = f"{COSIGN_VERIFY_CMD} {artifact_uri}"
-        proc = subprocess.run(cmd.split(), capture_output=True, text=True, check=False)
-        return {"returncode": proc.returncode, "stdout": proc.stdout, "stderr": proc.stderr}
-    except Exception as e:
-        raise HTTPException(status_code=500, detail=str(e))
+    try:
+        cmd = f"{COSIGN_VERIFY_CMD} {artifact_uri}"
+        proc = subprocess.run(cmd.split(), capture_output=True, text=True, check=False)
+        res = {"returncode": proc.returncode, "stdout": proc.stdout, "stderr": proc.stderr}
+        # If Rekor URL configured, verify Rekor entry exists for artifact
+        REKOR = os.environ.get("REKOR_URL")
+        if REKOR and proc.returncode == 0:
+            # best-effort: call Rekor search (operators should use canonical rekor-cli)
+            try:
+                r = requests.get(urljoin(REKOR, f"/api/v1/index/retrieve?hash={artifact_uri}"), timeout=5)
+                res["rekor_ok"] = (r.status_code == 200)
+            except Exception:
+                res["rekor_ok"] = False
+        return res
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=str(e))
*** End Patch
*** Begin Patch
*** Add File:infra/s3/lifecycle_policy.json
+{
+  "Rules": [
+    {
+      "ID": "AuditTransition",
+      "Prefix": "audit/",
+      "Status": "Enabled",
+      "Transitions": [
+        {
+          "Days": 30,
+          "StorageClass": "STANDARD_IA"
+        }
+      ],
+      "Expiration": {
+        "Days": 365
+      }
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/hardening/checklist.md
+# Security Hardening Checklist (Aegis)
+
+1. Ensure all images are signed in CI and Rekor entries are present.  
+2. Gatekeeper constraints applied in production must be deny (fail-closed). Start with dry-run in staging, then cutover.  
+3. Admission webhook TLS certs must be provisioned via cert-manager and valid caBundle applied.  
+4. Keys must be rotated periodically and secrets stored in SecretsManager with least-privilege access.  
+5. Ensure S3 audit prefix uses SSE-KMS and lifecycle policy (infra/s3/lifecycle_policy.json) is applied.  
+6. Verify DSAR tooling and role assignments for Data Stewards.  
+
*** End Patch
*** End Patch
