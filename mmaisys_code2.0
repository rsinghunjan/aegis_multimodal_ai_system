import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from PIL import Image
import requests
import os
import argparse
import re

# Try to import spaCy for NER
try:
    import spacy
    NER = spacy.load("en_core_web_sm")
except ImportError:
    NER = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultimodalAISystem:
    DEFAULT_MODEL_IDS = {
        "core_llm": "gpt2",  # Local-friendly default
        "vision_model": "Salesforce/blip-image-captioning-base",
        "image_gen": "CompVis/stable-diffusion-v1-4"
    }
    LARGE_MODEL_IDS = {
        "core_llm": "deepseek-ai/deepseek-llm-67b",
        "vision_model": "llava-hf/llava-1.5-7b-hf",
        "image_gen": "stabilityai/stable-diffusion-xl-base-1.0"
    }

    WEATHER_API_KEY = os.environ.get("WEATHER_API_KEY")  # Set as env var

    def __init__(self, model_ids=None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.models = {}
        self.tokenizer = None
        self.model_ids = model_ids if model_ids else self.DEFAULT_MODEL_IDS

    def load_core_llm(self):
        try:
            if "core_llm" not in self.models:
                logger.info(f"Loading core LLM model: {self.model_ids['core_llm']} ...")
                self.models["core_llm"] = AutoModelForCausalLM.from_pretrained(
                    self.model_ids["core_llm"], torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                ).to(self.device)
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_ids["core_llm"])
        except Exception as e:
            logger.error(f"Failed to load core LLM: {e}")
            print("Model download instructions: see top of the file docstring.")
            raise

    def load_vision_model(self):
        try:
            if "vision_model" not in self.models:
                logger.info(f"Loading vision model: {self.model_ids['vision_model']} ...")
                self.models["vision_model"] = pipeline(
                    "image-to-text", model=self.model_ids["vision_model"], device=0 if self.device == "cuda" else -1
                )
        except Exception as e:
            logger.error(f"Failed to load vision model: {e}")
            print("Model download instructions: see top of the file docstring.")
            raise

    def load_image_generator(self):
        try:
            if "image_gen" not in self.models:
                logger.info(f"Loading image generator: {self.model_ids['image_gen']} ...")
                self.models["image_gen"] = pipeline(
                    "text-to-image", model=self.model_ids["image_gen"], device=0 if self.device == "cuda" else -1
                )
        except Exception as e:
            logger.error(f"Failed to load image generator: {e}")
            print("Model download instructions: see top of the file docstring.")
            raise

    def handle_text_query(self, prompt):
        self.load_core_llm()
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.models["core_llm"].generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response[len(prompt):].strip()
            return answer
        except Exception as e:
            logger.error(f"Text query failed: {e}")
            return "Sorry, I could not process your request."

    def handle_image_query(self, image_path):
        self.load_vision_model()
        try:
            image = Image.open(image_path)
            result = self.models["vision_model"](image)
            return result[0]['generated_text'] if result else "No description generated."
        except Exception as e:
            logger.error(f"Image query failed: {e}")
            return "Sorry, I could not process your image."

    def generate_image(self, prompt):
        self.load_image_generator()
        try:
            result = self.models["image_gen"](prompt)
            if isinstance(result, list) and result:
                image = result[0]
                output_path = "generated_image.png"
                image.save(output_path)
                return f"Image generated and saved to {output_path}."
            elif isinstance(result, dict) and "images" in result:
                image = result["images"][0]
                output_path = "generated_image.png"
                image.save(output_path)
                return f"Image generated and saved to {output_path}."
            return "Image generation failed."
        except Exception as e:
            logger.error(f"Image generation failed: {e}")
            return "Sorry, I could not generate the image."

    def access_real_time_data(self, query):
        city = self.extract_city_from_query(query)
        if not city:
            return "Please specify a city for the weather information."
        if not self.WEATHER_API_KEY:
            return "Weather API key is not set."
        try:
            response = requests.get(
                f"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={self.WEATHER_API_KEY}&units=metric"
            )
            data = response.json()
            if response.status_code != 200 or 'main' not in data:
                return "Failed to retrieve weather data."
            return f"Current temperature in {city} is {data['main']['temp']}Â°C."
        except Exception as e:
            logger.error(f"Weather API failed: {e}")
            return "Failed to access real-time data."

    @staticmethod
    def extract_city_from_query(query):
        # Try spaCy NER if available
        if NER:
            doc = NER(query)
            cities = [ent.text for ent in doc.ents if ent.label_ == "GPE"]
            if cities:
                return cities[-1]
        # Fallback regex for "in <city>"
        match = re.search(r'in\s+([A-Za-z ]+)', query, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return None

    def generate_safe_response(self, query):
        q_lower = query.lower()
        if "generate an image" in q_lower:
            prompt = query.replace("generate an image", "", 1).strip()
            return self.generate_image(prompt)
        elif "describe image" in q_lower:
            image_path = query.replace("describe image", "", 1).strip()
            return self.handle_image_query(image_path)
        elif "weather" in q_lower:
            return self.access_real_time_data(query)
        else:
            return self.handle_text_query(query)

def parse_args():
    parser = argparse.ArgumentParser(description="Multimodal AI System with model size options")
    parser.add_argument("--core_llm", type=str, default=MultimodalAISystem.DEFAULT_MODEL_IDS['core_llm'],
                        help="Core LLM model (e.g., gpt2, deepseek-ai/deepseek-llm-67b)")
    parser.add_argument("--vision_model", type=str, default=MultimodalAISystem.DEFAULT_MODEL_IDS['vision_model'],
                        help="Vision model (e.g., Salesforce/blip-image-captioning-base, llava-hf/llava-1.5-7b-hf)")
    parser.add_argument("--image_gen", type=str, default=MultimodalAISystem.DEFAULT_MODEL_IDS['image_gen'],
                        help="Image generation model (e.g., CompVis/stable-diffusion-v1-4, stabilityai/stable-diffusion-xl-base-1.0)")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    model_ids = {
        "core_llm": args.core_llm,
        "vision_model": args.vision_model,
        "image_gen": args.image_gen
    }
    ai_system = MultimodalAISystem(model_ids=model_ids)
    print("Multimodal AI System Initialized. Type 'quit' to exit.")
    print("Model IDs in use:")
    for k, v in model_ids.items():
        print(f"  {k}: {v}")
    try:
        while True:
            user_query = input("\nUser: ")
            if user_query.lower() == 'quit':
                print("Goodbye!")
                break
            response = ai_system.generate_safe_response(user_query)
            print(f"Assistant: {response}")
    except KeyboardInterrupt:
        print("\nExiting gracefully.")
