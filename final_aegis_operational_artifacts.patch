*** Begin Patch
*** Add File:issues/FINAL_PRIORITIZED_ISSUES_BACKLOG.md
+# Final Prioritized Issues Backlog — Ready to create GitHub Issues
+
+Use this as the canonical backlog to hand to your teams. Each item includes title, description, acceptance criteria, estimate (days), priority, and suggested owner.
+
+- Title: Confirm telemetry collection & baseline health
+  Description: Deploy node-power-exporter DaemonSet to representative node types. Run telemetry/telemetry_healthcheck.py for 7–14 days and ensure coverage for each node_type and workload class. Ensure Prometheus targets are healthy.
+  Acceptance: S3 health JSON present at s3://<EVIDENCE_BUCKET>/telemetry/health/, Prom targets show data for representative nodes.
+  Estimate: 2–5 days
+  Priority: high
+  Owner: @platform
+
+- Title: Iterative calibration & holdout validation (staging)
+  Description: Run ci/telemetry_calibration_validate.yml workflow in staging. If median_relative_error > 0.10, run aegis_iterative_calibration.py until thresholds met. Produce validation reports in S3 under calibration/validation_reports/ and draft profiles under calibration/drafts/.
+  Acceptance: median_relative_error ≤ 0.10 on holdout reconciliations across representative workloads for a sustained window (suggest 14 days).
+  Estimate: 3–14 days (iterative)
+  Priority: high
+  Owner: @ml-platform
+
+- Title: Enforcement: stage scheduler extender + Argo admission enforcer and reach ≥95% coverage
+  Description: Deploy extender and enforcer to staging using manifests in enforcement/, ensure webhook certs via cert-manager corporate ClusterIssuer. Run enforcement/test_harness/enforcement_coverage_harness.py and iterate policy until enforcement_coverage ≥ 0.95. Produce coverage reports in s3://<EVIDENCE_BUCKET>/enforcement/coverage_reports/.
+  Acceptance: enforcement coverage ≥ 0.95 for test harness runs and representative job traffic.
+  Estimate: 7–14 days
+  Priority: high
+  Owner: @k8s-sre
+
+- Title: Bulk ingestion: parquet → manifest → dev DW load & MERGE validation
+  Description: Run etl/s3_parquet_pipeline_v3.py for a sample slice, generate manifest, run loader (in-cluster job or wrapper) into dev DW. Validate redshift_merge_from_staging.sql or equivalent MERGE/upsert is idempotent and performance acceptable. Tune parquet sizes/partitions accordingly.
+  Acceptance: idempotent loads (re-run does not duplicate), ingestion lag < 24h for daily data, acceptable query latency for marts.
+  Estimate: 5–10 days
+  Priority: high
+  Owner: @data-eng
+
+- Title: Provision DW & IAM for production COPY
+  Description: Use terraform/redshift/* to provision DW (or configure cloud DW) and bind least-privilege IAM role for COPY. Update CI and loader job envs with role ARN.
+  Acceptance: role ARN provisioned, role limited to specific S3 prefix, loader can COPY in dev with role.
+  Estimate: 3–7 days
+  Priority: high
+  Owner: @cloud-ops / @data-eng
+
+- Title: Security finalization: corporate PKI, KMS cosign, RBAC audit
+  Description: Create cert-manager `corporate-ca-issuer` secret, provision KMS key with restrictive principals (security/provision_kms.tf), rotate cosign KMS key in staging, update CI secrets, run security/rbac_audit.sh and reduce privileges.
+  Acceptance: ClusterIssuer active, COSIGN_KMS_KEY_ARN secret updated, no SAs with cluster-admin unless documented, cosign rotation recorded in Rekor (or equivalent).
+  Estimate: 7–14 days (depends on approvals)
+  Priority: high
+  Owner: @security
+
+- Title: Operationalize DR & resume drills, incident playbooks
+  Description: Run operational/dr_resume_test.py in staging, execute ETL failure and recovery drills, exercise runbooks and document playbook changes. Archive drill artifacts to evidence S3.
+  Acceptance: Drill executed, findings recorded in S3, runbooks updated, at least one remediation item closed.
+  Estimate: 3–5 days
+  Priority: medium
+  Owner: @sre
+
+- Title: BI SSO & RLS pilot
+  Description: Configure Superset OIDC using security/superset_oidc_rbac_snippet.py guidance and enable DB pre-query for app.team. Pilot RLS with two teams to validate dataset isolation.
+  Acceptance: SSO enabled, RLS enforced for pilot teams, no cross-team data visible.
+  Estimate: 3–7 days
+  Priority: medium
+  Owner: @analytics
+
+- Title: FinOps: ingest CUR & build cost allocation mart
+  Description: Ingest AWS Cost & Usage Report (CUR) to S3, create dbt models to allocate cost_by_team, integrate alerts for budget overrun.
+  Acceptance: Cost allocation mart producing daily allocated_cost_usd, alert on budget exceed.
+  Estimate: 5–10 days
+  Priority: medium
+  Owner: @finance-data
+
+---
+Notes:
+- Replace <EVIDENCE_BUCKET> placeholders with your real bucket.
+- Create these as Issues and assign owners; link to files in repo (runbooks, manifests, scripts).
+
*** End Patch
*** Begin Patch
*** Add File:prs/PR_CREATION_INSTRUCTIONS_AND_SCRIPT.md
+# PR creation instructions & small helper
+
+This file contains exact steps to open the PR that wires enforcement harness, dev DW loader workflow, backlog and operator handoff into your repo, plus a helper script (uses `gh` CLI).
+
+Steps to open PR:
+1. Ensure you have write access to the repository and `gh` CLI installed and authenticated.
+2. Update any placeholders (EVIDENCE_BUCKET, KUBECONFIG secrets) in the branch if required.
+3. Run the helper script to create a branch, push, and open a PR.
+
+Helper: create_pr.sh
+--------------------------------
+#!/usr/bin/env bash
+set -euo pipefail
+if [ $# -lt 2 ]; then
+  echo "Usage: $0 <owner/repo> <base-branch>"
+  exit 2
+fi
+REPO=$1
+BASE=$2
+BRANCH="aegis/production-finalize-$(date -u +%Y%m%dT%H%M%SZ)"
+git checkout -b "$BRANCH"
+git add .
+git commit -m "Aegis: finalize production readiness artifacts (telemetry, enforcement, loader, runbooks)"
+git push -u origin "$BRANCH"
+if command -v gh >/dev/null 2>&1; then
+  gh pr create --repo "$REPO" --base "$BASE" --head "$BRANCH" --title "Aegis: finalize production readiness" --body "This PR adds production readiness workflows, enforcement harness, DW loader job, prioritized backlog, operator handoff, and audit package. Deploy to staging and validate before promoting."
+else
+  echo "gh CLI not found. Branch pushed: origin/$BRANCH. Create PR manually at https://github.com/$REPO/compare/$BRANCH?expand=1"
+fi
+--------------------------------
+
+If you'd like, provide the repository name in owner/repo form and the base branch (e.g., main) and I will prepare to open the PR for you — I will not attempt to open it until you confirm and provide the repo name.
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/OPERATOR_HANDOFF_FINAL.md
+# Operator Handoff — Exact minimal steps to run and validate Aegis production readiness
+
+Purpose: a succinct checklist an operator or engineer can follow to run telemetry checks, enforcement harness, parquet pipeline, dev DW loader, calibration, and final promotion steps.
+
+Prereqs:
+- kubectl configured for staging and production contexts
+- AWS CLI with access to EVIDENCE_BUCKET
+- GitHub Actions secrets configured: EVIDENCE_BUCKET, KUBECONFIG_STAGING, KUBECONFIG_PROD, COSIGN_KMS_KEY_ARN, REDSHIFT_IAM_ROLE_ARN, REDSHIFT_DSN
+- `cosign`, `kubectl`, `aws`, `psql` (or DW client) installed in runner
+
+1) Telemetry healthcheck (staging)
+ - export EVIDENCE_BUCKET=<your-bucket>
+ - python3 telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 14
+ - Confirm S3 report: aws s3 ls s3://$EVIDENCE_BUCKET/telemetry/health/
+
+2) Enforcement harness (staging)
+ - Ensure extender+enforcer deployed in staging and MutatingWebhookConfiguration caBundle valid.
+ - python3 enforcement/test_harness/enforcement_coverage_harness.py
+ - Check S3: aws s3 ls s3://$EVIDENCE_BUCKET/enforcement/coverage_reports/
+ - If coverage < 0.95: inspect logs:
+   kubectl -n aegis logs -l app=aegis-argo-enforcer
+   kubectl -n kube-system logs -l app=aegis-scheduler-extender
+
+3) Parquet pipeline & manifest
+ - python3 etl/s3_parquet_pipeline_v3.py
+ - python3 etl/copy_manifest_generator.py
+ - Confirm manifest exists and lists parquet files (manifest printed to /tmp or uploaded to S3)
+
+4) Dev DW loader (run in-cluster)
+ - Update etl/redshift_copy_job.yaml envs with REDSHIFT_IAM_ROLE_ARN and EVIDENCE_BUCKET
+ - kubectl --kubeconfig=<staging-kubeconfig> -n aegis apply -f etl/redshift_copy_job.yaml
+ - kubectl --kubeconfig=<staging-kubeconfig> -n aegis wait --for=condition=complete job/redshift-copy-loader --timeout=1800s
+ - Fetch logs: kubectl --kubeconfig=<staging-kubeconfig> -n aegis logs job/redshift-copy-loader
+ - Validate in DW (select counts, dedupe)
+
+5) Run dbt (dev)
+ - cd dbt
+ - dbt deps && dbt run && dbt test
+ - If tests fail, inspect failing models and fix upstream ETL or expectations.
+
+6) Run Great Expectations validations
+ - python3 data_quality/run_expectations.py
+ - Check GE artifacts in s3://$EVIDENCE_BUCKET/data-quality/validations/
+
+7) Iterative calibration & validation
+ - python3 calibration/aegis_iterative_calibration.py
+ - Confirm validation report uploaded at s3://$EVIDENCE_BUCKET/calibration/validation_reports/
+ - If median_relative_error > 0.10, iterate calibration and collect more telemetry.
+
+8) Promotion (manual & auditable)
+ - Retrieve approved power_profiles.yaml from S3
+   aws s3 cp s3://$EVIDENCE_BUCKET/calibration/approved/power_profiles.yaml ./power_profiles.yaml
+ - Sign with cosign:
+   cosign sign --key "awskms://$COSIGN_KMS_KEY_ARN" ./power_profiles.yaml
+ - Apply to production (operator must perform):
+   kubectl --kubeconfig=<prod-kubeconfig> -n aegis create configmap aegis-power-profiles --from-file=power_profiles.yaml=./power_profiles.yaml --dry-run=client -o yaml | kubectl apply -f -
+ - Upload promotion note to S3 and link Rekor entry if applicable.
+
+9) Post-promotion checks
+ - Monitor Prometheus SLOs for 24–72 hours: median estimator error, enforcement coverage, ETL lag.
+ - Run enforcement_coverage_harness once in prod pilot namespace to confirm behavior.
+
+Contacts:
+- ML Platform: ml-platform@example.com
+- SRE: oncall@sre.example.com
+- Data Eng: data-eng@example.com
+- Security: security@example.com
+
*** End Patch
*** Begin Patch
*** Add File:audit/AUDIT_PACKAGE_TEMPLATE.md
+# Audit Package Template — what to collect and where
+
+Purpose: provide auditors a minimal, consistent package proving traceability, validation, and approvals for any production promotion related to profiles/models.
+
+1) Evidence artifacts (store under s3://<EVIDENCE_BUCKET>/audit/<release-id>/)
+ - power_profiles.yaml (approved) and cosign signature file (.sig)
+ - validation_report.json (from calibration/validation_reports/)
+ - enforcement_coverage_report.json (from enforcement/coverage_reports/)
+ - telemetry_health_report.json (from telemetry/health/)
+ - GE_validation_<timestamp>.json (from data-quality/validations/)
+ - DataHub lineage snapshot (manifest.json exported)
+
+2) Promotion metadata
+ - operator: name, email, approval timestamp
+ - PR/issue: link to GitHub PR that triggered promotion (if any)
+ - Rekor entries: rekor entry URLs or log IDs for signature events
+
+3) Configuration snapshot
+ - configmap yaml applied to production (aegis-power-profiles)
+ - KMS key ARN used for cosign signing
+ - ClusterIssuer name and CA information (for webhook TLS)
+
+4) Test & validation logs
+ - Enforcement harness run logs and analytics (coverage)
+ - Redshift/BigQuery loader logs and MERGE/upsert validation queries
+ - dbt run & test output (artifacts)
+
+5) Retention & access
+ - Audit package stored for minimum retention period per policy (e.g., 7 years) and access logged via CloudTrail.
+ - Ensure S3 bucket has object lock/versioning and access logging enabled.
+
+6) Checklist to include with package
+ - [ ] Telemetry health: OK
+ - [ ] Calibration median ≤ 0.10
+ - [ ] Enforcement coverage ≥ 0.95
+ - [ ] ETL idempotency validated
+ - [ ] Artifacts signed with cosign and Rekor entry exists
+ - [ ] Operator approval recorded
+
+Notes:
+- Use ISO8601 timestamps in all artifacts and include the operator's signed statement (can be simple text file uploaded and cosign-signed).
+
*** End Patch
*** End Patch
