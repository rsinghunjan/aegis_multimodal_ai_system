diff --git a/scripts/dgx/provision_and_set_ci_secrets.sh b/scripts/dgx/provision_and_set_ci_secrets.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/dgx/provision_and_set_ci_secrets.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Provision DGX cluster credentials and set required CI secrets in GitHub (repo or org)
+#
+# Requirements:
+#  - gh CLI authenticated with permission to write secrets for target repo
+#  - kubeconfig file path or slurm creds file
+#
+# Usage:
+#   ./scripts/dgx/provision_and_set_ci_secrets.sh --repo owner/repo --kubeconfig ./kubeconfigs/kubeconfig-dgx \
+#       --registry ghcr.io/org --registry-user ghuser --registry-pass ghpat --cosign-key ./keys/cosign.key \
+#       --alertmanager-url https://alertmanager.example.com --decision-log-url https://aegis.example.com/decision_log
+
+set -euo pipefail
+
+REPO=""
+KUBECONFIG=""
+SLURM_CREDS=""
+REGISTRY=""
+REG_USER=""
+REG_PASS=""
+COSIGN_KEY=""
+ALERTMANAGER_URL=""
+DECISION_LOG_URL=""
+
+print_usage() {
+  cat <<EOF
+Usage: $0 --repo owner/repo [--kubeconfig path] [--slurm-creds path] [--registry url --registry-user user --registry-pass pass]
+          [--cosign-key path] [--alertmanager-url url] [--decision-log-url url]
+EOF
+}
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --repo) REPO="$2"; shift 2;;
+    --kubeconfig) KUBECONFIG="$2"; shift 2;;
+    --slurm-creds) SLURM_CREDS="$2"; shift 2;;
+    --registry) REGISTRY="$2"; shift 2;;
+    --registry-user) REG_USER="$2"; shift 2;;
+    --registry-pass) REG_PASS="$2"; shift 2;;
+    --cosign-key) COSIGN_KEY="$2"; shift 2;;
+    --alertmanager-url) ALERTMANAGER_URL="$2"; shift 2;;
+    --decision-log-url) DECISION_LOG_URL="$2"; shift 2;;
+    -h|--help) print_usage; exit 0;;
+    *) echo "Unknown arg: $1"; print_usage; exit 2;;
+  esac
+done
+
+if [[ -z "$REPO" ]]; then
+  echo "ERROR: --repo is required"; print_usage; exit 2
+fi
+
+if ! command -v gh >/dev/null 2>&1; then
+  echo "ERROR: gh CLI is required: https://cli.github.com/"; exit 3
+fi
+
+tmpdir=$(mktemp -d)
+trap 'rm -rf "$tmpdir"' EXIT
+
+if [[ -n "$KUBECONFIG" ]]; then
+  if [[ ! -f "$KUBECONFIG" ]]; then echo "kubeconfig not found: $KUBECONFIG"; exit 4; fi
+  echo "Uploading KUBECONFIG_DGX to $REPO"
+  gh secret set KUBECONFIG_DGX --repo "$REPO" --body-file "$KUBECONFIG"
+fi
+
+if [[ -n "$SLURM_CREDS" ]]; then
+  if [[ ! -f "$SLURM_CREDS" ]]; then echo "slurm creds not found: $SLURM_CREDS"; exit 5; fi
+  echo "Uploading SLURM_CREDS to $REPO"
+  gh secret set SLURM_CREDS --repo "$REPO" --body-file "$SLURM_CREDS"
+fi
+
+if [[ -n "$REGISTRY" ]]; then
+  echo "Writing DGX_IMAGE_REGISTRY=$REGISTRY to $REPO"
+  printf '%s' "$REGISTRY" > "$tmpdir/_reg"
+  gh secret set DGX_IMAGE_REGISTRY --repo "$REPO" --body-file "$tmpdir/_reg"
+fi
+
+if [[ -n "$REG_USER" ]]; then
+  printf '%s' "$REG_USER" > "$tmpdir/_reg_user"
+  gh secret set DGX_REGISTRY_USER --repo "$REPO" --body-file "$tmpdir/_reg_user"
+fi
+if [[ -n "$REG_PASS" ]]; then
+  printf '%s' "$REG_PASS" > "$tmpdir/_reg_pass"
+  gh secret set DGX_REGISTRY_PASS --repo "$REPO" --body-file "$tmpdir/_reg_pass"
+fi
+
+if [[ -n "$COSIGN_KEY" ]]; then
+  if [[ ! -f "$COSIGN_KEY" ]]; then echo "COSIGN key not found: $COSIGN_KEY"; exit 6; fi
+  echo "Uploading COSIGN_KEY to $REPO"
+  gh secret set COSIGN_KEY --repo "$REPO" --body-file "$COSIGN_KEY"
+fi
+
+if [[ -n "$ALERTMANAGER_URL" ]]; then
+  printf '%s' "$ALERTMANAGER_URL" > "$tmpdir/_am"
+  gh secret set ALERTMANAGER_URL --repo "$REPO" --body-file "$tmpdir/_am"
+fi
+
+if [[ -n "$DECISION_LOG_URL" ]]; then
+  printf '%s' "$DECISION_LOG_URL" > "$tmpdir/_dl"
+  gh secret set DECISION_LOG_URL --repo "$REPO" --body-file "$tmpdir/_dl"
+fi
+
+echo "All requested secrets written to $REPO. Please verify in GitHub UI and protect them with Environments for production workflows."
+exit 0
+
diff --git a/scripts/dgx/register_self_hosted_runner.sh b/scripts/dgx/register_self_hosted_runner.sh
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/scripts/dgx/register_self_hosted_runner.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Register a GitHub Actions self-hosted runner on the host that will run DGX workflows.
+#
+# This script must be executed on the runner host (it downloads and configures the runner).
+# Usage:
+#   ./scripts/dgx/register_self_hosted_runner.sh --repo owner/repo --token RUNNER_TOKEN --labels dgx
+
+set -euo pipefail
+
+REPO=""
+TOKEN=""
+LABELS="dgx"
+WORKDIR="${WORKDIR:-/opt/actions-runner}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --repo) REPO="$2"; shift 2;;
+    --token) TOKEN="$2"; shift 2;;
+    --labels) LABELS="$2"; shift 2;;
+    --workdir) WORKDIR="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --repo owner/repo --token RUNNER_TOKEN --labels dgx"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$REPO" || -z "$TOKEN" ]]; then
+  echo "ERROR: --repo and --token are required"; exit 2
+fi
+
+mkdir -p "$WORKDIR"
+cd "$WORKDIR"
+
+if [[ ! -d actions-runner ]]; then
+  mkdir -p actions-runner
+  echo "Downloading self-hosted runner into $WORKDIR"
+  RUNNER_TAR="actions-runner-linux-x64-2.308.0.tar.gz"
+  curl -sSL -o "$RUNNER_TAR" "https://github.com/actions/runner/releases/download/v2.308.0/${RUNNER_TAR}"
+  tar xzf "$RUNNER_TAR" -C actions-runner
+  rm -f "$RUNNER_TAR"
+fi
+
+cd actions-runner
+./config.sh --url "https://github.com/${REPO}" --token "$TOKEN" --labels "$LABELS" --unattended --replace || true
+sudo ./svc.sh install || true
+sudo ./svc.sh start || true
+
+echo "Runner registered. Verify in GitHub repo actions -> runners. Ensure runner has network access to DGX control plane and necessary tools installed."
+exit 0
+
diff --git a/scripts/dgx/build_sign_publish_image.sh b/scripts/dgx/build_sign_publish_image.sh
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/dgx/build_sign_publish_image.sh
@@ -0,0 +1,300 @@
+#!/usr/bin/env bash
+#
+# Build, SBOM, scan, cosign-sign and publish pinned DeepSpeed images for DGX (H100/A100).
+#
+# Usage:
+#   REGISTRY=ghcr.io/org TARGET=h100 IMAGE_TAG=aegis-deepspeed:h100-cuda12.1-pytorch2.2 \
+#     ./scripts/dgx/build_sign_publish_image.sh
+
+set -euo pipefail
+
+REGISTRY="${REGISTRY:-}"
+TARGET="${TARGET:-h100}"
+IMAGE_TAG="${IMAGE_TAG:-}"
+BUILD_DIR="${BUILD_DIR:-.}"
+SBOM_DIR="${SBOM_DIR:-./artifacts/sbom}"
+IMAGE_MATRIX_FILE="${IMAGE_MATRIX_FILE:-docs/dgx/IMAGE_MATRIX.md}"
+
+if [[ -z "$REGISTRY" || -z "$IMAGE_TAG" ]]; then
+  echo "REGISTRY and IMAGE_TAG environment variables must be set"; exit 2
+fi
+
+case "$TARGET" in
+  h100) DOCKERFILE="docker/Dockerfile.deepspeed.h100";;
+  a100) DOCKERFILE="docker/Dockerfile.deepspeed.a100";;
+  *) echo "Unknown TARGET $TARGET"; exit 3;;
+esac
+
+if [[ ! -f "$DOCKERFILE" ]]; then
+  echo "Dockerfile missing: $DOCKERFILE"; exit 4
+fi
+
+IMAGE_FULL="${REGISTRY}/${IMAGE_TAG}"
+
+echo "Building image $IMAGE_FULL from $DOCKERFILE"
+docker build -t "$IMAGE_FULL" -f "$DOCKERFILE" "$BUILD_DIR"
+
+mkdir -p "$SBOM_DIR" ./artifacts
+SBOM_FILE="${SBOM_DIR}/${IMAGE_TAG//[:\/]/_}-sbom.json"
+if command -v syft >/dev/null 2>&1; then
+  syft "$IMAGE_FULL" -o json > "$SBOM_FILE" 2> ./artifacts/syft.log || true
+  echo "SBOM generated: $SBOM_FILE"
+else
+  echo "syft missing; SBOM not generated"
+fi
+
+if command -v trivy >/dev/null 2>&1; then
+  echo "Scanning image with trivy (CRITICAL/HIGH will fail build)"
+  trivy image --severity CRITICAL,HIGH --no-progress "$IMAGE_FULL" > "./artifacts/${IMAGE_TAG//[:\/]/_}-trivy.txt" 2>&1 || { echo "Trivy found critical/high issues; aborting"; exit 5; }
+else
+  echo "trivy missing; skipping vulnerability scan"
+fi
+
+echo "Pushing image $IMAGE_FULL"
+docker push "$IMAGE_FULL"
+
+DIGEST="unknown"
+if command -v skopeo >/dev/null 2>&1; then
+  skopeo inspect "docker://${IMAGE_FULL}" > ./artifacts/skopeo_inspect.json 2>/dev/null || true
+  DIGEST=$(jq -r '.Digest // empty' ./artifacts/skopeo_inspect.json || echo "")
+fi
+if [[ -z "$DIGEST" && command -v docker >/dev/null 2>&1 ]]; then
+  DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE_FULL" 2>/dev/null || echo "")
+fi
+echo "Image digest: $DIGEST" > ./artifacts/${IMAGE_TAG//[:\/]/_}-digest.txt
+
+echo "Signing image with cosign"
+if command -v cosign >/dev/null 2>&1; then
+  if [[ -n "${COSIGN_KEY:-}" ]]; then
+    cosign sign --key "$COSIGN_KEY" "$IMAGE_FULL"
+  else
+    cosign sign --oidc "$IMAGE_FULL" || echo "Keyless cosign sign failed or not available"
+  fi
+else
+  echo "cosign missing; image unsigned"
+fi
+
+echo "Recording provenance entry in $IMAGE_MATRIX_FILE"
+timestamp=$(date -u +%Y-%m-%d)
+mkdir -p "$(dirname "$IMAGE_MATRIX_FILE")"
+if ! grep -qF "$IMAGE_FULL" "$IMAGE_MATRIX_FILE" 2>/dev/null; then
+  cat >> "$IMAGE_MATRIX_FILE" <<EOF
+- image: ${IMAGE_FULL}
+  digest: ${DIGEST}
+  validated_on: ${timestamp}
+  sbom: ${SBOM_FILE}
+  signed: $(if command -v cosign >/dev/null 2>&1; then echo "true"; else echo "false"; fi)
+EOF
+  echo "Appended $IMAGE_FULL to $IMAGE_MATRIX_FILE"
+else
+  echo "IMAGE_MATRIX already contains $IMAGE_FULL"
+fi
+
+echo "Build/publish/sign complete. Artifacts in ./artifacts and SBOM in $SBOM_FILE"
+exit 0
+
diff --git a/scripts/dgx/deploy_gpu_operator_and_validate.sh b/scripts/dgx/deploy_gpu_operator_and_validate.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/deploy_gpu_operator_and_validate.sh
@@ -0,0 +1,240 @@
+#!/usr/bin/env bash
+#
+# Install/upgrade NVIDIA GPU Operator with chosen driver strategy (autoInstall true|false),
+# then run the driver-check daemonset and collect node-level driver/CUDA info.
+#
+# Usage:
+#   ./scripts/dgx/deploy_gpu_operator_and_validate.sh --autoInstall false --expected-cuda 12.1 --expected-driver 535
+
+set -euo pipefail
+
+AUTO_INSTALL="true"
+EXPECTED_CUDA=""
+EXPECTED_DRIVER=""
+NAMESPACE="gpu-operator"
+OUT_DIR="${OUT_DIR:-./artifacts/gpu_operator_validate}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --autoInstall) AUTO_INSTALL="$2"; shift 2;;
+    --expected-cuda) EXPECTED_CUDA="$2"; shift 2;;
+    --expected-driver) EXPECTED_DRIVER="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+VAL_DIR="k8s/manifests/dgx"
+if [[ "$AUTO_INSTALL" == "false" ]]; then
+  VAL_FILE="${VAL_DIR}/gpu-operator-values-no-driver.yaml"
+else
+  VAL_FILE="${VAL_DIR}/gpu-operator-values.yaml"
+fi
+
+if [[ ! -f "$VAL_FILE" ]]; then
+  echo "Values file not found: $VAL_FILE"; exit 3
+fi
+
+echo "Installing/upgrading GPU Operator with $VAL_FILE"
+helm repo add nvidia https://nvidia.github.io/gpu-operator || true
+helm repo update || true
+helm upgrade --install --namespace gpu-operator gpu-operator nvidia/gpu-operator -f "$VAL_FILE"
+
+echo "Waiting for operator to be available..."
+kubectl -n gpu-operator wait --for=condition=available deployment -l app.kubernetes.io/name=gpu-operator --timeout=180s || true
+kubectl -n gpu-operator get pods -o wide > "$OUT_DIR/operator_pods.txt" 2>&1 || true
+
+echo "Deploying driver-check daemonset to collect nvidia-smi output"
+kubectl create ns aegis-ml --dry-run=client -o yaml | kubectl apply -f - || true
+kubectl apply -f k8s/manifests/dgx/driver-check-daemonset.yaml -n aegis-ml || true
+sleep 5
+kubectl -n aegis-ml rollout status daemonset/dgx-driver-check --timeout=60s || true
+pods=$(kubectl -n aegis-ml get pods -l app=dgx-driver-check -o jsonpath='{.items[*].metadata.name}')
+for p in $pods; do
+  kubectl -n aegis-ml logs "$p" > "$OUT_DIR/${p}.log" 2>&1 || true
+done
+
+echo "Driver validation artifacts in $OUT_DIR"
+if [[ -n "$EXPECTED_CUDA" || -n "$EXPECTED_DRIVER" ]]; then
+  grep -E "Driver Version|CUDA Version" -n "$OUT_DIR"/*.log || true
+  if [[ -n "$EXPECTED_CUDA" ]]; then
+    if grep -R -q "$EXPECTED_CUDA" "$OUT_DIR"/*.log; then echo "Expected CUDA found"; else echo "Expected CUDA NOT found" ; fi
+  fi
+  if [[ -n "$EXPECTED_DRIVER" ]]; then
+    if grep -R -q "$EXPECTED_DRIVER" "$OUT_DIR"/*.log; then echo "Expected driver found"; else echo "Expected driver NOT found"; fi
+  fi
+fi
+
+echo "GPU Operator deploy & validation complete."
+exit 0
+
diff --git a/scripts/dgx/nccl_tune_apply_iterate.sh b/scripts/dgx/nccl_tune_apply_iterate.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/dgx/nccl_tune_apply_iterate.sh
@@ -0,0 +1,300 @@
+#!/usr/bin/env bash
+#
+# Run NCCL tuning, apply suggested envs to dgx-nccl-config, and iterate running multi-node test
+# until no NCCL fatal errors or max attempts reached.
+#
+# Usage:
+#   IMAGE=ghcr.io/org/aegis-deepspeed:h100... ./scripts/dgx/nccl_tune_apply_iterate.sh --nodes 2 --gpus-per-node 8 --max-attempts 3
+
+set -euo pipefail
+
+NODES=2
+GPUS_PER_NODE=8
+MAX_ATTEMPTS=3
+OUT_ROOT="${OUT_ROOT:-./artifacts/nccl_iterate}"
+JOB_MANIFEST="k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml"
+IMAGE="${IMAGE:-}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --gpus-per-node) GPUS_PER_NODE="$2"; shift 2;;
+    --max-attempts) MAX_ATTEMPTS="$2"; shift 2;;
+    --out) OUT_ROOT="$2"; shift 2;;
+    --job-manifest) JOB_MANIFEST="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$IMAGE" ]]; then
+  echo "IMAGE env var required (image to run multi-node test)"; exit 2
+fi
+
+mkdir -p "$OUT_ROOT"
+
+attempt=1
+while [[ $attempt -le $MAX_ATTEMPTS ]]; do
+  echo "Attempt $attempt/$MAX_ATTEMPTS: running NCCL tuning"
+  ./scripts/dgx/nccl_tuning.sh --nodes 1 --gpus-per-node "$GPUS_PER_NODE" --out "$OUT_ROOT/attempt-${attempt}/nccl" || true
+  ./scripts/dgx/generate_nccl_env_suggest.sh --nccl-output "$OUT_ROOT/attempt-${attempt}/nccl/all_reduce_perf.txt" --out "$OUT_ROOT/attempt-${attempt}/nccl/env_suggest.sh" || true
+  ./scripts/dgx/apply_nccl_config.sh --env-file "$OUT_ROOT/attempt-${attempt}/nccl/env_suggest.sh" --namespace aegis-ml || true
+
+  echo "Running multi-node test (attempt $attempt)..."
+  IMAGE="$IMAGE" ./scripts/dgx/apply_nccl_and_run_multinode.sh --nodes "$NODES" --gpus-per-node "$GPUS_PER_NODE" --out "$OUT_ROOT/attempt-${attempt}/multinode" || true
+
+  echo "Verifying multi-node logs for NCCL fatal errors"
+  if ./scripts/dgx/verify_multi_node_scaling_results.sh --artifact-dir "$OUT_ROOT/attempt-${attempt}/multinode"; then
+    echo "No obvious NCCL fatal errors detected in attempt $attempt"
+    break
+  else
+    echo "NCCL errors detected in attempt $attempt; will iterate if attempts remain"
+  fi
+  attempt=$((attempt+1))
+done
+
+if [[ $attempt -gt $MAX_ATTEMPTS ]]; then
+  echo "Reached max attempts ($MAX_ATTEMPTS); please inspect artifacts under $OUT_ROOT"
+  exit 1
+fi
+
+echo "NCCL tuning and multi-node validation succeeded on attempt $attempt. Artifacts in $OUT_ROOT"
+exit 0
+
diff --git a/scripts/dgx/ensure_pvc_and_offload.sh b/scripts/dgx/ensure_pvc_and_offload.sh
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/scripts/dgx/ensure_pvc_and_offload.sh
@@ -0,0 +1,240 @@
+#!/usr/bin/env bash
+#
+# Ensure fast checkpoint PVC exists and CronJob offloader is configured; run one-off offload and verify S3 objects.
+#
+# Usage:
+#   ./scripts/dgx/ensure_pvc_and_offload.sh --namespace aegis-ml --s3-bucket my-bucket
+
+set -euo pipefail
+
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+PVC_NAME="${PVC_NAME:-dgx-checkpoints-pvc}"
+CRONJOB_FILE="k8s/manifests/dgx/checkpoint-offloader-cronjob.yaml"
+S3_BUCKET=""
+OUT_DIR="${OUT_DIR:-./artifacts/offload_check}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --pvc) PVC_NAME="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "Checking PVC $PVC_NAME in namespace $NAMESPACE"
+if kubectl -n "$NAMESPACE" get pvc "$PVC_NAME" >/dev/null 2>&1; then
+  kubectl -n "$NAMESPACE" get pvc "$PVC_NAME" -o yaml > "$OUT_DIR/pvc.yaml"
+  echo "PVC found and recorded in $OUT_DIR/pvc.yaml"
+else
+  echo "PVC $PVC_NAME not found; applying example manifest"
+  kubectl apply -f k8s/manifests/dgx/checkpoints-pvc.yaml || true
+  kubectl -n "$NAMESPACE" get pvc "$PVC_NAME" -o yaml > "$OUT_DIR/pvc.yaml" || true
+fi
+
+if [[ -f "$CRONJOB_FILE" ]]; then
+  kubectl apply -f "$CRONJOB_FILE" -n "$NAMESPACE" || true
+  kubectl -n "$NAMESPACE" get cronjob dgx-checkpoint-offloader -o yaml > "$OUT_DIR/cronjob.yaml" 2>/dev/null || true
+else
+  echo "CronJob manifest $CRONJOB_FILE missing; cannot ensure offloader"
+fi
+
+echo "Triggering a one-off offloader job (from CronJob template) to validate offload"
+if kubectl -n "$NAMESPACE" get cronjob dgx-checkpoint-offloader >/dev/null 2>&1; then
+  kubectl -n "$NAMESPACE" create job --from=cronjob/dgx-checkpoint-offloader dgx-offloader-immediate-$(date +%s) || true
+  kubectl -n "$NAMESPACE" wait --for=condition=complete job -l job-name --timeout=300s || true
+  kubectl -n "$NAMESPACE" logs -l job-name -f > "$OUT_DIR/offloader_logs.txt" 2>&1 || true
+  if [[ -n "$S3_BUCKET" && command -v aws >/dev/null 2>&1 ]]; then
+    aws s3 ls "s3://${S3_BUCKET}/dgx-checkpoints/" > "$OUT_DIR/s3_list.txt" 2>&1 || true
+  fi
+else
+  echo "No dgx-checkpoint-offloader CronJob found; skip one-off offload"
+fi
+
+echo "Offload check artifacts in $OUT_DIR"
+exit 0
+
diff --git a/scripts/dgx/exercise_sre_and_alerts.sh b/scripts/dgx/exercise_sre_and_alerts.sh
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/scripts/dgx/exercise_sre_and_alerts.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Safely exercise SRE playbooks:
+#  - cordon/drain/uncordon a node (in staging)
+#  - trigger a test alert to Alertmanager to verify on-call routing
+#
+# Usage:
+#   ./scripts/dgx/exercise_sre_and_alerts.sh --node dgx-node-1 --alertmanager-url http://alertmanager:9093
+
+set -euo pipefail
+
+NODE=""
+ALERTMANAGER_URL=""
+GRACE=60
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --node) NODE="$2"; shift 2;;
+    --alertmanager-url) ALERTMANAGER_URL="$2"; shift 2;;
+    --grace) GRACE="$2"; shift 2;;
+    -h|--help) echo "Usage: $0 --node <node> --alertmanager-url <url>"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$NODE" ]]; then
+  echo "--node required"; exit 2
+fi
+
+echo "CORDON node $NODE"
+kubectl cordon "$NODE"
+
+echo "DRAIN node $NODE (grace $GRACE)"
+kubectl drain "$NODE" --ignore-daemonsets --delete-local-data --grace-period="$GRACE" || echo "Drain reported issues"
+
+echo "Sleeping 10s to simulate SRE mitigation steps"
+sleep 10
+
+echo "UNCORDON node $NODE"
+kubectl uncordon "$NODE" || true
+
+if [[ -n "$ALERTMANAGER_URL" ]]; then
+  echo "Sending test alert to $ALERTMANAGER_URL"
+  ./scripts/dgx/send_test_alert.sh --url "$ALERTMANAGER_URL" --summary "SRE exercise for node ${NODE}" --instance "sre-exercise-$(date +%s)" || true
+else
+  echo "ALERTMANAGER_URL not provided; skipping alert send"
+fi
+
+echo "SRE exercise complete. Confirm on-call received test notification and node recovered."
+exit 0
+
diff --git a/.github/workflows/dgx_onboard_and_validate.yml b/.github/workflows/dgx_onboard_and_validate.yml
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/.github/workflows/dgx_onboard_and_validate.yml
@@ -0,0 +1,300 @@
+name: DGX Onboard & Validate (Self-hosted)
+
+on:
+  workflow_dispatch:
+    inputs:
+      image:
+        description: "Image tag to validate (registry/image:tag)"
+        required: true
+      registry:
+        description: "Registry host (eg. ghcr.io/org)"
+        required: true
+      label_nodes:
+        description: "Comma separated node names to label (optional)"
+        required: false
+        default: ""
+      auto_install:
+        description: "GPU Operator driver autoInstall (true|false)"
+        required: false
+        default: "false"
+      s3_bucket:
+        description: "S3 bucket for checkpoint offload checks (optional)"
+        required: false
+        default: ""
+
+jobs:
+  onboard-validate:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Restore kubeconfig secret
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          if [ -z "$KUBECONFIG_DGX" ]; then echo "KUBECONFIG_DGX secret missing"; exit 1; fi
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          chmod 600 "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+
+      - name: Run image build/publish/sign (optional)
+        if: ${{ github.event.inputs.registry != '' }}
+        env:
+          REGISTRY: ${{ github.event.inputs.registry }}
+          IMAGE_TAG: ${{ github.event.inputs.image }}
+          TARGET: "h100"
+        run: |
+          chmod +x scripts/dgx/build_sign_publish_image.sh
+          REGISTRY="$REGISTRY" TARGET="$TARGET" IMAGE_TAG="$IMAGE_TAG" ./scripts/dgx/build_sign_publish_image.sh || true
+
+      - name: Deploy GPU Operator and run driver validation
+        env:
+          AUTO_INSTALL: ${{ github.event.inputs.auto_install }}
+        run: |
+          chmod +x scripts/dgx/deploy_gpu_operator_and_validate.sh
+          ./scripts/dgx/deploy_gpu_operator_and_validate.sh --autoInstall "$AUTO_INSTALL" --expected-cuda "12.1" --expected-driver "535" --out ./artifacts/gpu_operator || true
+
+      - name: Run NCCL tuning, apply envs and iterate multi-node test
+        env:
+          IMAGE: ${{ github.event.inputs.registry }}/${{ github.event.inputs.image }}
+        run: |
+          chmod +x scripts/dgx/nccl_tune_apply_iterate.sh
+          IMAGE="$IMAGE" ./scripts/dgx/nccl_tune_apply_iterate.sh --nodes 2 --gpus-per-node 8 --max-attempts 3 --out ./artifacts/nccl_iterate || true
+
+      - name: Ensure checkpoint PVC & offload
+        env:
+          S3_BUCKET: ${{ github.event.inputs.s3_bucket }}
+        run: |
+          chmod +x scripts/dgx/ensure_pvc_and_offload.sh
+          ./scripts/dgx/ensure_pvc_and_offload.sh --namespace aegis-ml --s3-bucket "$S3_BUCKET" --out ./artifacts/offload || true
+
+      - name: Exercise SRE playbooks & send test alert (staging only)
+        env:
+          ALERTMANAGER_URL: ${{ secrets.ALERTMANAGER_URL }}
+        run: |
+          chmod +x scripts/dgx/exercise_sre_and_alerts.sh
+          ./scripts/dgx/exercise_sre_and_alerts.sh --node "${{ secrets.DGX_TEST_NODE_NAME }}" --alertmanager-url "$ALERTMANAGER_URL" || true
+
+      - name: Run full provenance collection and archive
+        env:
+          REGISTRY: ${{ github.event.inputs.registry }}
+          IMAGE_TAG: ${{ github.event.inputs.image }}
+          DECISION_LOG_URL: ${{ secrets.DECISION_LOG_URL }}
+        run: |
+          chmod +x scripts/dgx/full_prod_validation_and_archive.sh
+          REGISTRY="$REGISTRY" IMAGE_TAG="$IMAGE_TAG" S3_BUCKET="${{ github.event.inputs.s3_bucket }}" DECISION_LOG_URL="$DECISION_LOG_URL" ./scripts/dgx/full_prod_validation_and_archive.sh || true
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-onboard-validation-artifacts
+          path: artifacts || .
+
+      - name: Cleanup kubeconfig
+        if: always()
+        run: rm -f "$HOME/.kube/config_dgx" || true
+
diff --git a/.github/workflows/promotion_gate_strict.yml b/.github/workflows/promotion_gate_strict.yml
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/.github/workflows/promotion_gate_strict.yml
@@ -0,0 +1,220 @@
+name: DGX Promotion Gate (Strict)
+
+on:
+  workflow_dispatch:
+    inputs:
+      image:
+        description: "Image to promote (registry/image:tag or digest)"
+        required: true
+
+jobs:
+  gate:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Install cosign
+        run: |
+          curl -sSfL https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64 -o /usr/local/bin/cosign || true
+          chmod +x /usr/local/bin/cosign || true
+
+      - name: Promotion checks
+        env:
+          IMAGE: ${{ github.event.inputs.image }}
+          IMAGE_MATRIX_FILE: docs/dgx/IMAGE_MATRIX.md
+          ARTIFACT_PATH: artifacts/dgx_full_validation
+        run: |
+          if ! grep -q "$IMAGE" "$IMAGE_MATRIX_FILE"; then
+            echo "Image not recorded in $IMAGE_MATRIX_FILE; aborting promotion"; exit 1
+          fi
+          if ! command -v cosign >/dev/null 2>&1; then echo "cosign missing"; exit 2; fi
+          if ! cosign verify "$IMAGE" > /tmp/cosign_verify.txt 2>&1; then
+            echo "cosign verify failed; see /tmp/cosign_verify.txt"; cat /tmp/cosign_verify.txt; exit 3
+          fi
+          if [ ! -d "$ARTIFACT_PATH" ] || [ -z "$(ls -A $ARTIFACT_PATH 2>/dev/null)" ]; then
+            echo "Validation artifacts missing at $ARTIFACT_PATH; aborting promotion"; exit 4
+          fi
+          echo "Promotion gate passed for $IMAGE"
+
+      - name: Done
+        run: echo "Promotion gating checks passed. Update production manifests to reference the digest."
+
