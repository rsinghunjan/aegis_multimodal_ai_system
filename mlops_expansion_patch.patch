*** Begin Patch
*** Add File:.github/workflows/ci_train_validate_register.yml
+name: CI — train, validate, register model candidate
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  unit-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install -r requirements.txt
+      - name: Run unit tests
+        run: pytest -q
+
+  train-validate-register:
+    needs: unit-tests
+    runs-on: ubuntu-latest
+    env:
+      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install -r requirements.txt mlflow boto3 scikit-learn joblib
+      - name: Run quick training (small)
+        id: train
+        run: |
+          python3 ops/train.py --data tests/fixtures/tiny_dataset --mlflow-tracking-uri "${MLFLOW_TRACKING_URI}" --experiment "ci-test" --output /tmp/ci_output
+          cat /tmp/ci_output/run_meta.json
+      - name: Package model and upload to S3
+        id: package
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          tar czf /tmp/ci_model.tar.gz -C /tmp/ci_output model
+          aws s3 cp /tmp/ci_model.tar.gz s3://${EVIDENCE_BUCKET}/ci_models/${{ github.run_id }}/model.tar.gz
+          echo "::set-output name=model_s3::s3://${EVIDENCE_BUCKET}/ci_models/${{ github.run_id }}/model.tar.gz"
+      - name: Validate model
+        id: validate
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          python3 ops/validate_model.py --model-s3 "${{ steps.package.outputs.model_s3 }}" --threshold 0.5 --mlflow-tracking-uri "${MLFLOW_TRACKING_URI}"
+      - name: Register model in MLflow
+        if: success()
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          python3 - <<PY
+import mlflow, os, json, boto3
+mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
+client = mlflow.tracking.MlflowClient()
+run = mlflow.start_run()
+mlflow.log_param("ci_run_id", "${{ github.run_id }}")
+mlflow.log_artifact("/tmp/ci_output", artifact_path="model")
+res = client.create_registered_model("ci-candidate-model")
+new_version = client.create_model_version("ci-candidate-model", f"runs:/{run.info.run_id}/model", run.info.run_id)
+print("Registered model version:", new_version.version)
+mlflow.end_run()
+PY
+      - name: Create GitHub PR to GitOps repo for promotion (optional)
+        if: success()
+        env:
+          GITOPS_REPO: ${{ secrets.GITOPS_REPO }}
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        run: |
+          echo "Create a PR in ${GITOPS_REPO} to update deployment manifests with the candidate model image/version. This step requires a workflow token and is org-specific."
+
*** End Patch
*** Begin Patch
*** Add File:kserve/canary_and_autoscale.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-serving
+
+---
+# Primary (stable) InferenceService
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-model-stable"
+  namespace: "aegis-serving"
+spec:
+  predictor:
+    serviceAccountName: "aegis-inference-sa"
+    sklearn:
+      storageUri: "s3://REPLACE_EVIDENCE_BUCKET/models/stable"
+      resources:
+        limits:
+          cpu: "500m"
+          memory: "1Gi"
+  minReplicas: 1
+  maxReplicas: 3
+
+---
+# Canary InferenceService (small subset of traffic)
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-model-canary"
+  namespace: "aegis-serving"
+spec:
+  predictor:
+    serviceAccountName: "aegis-inference-sa"
+    sklearn:
+      storageUri: "s3://REPLACE_EVIDENCE_BUCKET/models/canary"
+      resources:
+        limits:
+          cpu: "500m"
+          memory: "1Gi"
+  minReplicas: 0
+  maxReplicas: 2
+
+---
+# Note: Traffic split to canary/stable typically orchestrated by Istio/Knative/Seldon. Use your ServiceMesh routing to send e.g. 10% traffic to canary.
+# Example (pseudo): create VirtualService with weighted routing to the two InferenceService backends.
+
+---
+# HPA example for autoscaling (if not using KServe autoscaling)
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: ksvc-aegis-model
+  namespace: aegis-serving
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-model-deployment
+  minReplicas: 1
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 50
+
*** End Patch
*** Begin Patch
*** Add File:featurestore/feast_demo_materialize.sh
+#!/usr/bin/env bash
+#
+# Demo: materialize offline features into the online store (Feast)
+# Replace REPLACE_* placeholders with your environment values before running.
+set -euo pipefail
+export FEAST_REGISTRY="s3://REPLACE_EVIDENCE_BUCKET/feast/registry.db"
+export FEAST_PROJECT="aegis-feast"
+
+echo "Materializing features for project ${FEAST_PROJECT}..."
+# Requires feast CLI installed and configured
+feast materialize now --start $(date -d '1 hour ago' --utc +%Y-%m-%dT%H:%M:%SZ) --end $(date --utc +%Y-%m-%dT%H:%M:%SZ)
+echo "Materialize done. Test online feature retrieval:"
+python3 - <<PY
+from feast import FeatureStore
+fs = FeatureStore(repo_path="featurestore/feast_sample")
+print("Available feature views:", fs.list_feature_views())
+PY
+
*** End Patch
*** Begin Patch
*** Add File:gitops/promotion_workflow.yml
+name: GitOps — promote model candidate to staging (creates PR to GitOps repo)
+
+on:
+  workflow_dispatch:
+    inputs:
+      model_version:
+        description: 'MLflow model version to promote'
+        required: true
+
+jobs:
+  create-promotion-pr:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout (local workspace for editing)
+        uses: actions/checkout@v4
+      - name: Clone GitOps repo
+        env:
+          GITOPS_REPO: ${{ secrets.GITOPS_REPO }}
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        run: |
+          git clone https://x-access-token:${GITHUB_TOKEN}@github.com/${GITOPS_REPO} gitops
+          cd gitops
+          # Replace placeholder in kustomize overlay or manifest
+          sed -i "s/REPLACE_MODEL_VERSION/${{ github.event.inputs.model_version }}/g" overlays/staging/kustomization.yaml || true
+          git add -A
+          git commit -m "Promote model ${{ github.event.inputs.model_version }} to staging" || true
+          git push origin HEAD
+          gh pr create --repo ${GITOPS_REPO} --title "Promote model ${{ github.event.inputs.model_version }} to staging" --body "Automated promotion PR created by CI: model_version=${{ github.event.inputs.model_version }}"
+
+    # NOTE: Requires GITOPS_REPO secret and workflow runner permissions to create PRs.
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_rule_model_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-model-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: model.rules
+      rules:
+        - alert: ModelLatencyHigh
+          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="kserve"}[5m])) by (le)) > 0.5
+          for: 5m
+          labels:
+            severity: page
+          annotations:
+            summary: "Model latency high (95th percentile > 500ms)"
+            description: "Check KServe pods and HPA. Workflow: Scale up or roll back canary."
+
+        - alert: ModelPredictionDrift
+          expr: increase(model_prediction_drift_total[1h]) > 10
+          for: 10m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Model prediction drift detected"
+            description: "Trigger drift evaluation job and open incident if confirmed."
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana_aegis_model_dashboard.json
+{
+  "dashboard": {
+    "id": null,
+    "title": "Aegis Model - Overview",
+    "panels": [
+      {
+        "type": "graph",
+        "title": "Prediction Latency (P95)",
+        "targets": [
+          {
+            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"kserve\"}[5m])) by (le))",
+            "legendFormat": "p95"
+          }
+        ],
+        "gridPos": { "x": 0, "y": 0, "w": 12, "h": 6 }
+      },
+      {
+        "type": "graph",
+        "title": "Prediction Count",
+        "targets": [
+          { "expr": "sum(rate(http_requests_total{job=\"kserve\"}[5m]))", "legendFormat": "req/s" }
+        ],
+        "gridPos": { "x": 12, "y": 0, "w": 12, "h": 6 }
+      },
+      {
+        "type": "stat",
+        "title": "Drift Alerts (last 1h)",
+        "targets": [
+          { "expr": "increase(model_prediction_drift_total[1h])", "legendFormat": "drift_inc" }
+        ],
+        "gridPos": { "x": 0, "y": 6, "w": 6, "h": 3 }
+      }
+    ],
+    "schemaVersion": 27,
+    "version": 0
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/evidently_drift_config.json
+{
+  "drift_config": {
+    "target_column": "label",
+    "prediction_column": "prediction",
+    "numerical_features": ["f1","f2","f3"],
+    "categorical_features": ["cat1","cat2"],
+    "drift_threshold": 0.1,
+    "batch_size": 1000
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/run_drift_evaluation.sh
+#!/usr/bin/env bash
+#
+# Run Evidently drift evaluation on recent production data.
+# Requires python env with evidently installed and AWS access to fetch data.
+set -euo pipefail
+CONFIG=monitoring/evidently_drift_config.json
+MODEL_S3=${1:-"s3://REPLACE_EVIDENCE_BUCKET/models/stable/model.tar.gz"}
+DATA_S3=${2:-"s3://REPLACE_EVIDENCE_BUCKET/inputs/prod_recent.csv"}
+
+echo "Running drift evaluation for model ${MODEL_S3} against data ${DATA_S3}"
+python3 - <<PY
+import json, tempfile, subprocess, os
+from evidently.report import Report
+from evidently.metric_preset import DataDriftPreset
+cfg=json.load(open("${CONFIG}"))
+tmpdir=tempfile.mkdtemp()
+subprocess.check_call(["aws","s3","cp","${DATA_S3}", os.path.join(tmpdir,"data.csv")])
+import pandas as pd
+data = pd.read_csv(os.path.join(tmpdir,"data.csv"))
+report = Report(metrics=[DataDriftPreset()])
+report.run(reference_data=data.sample(frac=0.5), current_data=data.sample(frac=0.5))
+res_html = os.path.join(tmpdir,"drift_report.html")
+report.save_html(res_html)
+print("Saved drift report to", res_html)
+subprocess.check_call(["aws","s3","cp", res_html, "s3://REPLACE_EVIDENCE_BUCKET/monitoring/drift_reports/"])
+print("Drift evaluation complete.")
+PY
+
*** End Patch
*** End Patch
