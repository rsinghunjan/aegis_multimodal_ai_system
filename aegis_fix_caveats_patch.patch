*** Begin Patch
*** Add File:feature_store/feast_online_redis.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: feast-redis
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: feast-redis
+  template:
+    metadata:
+      labels:
+        app: feast-redis
+    spec:
+      containers:
+        - name: redis
+          image: redis:6.2-alpine
+          ports:
+            - containerPort: 6379
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "256Mi"
+            limits:
+              cpu: "500m"
+              memory: "512Mi"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: feast-redis
+  namespace: aegis
+spec:
+  selector:
+    app: feast-redis
+  ports:
+    - port: 6379
+      targetPort: 6379
+  type: ClusterIP
+
+# Provides an online store (Redis) for Feast online lookup. Configure Feast repo to use this host for online store.
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/feature_server.py
+#!/usr/bin/env python3
+"""
+Minimal feature serving microservice that proxies feature lookups from Redis for online inference.
+Exposes an HTTP endpoint /features?ids=1,2,3 and returns JSON feature vectors.
+"""
+import os
+import json
+from flask import Flask, request, jsonify
+import redis
+
+REDIS_HOST = os.environ.get("FEAST_REDIS_HOST", "feast-redis.aegis.svc.cluster.local")
+REDIS_PORT = int(os.environ.get("FEAST_REDIS_PORT", "6379"))
+
+r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
+app = Flask("feature-server")
+
+@app.route("/features")
+def features():
+    ids = request.args.get("ids","")
+    ids = [i for i in ids.split(",") if i]
+    out = {}
+    for _id in ids:
+        key = f"feast:job_run_features:{_id}"
+        val = r.hgetall(key)
+        out[_id] = val
+    return jsonify(out)
+
+@app.route("/healthz")
+def health():
+    try:
+        r.ping()
+        return "ok"
+    except Exception:
+        return "redis_unavailable", 503
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8080")))
+
*** End Patch
*** Begin Patch
*** Add File:serving/seldon_canary_istio.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: model-runtime-rollout
+  namespace: aegis
+spec:
+  replicas: 3
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+        - pause: {duration: 120}
+        - setWeight: 50
+        - pause: {duration: 300}
+  selector:
+    matchLabels:
+      app: model-runtime
+  template:
+    metadata:
+      labels:
+        app: model-runtime
+    spec:
+      containers:
+        - name: runtime
+          image: ghcr.io/yourorg/aegis-runtime:latest
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+
+---
+# Istio VirtualService snippet (example) to route traffic using weight
+apiVersion: networking.istio.io/v1alpha3
+kind: VirtualService
+metadata:
+  name: model-runtime-vs
+  namespace: aegis
+spec:
+  hosts:
+    - model-runtime.aegis.svc.cluster.local
+  http:
+    - route:
+        - destination:
+            host: model-runtime
+            subset: stable
+          weight: 90
+        - destination:
+            host: model-runtime
+            subset: canary
+          weight: 10
+
*** End Patch
*** Begin Patch
*** Add File:gpu/gpu_training_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-gpu-train
+  namespace: aegis
+spec:
+  template:
+    spec:
+      nodeSelector:
+        accelerator: nvidia-gpu
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      containers:
+        - name: trainer
+          image: ghcr.io/yourorg/aegis-trainer:gpu-latest
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "8"
+              memory: "32Gi"
+            requests:
+              cpu: "4"
+              memory: "16Gi"
+          env:
+            - name: EVIDENCE_BUCKET
+              value: "REPLACE_WITH_EVIDENCE_BUCKET"
+            - name: MLFLOW_TRACKING_URI
+              value: "http://mlflow.aegis.svc.cluster.local"
+          command: ["/bin/sh", "-c"]
+          args:
+            - |
+              set -e
+              /opt/train/run_train.sh
+      restartPolicy: Never
+  backoffLimit: 1
+
+# Use this job to run GPU training on GPU nodes; create a node pool in your cloud with label accelerator=nvidia-gpu
+
*** End Patch
*** Begin Patch
*** Add File:terraform/eks/gpu_node_group.tf
+variable "cluster_name" {}
+variable "node_group_name" { default = "aegis-gpu-nodes" }
+variable "instance_types" { type = list(string) default = ["p3.2xlarge"] }
+variable "min_size" { default = 0 }
+variable "max_size" { default = 5 }
+
+resource "aws_eks_node_group" "gpu_nodes" {
+  cluster_name    = var.cluster_name
+  node_group_name = var.node_group_name
+  node_role_arn   = var.node_role_arn
+  subnet_ids      = var.subnet_ids
+  scaling_config {
+    desired_size = 1
+    max_size     = var.max_size
+    min_size     = var.min_size
+  }
+  instance_types = var.instance_types
+  labels = {
+    "accelerator" = "nvidia-gpu"
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:workflows/trigger_gpu_training.yml
+name: Trigger GPU Training Job (manual)
+
+on:
+  workflow_dispatch:
+
+jobs:
+  trigger:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Configure kubectl
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+      - name: Trigger GPU training job
+        run: |
+          kubectl -n aegis apply -f gpu/gpu_training_job.yaml
+          kubectl -n aegis wait --for=condition=complete job/aegis-gpu-train --timeout=7200s
+
*** End Patch
*** Begin Patch
*** Add File:secrets/externalsecret_dw_credentials.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: SecretStore
+metadata:
+  name: aws-sm
+  namespace: aegis
+spec:
+  provider:
+    aws:
+      service: SecretsManager
+      region: REPLACE_WITH_AWS_REGION
+      auth:
+        jwt:
+          serviceAccountRef:
+            name: external-secrets-sa
+
+---
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: dw-credentials
+  namespace: aegis
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: aws-sm
+    kind: SecretStore
+  target:
+    name: dw-credentials
+  data:
+    - secretKey: DW_DSN
+      remoteRef:
+        key: aegis/dw/dsn
+
+# Mirrors DW credentials from AWS Secrets Manager to Kubernetes secret `dw-credentials` for Airflow/Superset
+
*** End Patch
*** Begin Patch
*** Add File:superset/superset_rls_setup.py
+#!/usr/bin/env python3
+"""
+Create Superset roles and RLS policies programmatically via Superset REST API.
+Requires SUPERSET_API_URL and SUPERSET_API_TOKEN env vars.
+"""
+import os, requests, json
+
+SUPERSET_URL = os.environ.get("SUPERSET_API_URL")
+TOKEN = os.environ.get("SUPERSET_API_TOKEN")
+
+def api(path, method="GET", data=None):
+    headers = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}
+    url = SUPERSET_URL.rstrip("/") + path
+    if method == "GET":
+        return requests.get(url, headers=headers).json()
+    elif method == "POST":
+        return requests.post(url, headers=headers, json=data).json()
+    elif method == "PUT":
+        return requests.put(url, headers=headers, json=data).json()
+
+def create_role(name):
+    return api("/api/v1/security/role", method="POST", data={"name": name})
+
+def create_rls_rule(dataset_id, clause, role_name):
+    payload = {"dataset_id": dataset_id, "clause": clause, "roles": [role_name], "name": f"rls_{dataset_id}_{role_name}"}
+    return api("/api/v1/rls", method="POST", data=payload)
+
+def main():
+    # Example: create role team_alpha and RLS clause limiting to team='alpha'
+    team = os.environ.get("TEAM", "alpha")
+    role = f"team_{team}"
+    print("Creating role", role)
+    create_role(role)
+    # List datasets and add RLS for daily_team_rollup dataset (you may need to map dataset id)
+    datasets = api("/api/v1/dataset")
+    # best-effort: find dataset named marts.daily_team_rollup
+    dataset_id = None
+    for ds in datasets.get("result",[]):
+        if "marts.daily_team_rollup" in ds.get("table_name",""):
+            dataset_id = ds["id"]
+            break
+    if dataset_id:
+        clause = f"team = '{team}'"
+        create_rls_rule(dataset_id, clause, role)
+        print("Created RLS for dataset", dataset_id)
+    else:
+        print("Dataset not found; please run after Superset metadata sync.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:reports/generate_signed_report.py
+#!/usr/bin/env python3
+"""
+Generate a signed daily validation/report (calibration/ETL) artifact:
+ - collects latest validation + enforcement + telemetry summaries
+ - composes a JSON report, uploads to S3 under calibration/approved/
+ - signs the local JSON using cosign with KMS and uploads signature (and Rekor is automatically updated by cosign)
+"""
+import os, json, tempfile, subprocess, time
+import boto3
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN")
+
+def fetch_latest(s3, prefix):
+    paginator = s3.get_paginator("list_objects_v2")
+    latest = None
+    for page in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=prefix):
+        for o in page.get("Contents", []):
+            latest = o["Key"]
+    if not latest:
+        return None
+    return s3.get_object(Bucket=EVIDENCE_BUCKET, Key=latest)["Body"].read().decode()
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET env required")
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    calib = fetch_latest(s3, "calibration/validation_reports/")
+    enforce = fetch_latest(s3, "enforcement/coverage_reports/")
+    telemetry = fetch_latest(s3, "telemetry/health/")
+    report = {
+        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
+        "calibration": json.loads(calib) if calib else None,
+        "enforcement": json.loads(enforce) if enforce else None,
+        "telemetry": json.loads(telemetry) if telemetry else None,
+    }
+    tmp = tempfile.mktemp(suffix=".json")
+    with open(tmp, "w") as f:
+        json.dump(report, f, indent=2)
+    key = f"calibration/approved/validation_report_{int(time.time())}.json"
+    s3.upload_file(tmp, EVIDENCE_BUCKET, key)
+    print("Uploaded report", key)
+    # sign using cosign via KMS
+    if COSIGN_KMS:
+        subprocess.run(["cosign","sign","--key",f"awskms://{COSIGN_KMS}", tmp], check=True)
+        sig = tmp + ".sig"
+        if os.path.exists(sig):
+            s3.upload_file(sig, EVIDENCE_BUCKET, key + ".sig")
+            print("Uploaded signature", key + ".sig")
+    else:
+        print("COSIGN_KMS_KEY_ARN not set; report uploaded unsigned")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ci/auto_calibration_and_sign.yml
+name: Auto Calibration and Signed Report (staging)
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 3 * * *' # daily
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+  COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+
+jobs:
+  converge-and-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install boto3 kubernetes
+      - name: Configure kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+      - name: Run calibration converge
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          MEDIAN_THRESHOLD: "0.10"
+        run: |
+          python3 calibration/iterative_convergence_runner.py || true
+      - name: Generate signed report
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+        run: |
+          python3 reports/generate_signed_report.py
+
*** End Patch
*** Begin Patch
*** Add File:ci/verify_cosign_rekor.py
+#!/usr/bin/env python3
+"""
+Verify cosign signature and Rekor entry for a signed artifact in S3.
+Requires cosign CLI configured with Rekor server.
+Usage:
+  python3 ci/verify_cosign_rekor.py s3://bucket/path/to/file.json
+"""
+import sys, tempfile, subprocess, boto3, os
+
+def download_s3(uri):
+    parts = uri[len("s3://"):].split("/",1)
+    b, k = parts[0], parts[1]
+    tmp = tempfile.mktemp()
+    boto3.client("s3").download_file(b, k, tmp)
+    return tmp
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("usage: ... s3://bucket/key")
+        sys.exit(2)
+    uri = sys.argv[1]
+    local = download_s3(uri)
+    try:
+        subprocess.run(["cosign","verify","--key","awskms://${COSIGN_KMS_KEY_ARN}", local], check=True)
+        print("cosign verify OK")
+    except Exception as e:
+        print("cosign verify failed", e)
+        sys.exit(1)
+
*** End Patch
*** Begin Patch
*** Add File:superset/superset_dataset_ownership.py
+#!/usr/bin/env python3
+"""
+Assign dataset ownership in Superset based on DataHub / dbt manifest owners.
+Requires SUPERSET_API_URL and SUPERSET_API_TOKEN.
+"""
+import os, requests, json
+
+SUPERSET_URL = os.environ.get("SUPERSET_API_URL")
+TOKEN = os.environ.get("SUPERSET_API_TOKEN")
+MANIFEST = os.environ.get("DBT_MANIFEST_PATH", "dbt/target/manifest.json")
+
+def superset_headers():
+    return {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}
+
+def upsert_owner(dataset_name, owner_email):
+    # crude: search dataset then add owner via API (example)
+    r = requests.get(f"{SUPERSET_URL}/api/v1/dataset?q={{\"filters\":[{{\"col\":\"table_name\",\"opr\":\"eq\",\"value\":\"{dataset_name}\"}}]}}", headers=superset_headers())
+    if r.status_code != 200:
+        print("Superset query failed", r.text)
+        return
+    res = r.json().get("result", [])
+    if not res:
+        print("Dataset not found in Superset:", dataset_name)
+        return
+    ds_id = res[0]["id"]
+    # assign owner via dashboard metadata endpoints (implementation dependent)
+    print("Would assign owner", owner_email, "to dataset id", ds_id)
+
+def main():
+    mf = json.load(open(MANIFEST))
+    for uid, node in mf.get("nodes", {}).items():
+        owner = node.get("meta", {}).get("owner")
+        name = node.get("metadata", {}).get("name") or node.get("unique_id")
+        if owner:
+            upsert_owner(name, owner)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/roles_self_service_onboarding.md
+# Role-based Self-Service Onboarding (Data Scientist, Data Analyst, BI/Business Analyst)
+
+This document describes the steps and automation we added to remove caveats and enable self-service.
+
+Data Scientist
+- Online features: Feast online Redis deployed (feature_store/feast_online_redis.yaml) and feature-server (feature_store/feature_server.py) for low-latency lookups.
+- GPU training: GPU node pool Terraform & GPU Job (gpu/gpu_training_job.yaml). Use workflows/trigger_gpu_training.yml to run.
+- Production promotions: CI workflow ci/auto_calibration_and_sign.yml will run calibration and generate a signed report; gated_promotion.yml and ci/gated_promotion.yml enforce manual approvals and cosign checks.
+- Retrain trigger: retrain/trigger_retrain.py and workflow .github/workflows/drift_trigger_retrain.yml will detect drift and schedule retrain jobs.
+
+Data Analyst / BI Analyst
+- DW credentials are mirrored to k8s via secrets/externalsecret_dw_credentials.yaml.
+- Superset RLS helper superset/superset_rls_setup.py creates roles and RLS rules for team-level isolation.
+- Superset dataset ownership automation via superset/superset_dataset_ownership.py + datahub/datahub_owner_mapper.py assign dataset owners.
+
+Business Analyst
+- Signed, auditable reports: reports/generate_signed_report.py produces signed validation reports and uploads signature; verify via ci/verify_cosign_rekor.py.
+- Audit packages: audit/audit_package_collector.py + CI upload workflow ci/upload_audit_package.yml collect evidence for approvals.
+
+Operational notes
+- Replace placeholders (EVIDENCE_BUCKET, COSIGN_KMS_KEY_ARN, SUPERSET_API_URL, SECRET ARNs) before using.
+- CI and cluster contexts require secrets: KUBECONFIG, COSIGN_KMS_KEY_ARN, EVIDENCE_BUCKET, AWS creds.
+- Operators: run devsecops/ci/devsecops_ci_checks.yml to validate manifests and RBAC prior to merge.
+
*** End Patch
*** End Patch
