*** Begin Patch
*** Add File: k8s/postgres/statefulset-postgres.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: postgres
+  labels:
+    app: postgres
+spec:
+  ports:
+    - port: 5432
+      name: pg
+  clusterIP: None
+  selector:
+    app: postgres
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: postgres
+  labels:
+    app: postgres
+spec:
+  serviceName: "postgres"
+  replicas: 3
+  selector:
+    matchLabels:
+      app: postgres
+  template:
+    metadata:
+      labels:
+        app: postgres
+    spec:
+      containers:
+        - name: postgres
+          image: postgres:14
+          ports:
+            - containerPort: 5432
+          env:
+            - name: POSTGRES_DB
+              value: "aegis"
+            - name: POSTGRES_USER
+              value: "aegis"
+            - name: POSTGRES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: postgres-secret
+                  key: password
+          volumeMounts:
+            - name: pgdata
+              mountPath: /var/lib/postgresql/data
+          readinessProbe:
+            exec:
+              command:
+                - sh
+                - -c
+                - "pg_isready -U aegis -d aegis"
+            initialDelaySeconds: 10
+            periodSeconds: 10
+  volumeClaimTemplates:
+    - metadata:
+        name: pgdata
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 50Gi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/postgres/backup-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: postgres-backup
+  namespace: aegis
+spec:
+  schedule: "0 2 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: pg-backup
+              image: postgres:14
+              env:
+                - name: PGPASSWORD
+                  valueFrom:
+                    secretKeyRef:
+                      name: postgres-secret
+                      key: password
+              command:
+                - sh
+                - -c
+                - |
+                  pg_dump -h postgres -U aegis aegis | gzip > /backup/aegis-$(date +%F-%s).sql.gz
+              volumeMounts:
+                - name: backup
+                  mountPath: /backup
+          restartPolicy: OnFailure
+          volumes:
+            - name: backup
+              persistentVolumeClaim:
+                claimName: postgres-backup-pvc
+
*** End Patch
*** Begin Patch
*** Add File: scripts/setup_cosign_kms_gcp.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create a Google Cloud KMS key for cosign usage and print COSIGN_KMS_KEY URI
+# Requires: gcloud configured with appropriate project and permissions.
+PROJECT="${1:-$(gcloud config get-value project)}"
+LOCATION="${2:-global}"
+KEYRING="${3:-aegis-cosign}"
+KEYNAME="${4:-cosign-key}"
+
+gcloud kms keyrings create "${KEYRING}" --location "${LOCATION}" || true
+gcloud kms keys create "${KEYNAME}" --location "${LOCATION}" --keyring "${KEYRING}" --purpose asymmetric-signing --default-algorithm rsa-sign-pkcs1-2048-sha256 || true
+KEY_RESOURCE="gcpkms://projects/${PROJECT}/locations/${LOCATION}/keyRings/${KEYRING}/cryptoKeys/${KEYNAME}"
+echo "COSIGN_KMS_KEY=${KEY_RESOURCE}"
+echo "Grant signer permission to the CI service account:"
+echo "gcloud kms keys add-iam-policy-binding ${KEYNAME} --location=${LOCATION} --keyring=${KEYRING} --member='serviceAccount:CI_SA@${PROJECT}.iam.gserviceaccount.com' --role='roles/cloudkms.signerVerifier'"
+echo "Use the COSIGN_KMS_KEY value in your CI secrets."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/setup_cosign_kms_aws.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create AWS KMS key alias for cosign usage and print COSIGN_KMS_KEY URI
+# Requires: aws CLI configured
+KEY_ALIAS="${1:-alias/aegis-cosign}"
+
+KEY_ID=$(aws kms create-key --description "Cosign key for Aegis CI" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name "${KEY_ALIAS}" --target-key-id "${KEY_ID}"
+echo "COSIGN_KMS_KEY=awskms:///$KEY_ID"
+echo "Grant kms:Sign permission to your CI role/principal"
+
*** End Patch
*** Begin Patch
*** Add File: ci/verify_rekor_before_promote.py
+#!/usr/bin/env python3
+"""
+Check Rekor for a given manifest before promotion.
+Usage:
+  python ci/verify_rekor_before_promote.py --manifest manifest.json
+Exits 0 if Rekor entry exists, non-zero otherwise.
+"""
+import argparse
+import hashlib
+import subprocess
+import sys
+
+def parse():
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    return p.parse_args()
+
+def main():
+    args = parse()
+    with open(args.manifest, "rb") as fh:
+        digest = hashlib.sha256(fh.read()).hexdigest()
+    try:
+        out = subprocess.check_output(["rekor-cli", "search", "hash", digest], stderr=subprocess.STDOUT, timeout=20)
+        if out and b"UUID" in out:
+            print("Rekor entry found")
+            sys.exit(0)
+        else:
+            print("No Rekor entry")
+            sys.exit(2)
+    except subprocess.CalledProcessError as e:
+        print("rekor-cli error:", e.output.decode() if e.output else str(e))
+        sys.exit(3)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: policies/gatekeeper/constrainttemplate_signed_images.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: k8ssignedimages
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sSignedImages
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8ssignedimages
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Pod"
+          annotations := input.review.object.metadata.annotations
+          not annotations["aegis.cosign.signed"]
+          msg := sprintf("Pod %v does not have aegis.cosign.signed annotation", [input.review.object.metadata.name])
+        }
+
*** End Patch
*** Begin Patch
*** Add File: policies/gatekeeper/constraint_require_signed_images.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sSignedImages
+metadata:
+  name: require-signed-images
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
*** End Patch
*** Begin Patch
*** Add File: scripts/vault_publish_crl_s3.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Export CRL from Vault PKI and upload to S3 for devices to fetch.
+VAULT_ADDR="${VAULT_ADDR:-http://127.0.0.1:8200}"
+OUT="/tmp/aegis_crl.pem"
+S3_BUCKET="${1:-}"
+
+: "${S3_BUCKET:?S3 bucket required (e.g., my-bucket/aegis-crl)}"
+
+echo "Generating CRL from Vault"
+vault write -format=json pki_int/crl > /tmp/_crl.json
+jq -r '.data.certificate' /tmp/_crl.json > "${OUT}" || true
+echo "Uploading to s3://${S3_BUCKET}"
+aws s3 cp "${OUT}" "s3://${S3_BUCKET}/aegis_crl.pem"
+echo "Published CRL"
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/leader_election.py
+"""
+Leader election helper using Kubernetes Lease API.
+Acquires a Lease and only the holder should run scheduler work.
+Requires KUBERNETES_SERVICE_HOST in environment (i.e., running in cluster) and kubernetes python client.
+"""
+import os
+import time
+import uuid
+import logging
+from kubernetes import client, config
+
+LOG = logging.getLogger("aegis.leader_election")
+
+class K8sLeaderElection:
+    def __init__(self, lease_name="aegis-fleet-controller-leader", namespace="aegis", identity=None, lease_duration=30):
+        self.lease_name = lease_name
+        self.namespace = namespace
+        self.identity = identity or str(uuid.uuid4())
+        self.lease_duration = lease_duration
+        if os.getenv("KUBERNETES_SERVICE_HOST"):
+            config.load_incluster_config()
+            self.v1 = client.CoordinationV1Api()
+        else:
+            config.load_kube_config()
+            self.v1 = client.CoordinationV1Api()
+
+    def try_acquire(self):
+        now = client.V1MicroTime()
+        now = client.V1MicroTime(str(time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())))
+        try:
+            lease = self.v1.read_namespaced_lease(self.lease_name, self.namespace)
+            # attempt update if expired
+            holder = lease.spec.holder_identity
+            renew_time = lease.spec.renew_time
+            # Simple approach: try to replace lease holder
+            lease.spec.holder_identity = self.identity
+            lease.spec.renew_time = now
+            self.v1.replace_namespaced_lease(self.lease_name, self.namespace, lease)
+            LOG.info("acquired lease %s as %s", self.lease_name, self.identity)
+            return True
+        except client.exceptions.ApiException as e:
+            # create lease if not exists
+            if e.status == 404:
+                body = client.V1Lease(
+                    metadata=client.V1ObjectMeta(name=self.lease_name, namespace=self.namespace),
+                    spec=client.V1LeaseSpec(holder_identity=self.identity, renew_time=now, lease_duration_seconds=self.lease_duration)
+                )
+                try:
+                    self.v1.create_namespaced_lease(self.namespace, body)
+                    LOG.info("created and acquired lease %s as %s", self.lease_name, self.identity)
+                    return True
+                except Exception as ex:
+                    LOG.exception("failed to create lease: %s", ex)
+                    return False
+            LOG.exception("lease read error: %s", e)
+            return False
+        except Exception as e:
+            LOG.exception("leader election unexpected error: %s", e)
+            return False
+
*** End Patch
*** Begin Patch
*** Add File: ci/jetson_runner_ssh_provisioner.py
+#!/usr/bin/env python3
+"""
+SSH-based provisioner for Jetson/ARM runners.
+Reads a CSV of host,user,token and registers runners via SSH, installs runner as systemd.
+"""
+import csv
+import subprocess
+import sys
+import os
+from pathlib import Path
+
+def provision_row(host, user, token, repo, name):
+    # Uses ssh to run a provisioning script that is already present on host or copy via scp
+    script = Path("ci/hardware_runners/provision_runner_remote.sh")
+    # Ensure script is present
+    if not script.exists():
+        print("provision_runner_remote.sh missing")
+        return
+    subprocess.check_call(["scp", str(script), f"{user}@{host}:/tmp/provision_runner_remote.sh"])
+    cmd = f"ssh {user}@{host} 'bash /tmp/provision_runner_remote.sh {repo} {token} {name}'"
+    subprocess.check_call(cmd, shell=True)
+
+def main(csvfile):
+    with open(csvfile) as fh:
+        rdr = csv.DictReader(fh)
+        for i,row in enumerate(rdr):
+            host=row['host']; user=row['user']; token=row['token']; repo=row['repo']; name=row.get('name', f"runner-{i}")
+            print("Provisioning", host)
+            provision_row(host, user, token, repo, name)
+
+if __name__=="__main__":
+    if len(sys.argv)<2:
+        print("usage: jetson_runner_ssh_provisioner.py hosts.csv")
+        sys.exit(2)
+    main(sys.argv[1])
+
*** End Patch
*** Begin Patch
*** Add File: ci/hardware_runners/provision_runner_remote.sh
+#!/usr/bin/env bash
+set -euo pipefail
+REPO="$1"
+TOKEN="$2"
+NAME="$3"
+ARCH=$(uname -m)
+mkdir -p actions-runner && cd actions-runner
+curl -O -L https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-linux-${ARCH}-2.308.0.tar.gz
+tar xzf actions-runner-linux-${ARCH}-2.308.0.tar.gz
+./config.sh --url https://github.com/${REPO} --token "${TOKEN}" --name "${NAME}" --labels "self-hosted,jetson,${ARCH}" --unattended
+sudo tee /etc/systemd/system/github-runner.service >/dev/null <<EOF
+[Unit]
+Description=GitHub Actions Runner
+After=network.target
+
+[Service]
+Type=simple
+User=$(whoami)
+WorkingDirectory=$(pwd)
+ExecStart=$(pwd)/run.sh
+Restart=always
+RestartSec=5
+
+[Install]
+WantedBy=multi-user.target
+EOF
+sudo systemctl daemon-reload
+sudo systemctl enable --now github-runner.service
+echo "Runner installed and running"
+
*** End Patch
*** Begin Patch
*** Add File: security/opa/admission_signed_images.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sSignedImages
+metadata:
+  name: require-cosign-signed
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+  parameters:
+    message: "Images must be cosign-signed and have aegis.cosign.signed=true annotation"
+
*** End Patch
*** Begin Patch
*** Add File: docs/production_release_runbook.md
+# Production Release Runbook (Edge & Quantum)
+
+This runbook describes the steps and checks required to promote a model to edge and/or QPU production.
+
+Preconditions:
+- Manifest.json signed via cosign with KMS/HSM; Rekor entry present.
+- model_card contains required generative fields (if applicable).
+- Hardware validation passed for required target devices.
+- Controller, Postgres and scheduler are green and leader-elected instance available.
+- Billing quotas for QPU are sufficient for intended shots.
+
+Promotion steps:
+1) CI signs manifest using COSIGN_KMS_KEY and validates Rekor entry (ci/verify_rekor_before_promote.py).
+2) CI runs OPA checks and generative safety/hallucination tests.
+3) CI triggers hardware validation job on self-hosted runners (Jetson); job must return success.
+4) If promoting to QPU: run simulator validation, then call controller promote endpoint. Controller will enqueue job and scheduler will check quota and submit to provider.
+5) Controller will store provider receipt, call attestation and sign manifest that includes provider_job_id (quantum/attestation/provider_receipt.py).
+6) Monitor job via Prometheus dashboards and alerts (quantum_prom_rules.yaml). If error thresholds exceeded, trigger runbook.
+
+Rollback criteria:
+- High error rate (QPUErrorRateHigh) or edge high error rate -> abort rollout and mark release inactive.
+- Missing Rekor entry or unsigned image discovered -> abort promotion and revoke artifacts.
+
*** End Patch
*** Begin Patch
*** Add File: docs/acceptance/edge_scale_test_plan.md
+# Edge Scale Test Plan
+
+Purpose: validate controller and agent behavior at fleet scale (target 10k device polls/min).
+
+Steps:
+1) Provision 3+ controller replicas behind ingress; ensure Postgres is HA and scaled.
+2) Run Locust scenario (k8s/fleet/controller/load_test/locustfile.py) with many simulated device clients from multiple worker EC2 instances / k8s jobs.
+3) Monitor DB CPU/IO, controller pod CPU, pod restarts and telemetry ingestion lag.
+4) Tune DB pool sizes and split write-heavy telemetry ingestion into async workers (e.g., Kafka + workers).
+5) Validate canary rollout: create a release and start rollout with small pct, then increase; ensure automated rollback triggers on synthetic bad telemetry.
+
+Success criteria:
+- No lost requests, controller used <70% CPU and DB P95 latencies <200ms under target load.
+- Canary rollback occurs within configured time when injected failures observed.
+
*** End Patch
*** Begin Patch
*** Add File: docs/acceptance/quantum_prod_test_plan.md
+# Quantum Production Acceptance Test Plan
+
+Objective: validate provider integration, cost accounting, attestation and scheduler behavior.
+
+Steps:
+1) Configure QPU provider credentials in Vault and ensure scheduler can access them.
+2) Submit a set of simulator jobs and ensure simulatorâ†’promote flow works end-to-end.
+3) Submit small test jobs to real QPU with low shots; validate receipt stored, attestations created (Rekor entry), and MLflow provenance artifacts logged.
+4) Inject a simulated provider outage and verify scheduler routes to alternative provider or simulator and alerts fire.
+5) Validate quota enforcement by creating a job exceeding tenant quota and asserting job status becomes blocked_quota.
+
+Success criteria:
+- Provider receipts recorded and Rekor entries created for each QPU run.
+- Quota/chargeback records reflect usage and prevent overuse.
+- SLA thresholds (queue latency, error rate) respected in test conditions.
+
*** End Patch
*** End Patch
