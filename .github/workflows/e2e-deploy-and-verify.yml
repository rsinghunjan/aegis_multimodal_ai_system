  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
name: E2E Deploy & Verify â€” Staging (AWS OIDC)

on:
  workflow_dispatch:
    inputs:
      terraform_dir:
        description: 'Terraform overlay directory (infra/terraform/overlays/aws)'
        required: true
        default: infra/terraform/overlays/aws
      helm_image_tag:
        description: 'Image tag to deploy to staging'
        required: true

permissions:
  id-token: write
  contents: read

jobs:
  deploy-and-verify:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS via OIDC
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform init/plan/apply (staging overlay)
        working-directory: ${{ github.event.inputs.terraform_dir }}
        run: |
          terraform init -input=false
          terraform plan -out=tfplan -input=false
          terraform apply -input=false -auto-approve tfplan
          terraform output -json > tf_outputs.json

      - name: Export Terraform outputs
        working-directory: ${{ github.event.inputs.terraform_dir }}
        run: |
          cat tf_outputs.json
          ./../../../scripts/terraform/output_to_env.sh tf_outputs.json > $GITHUB_WORKSPACE/staging.env
          cat $GITHUB_WORKSPACE/staging.env

      - name: Set env
        run: |
          source $GITHUB_WORKSPACE/staging.env
          echo "OBJECT_STORE_BUCKET=$bucket_name" >> $GITHUB_ENV
          echo "DATABASE_URL=$database_url" >> $GITHUB_ENV

      - name: Configure kubeconfig for EKS (if outputs include cluster name)
        run: |
          if jq -e '.eks_cluster_name' ${{ github.event.inputs.terraform_dir }}/tf_outputs.json >/dev/null 2>&1; then
            CLUSTER_NAME=$(jq -r '.eks_cluster_name.value' ${{ github.event.inputs.terraform_dir }}/tf_outputs.json)
            aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "${{ secrets.AWS_REGION }}"
          else
            echo "No EKS cluster output found; skipping kubeconfig step."
          fi

      - name: Helm upgrade/install Aegis
        run: |
          helm upgrade --install aegis ./helm -n aegis --create-namespace \
            -f helm/values.aws.yaml \
            --set image.tag=${{ github.event.inputs.helm_image_tag }}

      - name: Wait for pods to be ready
        run: |
          kubectl -n aegis rollout status deployment/aegis-orchestrator --timeout=300s || true
          kubectl -n aegis get pods -o wide

      - name: Run verifier smoke (uses configured AWS creds via OIDC)
        env:
          OBJECT_STORE_TYPE: s3
        run: |
          python3 scripts/verify_model_signatures.py || true

      - name: Run canary simulation (simple)
        run: |
          pip install httpx
          python3 scripts/simulate_canary_load.py --url "http://aegis-staging.example.com/infer" --qps 10 --duration 60 --error-rate 0.02 || true

      - name: Snapshot DB backup & restore test (optional)
        run: |
          ./scripts/backup/backup_db.py || true
          echo "Manual restore test recommended; see docs/production_ready_runbook.md"
.github/workflows/e2e-deploy-and-veri
