*** Begin Patch
*** Add File: argo/apps/postgres-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-ml-postgres
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://charts.bitnami.com/bitnami
+    chart: postgresql
+    targetRevision: 12.10.7
+    helm:
+      values: |
+        global:
+          postgresql:
+            postgresqlDatabase: aegis_mlflow
+        persistence:
+          enabled: true
+          size: 5Gi
+        resources:
+          requests:
+            cpu: "100m"
+            memory: "256Mi"
+          limits:
+            cpu: "500m"
+            memory: "1Gi"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/minio-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-minio
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://charts.bitnami.com/bitnami
+    chart: minio
+    targetRevision: 6.0.0
+    helm:
+      values: |
+        persistence:
+          enabled: true
+          size: 10Gi
+        resources:
+          requests:
+            cpu: "100m"
+            memory: "256Mi"
+          limits:
+            cpu: "1"
+            memory: "1Gi"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/mlflow-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-mlflow
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    # Point this to the git repo containing the k8s/mlflow manifests
+    repoURL: https://github.com/yourorg/yourrepo
+    path: k8s/mlflow
+    targetRevision: HEAD
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/kserve-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-kserve
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://kserve.github.io/helm-charts
+    chart: kserve
+    targetRevision: 0.9.0
+    helm:
+      values: |
+        controller:
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "512Mi"
+            limits:
+              cpu: "2"
+              memory: "2Gi"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: k8s/mlflow/pvc.yaml
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: mlflow-artifacts-pvc
+  namespace: aegis
+spec:
+  accessModes:
+    - ReadWriteOnce
+  resources:
+    requests:
+      storage: 10Gi
+  storageClassName: standard
+
*** End Patch
*** Begin Patch
*** Add File: k8s/mlflow/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: mlflow
+  namespace: aegis
+  labels:
+    app: mlflow
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: mlflow
+  template:
+    metadata:
+      labels:
+        app: mlflow
+    spec:
+      containers:
+        - name: mlflow
+          # This is a small, demo-focused mlflow server image. Replace with your pinned image for production.
+          image: python:3.9-slim
+          imagePullPolicy: IfNotPresent
+          command: ["/bin/sh", "-c"]
+          args:
+            - |
+              pip install --no-cache-dir mlflow[extras]==2.3.2 && \
+              mkdir -p /mlflow/artifacts && \
+              mlflow server \
+                --backend-store-uri sqlite:///mlflow.db \
+                --default-artifact-root /mlflow/artifacts \
+                --host 0.0.0.0 --port 5000
+          ports:
+            - containerPort: 5000
+          volumeMounts:
+            - name: mlflow-artifacts
+              mountPath: /mlflow/artifacts
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "256Mi"
+            limits:
+              cpu: "500m"
+              memory: "1Gi"
+      volumes:
+        - name: mlflow-artifacts
+          persistentVolumeClaim:
+            claimName: mlflow-artifacts-pvc
+
*** End Patch
*** Begin Patch
*** Add File: k8s/mlflow/service.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: mlflow
+  namespace: aegis
+spec:
+  selector:
+    app: mlflow
+  ports:
+    - protocol: TCP
+      port: 5000
+      targetPort: 5000
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File: k8s/examples/train_script.py
+"""
+Simple demo training script that logs a scalar to MLflow.
+This script is intended to run inside a container that has tensorflow and mlflow installed.
+"""
+import time
+import mlflow
+import numpy as np
+
+def train(steps=10):
+    # dummy training loop
+    for step in range(steps):
+        loss = np.exp(-0.1 * step) + np.random.rand() * 0.01
+        mlflow.log_metric("loss", float(loss), step=step)
+        time.sleep(0.5)
+
+if __name__ == "__main__":
+    mlflow.set_tracking_uri("http://mlflow:5000")
+    mlflow.set_experiment("aegis-demo")
+    with mlflow.start_run():
+        mlflow.log_param("model", "demo-net")
+        train(steps=20)
+        mlflow.log_artifact(__file__)
+        print("Training complete")
+
*** End Patch
*** Begin Patch
*** Add File: k8s/examples/ml_training_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: demo-ml-training
+  namespace: aegis
+  labels:
+    app: demo-ml
+spec:
+  template:
+    spec:
+      restartPolicy: Never
+      containers:
+        - name: trainer
+          image: tensorflow/tensorflow:2.11.0
+          command: ["/bin/sh", "-c"]
+          args:
+            - pip install --no-cache-dir mlflow==2.3.2 && \
+              mkdir -p /scripts && \
+              cp /config/train_script.py /scripts/train_script.py && \
+              python /scripts/train_script.py
+          volumeMounts:
+            - name: train-script
+              mountPath: /config
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+      volumes:
+        - name: train-script
+          configMap:
+            name: demo-train-script
+
*** End Patch
*** Begin Patch
*** Add File: k8s/examples/demo-train-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: demo-train-script
+  namespace: aegis
+data:
+  train_script.py: |
+    #!/usr/bin/env python3
+    # wrapper to call the real script mounted at /config/train_script.py
+    from train_script import *
+    if __name__ == "__main__":
+      train(steps=20)
+
*** End Patch
*** Begin Patch
*** Add File: argo/apps/ml-examples-argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-ml-examples
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://github.com/yourorg/yourrepo
+    path: k8s/examples
+    targetRevision: HEAD
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: docs/converged_platform.md
+Converged AI/ML + Quantum on Aegis
+==================================
+
+Overview
+--------
+This repository now contains converged platform artifacts to run both AI/ML and Quantum workloads on Aegis. The goal is a single control plane for:
+- classical ML training and experiment tracking (MLflow)
+- model serving (KServe)
+- quantum experiments and provider orchestration (Qiskit/Braket adapters)
+
+What was added
+--------------
+- MLflow: lightweight MLflow server deployment with PVC-backed artifact storage (k8s/mlflow/*) and Argo app (argo/apps/mlflow-argo-app.yaml).
+- Postgres and MinIO: optional components managed as Argo apps (postgres-argo-app.yaml, minio-argo-app.yaml) if you want production-type stores.
+- Example training job: a Kubernetes Job that runs a simple TensorFlow training script and logs metrics to MLflow (k8s/examples/*).
+- KServe Argo app to enable model serving capability (argo/apps/kserve-argo-app.yaml).
+- Argo app to deploy the ML examples (argo/apps/ml-examples-argo-app.yaml).
+
+How to try it (quick)
+---------------------
+1) Ensure namespace `aegis` exists:
+   kubectl create ns aegis
+
+2) Sync Argo apps (or apply k8s manifests directly):
+   kubectl apply -f k8s/mlflow/pvc.yaml
+   kubectl apply -f k8s/mlflow/deployment.yaml
+   kubectl apply -f k8s/mlflow/service.yaml
+
+3) Deploy the example training job:
+   kubectl apply -f k8s/examples/demo-train-configmap.yaml
+   kubectl apply -f k8s/examples/ml_training_job.yaml
+
+4) Open MLflow (ClusterIP service) via port-forward:
+   kubectl port-forward svc/mlflow -n aegis 5000:5000
+   Visit: http://localhost:5000
+
+Converged workflows
+-------------------
+- Hybrid runs: training jobs can enqueue quantum jobs via the existing Redis queue or call quantum provider adapters directly (via the controller).
+- Experiment provenance: MLflow stores classical metrics and artifacts; quantum results are signed and attested via Rekor and can be referenced from MLflow runs.
+- Billing: cost_meter and billing reporter already present; they can be extended to capture GPU/TPU and QPU cost metadata.
+
+Next steps / recommendations
+---------------------------
+- For production MLflow use Postgres + MinIO (sync the Argo apps postgres/minio and update mlflow deployment env to use these stores).
+- Add GPU nodeSelectors/taints to training Job manifests for hardware accel (update resource requests/limits).
+- Integrate MLflow experiment lifecycle into your CI/CD (e.g., workflows that register models to KServe for serving).
+- Use the existing RBAC and Vault patterns to store dataset and cloud credentials, and run the operator bootstrap to populate secrets.
+
*** End Patch
*** Begin Patch
*** Update File: README.edge.md
@@
 - RBAC manifests (k8s/rbac/quantum-controller-rbac.yaml, k8s/rbac/quantum-admin-rbac.yaml)
 - Production checklist (docs/runbooks/prod_checklist.md)
+- Converged ML components:
+  - MLflow server manifests (k8s/mlflow/*) and examples (k8s/examples/*)
+  - Argo apps for Postgres and MinIO (argo/apps/postgres-argo-app.yaml, argo/apps/minio-argo-app.yaml)
+  - KServe app manifest for model serving (argo/apps/kserve-argo-app.yaml)
+
+If you plan to run both ML and quantum workloads:
+- Deploy MLflow (or enable Postgres+MinIO Argo apps), then run the example Job in k8s/examples to validate pipeline.
+- Use the Redis-backed queue and controller to orchestrate hybrid workflows that call quantum providers from ML experiments.
+
*** End Patch
*** End Patch
