*** Begin Patch
*** Add File: devcontainer/Dockerfile
+FROM mcr.microsoft.com/vscode/devcontainers/python:3.10
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    build-essential git curl jq sqlite3 apt-transport-https ca-certificates \
+    && rm -rf /var/lib/apt/lists/*
+
+RUN pip install --no-cache-dir mlflow[extras] nbformat jupyterlab qiskit qiskit-aer \
+    mlxtend black pre-commit requests
+
+WORKDIR /workspace
+EXPOSE 8888 5000 3000
+CMD [ "sleep", "infinity" ]
+
*** End Patch
*** Begin Patch
*** Add File: devcontainer/devcontainer.json
+{
+  "name": "Aegis Devcontainer",
+  "build": {
+    "dockerfile": "Dockerfile"
+  },
+  "workspaceFolder": "/workspace",
+  "extensions": [
+    "ms-python.python",
+    "ms-toolsai.jupyter",
+    "eamodio.gitlens"
+  ],
+  "forwardPorts": [8888, 5000, 3000],
+  "postCreateCommand": "pip install -r requirements-dev.txt || true"
+}
+
*** End Patch
*** Begin Patch
*** Add File: requirements-dev.txt
+mlflow[extras]
+nbformat
+jupyterlab
+black
+pre-commit
+qiskit
+requests
+
*** End Patch
*** Begin Patch
*** Add File: mlflow/docker-compose.yml
+version: '3.7'
+services:
+  mlflow:
+    image: python:3.10-slim
+    working_dir: /opt/mlflow
+    volumes:
+      - ./mlflow_store:/opt/mlflow/store
+    ports:
+      - "5000:5000"
+    command: >
+      sh -c "pip install mlflow && 
+      mlflow server --backend-store-uri sqlite:///mlflow_store/mlflow.db --default-artifact-root /opt/mlflow/store/artifacts --host 0.0.0.0 --port 5000"
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/feature_store.py
+#!/usr/bin/env python3
+"""
+Tiny feature store prototype (file-backed JSON).
+ - register_feature(name, vector)
+ - fetch_feature(name)
+ - list_features()
+ Intended for local dev / experiments. Replace with Feast or other store in production.
+"""
+import json
+import os
+from typing import Any
+
+STORE_PATH = os.environ.get("FEATURE_STORE_PATH", "feature_store/store.json")
+
+def _ensure():
+    d = os.path.dirname(STORE_PATH)
+    if d and not os.path.exists(d):
+        os.makedirs(d, exist_ok=True)
+    if not os.path.exists(STORE_PATH):
+        json.dump({}, open(STORE_PATH, "w"))
+
+def register_feature(name: str, value: Any):
+    _ensure()
+    data = json.load(open(STORE_PATH))
+    data[name] = {"value": value}
+    json.dump(data, open(STORE_PATH, "w"), indent=2)
+    return True
+
+def fetch_feature(name: str):
+    _ensure()
+    data = json.load(open(STORE_PATH))
+    return data.get(name)
+
+def list_features():
+    _ensure()
+    return list(json.load(open(STORE_PATH)).keys())
+
+if __name__ == "__main__":
+    # simple CLI for local dev
+    import sys
+    if len(sys.argv) < 2:
+        print("usage: feature_store.py register|fetch|list ...")
+        sys.exit(2)
+    cmd = sys.argv[1]
+    if cmd == "register":
+        register_feature(sys.argv[2], json.loads(sys.argv[3]))
+        print("ok")
+    elif cmd == "fetch":
+        print(fetch_feature(sys.argv[2]))
+    else:
+        print(list_features())
+
*** End Patch
*** Begin Patch
*** Add File: artifact_store/upload_artifact.sh
+#!/usr/bin/env bash
+# Simple local artifact uploader for dev: copy artifact into artifacts/ and record metadata
+set -euo pipefail
+ARTIFACT=$1
+DEST_DIR=${2:-artifacts/staging}
+mkdir -p "$DEST_DIR"
+cp "$ARTIFACT" "$DEST_DIR/"
+echo "{\"artifact\":\"$(basename $ARTIFACT)\",\"uploaded\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > "${DEST_DIR}/$(basename $ARTIFACT).meta.json"
+echo "Uploaded to $DEST_DIR/$(basename $ARTIFACT)"
+
*** End Patch
*** Begin Patch
*** Add File: experiment/templates/experiment_template.py
+#!/usr/bin/env python3
+"""
+Experiment template:
+ - trains a trivial model (placeholder)
+ - logs parameters, metrics, and artifacts to MLflow
+ - saves model artifact and model card (calls tools/generate_model_card.py)
+ - demonstrates reproducible experiment structure for developers
+"""
+import argparse
+import mlflow
+import json
+import os
+import random
+from datetime import datetime
+
+def run_experiment(name="demo", mlflow_uri=None):
+    if mlflow_uri:
+        mlflow.set_tracking_uri(mlflow_uri)
+    mlflow.set_experiment("dev_experiments")
+    with mlflow.start_run(run_name=f"{name}_{datetime.utcnow().isoformat()}") as run:
+        mlflow.log_param("seed", 42)
+        # fake training
+        val = random.random()
+        mlflow.log_metric("val_accuracy", val)
+        # write a small model file
+        model_path = f"/tmp/{name}_model.pt"
+        with open(model_path, "w") as fh:
+            fh.write("fake-model")
+        mlflow.log_artifact(model_path, artifact_path="models")
+        # generate model card
+        mc_out = f"/tmp/{name}_model_card.json"
+        os.system(f"python tools/generate_model_card.py --model-path {model_path} --out {mc_out}")
+        mlflow.log_artifact(mc_out, artifact_path="model_card")
+        print("Run complete. MLflow run id:", run.info.run_id)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--mlflow-uri", default=os.environ.get("MLFLOW_TRACKING_URI"))
+    p.add_argument("--name", default="demo")
+    args = p.parse_args()
+    run_experiment(args.name, args.mlflow_uri)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/precommit_checks.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Running SBOM & package pinning checks..."
+# SBOM: use syft if present
+if command -v syft >/dev/null 2>&1; then
+  syft . -o json > /tmp/sbom_precommit.json || true
+  echo "SBOM written to /tmp/sbom_precommit.json"
+else
+  echo "syft not installed; skipping SBOM generation"
+fi
+
+# Ensure requirements.txt exists and packages are pinned (no loose >= or >)
+if [ -f requirements.txt ]; then
+  if grep -E ">=|>|~=" requirements.txt >/dev/null 2>&1; then
+    echo "ERROR: requirements.txt contains unpinned packages (>=, >, ~=). Please pin exact versions."
+    exit 1
+  else
+    echo "requirements.txt appears pinned"
+  fi
+else
+  echo "No requirements.txt found; skipping pin check"
+fi
+
+# Notebook provenance for changed notebooks (git diff)
+NB_CHANGED=$(git diff --name-only --staged | grep '\.ipynb$' || true)
+if [ -n "$NB_CHANGED" ]; then
+  for nb in $NB_CHANGED; do
+    echo "Extracting provenance for $nb"
+    python sbom/notebook_provenance.py --notebook "$nb" --out "/tmp/prov_$(basename $nb .ipynb).json" || true
+  done
+fi
+
+echo "Precommit checks passed"
+
*** End Patch
*** Begin Patch
*** Add File: .pre-commit-config.yaml
+repos:
+  - repo: https://github.com/psf/black
+    rev: 23.3.0
+    hooks:
+      - id: black
+        language_version: python3
+  - repo: local
+    hooks:
+      - id: aegis-precommit
+        name: Aegis precommit checks (SBOM & pinning)
+        entry: scripts/precommit_checks.sh
+        language: script
+        files: '\\.(py|ipynb)$'
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/dev_to_staging.yml
+name: Dev -> Staging Promotion (one-click)
+on:
+  workflow_dispatch:
+    inputs:
+      artifact-path:
+        description: 'Path to artifact relative to repo (e.g. artifacts/latest/model.pt)'
+        required: true
+
+jobs:
+  promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Prepare environment
+        run: |
+          python -m pip install --upgrade pip
+          pip install cosign nbformat
+      - name: Verify artifact signature & Rekor entry
+        env:
+          COSIGN_PUBLIC_KEY: ${{ secrets.COSIGN_PUBLIC_KEY }}
+          REKOR_SERVER: ${{ secrets.REKOR_SERVER }}
+        run: |
+          ART="${{ github.event.inputs.artifact-path }}"
+          if [ ! -f "$ART" ]; then echo "Artifact $ART not found in repo"; exit 2; fi
+          if [ -n "$COSIGN_PUBLIC_KEY" ]; then
+            echo "$COSIGN_PUBLIC_KEY" > /tmp/cosign_pub.pem
+            cosign verify --key /tmp/cosign_pub.pem "$ART" || (echo "cosign verify failed" && exit 3)
+          else
+            echo "COSIGN_PUBLIC_KEY not set; failing promotion"
+            exit 4
+          fi
+          if [ -n "$REKOR_SERVER" ]; then
+            rekor-cli search --rekor_server "$REKOR_SERVER" --artifact "$ART" >/dev/null 2>&1 || (echo "No Rekor entry for $ART" && exit 5)
+          else
+            echo "REKOR_SERVER not set; failing promotion"
+            exit 6
+          fi
+      - name: Promote artifact to staging (local prototype)
+        run: |
+          ART="${{ github.event.inputs.artifact-path }}"
+          mkdir -p artifacts/staging
+          cp "$ART" artifacts/staging/
+          echo "Promoted $ART to artifacts/staging/"
+      - name: Create promotion note (MLflow tag & artifact metadata)
+        run: |
+          ART="${{ github.event.inputs.artifact-path }}"
+          echo "{\"promoted_by\":\"${{ github.actor }}\",\"promoted_at\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"artifact\":\"$ART\"}" > artifacts/staging/$(basename $ART).promote.json
+      - name: Upload promotion artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: promotion-artifacts
+          path: artifacts/staging/
+
*** End Patch
*** Begin Patch
*** Add File: docs/dev_sandbox_experiments.md
+# Dev Sandboxes, Experiment Platform & Reproducibility
+
+This patch adds:
+- Devcontainer Dockerfile and devcontainer.json for local Codespaces / VS Code dev containers.
+- A simple MLflow docker-compose for a local experiment server.
+- Tiny feature store prototype (feature_store/feature_store.py).
+- Local artifact uploader for dev (artifact_store/upload_artifact.sh).
+- Experiment template that logs to MLflow and generates a model card (experiment/templates/experiment_template.py).
+- Pre-commit checks that generate SBOMs (if syft present), ensure package pinning and capture notebook provenance (scripts/precommit_checks.sh + .pre-commit-config.yaml).
+- One-click dev->staging promotion GH Action that enforces cosign + Rekor verification before promoting artifact to a staging folder.
+
+Developer quickstart:
+1. Open the repo in Codespaces or use the devcontainer via VS Code Remote - Containers.
+2. Start MLflow: docker-compose -f mlflow/docker-compose.yml up
+3. Run an experiment:
+   python experiment/templates/experiment_template.py --mlflow-uri http://localhost:5000
+4. Use the precommit hooks:
+   pip install pre-commit && pre-commit install && pre-commit run --all-files
+5. Promote an artifact via Actions -> Dev -> Staging workflow (or run locally via script).
+
+Notes:
+- These are developer-stage tools to increase reproducibility and velocity. For production, integrate Feast, a durable artifact store (S3/GCS/Artifactory), and a managed MLflow backend (RDB + object store).
+- Ensure COSIGN public key and REKOR server settings are stored in GitHub secrets for promotions to work.
+
*** End Patch
*** End Patch
