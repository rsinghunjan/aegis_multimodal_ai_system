*** Begin Patch
*** Add File:compliance/dsar/README.md
+# DSAR (Data Subject Access Request) & Personal Data Handling
+
+This directory contains operator-run tooling and examples to:
+- receive and track DSARs (export / erase / rectify / port requests),
+- locate potential personal data in the repository or collected artifacts (DLP-style scan),
+- export a consistent evidence bundle for a subject request,
+- record proof that a DSAR was fulfilled (hashes + append-only log / optional Vault write).
+
+Important guidance
+- These scripts are templates and operator-run tools — they do not automatically delete or expose secrets.
+- Always review matches from `search_pii.py` manually before taking action; false positives are expected.
+- Move any discovered secret-like artifacts to Vault and follow your legal team's guidance before returning data to a data subject.
+
+Typical workflow (operator)
+1. A data subject submits a DSAR (email/form). Operator creates a DSAR ticket in the DSAR service:
+   curl -X POST http://localhost:8081/dsar -H "Content-Type: application/json" -d '{"subject_id":"alice@example.com","request_type":"export"}'
+2. Operator reviews matches (search results) and runs the export tool to assemble evidence bundle:
+   compliance/dsar/tools/export_bundle.sh --dsar-id <id> --output /tmp/dsar_bundle_<id>.tar.gz
+3. Record proof of fulfillment:
+   compliance/dsar/tools/record_proof.sh --dsar-id <id> --bundle /tmp/dsar_bundle_<id>.tar.gz
+4. Send the bundle to the requester via secure channel (SFTP / secure email) and document the delivery.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsar/api/dsar_service.py
+#!/usr/bin/env python3
+"""
+Lightweight DSAR service (FastAPI) for operator-run environments.
+
+Endpoints:
+ - POST /dsar          create a DSAR (export/erase/rectify/port)
+ - GET  /dsar/{id}     get status and matches
+ - POST /dsar/{id}/fulfill   mark as fulfilled and record proof (operator)
+
+This is a scaffold: in production you should place this behind auth, rate-limit, and an operator approval flow.
+"""
+import os
+import json
+import uuid
+import hashlib
+import subprocess
+from datetime import datetime
+from pathlib import Path
+from typing import List
+
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+
+ROOT = Path(__file__).resolve().parents[2]
+DSAR_DIR = ROOT / "data" / "dsar"
+DSAR_DIR.mkdir(parents=True, exist_ok=True)
+DSAR_LOG = DSAR_DIR / "dsar_requests.jsonl"
+
+app = FastAPI(title="Aegis DSAR Service (scaffold)")
+
+
+class DSARRequest(BaseModel):
+    subject_id: str
+    request_type: str  # export | erase | rectify | port
+    contact_email: str = ""
+    note: str = ""
+
+
+def write_request(record: dict):
+    record["created_at"] = datetime.utcnow().isoformat() + "Z"
+    with open(DSAR_LOG, "a") as f:
+        f.write(json.dumps(record) + "\n")
+
+
+def run_dlp_for_subject(subject_id: str) -> dict:
+    """
+    Run the repo-local DLP scan (search_pii.py) and return matches.
+    This is a conservative scan; operator must review results.
+    """
+    script = ROOT / "compliance" / "dsar" / "tools" / "search_pii.py"
+    if not script.exists():
+        return {"matches": [], "note": "dlp script not found"}
+    proc = subprocess.run(["python3", str(script), "--query", subject_id, "--json"], capture_output=True, text=True)
+    if proc.returncode != 0:
+        return {"matches": [], "error": proc.stderr.strip()}
+    try:
+        return json.loads(proc.stdout)
+    except Exception as e:
+        return {"matches": [], "error": f"failed parse: {e}"}
+
+
+@app.post("/dsar")
+def create_dsar(req: DSARRequest):
+    rid = str(uuid.uuid4())
+    rec = {
+        "id": rid,
+        "subject_id": req.subject_id,
+        "request_type": req.request_type,
+        "contact_email": req.contact_email,
+        "note": req.note,
+        "status": "pending",
+    }
+    # Run DLP search to surface potential matches
+    dlp = run_dlp_for_subject(req.subject_id)
+    rec["dlp"] = dlp
+    write_request(rec)
+    # write per-request file
+    with open(DSAR_DIR / f"{rid}.json", "w") as f:
+        json.dump(rec, f, indent=2)
+    return {"id": rid, "dlp_summary": {"matches": len(dlp.get("matches", []))}}
+
+
+@app.get("/dsar/{rid}")
+def get_dsar(rid: str):
+    fpath = DSAR_DIR / f"{rid}.json"
+    if not fpath.exists():
+        raise HTTPException(status_code=404, detail="DSAR not found")
+    with open(fpath) as f:
+        return json.load(f)
+
+
+@app.post("/dsar/{rid}/fulfill")
+def fulfill_dsar(rid: str, proof_hash: str = ""):
+    """
+    Mark DSAR as fulfilled. Operator should compute proof hash of the delivered bundle
+    and optionally call record_proof.sh to store proof in Vault / append-only log.
+    """
+    fpath = DSAR_DIR / f"{rid}.json"
+    if not fpath.exists():
+        raise HTTPException(status_code=404, detail="DSAR not found")
+    rec = json.loads(fpath.read_text())
+    rec["status"] = "fulfilled"
+    rec["fulfilled_at"] = datetime.utcnow().isoformat() + "Z"
+    rec["proof_hash"] = proof_hash
+    with open(fpath, "w") as f:
+        json.dump(rec, f, indent=2)
+    # append to global log
+    write_request({"id": rid, "event": "fulfilled", "proof_hash": proof_hash})
+    return {"id": rid, "status": "fulfilled"}
+
+
+if __name__ == "__main__":
+    import uvicorn
+
+    uvicorn.run(app, host="0.0.0.0", port=8081)
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsar/tools/search_pii.py
+#!/usr/bin/env python3
+"""
+Simple repository/artifact scanner to find likely personal data (emails, phone numbers, basic IDs).
+This scanner is intentionally simple and conservative — treat results as indicators for manual review.
+
+Usage:
+  python3 search_pii.py --query alice@example.com --json
+  python3 search_pii.py --path /path/to/artifacts --json
+"""
+import re
+import os
+import json
+import argparse
+from pathlib import Path
+
+EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
+PHONE_RE = re.compile(r"(?:\+?\d{1,3}[\s-]?)?(?:\(?\d{3}\)?[\s-]?)?\d{3}[\s-]?\d{4}")
+SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")
+
+def scan_file(path: Path, query=None):
+    matches = []
+    try:
+        text = path.read_text(errors="ignore")
+    except Exception:
+        return matches
+    if query and query in text:
+        matches.append({"type": "query", "snippet": text[:200]})
+    for regex, typ in ((EMAIL_RE, "email"), (PHONE_RE, "phone"), (SSN_RE, "ssn")):
+        for m in regex.finditer(text):
+            matches.append({"type": typ, "match": m.group(0), "pos": m.start()})
+    return matches
+
+def scan_path(root: Path, query=None, max_files=10000):
+    results = []
+    count = 0
+    for dirpath, dirnames, filenames in os.walk(root):
+        # skip common binary or dependency directories
+        if any(skip in dirpath for skip in ("/.git", "/node_modules", "/venv", "/.venv", "/__pycache__")):
+            continue
+        for fn in filenames:
+            if count >= max_files:
+                return results
+            fpath = Path(dirpath) / fn
+            matches = scan_file(fpath, query=query)
+            if matches:
+                results.append({"path": str(fpath), "matches": matches})
+            count += 1
+    return results
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--path", default=".", help="Path to scan")
+    p.add_argument("--query", default=None, help="Optional query string to look for (email/ID)")
+    p.add_argument("--json", action="store_true", help="Output JSON")
+    args = p.parse_args()
+
+    root = Path(args.path)
+    matches = scan_path(root, query=args.query)
+    out = {"matches": matches, "count": len(matches)}
+    if args.json:
+        print(json.dumps(out))
+    else:
+        for m in matches:
+            print("MATCH:", m["path"])
+            for mm in m["matches"][:5]:
+                print("  -", mm)
+        print("Total matches:", len(matches))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsar/tools/export_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Assemble an export bundle for a DSAR. This script collects:
+#  - MLflow artifacts (if present)
+#  - Argo workflow outputs (if present)
+#  - k8s pod logs in staging
+#  - DSAR metadata file
+#  - Rekor / signing artifacts (if present)
+#
+# Usage:
+#   export_bundle.sh --dsar-id <id> --output /tmp/dsar_<id>.tar.gz
+#
+DSAR_ID=""
+OUTPUT=""
+ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
+ARTIFACT_DIR="${ROOT}/artifacts"
+TMPDIR="/tmp/dsar_${RANDOM}"
+mkdir -p "$TMPDIR"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --dsar-id) DSAR_ID="$2"; shift 2;;
+    --output) OUTPUT="$2"; shift 2;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$DSAR_ID" ]; then
+  echo "Usage: $0 --dsar-id <id> --output <file>"
+  exit 2
+fi
+
+echo "Assembling DSAR export for $DSAR_ID into $OUTPUT"
+
+# copy DSAR metadata
+DSAR_META="$ROOT/data/dsar/${DSAR_ID}.json"
+if [ -f "$DSAR_META" ]; then
+  mkdir -p "$TMPDIR/dsar"
+  cp "$DSAR_META" "$TMPDIR/dsar/"
+fi
+
+# grab mlflow runs if present
+if [ -d "$ARTIFACT_DIR/mlflow" ]; then
+  cp -a "$ARTIFACT_DIR/mlflow" "$TMPDIR/" || true
+fi
+if [ -d "$ARTIFACT_DIR/mlruns" ]; then
+  cp -a "$ARTIFACT_DIR/mlruns" "$TMPDIR/" || true
+fi
+
+# grab argo outputs
+if [ -d "$ARTIFACT_DIR/argo" ]; then
+  cp -a "$ARTIFACT_DIR/argo" "$TMPDIR/" || true
+fi
+
+# grab k8s logs (staging)
+if [ -d "$ARTIFACT_DIR/k8s/staging" ]; then
+  cp -a "$ARTIFACT_DIR/k8s/staging" "$TMPDIR/k8s_staging" || true
+fi
+
+# grab rekor/cosign signatures if present
+if [ -d "$ARTIFACT_DIR/rekor" ]; then
+  cp -a "$ARTIFACT_DIR/rekor" "$TMPDIR/" || true
+fi
+
+# create a manifest of included files
+find "$TMPDIR" -type f -print > "$TMPDIR/manifest.txt"
+
+tar -czf "${OUTPUT:-/tmp/dsar_${DSAR_ID}.tar.gz}" -C "$TMPDIR" .
+
+echo "Created bundle: ${OUTPUT:-/tmp/dsar_${DSAR_ID}.tar.gz}"
+echo "Temporary files at $TMPDIR (operator: delete when done)"
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsar/tools/record_proof.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Record a proof hash for a DSAR bundle.
+# - computes sha256 of bundle
+# - writes an append-only log entry to data/dsar/proofs.jsonl
+# - optionally writes proof into Vault KV at secret/data/dsar/<id>/proof (requires VAULT_ADDR & VAULT_TOKEN)
+#
+DSAR_ID=""
+BUNDLE=""
+ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
+PROOF_LOG="$ROOT/data/dsar/proofs.jsonl"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --dsar-id) DSAR_ID="$2"; shift 2;;
+    --bundle) BUNDLE="$2"; shift 2;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$DSAR_ID" ] || [ -z "$BUNDLE" ]; then
+  echo "Usage: $0 --dsar-id <id> --bundle <bundle.tar.gz>"
+  exit 2
+fi
+
+if [ ! -f "$BUNDLE" ]; then
+  echo "Bundle not found: $BUNDLE"
+  exit 2
+fi
+
+SHA=$(sha256sum "$BUNDLE" | awk '{print $1}')
+TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
+ENTRY="{\"dsar_id\":\"$DSAR_ID\",\"bundle\":\"$BUNDLE\",\"sha256\":\"$SHA\",\"timestamp\":\"$TIMESTAMP\"}"
+echo "$ENTRY" >> "$PROOF_LOG"
+echo "Recorded proof in $PROOF_LOG"
+
+# Optionally write to Vault
+if [ -n "${VAULT_ADDR:-}" ] && [ -n "${VAULT_TOKEN:-}" ]; then
+  # kv v2 path: secret/data/dsar/<id>
+  KEY_PATH="secret/data/dsar/${DSAR_ID}"
+  echo "Storing proof in Vault at ${KEY_PATH#secret/data/}"
+  vault kv put "${KEY_PATH#secret/data/}" proof="$SHA" bundle="$BUNDLE" timestamp="$TIMESTAMP"
+fi
+
+echo "Proof: $SHA"
+
*** End Patch
*** Begin Patch
*** Add File:compliance/pseudonymization/pseudonymize.py
+#!/usr/bin/env python3
+"""
+Simple pseudonymization helpers.
+
+Provides:
+ - deterministic_pseudonym(value): HMAC-SHA256 with repo-specified salt (ENV PSEUDO_SALT) - useful for stable pseudonyms.
+ - tokenization placeholder: notes on using Vault transit or external tokenization service for reversible tokenization.
+"""
+import os
+import hmac
+import hashlib
+
+def deterministic_pseudonym(value: str, salt_env: str = "PSEUDO_SALT") -> str:
+    """
+    Deterministic pseudonymization using HMAC-SHA256 and salt from environment.
+    Not reversible. Use Vault transit or a token service for reversible tokenization.
+    """
+    salt = os.environ.get(salt_env)
+    if not salt:
+        raise RuntimeError(f"Environment variable {salt_env} must be set to a secure random value")
+    hm = hmac.new(salt.encode("utf-8"), value.encode("utf-8"), hashlib.sha256)
+    return hm.hexdigest()
+
+def example_usage():
+    print(deterministic_pseudonym("alice@example.com"))
+
+if __name__ == "__main__":
+    example_usage()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/retention/retention_policy.yaml
+# Retention policy template for Aegis artifacts and data sets
+#
+# Define dataset identifiers and retention days. Operator must fill and tailor to legal/regulatory needs.
+datasets:
+  mlflow_artifacts:
+    description: "MLflow artifacts (models, checkpoints)"
+    retention_days: 365
+  rekor_entries:
+    description: "Rekor entries for signed artifacts"
+    retention_days: 1095
+  dsar_temp_exports:
+    description: "Temporary DSAR export bundles"
+    retention_days: 30
+  k8s_logs:
+    description: "Kubernetes pod logs collected by orchestrator"
+    retention_days: 365
+
+# Operator note: implement secure deletion for backups and off-site copies as well.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/retention/enforcer.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Enforce retention policy by scanning artifact directories and deleting files older than retention days.
+# This is operator-run and must be reviewed before enabling.
+#
+ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
+POLICY="$ROOT/compliance/retention/retention_policy.yaml"
+ARTIFACT_DIR="$ROOT/artifacts"
+DRY_RUN=true
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --apply) DRY_RUN=false; shift ;;
+    --artifacts) ARTIFACT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ ! -f "$POLICY" ]; then
+  echo "Retention policy not found: $POLICY"
+  exit 2
+fi
+
+python3 - <<PY
+import yaml, os, time
+root = os.getcwd()
+with open("$POLICY") as f:
+    policy = yaml.safe_load(f)
+datasets = policy.get("datasets", {})
+for key, meta in datasets.items():
+    days = meta.get("retention_days", 0)
+    path = "$ARTIFACT_DIR"
+    cutoff = time.time() - days * 86400
+    print(f"Dataset: {key}, retention_days={days}")
+    # naive: remove files matching dataset key in path (operator should adapt)
+    for dirpath, dirnames, files in os.walk(path):
+        for fn in files:
+            if key in fn or key in dirpath:
+                fp = os.path.join(dirpath, fn)
+                mtime = os.path.getmtime(fp)
+                if mtime < cutoff:
+                    print("TO_DELETE:", fp)
+PY
+
+if [ "$DRY_RUN" = false ]; then
+  echo "Apply mode: deleting files identified above (operator must ensure backups exist)"
+  # For safety, the actual deletion should be done by the operator after review; implement carefully per environment
+  echo "Operator: implement the deletion step here (rm or secure-delete) after manual review"
+fi
+
+echo "Retention scan complete (dry_run=$DRY_RUN)."
+
*** End Patch
*** Begin Patch
*** Add File:compliance/incident/IR_PLAYBOOK.md
+# Incident Response Playbook (Breach / Data Incident)
+
+Purpose
+- Provide a documented, step-by-step IR flow for data breaches affecting personal data or PHI. This playbook is an operator-run guide and must be integrated into organizational incident response procedures.
+
+High-level timeline
+1. Detection (T0)
+2. Triage & containment (T0+0-24h)
+3. Forensic evidence collection (T0+0-48h)
+4. Notification to supervisory authority (GDPR: within 72 hours if required) (T0+72h)
+5. Notification to affected data subjects (as required) (T0+as soon as feasible)
+6. Remediation & lessons learned (post-incident)
+
+Immediate actions (operator checklist)
+- Triage: capture current system state (orchestrator artifact bundle recommended - run_full_end_to_end.sh)
+- Containment: isolate affected services, revoke compromised credentials (Vault rotate keys), block ingress IPs
+- Evidence: export Vault audit logs, application logs, MLflow run details, Argo workflow outputs into an artifacts bundle
+- Notify CISO & Legal immediately; open an incident ticket in your tracking system
+
+Breach notification template: see compliance/incident/notification_templates/breach_notification_email.md
+
+Forensic collection commands (examples)
+- Export Vault audit logs:
+  vault audit list
+  vault audit enable file file_path=/vault/logs/vault_audit.log
+- Run orchestrator to collect current state:
+  ./scripts/orchestrator/run_full_end_to_end.sh --skip-images --skip-tpu --skip-dgx
+
+Post-incident
+- Produce incident report with timeline, root cause, remediation steps, and evidence bundle.
+- Run a security retrospective and schedule remediation follow-ups.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/incident/notification_templates/breach_notification_email.md
+Subject: Data Security Incident Notification — [Short description]
+
+Dear [Data Protection Officer / Contact],
+
+We are contacting you to report a potential data security incident detected on [date/time UTC]. Summary:
+- Affected systems: [list services / clusters]
+- Scope: [number] users / records potentially affected (preliminary)
+- Data types: [e.g., names, emails, device identifiers, health data]
+- Containment status: [contained / in progress]
+
+Actions taken:
+1. Immediate containment (describe)
+2. Credentials rotated and compromised access removed
+3. Forensic artifacts collected and archived at [location / signed bundle]
+4. Ongoing investigation by [team]
+
+Next steps:
+- We will provide an interim report within 72 hours and a full report as soon as practical.
+- Contact for this incident: [CISO contact name, email, phone]
+
+Regards,
+[Your security team]
+
*** End Patch
*** Begin Patch
*** Add File:compliance/admin/HIPAA_POLICIES.md
+# HIPAA Administrative & Policy Checklist (template)
+
+This file contains a checklist of administrative controls and policy templates necessary to demonstrate HIPAA administrative safeguards. Operators and legal teams must adapt and complete these items prior to handling PHI.
+
+Required items
+- Designated Security Officer & Privacy Officer (name/contact)
+- Workforce training program (topics, frequency, attendance records)
+- Access Management policy (least privilege, role assignments)
+- Sanction policy for workforce violations
+- Contingency planning (backup, restore, availability)
+- Risk analysis and risk management plan (documented)
+- Vendor management & BAAs for all subprocessors
+- Incident Response policy (align with compliance/incident/IR_PLAYBOOK.md)
+
+Record-keeping recommendations
+- Keep training records, access logs, risk assessments, BAAs, and incident reports in a secure evidence repository (artifacts/security/).
+
*** End Patch
*** Begin Patch
*** Add File:compliance/admin/workforce_training.md
+# Workforce Security & Privacy Training — Template
+
+Contents (minimum)
+- Introduction to privacy & data protection (GDPR overview)
+- Handling of personal data & PHI
+- DSAR processing steps and confidentiality requirements
+- Using Vault for secrets (never commit credentials to Git)
+- Incident reporting and escalation
+- Role-based access and least privilege
+- Secure coding & data minimization for ML pipelines
+
+Suggested cadence
+- Onboarding: mandatory within 30 days
+- Refresher: annually (or on major process change)
+- Records: maintain attendance logs in artifacts/security/training/
+
+Quiz & acknowledgement
+- Operators must sign an acknowledgement after training; store signed acknowledgements securely.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/audit/AUDIT_CHECKLIST.md
+# Compliance & Audit Checklist (GDPR / HIPAA readiness)
+
+Use this checklist when preparing for a compliance review or third‑party audit.
+
+Legal & Governance
+- DPA / BAA agreements in place with providers and subprocessors
+- Appointed DPO / Security Officer
+- Data inventory & DPIA completed
+
+Technical
+- Vault HA + auto-unseal configured, audit logs forwarded to SIEM
+- KMS usage for encryption at rest and transit (HSM-backed keys for signing)
+- cosign + Rekor signing in CI and image verification at admission
+- DSAR tooling in place with logs/proof of fulfillment
+- Retention enforcement and secure deletion documented and tested
+
+Operational
+- Incident response playbook and breach templates exist and tested
+- Workforce training & acknowledgement records present
+- Penetration test & remediation tickets available
+
+Evidence artifacts
+- Orchestrator artifact bundle (artifacts/run_*)
+- Terraform outputs, Vault policies, MLflow runs, Rekor entries
+
*** End Patch
*** Begin Patch
*** Add File:compliance/audit/pen_test_request.md
+Subject: Penetration Test Request — Aegis Platform (staging)
+
+Purpose:
+Requesting an external penetration test engagement against the Aegis staging environment to validate security controls and provide remediation guidance.
+
+Scope (suggested):
+- Staging cluster (K8s) publicly exposed endpoints
+- Vault access controls and audit logging review
+- CI/CD pipelines, GitHub OIDC flow, and secret handling
+- Quantum controller & worker endpoints
+- Artifact storage (S3/GCS/COS) access controls
+
+Deliverables:
+- Executive summary, detailed findings, severity classification, and remediation recommendations
+- Sample exploit POC where safe
+- Final retest confirmation
+
+Scheduling / Contacts:
+- Primary contact: [Name, email, phone]
+- Preferred test window: [dates]
+
*** End Patch
*** Begin Patch
*** Add File:scripts/compliance/run_dlp_scan.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run repo-local DLP scan and write JSON results to artifacts/dlp_scan.json
+ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+OUT="${ROOT}/artifacts/dlp_scan.json"
+mkdir -p "$(dirname "$OUT")"
+python3 "$ROOT/compliance/dsar/tools/search_pii.py" --path "$ROOT" --json > "$OUT"
+echo "DLP scan results written to $OUT"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/compliance_dlp.yml
+name: Compliance DLP scan (PR)
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  dlp-scan:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Run DLP scan (repo)
+        run: |
+          pip install --user --upgrade pip
+          python3 compliance/dsar/tools/search_pii.py --path . --json > artifacts/dlp_scan.json || true
+          cat artifacts/dlp_scan.json
+      - name: Fail on likely PII (adjust threshold)
+        run: |
+          python3 - <<PY
+import json,sys
+data=json.load(open("artifacts/dlp_scan.json"))
+count=len(data.get("matches",[]))
+print("PII match count:",count)
+if count>0:
+    print("Potential PII found in PR. Please review and remove sensitive data.")
+    sys.exit(1)
+PY
+
*** End Patch
*** Begin Patch
*** Add File:docs/compliance_overview.md
+# Compliance support added to Aegis
+
+This patch adds scaffolding and operator-run tools to help handle:
+- Data Subject Access Requests (DSARs): lightweight DSAR API, DLP scan, export bundle & proof recording.
+- Pseudonymization & minimization helpers: deterministic pseudonymization function.
+- Retention policy template & enforcer (dry-run mode) for artifact pruning.
+- Incident response playbook and breach notification templates (GDPR 72h guidance & HIPAA).
+- Workforce training & HIPAA administrative checklist templates.
+- Audit & penetration test request templates.
+- CI integration: DLP scan on PRs to catch accidental PII commits.
+
+Operator responsibilities
+- Review all templates, update placeholders (Vault/KMS references), and integrate with your ticketing & secure communication channels.
+- Do not run deletion (retention enforcer apply) until you have backups and approvals.
+- Review DLP matches manually and follow legal guidance before exporting or deleting any personal data.
+
*** End Patch
*** End Patch
