*** Begin Patch
*** Add File:ops/operator_playbook.md
+# Operator Playbook — Bootstrap, Hardening, Backups, Secrets, Verifier and DSAR
+
+This playbook contains exact commands, sequence and checks to complete the remaining operational tasks required to bring Aegis from scaffold → hardened staging. Read it top-to-bottom and run commands from the cluster operator workstation with appropriate IAM privileges.
+
+Prerequisites
+- AWS CLI, kubectl, helm, terraform, gh (GitHub CLI), jq, yq installed and configured.
+- GitHub repo has CI images build workflow `image_scan_sign_rekor_block.yml`.
+- Secrets available: AWS creds for provisioning, GitHub token (GITHUB_TOKEN) for secret updates (or use GH Actions).
+- KUBECONFIG for staging cluster exported as KUBECONFIG.
+
+1) Run Terraform to provision KMS, S3 evidence bucket, RDS/Postgres (or use managed Helm chart)
+ - cd infra/terraform
+ - terraform init
+ - terraform plan -var 'aws_region=us-west-2'
+ - terraform apply -auto-approve -var 'aws_region=us-west-2'
+ - terraform output -json > /tmp/aegis_tf_output.json
+ - Verify outputs:
+   - jq .cosign_kms_arn /tmp/aegis_tf_output.json
+   - jq .evidence_bucket /tmp/aegis_tf_output.json
+
+2) Package & upload KMS rotation lambda used by Terraform (if present)
+ - bash infra/scripts/kms_rotation_package.sh
+ - Confirm file infra/terraform/kms_rotation_lambda.zip exists
+
+3) Populate SecretsManager/SSM and GitHub secrets
+ - ops/populate_and_apply_external_secrets.sh /tmp/aegis_tf_output.json
+ - Update GitHub secrets (if not using ExternalSecrets to sync):
+   - gh secret set COSIGN_KEY --body "awskms://..." --repo owner/repo
+   - gh secret set REKOR_URL --body "https://rekor...." --repo owner/repo
+   - gh secret set EVIDENCE_BUCKET --body "$(jq -r '.evidence_bucket.value' /tmp/aegis_tf_output.json)" --repo owner/repo
+
+4) Build & publish pipeline images (locally or via CI)
+ - docker build -t ghcr.io/<org>/aegis-tools:latest -f tools/Dockerfile .
+ - docker push ghcr.io/<org>/aegis-tools:latest
+ - docker build -t ghcr.io/<org>/aegis-train:latest -f train/Dockerfile .
+ - docker push ghcr.io/<org>/aegis-train:latest
+ - Or trigger your image CI: use GitHub Actions dispatch `image_scan_sign_rekor_block.yml`
+
+5) Run one-button bootstrap in staging (or use the GitHub one-button Action)
+ - Ensure KUBECONFIG points to staging
+ - ./ops/full_auto_provision_and_hardening.sh
+ - Or trigger `.github/workflows/one_button_bootstrap.yml` in GitHub UI (review logs)
+ - Validate ExternalSecrets synced: kubectl -n security get secret aegis-cosign
+
+6) Postgres HA sizing, PVC selection and backups
+ - Review registry/postgres/ha_values.yaml and set `size`, `storageClass` to your disk class.
+ - Apply chart: helm upgrade --install postgres bitnami/postgresql -n aegis -f registry/postgres/ha_values.yaml
+ - Enable backups: apply k8s/postgres/pgdump-to-s3-cronjob.yaml (will create scheduled pg_dump to S3 using credentials in ExternalSecrets). Verify S3 objects appear in evidence bucket under `backups/postgres/`.
+ - Consider enabling Velero for full cluster backup/restore.
+
+7) Rekor HA sizing and DR testing
+ - Edit rekor/rekor_values_ha.yaml to match production replica counts and storage class.
+ - helm upgrade --install rekor <chart> -n security -f rekor/rekor_values_ha.yaml
+ - Verify StatefulSet PVCs and run manual restore drill: take backup of rekor DB/prefix and test restore to a DR namespace.
+
+8) IRSA & least-privilege roles verification
+ - Use iam/irsa_roles.tpl.yaml as a template to create IAM roles for service accounts (cosign signer, reconcilers).
+ - Annotate service accounts (k8s/irsa/serviceaccount_cosign.yaml) with role ARN returned by Terraform or IAM team.
+ - Validate with: kubectl -n security exec -it <pod> -- aws sts get-caller-identity
+
+9) ExternalSecrets verification
+ - kubectl apply -f k8s/external-secrets/externalsecret_cosign.yaml
+ - kubectl -n security get externalsecret
+ - Apply k8s/external-secrets/verify_external_secrets_job.yaml to verify secrets exist as Kubernetes Secrets and have expected keys.
+
+10) Run verifier benchmark & propose thresholds
+ - Trigger CI job `.github/workflows/verifier_benchmark_and_pr.yml` or run locally:
+   - python ops/verifier_model_selector.py --runs 10 --out ops/verifier_recommendation.json
+   - python ops/eval_harness/run_eval.py --model <choice> --dev ops/eval_harness/eval_dataset_template/nli/dev.jsonl --out eval_results.json
+   - python ops/verifier/apply_threshold_pr.py (requires GITHUB_TOKEN)
+ - Review the PR and merge to apply thresholds to CI gating.
+
+11) Monitoring tuning & PagerDuty
+ - Edit monitoring/alertmanager/alertmanager_production.yml.tpl and monitoring/prometheus_rules_prod_tuned.yaml to set your PagerDuty/service keys and adjust thresholds to your SLOs.
+ - Run the runbook drill: .github/workflows/runbook_drill_and_audit.yml monthly to exercise alerting and tune rules.
+
+12) DSAR & legal process
+ - For complex DSAR cases, Aegis will create Approval Service requests. Legal must review these PRs/requests.
+ - Run dsar/automated_dsar.py in dry-run to gather candidates prior to deletion/redaction.
+
+13) Hardening & tests
+ - Replace any demo synthetic data in pipelines with real S3/lakeFS paths (see pipelines/ssl/*_prod variants).
+ - Run tests: pytest tests/integration -q
+ - Add unit tests for verifier, admission webhook and rekor reconciler to CI.
+
+14) Acceptance checks (smoke)
+ - MLflow runs present and tagged with dataset_snapshot
+ - Rekor health: curl ${REKOR_URL}/api/v1/log/publicKey
+ - ExternalSecrets synced to k8s secrets
+ - Postgres backups present in S3
+ - Approval Service reachable
+ - CI image scan/sign workflow completed successfully
+
+If you want, I will generate a follow-up patch that replaces the demo SSL scripts in Argo with the production S3/lakeFS versions and a templated `values.prod.yaml` file with commentary for your infra values.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/full_bootstrap_with_build.yml
+name: Full Bootstrap with Build & Manual Secret Approval
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: write
+  id-token: write
+
+jobs:
+  build-images:
+    runs-on: ubuntu-latest
+    outputs:
+      image_tags: ${{ steps.push.outputs.tags || '' }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to GHCR
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build and push aegis-tools
+        id: push
+        run: |
+          docker build -t ghcr.io/${{ github.repository_owner }}/aegis-tools:latest -f tools/Dockerfile .
+          docker push ghcr.io/${{ github.repository_owner }}/aegis-tools:latest
+          echo "::set-output name=tags::aegis-tools:latest"
+
+  await-secret-approval:
+    needs: build-images
+    runs-on: ubuntu-latest
+    environment:
+      name: bootstrap-approve
+    steps:
+      - name: Wait for operator approval (environment protection rules)
+        run: echo "Waiting for operator to approve secrets update in GitHub environment 'bootstrap-approve'"
+
+  run-bootstrap:
+    needs: await-secret-approval
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Configure AWS
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          aws-region: us-west-2
+          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+      - name: Run full auto-provision and hardening (operator-reviewed)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+        run: |
+          chmod +x ops/full_auto_provision_and_hardening.sh
+          ./ops/full_auto_provision_and_hardening.sh
+      - name: Trigger image CI workflow dispatch
+        uses: actions/github-script@v6
+        with:
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          script: |
+            await github.actions.createWorkflowDispatch({ owner: context.repo.owner, repo: context.repo.repo, workflow_id: 'image_scan_sign_rekor_block.yml', ref: 'main' });
+      - name: Run verification script
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+          APPROVAL_SERVICE_URL: ${{ secrets.APPROVAL_SERVICE_URL }}
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+        run: python3 pipelines/ssl/verify_ssl_signals.py
+
*** End Patch
*** Begin Patch
*** Add File:k8s/postgres/pgdump-to-s3-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: pgdump-to-s3
+  namespace: aegis
+spec:
+  schedule: "0 3 * * *" # daily at 03:00
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: pgdump
+              image: bitnami/postgres:13
+              env:
+                - name: PGPASSWORD
+                  valueFrom:
+                    secretKeyRef:
+                      name: postgres-credentials
+                      key: password
+                - name: PGHOST
+                  value: "postgresql.aegis.svc.cluster.local"
+                - name: PGUSER
+                  value: "aegis"
+                - name: BUCKET
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-s3-creds
+                      key: bucket
+                - name: AWS_ACCESS_KEY_ID
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-s3-creds
+                      key: access_key
+                - name: AWS_SECRET_ACCESS_KEY
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-s3-creds
+                      key: secret_key
+              command:
+                - sh
+                - -c
+                - |
+                  set -e
+                  ts=$(date -u +"%Y%m%dT%H%M%SZ")
+                  outfile="/tmp/aegis_postgres_dump_${ts}.sql.gz"
+                  pg_dump -h $PGHOST -U $PGUSER -d aegis_registry | gzip > $outfile
+                  aws s3 cp $outfile s3://$BUCKET/backups/postgres/$(basename $outfile)
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:k8s/external-secrets/verify_external_secrets_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: verify-external-secrets
+  namespace: security
+spec:
+  template:
+    spec:
+      containers:
+        - name: verify
+          image: python:3.10-slim
+          command: ["sh","-c"]
+          args:
+            - pip install boto3 kubernetes && python - <<'PY'
+from kubernetes import client, config
+import os, sys
+config.load_incluster_config()
+v1 = client.CoreV1Api()
+secrets = ["aegis-cosign","aegis-rekor"]
+ns="security"
+ok=True
+for s in secrets:
+  try:
+    v1.read_namespaced_secret(s, ns)
+    print("Found secret", s)
+  except Exception as e:
+    print("Missing secret", s, e); ok=False
+sys.exit(0 if ok else 2)
+PY
+      restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:iam/irsa_roles.tpl.yaml
+##
+# Template: create IAM role for IRSA binding (replace placeholders)
+#
+AWSPOLICY:
+  Version: "2012-10-17"
+  Statement:
+    - Effect: "Allow"
+      Action:
+        - "kms:Sign"
+        - "kms:DescribeKey"
+        - "kms:GetPublicKey"
+      Resource: "arn:aws:kms:REPLACE_REGION:REPLACE_ACCOUNT:key/REPLACE_KEY_ID"
+
+IAM_ROLE:
+  RoleName: "aegis-cosign-irsa-role"
+  AssumeRolePolicyDocument:
+    Version: "2012-10-17"
+    Statement:
+      - Effect: Allow
+        Principal:
+          Federated: "arn:aws:iam::REPLACE_ACCOUNT:oidc-provider/REPLACE_OIDC_PROVIDER"
+        Action:
+          - "sts:AssumeRoleWithWebIdentity"
+        Condition:
+          StringEquals:
+            "REPLACE_OIDC_PROVIDER:sub": "system:serviceaccount:security:cosign-signer"
+
+Notes:
+- Create role using Terraform or AWS console; attach AWSPOLICY as inline policy.
+- Replace OIDC provider, account and key ARNs.
+
*** End Patch
*** Begin Patch
*** Add File:infra/scripts/update_github_secret.sh
+#!/usr/bin/env bash
+#
+# Update GitHub repository secret using gh CLI (operator-run)
+#
+set -euo pipefail
+if [ -z "${GITHUB_REPOSITORY:-}" ] || [ -z "${GITHUB_TOKEN:-}" ]; then
+  echo "Set GITHUB_REPOSITORY and GITHUB_TOKEN environment variables"
+  exit 2
+fi
+secret_name=${1:-COSIGN_KEY}
+secret_value=${2:-}
+if [ -z "$secret_value" ]; then
+  echo "Usage: $0 SECRET_NAME SECRET_VALUE"
+  exit 2
+fi
+echo "$secret_value" | gh secret set "$secret_name" --repo "$GITHUB_REPOSITORY" --body -
+echo "Updated GitHub secret $secret_name"
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/threshold_workflow.md
+# Verifier Threshold Workflow (operator)
+
+1. Run benchmark and evaluation:
+ - CI: .github/workflows/verifier_benchmark_and_pr.yml or locally:
+   - python ops/verifier_model_selector.py --runs 10 --out ops/verifier_recommendation.json
+   - python ops/eval_harness/run_eval.py --model <ci_model> --dev ops/eval_harness/eval_dataset_template/nli/dev.jsonl --out eval_results.json
+
+2. Auto-propose PR:
+ - python ops/verifier/apply_threshold_pr.py (requires GITHUB_TOKEN + repository access).
+ - Review PR, ensure thresholds make sense for FP/FN tradeoffs and pipeline capacity.
+
+3. Merge after review:
+ - CI gating reads config/verifier_thresholds.yaml (implement reading in your verifier CI logic).
+
+4. Monitor:
+ - After merge, run verifier in staging and check alerting for VerifierHighLatencyP95 and false positive/negative rates.
+
*** End Patch
*** Begin Patch
*** Add File:tests/verifier/test_threshold_pipeline.py
+import json
+import os
+
+def test_verifier_recommendation_file():
+    path = "ops/verifier_recommendation.json"
+    assert os.path.exists(path), "Run verifier_model_selector CI job to produce recommendation"
+    with open(path) as f:
+        j = json.load(f)
+    assert "ci_model" in j and "offline_model" in j
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_admission_webhook.py
+import requests
+import os
+
+def test_admission_webhook_health():
+    url = os.environ.get("ADMISSION_WEBHOOK_HEALTH","http://gateway.aegis.svc.cluster.local/health")
+    r = requests.get(url, timeout=5)
+    assert r.status_code in (200,204)
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_rekor_remediate.py
+import subprocess
+import os
+
+def test_rekor_remediate_runs():
+    rc = subprocess.call(["python","rekor/rekor_reconcile_remediate.py"])
+    assert rc in (0,1)
+
*** End Patch
*** Begin Patch
*** Add File:pipelines/ssl/infer_and_score_prod.py
+#!/usr/bin/env python3
+"""
+Production-friendly infer+score: load unlabeled dataset from S3 or lakeFS,
+apply saved base model artifact (downloaded from MLflow), produce pseudo labels CSV with confidences.
+"""
+import os
+import mlflow
+import boto3
+import tempfile
+import pandas as pd
+import pickle
+import numpy as np
+
+MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow.aegis.svc.cluster.local:5000")
+UNLABELED_S3_PATH = os.environ.get("UNLABELED_S3_PATH", "")
+MLFLOW_MODEL_RUN_ID = os.environ.get("BASE_MODEL_RUN_ID", "")
+mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
+s3 = boto3.client("s3")
+
+def download_unlabeled_from_s3(s3uri, local_path="/tmp/unlabeled.csv"):
+    # s3uri = s3://bucket/path/to/file.csv or prefix; for prefix we list and concatenate small files
+    if not s3uri.startswith("s3://"):
+        raise SystemExit("UNLABELED_S3_PATH must be s3://...")
+    parts = s3uri[5:].split("/",1)
+    bucket = parts[0]
+    key = parts[1] if len(parts)>1 else ""
+    tmp = local_path
+    if key.endswith(".csv"):
+        s3.download_file(bucket, key, tmp)
+    else:
+        # if prefix, concatenate first N objects (safety)
+        objs = s3.list_objects_v2(Bucket=bucket, Prefix=key, MaxKeys=50).get("Contents",[])
+        frames=[]
+        for o in objs:
+            k = o["Key"]
+            tmpf = tempfile.mktemp(suffix=".csv")
+            s3.download_file(bucket, k, tmpf)
+            frames.append(pd.read_csv(tmpf))
+        if frames:
+            pd.concat(frames, ignore_index=True).to_csv(tmp, index=False)
+        else:
+            pd.DataFrame().to_csv(tmp, index=False)
+    return tmp
+
+def load_model_from_mlflow(run_id):
+    if not run_id:
+        raise SystemExit("BASE_MODEL_RUN_ID not provided")
+    artifact_path = f"runs:/{run_id}/model/model.pkl"
+    local = "/tmp/base_model.pkl"
+    mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path="model/model.pkl", dst_path="/tmp")
+    with open(local,"rb") as f:
+        return pickle.load(f)
+
+def infer_and_write(model, data_csv, out="/tmp/pseudo_labels.csv"):
+    df = pd.read_csv(data_csv)
+    feature_cols = [c for c in df.columns if c.startswith("x")]
+    X = df[feature_cols].values
+    probs = model.predict_proba(X)
+    preds = model.predict(X)
+    confs = np.max(probs, axis=1)
+    outdf = df[feature_cols].copy()
+    outdf["pseudo_label"] = preds
+    outdf["confidence"] = confs
+    outdf.to_csv(out, index=False)
+    return out
+
+def main():
+    if not UNLABELED_S3_PATH:
+        print("UNLABELED_S3_PATH not set; exiting")
+        raise SystemExit(2)
+    local_unl = download_unlabeled_from_s3(UNLABELED_S3_PATH)
+    model = load_model_from_mlflow(MLFLOW_MODEL_RUN_ID)
+    out = infer_and_write(model, local_unl)
+    # log artifact back to MLflow under a new run
+    with mlflow.start_run() as run:
+        mlflow.log_artifact(out, artifact_path="pseudo_labels")
+    print("Wrote pseudo labels to", out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/monitoring_tuning.md
+# Monitoring Tuning & Alerting Runbook
+
+1) Tune Alert thresholds:
+ - Start with conservative for critical signals (Rekor down >3m, Postgres replica lag >30s).
+ - Run alert drill and record false positives for 2 weeks, then adjust `prometheus/prometheus_rules_prod_tuned.yaml`.
+
+2) Inhibition rules:
+ - Ensure Alertmanager has inhibit_rules for warning -> critical to avoid multiple pages.
+
+3) PagerDuty mapping:
+ - Map `severity: critical` to on-call rotation, `severity: warning` to Slack.
+ - Update monitoring/alertmanager/alertmanager_production.yml.tpl with your PagerDuty service key.
+
+4) Alert fatigue mitigation:
+ - Use group_wait/group_interval/repeat_interval sensible defaults; use silence windows during maintenance.
+
+5) Run drills:
+ - Schedule .github/workflows/runbook_drill_and_audit.yml monthly and review outcomes.
+
*** End Patch
*** End Patch
