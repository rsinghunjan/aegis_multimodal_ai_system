*** Begin Patch
*** Add File:serving/kserve/README.md
+# KServe / Triton serving for Aegis
+
+This folder contains example manifests and small runbooks to deploy a GPU-backed Triton inference service using KServe, plus autoscaling and a simple inference benchmark.
+
+What is included
+- Triton InferenceService example (serving/kserve/triton-inferenceservice.yaml)
+- HPA autoscaler example to complement KServe scaling (serving/kserve/ksvc-hpa.yaml)
+- Simple Python inference benchmark script (serving/kserve/run_inference_bench.sh)
+- Notes on metrics & Prometheus monitoring for SLOs
+
+Operator notes
+- Replace image and model repository placeholders with your validated Triton model repository.
+- Ensure nodes have GPUs and that KServe & KNative (or appropriate autoscaling components) are installed.
+- For production, use KServe autoscaling (KServe inference autoscaler/KPA/KEDA) and configure GPU node selectors and tolerations.
+
*** End Patch
*** Begin Patch
*** Add File:serving/kserve/triton-inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-triton-model"
+  namespace: "ml"
+spec:
+  predictor:
+    minReplicas: 1
+    maxReplicas: 4
+    podSpec:
+      containers:
+      - name: kserve-container
+        # Replace with a validated Triton server image and model repo mount
+        image: "nvcr.io/nvidia/tritonserver:24.08-py3"
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+        env:
+        - name: MODEL_REPOSITORY
+          value: "s3://aegis-model-repo/triton/aegis-model"
+    triton:
+      runtimeVersion: "2.26.0"
+      resources:
+        limits:
+          nvidia.com/gpu: 1
+      storage:
+        s3:
+          endpoint: "s3.amazonaws.com"   # Replace with your object store endpoint
+          bucket: "aegis-model-repo"
+          region: "us-east-1"
+          accessKeyIDSecret:
+            name: aegis-s3-creds
+            key: access_key
+          secretAccessKeySecret:
+            name: aegis-s3-creds
+            key: secret_key
+
+# Notes:
+# - Namespace 'ml' must exist.
+# - For GCP/Azure/IBM COS, adapt storage stanza to provider-specific connector.
+
*** End Patch
*** Begin Patch
*** Add File:serving/kserve/ksvc-hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-triton-hpa
+  namespace: ml
+spec:
+  scaleTargetRef:
+    apiVersion: serving.kserve.io/v1beta1
+    kind: InferenceService
+    name: aegis-triton-model
+  minReplicas: 1
+  maxReplicas: 4
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
+# Note: For GPU-aware autoscaling, consider KEDA or cluster-level policies; this HPA demonstrates CPU-based scaling as a fallback.
+
*** End Patch
*** Begin Patch
*** Add File:serving/kserve/run_inference_bench.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple inference benchmark script that sends HTTP requests to a KServe inference endpoint.
+#
+# Usage:
+#   ./run_inference_bench.sh --host <inference-host> --path <predict-path> --requests 100
+#
+HOST=""
+PATH_URL="/v1/models/aegis-triton-model:predict"
+REQS=100
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --host) HOST="$2"; shift 2;;
+    --path) PATH_URL="$2"; shift 2;;
+    --requests) REQS="$2"; shift 2;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$HOST" ]; then
+  echo "Usage: $0 --host <inference-host> [--path <predict-path>] [--requests N]"
+  exit 1
+fi
+
+URL="http://${HOST}${PATH_URL}"
+echo "Benchmarking $URL with $REQS requests"
+
+# Simple looped requests; for production use wrk/locust/ab
+START=$(date +%s)
+for i in $(seq 1 $REQS); do
+  curl -s -X POST "$URL" -H "Content-Type: application/json" -d '{"inputs":[{"name":"input__0","shape":[1,3,224,224],"datatype":"FP32","data":[0] }]}' >/dev/null &
+done
+wait
+END=$(date +%s)
+ELAPSED=$((END-START))
+echo "Sent $REQS requests in ${ELAPSED}s -> $(echo "$REQS / $ELAPSED" | bc -l) rps"
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/README.md
+# MLflow registry promotion, signing and Rekor automation
+
+This directory contains scripts and a CI workflow to:
+- Promote a model version in MLflow (example uses MLflow REST API).
+- Download the model artifact, sign it with Vault Transit (via scripts/vault/vault_transit_sign.sh), and record the signature in Rekor (cosign --rekor-server).
+- Tag the MLflow model with promotion and provenance metadata.
+
+Files added
+- mlflow/scripts/mlflow_promote_and_sign.sh
+- .github/workflows/mlflow_promote_and_sign.yml
+- runbooks/mlflow_promotion_runbook.md
+
+Operator notes
+- Requires VAULT_ADDR & REKOR_SERVER repo secrets (or a Vault OIDC auth flow configured).
+- MLflow server URL and API token expected via environment variables or CI secret.
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/scripts/mlflow_promote_and_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# mlflow_promote_and_sign.sh
+# Promote a model version in MLflow, download the model artifact, sign via Vault Transit and record Rekor entry.
+#
+# Required env:
+#   MLFLOW_TRACKING_URI, MLFLOW_TOKEN
+#   VAULT_ADDR (and VAULT_TOKEN or OIDC flow available in CI)
+#   REKOR_SERVER
+#
+usage() {
+  cat <<EOF
+Usage: $0 --model-name <name> --version <version> --promote-to <stage>
+
+Example:
+  MLFLOW_TRACKING_URI=https://mlflow.example.com MLFLOW_TOKEN=xxx VAULT_ADDR=https://vault.example.com REKOR_SERVER=https://rekor.example.com \\
+    ./mlflow_promote_and_sign.sh --model-name my-model --version 3 --promote-to Production
+EOF
+  exit 1
+}
+
+MODEL_NAME=""
+VERSION=""
+PROMOTE_TO=""
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --model-name) MODEL_NAME="$2"; shift 2;;
+    --version) VERSION="$2"; shift 2;;
+    --promote-to) PROMOTE_TO="$2"; shift 2;;
+    *) usage;;
+  esac
+done
+
+if [ -z "$MODEL_NAME" ] || [ -z "$VERSION" ] || [ -z "$PROMOTE_TO" ]; then
+  usage
+fi
+
+if [ -z "${MLFLOW_TRACKING_URI:-}" ] || [ -z "${MLFLOW_TOKEN:-}" ]; then
+  echo "Set MLFLOW_TRACKING_URI and MLFLOW_TOKEN environment variables"
+  exit 2
+fi
+
+API="$MLFLOW_TRACKING_URI/api/2.0/preview/mlflow"
+
+echo "Promoting model $MODEL_NAME version $VERSION to stage $PROMOTE_TO"
+curl -s -X POST -H "Authorization: Bearer ${MLFLOW_TOKEN}" -H "Content-Type: application/json" \
+  -d "{\"name\": \"$MODEL_NAME\", \"version\": \"$VERSION\", \"stage\": \"$PROMOTE_TO\", \"archive_existing_versions\": false}" \
+  "$API/transition-requests/create" | jq .
+
+# Retrieve artifact download URI for the model version
+echo "Fetching model version info..."
+MV=$(curl -s -H "Authorization: Bearer ${MLFLOW_TOKEN}" "$API/model-versions/get?name=${MODEL_NAME}&version=${VERSION}")
+ART_URI=$(echo "$MV" | jq -r '.model_version.source')
+echo "Model artifact source: $ART_URI"
+
+# Download model artifact (assumes artifact is accessible via HTTP/GS/S3; operator may need to adapt)
+TMPDIR=$(mktemp -d)
+echo "Downloading model artifact to $TMPDIR/artifact.tar.gz (operator: adapt download method to your storage)"
+# Naive attempt using cp (if source is local path) or use aws/gsutil/ibmcloud COS tooling as appropriate
+if [[ "$ART_URI" == s3* ]]; then
+  echo "S3 artifact detected; use aws cli to download (operator must ensure credentials are present)"
+  aws s3 cp "$ART_URI" "$TMPDIR/artifact.tar.gz"
+elif [[ "$ART_URI" == http* ]]; then
+  curl -sSL "$ART_URI" -o "$TMPDIR/artifact.tar.gz"
+else
+  # fallback - copy if path
+  cp -r "$ART_URI" "$TMPDIR/"
+  tar -czf "$TMPDIR/artifact.tar.gz" -C "$TMPDIR" .
+fi
+
+SIG_OUT="$TMPDIR/artifact.sig"
+echo "Signing artifact via Vault Transit..."
+chmod +x ../../scripts/vault/vault_transit_sign.sh
+VAULT_ADDR="${VAULT_ADDR:-}" VAULT_TOKEN="${VAULT_TOKEN:-}" ../../scripts/vault/vault_transit_sign.sh --key aegis-signing-key --file "$TMPDIR/artifact.tar.gz" --out "$SIG_OUT"
+
+echo "Uploading signature to Rekor via cosign verify/record semantics (cosign will upload to Rekor when signing an image; for blobs use cosign sign-blob --rekor-server ...)."
+if ! command -v cosign >/dev/null 2>&1; then
+  echo "cosign binary required; install in CI image"
+  exit 2
+fi
+cosign sign-blob --key "vault://transit/keys/aegis-signing-key" --rekor-server "${REKOR_SERVER:-https://rekor.sigstore.dev}" --output-signature "$SIG_OUT" --output-cose "$TMPDIR/artifact.cose" "$TMPDIR/artifact.tar.gz" || true
+
+echo "Attach promotion metadata to MLflow model version (provenance)"
+curl -s -X POST -H "Authorization: Bearer ${MLFLOW_TOKEN}" -H "Content-Type: application/json" \
+  -d "{\"name\":\"$MODEL_NAME\",\"version\":\"$VERSION\",\"description\":\"Promoted to $PROMOTE_TO; signed via Vault transit; rekor_server=${REKOR_SERVER:-https://rekor.sigstore.dev}\"}" \
+  "$API/model-versions/set-tag" | jq .
+
+echo "Promotion and signing completed. Artifacts in $TMPDIR"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/mlflow_promote_and_sign.yml
+name: MLflow Promote & Sign
+
+on:
+  workflow_dispatch:
+    inputs:
+      model-name:
+        required: true
+        type: string
+      version:
+        required: true
+        type: string
+      promote-to:
+        required: true
+        type: string
+
+permissions:
+  contents: read
+  id-token: write
+
+jobs:
+  promote-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Install deps
+        run: |
+          sudo apt-get update && sudo apt-get install -y jq curl
+          curl -sSL https://github.com/sigstore/cosign/releases/download/v2.1.0/cosign-linux-amd64 -o /usr/local/bin/cosign
+          chmod +x /usr/local/bin/cosign
+          wget -qO- https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_linux_amd64.zip | bsdtar -xvf- -C /usr/local/bin
+          chmod +x /usr/local/bin/vault
+
+      - name: Run promote and sign
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          MLFLOW_TOKEN: ${{ secrets.MLFLOW_TOKEN }}
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          REKOR_SERVER: ${{ secrets.REKOR_SERVER }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+        run: |
+          chmod +x mlflow/scripts/mlflow_promote_and_sign.sh
+          ./mlflow/scripts/mlflow_promote_and_sign.sh --model-name "${{ github.event.inputs.model-name }}" --version "${{ github.event.inputs.version }}" --promote-to "${{ github.event.inputs.promote-to }}"
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/runbooks/mlflow_promotion_runbook.md
+# MLflow promotion & signing runbook
+
+Overview
+- This runbook explains how to run the MLflow promotion + sign workflow and how to validate provenance recorded in Rekor.
+
+Prereqs
+- MLflow server URL and API token (MLFLOW_TRACKING_URI, MLFLOW_TOKEN)
+- Vault transit signing configured with key aegis-signing-key
+- REKOR_SERVER reachable (use sigstore public Rekor for testing)
+- cosign & vault installed in the CI runner (workflow installs them)
+
+Steps
+1. Trigger the GitHub Action "MLflow Promote & Sign" or run mlflow/scripts/mlflow_promote_and_sign.sh locally with env vars set.
+2. After workflow completes, verify:
+   - cosign verify --rekor-server $REKOR_SERVER --key <pub> image (for images) or cosign verify-blob --rekor-server <rekor> --signature <sig> <artifact.tar.gz> (for blobs)
+   - MLflow model version has tag describing promotion and provenance.
+
+Validation
+- Use scripts/validate/* to run additional tests (artifact present, Rekor entry, webhook acceptance).
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/feast/README.md
+# Feast feature store scaffold
+
+This directory contains a minimal Feast feature-repo scaffold and a sample ingestion script to demonstrate feature ingestion and retrieval.
+
+What is included
+- feature_repo.yaml — basic Feast repo configuration
+- ingest_sample.py — example script to write sample features to the online store
+
+Operator notes
+- This scaffold is provider agnostic. For production, choose an online store (Redis/Bigtable) and offline store (BigQuery/S3) supported by Feast.
+- Configure access credentials as secrets and update the config accordingly.
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/feast/feature_repo.yaml
+project: aegis-features
+registry: registry.db
+provider: local
+online_store:
+  type: redis
+  connection_string: "redis://redis:6379"
+offline_store:
+  type: file
+  path: ./offline_store
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/feast/ingest_sample.py
+#!/usr/bin/env python3
+"""
+Sample Feast ingestion script (local provider).
+Requires feast package: pip install feast
+"""
+from datetime import datetime
+from feast import FeatureStore, Entity, FeatureView, Field, ValueType
+import pandas as pd
+
+fs = FeatureStore(repo_path=".")
+
+def create_demo_features():
+    # Define an entity and feature view (for local demo)
+    entity = Entity(name="user_id", value_type=ValueType.INT64, description="user id")
+    # sample feature DataFrame
+    df = pd.DataFrame({
+        "user_id": [1,2,3],
+        "feature_x": [0.1, 0.2, 0.3],
+        "event_timestamp": [datetime.utcnow(), datetime.utcnow(), datetime.utcnow()]
+    })
+    # For local provider, write to offline store directory
+    df.to_parquet("./offline_store/demo_features.parquet")
+    print("Wrote demo features to offline_store/demo_features.parquet")
+
+if __name__ == "__main__":
+    create_demo_features()
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/lakefs/README.md
+# lakeFS / dataset versioning scaffold
+
+This directory documents using lakeFS (or DVC) for dataset versioning. We include a brief example using lakeFS concepts:
+- create repository, branch, and add dataset snapshots
+- use dataset snapshots as reproducible inputs for training
+
+Operator notes
+- Install lakeFS and configure your object store (S3/GCS/IBM COS) per lakeFS docs.
+- Replace placeholders with your bucket and credentials.
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/dvc/README.md
+# DVC dataset versioning scaffold
+
+This folder contains guidelines to version datasets using DVC:
+- create dvc.yaml pipelines for data ingestion and preprocessing
+- store data in remote storage (S3/GCS/COS)
+- use dvc repro to reproduce datasets for training
+
+Operator notes
+- Example (init + remote set):
+  dvc init
+  dvc remote add -d storage s3://aegis-data
+  dvc add data/raw
+  git add data.dvc .gitignore
+  dvc push
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/acceptance_ai_platform.md
+# AI Platform acceptance tests & runbook
+
+This runbook consolidates acceptance tests for the three new capabilities: Serving, MLflow promotion/signing, and Feature store / dataset versioning.
+
+Serving (KServe/Triton)
+-----------------------
+1. Deploy KServe & supporting CRDs (follow KServe docs).
+2. Apply `serving/kserve/triton-inferenceservice.yaml` in namespace `ml`.
+3. Ensure pod is scheduled on GPU node and readiness checks pass.
+4. Run `serving/kserve/run_inference_bench.sh --host <host> --requests 50`:
+   - Acceptance: endpoint responds and baseline RPS/latency meet SLO (operator-defined).
+5. Scale test:
+   - Increase load and verify HPA / KServe scaling behavior; ensure model stays healthy and GPU utilization appropriate.
+
+MLflow promotion & signing
+--------------------------
+1. Ensure MLflow server credentials are available (MLFLOW_TRACKING_URI, MLFLOW_TOKEN).
+2. Confirm Vault Transit key `aegis-signing-key` exists and Vault JWT/OIDC auth for CI is configured.
+3. Trigger GitHub Action `MLflow Promote & Sign` or run `mlflow/scripts/mlflow_promote_and_sign.sh`.
+   - Acceptance:
+     - MLflow model version has been transitioned to the target stage.
+     - The model artifact was signed via Vault Transit; a signature and Rekor entry exist.
+     - cosign verify-blob --rekor-server <rekor> --signature <sig> <artifact> succeeds.
+
+Feature store & dataset versioning
+---------------------------------
+1. Run `feature-store/feast/ingest_sample.py` (ensure Feast local dev dependencies installed).
+   - Acceptance: offline_store/demo_features.parquet created; features retrievable via Feast SDK.
+2. For dataset versioning:
+   - Initialize lakeFS or DVC in a sandbox and add a dataset snapshot.
+   - Acceptance: dataset snapshot can be referenced by training pipeline and reproduces identical training inputs.
+
+End-to-end smoke (optional)
+--------------------------
+1. Train a small model using versioned dataset (DVC / lakeFS) and log artifact into MLflow.
+2. Promote model via MLflow promotion flow and sign artifact.
+3. Deploy promoted model to KServe from the same artifact (update triton InferenceService storage to point to artifact).
+4. Run inference bench and validate correctness and SLOs.
+
+If any test fails
+- Collect logs:
+  - kubectl logs -n ml <pod>
+  - Vault logs and audit (if signing failed)
+  - GitHub Action run logs (MLflow promote + sign)
+- Inspect Rekor entries and cosign outputs.
+
*** End Patch
*** End Patch
