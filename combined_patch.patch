*** Begin Patch
*** Add File: scripts/run_gate_and_deploy.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# run_gate_and_deploy.sh
+# Single pasteable operator script to:
+#  1) dispatch the orchestrator GH workflow (Agentic chaos 3× → Edge rotation → Quantum reconcile → RLHF pilot)
+#  2) download artifacts and validate key reports
+#  3) deploy a small device cohort via Ansible and trigger enroll
+#  4) prompt operator to submit one real QPU job per provider, then run reconcile
+#  5) download RLHF artifacts and run checkpoint/adversarial checks
+#  6) upload artifacts to COMPLIANCE_BUCKET (if set)
+#
+# Usage:
+#   COMPLIANCE_BUCKET=my-bucket KUBECONFIG=~/.kube/staging \
+#     ANSIBLE_INVENTORY=inventory/devices.ini DEVICE_SSH_USER=ubuntu \
+#     ./scripts/run_gate_and_deploy.sh
+#
+
+COMPLIANCE_BUCKET=${COMPLIANCE_BUCKET:-}
+KUBECONFIG=${KUBECONFIG:-$HOME/.kube/config}
+ANSIBLE_INVENTORY=${ANSIBLE_INVENTORY:-inventory/devices.ini}
+DEVICE_SSH_USER=${DEVICE_SSH_USER:-root}
+BRANCH=${BRANCH:-main}
+ANSIBLE_PLAYBOOK=${ANSIBLE_PLAYBOOK:-edge/device_deploy/ansible/deploy_tpm2_tools.yml}
+
+info(){ echo; echo "=== $*"; echo; }
+err(){ echo "ERROR: $*" >&2; exit 2; }
+
+# Ensure prerequisites
+for cmd in gh aws kubectl ansible-playbook jq; do
+  if ! command -v "$cmd" >/dev/null 2>&1; then
+    err "$cmd CLI required but not installed"
+  fi
+done
+
+if [ -z "$COMPLIANCE_BUCKET" ]; then
+  echo "WARNING: COMPLIANCE_BUCKET not set — artifacts will not be uploaded to S3."
+fi
+
+# 1) Dispatch orchestrator workflow
+info "Dispatching Run Production Orchestrator workflow"
+gh workflow run run_production_orchestrator.yml --ref "${BRANCH}" -f branch="${BRANCH}"
+sleep 4
+
+RUN_ID=$(gh run list --workflow run_production_orchestrator.yml --limit 1 --json database --jq '.[0].database' 2>/dev/null || true)
+if [ -z "$RUN_ID" ]; then
+  RUN_ID=$(gh run list --workflow run_production_orchestrator.yml --limit 1 | awk 'NR==1{print $1}' || true)
+fi
+[ -n "$RUN_ID" ] || err "Cannot determine orchestrator run id"
+
+info "Watching orchestrator run id=${RUN_ID} (this will block until completion)"
+if ! gh run watch "$RUN_ID" --exit-status; then
+  echo "Orchestrator run finished with non-zero conclusion; continue to download artifacts for triage."
+fi
+
+TMP_ORCH=$(mktemp -d /tmp/orch_artifacts_XXXX)
+info "Downloading orchestrator artifacts to $TMP_ORCH"
+gh run download "$RUN_ID" -D "$TMP_ORCH" || true
+
+# quick artifact checks
+info "Looking for chaos summary and reconcile reports"
+find "$TMP_ORCH" -type f -iname '*chaos*summary*.json' -print -quit || true
+find "$TMP_ORCH" -type f -iname '*reconcile*.json' -print -quit || true
+
+orch_conclusion=$(gh run view "$RUN_ID" --json conclusion --jq .conclusion 2>/dev/null || echo "unknown")
+if echo "$orch_conclusion" | grep -Eiq "failure|cancelled|timed_out"; then
+  err "Orchestrator concluded with issues (${orch_conclusion}). Inspect artifacts in ${TMP_ORCH} and s3://$COMPLIANCE_BUCKET/"
+fi
+
+# 2) Deploy device cohort
+info "Deploying device cohort via Ansible: ${ANSIBLE_PLAYBOOK}"
+[ -f "$ANSIBLE_INVENTORY" ] || err "Ansible inventory not found at $ANSIBLE_INVENTORY"
+ansible-playbook -i "${ANSIBLE_INVENTORY}" "${ANSIBLE_PLAYBOOK}"
+
+info "Triggering enroll_ondevice.py on devices via Ansible ad-hoc (group: devices)"
+ansible -i "${ANSIBLE_INVENTORY}" devices -m shell -a "sudo python3 /opt/device_agent/enroll_ondevice.py" || echo "Enroll had partial failures; inspect output"
+
+info "Waiting 30s for devices to upload attestations"
+sleep 30
+
+if [ -n "$COMPLIANCE_BUCKET" ]; then
+  info "Listing potential attestations in s3://${COMPLIANCE_BUCKET}/attestations and s3://${COMPLIANCE_BUCKET}/hsm/"
+  aws s3 ls "s3://${COMPLIANCE_BUCKET}/attestations/" --recursive || echo "No attestations prefix found"
+  aws s3 ls "s3://${COMPLIANCE_BUCKET}/hsm/" --recursive || echo "No hsm prefix found"
+fi
+
+# 3) Prompt operator to submit one real QPU job per provider
+info "Operator: submit one real QPU job per provider now. Example:"
+echo "  python3 quantum/staging/submit_and_wait.py --backend ibm --qasm /path/sample.qasm --tenant staging"
+echo "  (or use your provider submission tooling)"
+read -p "Press ENTER after provider jobs submitted (or CTRL-C to abort)"
+
+# 4) Trigger Quantum reconcile
+info "Dispatching quantum_full_prod_reconcile.yml"
+gh workflow run quantum_full_prod_reconcile.yml --ref "${BRANCH}"
+sleep 3
+QR_RUN_ID=$(gh run list --workflow quantum_full_prod_reconcile.yml --limit 1 --json database --jq '.[0].database' 2>/dev/null || true)
+if [ -z "$QR_RUN_ID" ]; then
+  QR_RUN_ID=$(gh run list --workflow quantum_full_prod_reconcile.yml --limit 1 | awk 'NR==1{print $1}' || true)
+fi
+[ -n "$QR_RUN_ID" ] || err "Cannot determine quantum reconcile run id"
+
+info "Watching quantum reconcile run id=${QR_RUN_ID}"
+if ! gh run watch "$QR_RUN_ID" --exit-status; then
+  echo "Quantum reconcile finished with non-zero conclusion; fetch artifacts."
+fi
+Q_TMP=$(mktemp -d /tmp/quantum_artifacts_XXXX)
+gh run download "$QR_RUN_ID" -D "$Q_TMP" || true
+info "Downloaded quantum artifacts to $Q_TMP"
+
+ANOMALIES=$(find "$Q_TMP" -type f -iname '*reconcile*.json' -exec jq '.anomalies | length' {} + 2>/dev/null | awk '{s+=$1}END{print s+0}')
+if [ -n "$ANOMALIES" ] && [ "$ANOMALIES" -gt 0 ]; then
+  err "Quantum reconcile reported anomalies (count=${ANOMALIES}). Inspect ${Q_TMP} and s3://$COMPLIANCE_BUCKET/quantum/reconcile/"
+fi
+
+# 5) Trigger RLHF pilot
+info "Dispatching RLHF production pilot (profile=pilot_medium)"
+gh workflow run rlhf_prod_pilot.yml --ref "${BRANCH}" -f profile=pilot_medium
+sleep 3
+RLHF_RUN_ID=$(gh run list --workflow rlhf_prod_pilot.yml --limit 1 --json database --jq '.[0].database' 2>/dev/null || true)
+if [ -z "$RLHF_RUN_ID" ]; then
+  RLHF_RUN_ID=$(gh run list --workflow rlhf_prod_pilot.yml --limit 1 | awk 'NR==1{print $1}' || true)
+fi
+[ -n "$RLHF_RUN_ID" ] || err "Cannot determine RLHF run id"
+
+info "Watching RLHF run id=${RLHF_RUN_ID}"
+if ! gh run watch "$RLHF_RUN_ID" --exit-status; then
+  echo "RLHF run finished with non-zero conclusion; download artifacts."
+fi
+R_TMP=$(mktemp -d /tmp/rlhf_artifacts_XXXX)
+gh run download "$RLHF_RUN_ID" -D "$R_TMP" || true
+info "Downloaded RLHF artifacts to $R_TMP"
+
+CKPT=$(find "$R_TMP" -type f -iname '*.tar.gz' -print -quit || true)
+if [ -n "$CKPT" ]; then
+  info "Running checkpoint restore test on $CKPT"
+  python3 rl/checkpoint_restore_test.py --ckpt "$CKPT" || err "Checkpoint restore test failed"
+fi
+
+if [ -f "scripts/adversarial_harness_enhanced.py" ]; then
+  info "Running adversarial harness check"
+  python3 scripts/adversarial_harness_enhanced.py || err "Adversarial harness failed"
+fi
+
+# 6) Upload artifacts to S3
+info "Uploading artifacts to S3 (if COMPLIANCE_BUCKET configured)"
+if [ -n "$COMPLIANCE_BUCKET" ]; then
+  aws s3 cp "$TMP_ORCH" "s3://${COMPLIANCE_BUCKET}/orchestrator/${RUN_ID}/" --recursive || true
+  aws s3 cp "$Q_TMP" "s3://${COMPLIANCE_BUCKET}/quantum/${QR_RUN_ID}/" --recursive || true
+  aws s3 cp "$R_TMP" "s3://${COMPLIANCE_BUCKET}/rlhf/${RLHF_RUN_ID}/" --recursive || true
+  info "Artifacts uploaded to s3://${COMPLIANCE_BUCKET}/"
+else
+  echo "COMPLIANCE_BUCKET not set; skipping uploads."
+fi
+
+info "Full gate + device deploy completed. Local artifacts in: $TMP_ORCH, $Q_TMP, $R_TMP"
+echo "If any step failed, inspect those directories and s3://$COMPLIANCE_BUCKET/ for details."
+
*** End Patch
*** Begin Patch
*** Add File: docs/pr_commands_and_body.md
+# PR + Git Commands (exact) — push orchestrator/autonomy patches & open PR
+
+Run from repo root.
+
+1) Create branch, stage files and push:
+```bash
+git checkout -b feat/prod-orchestrator-and-av
+# Stage the files created by the patches (adjust if paths differ)
+git add orchestrator/production_orchestrator.py .github/workflows/run_production_orchestrator.yml orchestrator/README.md \
+    scripts/run_full_gate_and_deploy.sh scripts/run_gate_and_deploy.sh \
+    runbooks/one_page_checklist.md docs/pr_and_git_commands.md docs/av_integration_design.md \
+    k8s/manifests/av-simulation-job.yaml .github/workflows/av_simulation_train.yml \
+    docs/s3_layout.md monitoring/prometheus/av_rules.yaml \
+    docs/iso26262_checklist.md docs/pr_template_iso26262.md docs/av_phased_roadmap.md \
+    ansible/av_canary_playbook.yml runbooks/av_incident_playbook.md
+git commit -m "chore(prod): add orchestrator, operator run scripts, AV integration artifacts and checklists"
+git push --set-upstream origin feat/prod-orchestrator-and-av
+```
+
+2) Create PR (paste PR body below)
+```bash
+gh pr create --title "chore(prod): orchestrator + operator scripts + AV integration artifacts" --body-file - <<'PRBODY'
+This PR adds production orchestration, operator run scripts, autonomy prototypes (anomaly detector & remediation assistant), and Autonomous Vehicle (AV) integration artifacts and runbooks.
+
+Key additions:
+- orchestrator/production_orchestrator.py + GH workflow run_production_orchestrator.yml
+- scripts/run_full_gate_and_deploy.sh and scripts/run_gate_and_deploy.sh (operator scripts)
+- ml/anomaly prototype and remediation scaffolds (auto-triage, assistant, executor)
+- AV integration: manifests, CI workflow, S3 layout, Prometheus rules, ISO26262 checklist, canary playbooks, incident playbook
+- runbooks/one_page_checklist.md and runbooks/av_incident_playbook.md
+
+Purpose:
+- Provide an end-to-end gated promote flow (Agentic chaos → Edge rotation → Quantum reconcile → RLHF pilot), operator tools to run the gate, and AV-specific templates for simulation → training → canary → cert workflows.
+- All artifacts must be reviewed by Ops, Security and ML teams before merging and running in staging/production.
+
+Required secrets (set in repo settings):
+- COMPLIANCE_BUCKET, PROM_URL, KUBECONFIG_STAGING, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, GITHUB_TOKEN, MODEL_COSIGN_PUBKEY
+
+Review & merge policy:
+- Request review from Ops, Security and ML leads.
+- Do not merge until at least one SRE and one Security reviewer approve.
+
+PRBODY
+```
+
+3) Add reviewers/labels (example; replace handles)
+```bash
+gh pr edit --add-reviewer ops-team --add-reviewer security --add-reviewer ml-team --add-label production,automation
+```
+
+Notes
+- Verify required secrets exist before running workflows that interact with staging or S3.
+- The PR contains safety-sensitive automation — coordinate with Security and Legal for AV artifacts & pen-test plans.
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/one_page_checklist.md
+```markdown
+# Production Gate — One‑Page Checklist (printable)
+
+Project: Aegis  
+Gate: Full Production Gate — Agentic → Edge → Quantum → Generative  
+Operator: ______________________    Date/time start: _______________
+
+Prechecks
+- [ ] gh CLI authenticated (run `gh auth status`)
+- [ ] AWS creds configured and COMPLIANCE_BUCKET set: s3://________________
+- [ ] KUBECONFIG for staging available: __________________
+- [ ] Operators & SRE on call (names): __________________
+- [ ] Vault operator token available for provider secrets (if needed)
+
+1) Agentic chaos (3 runs)
+- Command:
+  gh workflow run agentic_full_chaos_sequence.yml --ref main -f runs=3 -f duration=600 -f namespace=aegis
+- Monitor:
+  gh run watch <run-id>
+- Artifacts:
+  - Local download: /tmp/agentic_chaos_run/
+  - S3: s3://<COMPLIANCE_BUCKET>/chaos/<artifact>.tgz
+  - Required file: chaos_summary.json
+- Acceptance:
+  - chaos_summary.json: stuck_tx_ids == [] AND fatal_errors == []
+
+2) Adapter tuning & rollout
+- Commands:
+  PROM_URL=<prom> python3 participant/adapter_timeout_tuner.py
+  KUBECONFIG=<kubeconfig> python3 agentic/auto_tuner_apply.py
+- Verify:
+  kubectl -n aegis rollout status deploy -l app=example-tool-adapter
+
+3) Edge device cohort deploy & enroll
+- Ansible deploy:
+  ansible-playbook -i <ANSIBLE_INVENTORY> edge/device_deploy/ansible/deploy_tpm2_tools.yml
+- Enroll devices:
+  ansible -i <ANSIBLE_INVENTORY> devices -m shell -a "sudo python3 /opt/device_agent/enroll_ondevice.py"
+- Artifacts:
+  - s3://<COMPLIANCE_BUCKET>/attestations/
+  - s3://<COMPLIANCE_BUCKET>/hsm/rotation_*.json
+  - Rekor entries (URL/ID)
+- Acceptance:
+  - Each device has attestation in S3 or Rekor entry
+
+4) Edge rotation drill
+- Command:
+  gh workflow run edge_rotation_drill_ci.yml --ref main -f total_devices=500
+- Artifacts:
+  - s3://<COMPLIANCE_BUCKET>/hsm/rotation_*.json
+- Acceptance:
+  - Canary enrolls succeed and evidence uploaded; critical outage <1%
+
+5) Quantum provider job submissions & reconcile
+- Operator submit examples:
+  python3 quantum/staging/submit_and_wait.py --backend ibm --qasm /path/sample.qasm --tenant staging
+- Trigger reconcile:
+  gh workflow run quantum_full_prod_reconcile.yml --ref main
+- Artifacts:
+  - s3://<COMPLIANCE_BUCKET>/provider_receipts/<provider>/
+  - s3://<COMPLIANCE_BUCKET>/quantum/reconcile/v4/quantum_reconcile_v4_*.json
+- Acceptance:
+  - reconcile report: anomalies == 0 OR remediation tickets created & assigned
+
+6) Generative RLHF pilot
+- Launch:
+  gh workflow run rlhf_prod_pilot.yml --ref main -f profile=pilot_medium
+- Artifacts:
+  - RLHF checkpoint tar (.tar.gz) in GH artifacts or MLflow
+  - s3://<COMPLIANCE_BUCKET>/rlhf/<run>/
+- Local checks:
+  - python3 rl/checkpoint_restore_test.py --ckpt <ckpt>
+  - python3 scripts/adversarial_harness_enhanced.py
+  - python3 scripts/ensure_cosign_signed.py --artifact <ckpt> --pubkey /path/pubkey.pem
+- Acceptance:
+  - checkpoint restore OK; adversarial harness OK; cosign verify OK; billing_reconcile no anomalies
+
+7) Final evidence upload
+- Command:
+  python3 scripts/upload_runbook_evidence.py --paths "/tmp/*_artifacts/*" --bucket <COMPLIANCE_BUCKET>
+- Verify S3 paths:
+  - s3://<COMPLIANCE_BUCKET>/chaos/
+  - s3://<COMPLIANCE_BUCKET>/orchestrator/<run-id>/
+  - s3://<COMPLIANCE_BUCKET>/hsm/
+  - s3://<COMPLIANCE_BUCKET>/quantum/reconcile/
+  - s3://<COMPLIANCE_BUCKET>/rlhf/
+  - s3://<COMPLIANCE_BUCKET>/pen_tests/
+  - s3://<COMPLIANCE_BUCKET>/backups/approvals_*.db
+
+Post-run
+- If any [FAIL], capture DB snapshot, pod logs, upload to S3, and open incident ticket in GitHub with artifact links.
+- Document run summary in runbooks/reports/ e.g., chaos_run_<ts>.json, rlhf_pilot_<ts>.json
+
+Operator signature: ______________________   Date/time end: _______________
+```
+
*** End Patch
