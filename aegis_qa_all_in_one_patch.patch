*** Begin Patch
*** Add File:qa/argo/qa_e2e.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-qa-e2e-
+spec:
+  entrypoint: qa-e2e
+  serviceAccountName: aegis-agent
+  templates:
+  - name: qa-e2e
+    steps:
+      - - name: setup-test-data
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "bash qa/scripts/setup_test_data.sh /workspace/data/test_input.csv"
+      - - name: train
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "python3 production/pipeline/train/train.py --data /workspace/data/test_input.csv --out /workspace/artifacts/test-model.pkl --mlflow ${MLFLOW_URL:-}"
+      - - name: validate
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "python3 production/pipeline/validate/validate.py --model /workspace/artifacts/test-model.pkl --data /workspace/data/test_input.csv --mlflow ${MLFLOW_URL:-} --threshold 0.0"
+      - - name: sign-with-mock-hsm
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "python3 qa/mocks/mock_sign_client.py --artifact /workspace/artifacts/test-model.pkl --out /workspace/artifacts/test-model.pkl.sig --hsm-url ${MOCK_HSM_URL:-http://mock-hsm.aegis:5000}"
+      - - name: deploy-canary
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "bash production/pipeline/deploy/canary_deploy.sh --image ${REGISTRY}/aegis-model:${IMAGE_TAG} --deployment aegis-model-test --namespace aegis-test"
+      - - name: monitor-canary
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "python3 production/pipeline/monitor/monitor_check.py --prom-url ${PROM_URL} --max-latency 10.0 --max-error 1.0 || true"
+      - - name: collect-evidence
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "python3 aiops/incident/incident_bundler.py --deployment aegis-model-test --out /tmp/qa_incident_{{workflow.name}}.tar.gz --s3-bucket ${EVIDENCE_BUCKET:-}"
+      - - name: report-to-cr
+          template: run-script
+          arguments:
+            parameters:
+              - name: script
+                value: "kubectl patch testrun -n aegis-test qa-testrun-example --type merge -p '{\"status\":{\"state\":\"Completed\",\"result\":\"Succeeded\",\"artifact\":\"/tmp/qa_incident_{{workflow.name}}.tar.gz\"}}' || true"
+
+  - name: run-script
+    inputs:
+      parameters:
+        - name: script
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args: ["{{inputs.parameters.script}}"]
+
+  arguments:
+    parameters:
+      - name: MLFLOW_URL
+        value: ""
+      - name: REGISTRY
+        value: ""
+      - name: IMAGE_TAG
+        value: "qa"
+      - name: PROM_URL
+        value: ""
+      - name: MOCK_HSM_URL
+        value: "http://mock-hsm.aegis:5000"
+      - name: EVIDENCE_BUCKET
+        value: ""
+
*** End Patch
*** Begin Patch
*** Add File:qa/scripts/setup_test_data.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUT="${1:-/workspace/data/test_input.csv}"
+mkdir -p "$(dirname "$OUT")"
+cat > "$OUT" <<'CSV'
+label
+1
+0
+1
+1
+0
+CSV
+ln -sf "$OUT" "$(dirname "$OUT")/snapshot-latest.csv"
+echo "Test data prepared at $OUT"
+
*** End Patch
*** Begin Patch
*** Add File:qa/mocks/mock_hsm.py
+#!/usr/bin/env python3
+"""
+Mock HSM HTTP service for QA: accepts a POST /sign with a binary blob and returns a deterministic signature.
+This is intended for CI and QA; do NOT use in production.
+"""
+from flask import Flask, request, jsonify
+import base64, hashlib
+
+app = Flask(__name__)
+
+@app.route("/sign", methods=["POST"])
+def sign():
+    data = request.get_data()
+    # deterministic pseudo-signature: sha256(data) hex
+    digest = hashlib.sha256(data).hexdigest()
+    sig_b64 = base64.b64encode(digest.encode()).decode()
+    return jsonify({"signature": sig_b64, "signed_by": "mock-hsm"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(__import__("os").environ.get("PORT", "5000")))
+
*** End Patch
*** Begin Patch
*** Add File:qa/mocks/mock_sign_client.py
+#!/usr/bin/env python3
+"""
+Client script used in Argo QA workflow to call mock HSM and write signature file.
+"""
+import argparse, requests, base64
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--artifact", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--hsm-url", default="http://localhost:5000")
+    args = p.parse_args()
+    with open(args.artifact, "rb") as f:
+        data = f.read()
+    resp = requests.post(args.hsm_url + "/sign", data=data, timeout=10)
+    resp.raise_for_status()
+    j = resp.json()
+    sig = j.get("signature")
+    # write signature (base64) to out
+    with open(args.out, "w") as f:
+        f.write(sig)
+    print("Wrote mock signature to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:qa/mocks/mock_qpu.py
+#!/usr/bin/env python3
+"""
+Mock QPU service for QA: accepts a POST /run with program, returns a fake job id, noise profile and simple result.
+This is intended for CI and QA only.
+"""
+from flask import Flask, request, jsonify
+import uuid, json, os
+
+app = Flask(__name__)
+
+@app.route("/run", methods=["POST"])
+def run():
+    payload = request.json or {}
+    program = payload.get("program","")
+    jid = str(uuid.uuid4())
+    noise = {"qubits": 4, "calibration": [0.01,0.02,0.03,0.04]}
+    # optionally persist noise to a path for evidence collection
+    outdir = os.environ.get("QA_ARTIFACT_DIR","/tmp/qa_qpu")
+    os.makedirs(outdir, exist_ok=True)
+    with open(os.path.join(outdir, f"noise_{jid}.json"), "w") as f:
+        json.dump(noise, f)
+    result = {"job_id": jid, "result": {"counts":{"00": 10, "11":5}}, "noise_path": os.path.join(outdir, f"noise_{jid}.json")}
+    return jsonify(result)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(__import__("os").environ.get("PORT","6000")))
+
*** End Patch
*** Begin Patch
*** Add File:qa/vault/write_test_creds.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Write mock QA credentials into Vault for pilots/tests.
+VAULT_PATH="${1:-secret/data/qa/providers}"
+BRAKET_DEVICE="${BRAKET_DEVICE:-arn:aws:braket:us-west-2::device/quantum-simulator}"
+IBM_TOKEN="${IBM_TOKEN:-mock-ibm-token}"
+
+if [ -z "${VAULT_ADDR:-}" ] || [ -z "${VAULT_TOKEN:-}" ]; then
+  echo "Please set VAULT_ADDR and VAULT_TOKEN in environment"
+  exit 2
+fi
+
+vault kv put "${VAULT_PATH}" braket_device="${BRAKET_DEVICE}" ibm_token="${IBM_TOKEN}"
+echo "Wrote QA provider creds to ${VAULT_PATH}"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/crd/testrun_crd.yaml
+apiVersion: apiextensions.k8s.io/v1
+kind: CustomResourceDefinition
+metadata:
+  name: testruns.aegis.ai
+spec:
+  group: aegis.ai
+  names:
+    kind: TestRun
+    plural: testruns
+    singular: testrun
+    shortNames:
+      - trun
+  scope: Namespaced
+  versions:
+    - name: v1
+      served: true
+      storage: true
+      schema:
+        openAPIV3Schema:
+          type: object
+          properties:
+            spec:
+              type: object
+              properties:
+                test_type:
+                  type: string
+                parameters:
+                  type: object
+            status:
+              type: object
+              properties:
+                state:
+                  type: string
+                result:
+                  type: string
+                artifact:
+                  type: string
+      subresources:
+        status: {}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/testrun_rbac.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: testrun-controller
+  namespace: aegis-test
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: testrun-role
+  namespace: aegis-test
+rules:
+  - apiGroups: ["aegis.ai"]
+    resources: ["testruns"]
+    verbs: ["get","list","watch","update","patch","create"]
+  - apiGroups: [""]
+    resources: ["pods","jobs","pods/log"]
+    verbs: ["get","list","create","patch"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: testrun-rb
+  namespace: aegis-test
+subjects:
+  - kind: ServiceAccount
+    name: testrun-controller
+    namespace: aegis-test
+roleRef:
+  kind: Role
+  name: testrun-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File:controllers/testrun_controller.py
+#!/usr/bin/env python3
+"""
+Minimal TestRun controller: watches TestRun CRs in namespace and executes the QA Argo workflow.
+On creation it will submit the qa_e2e workflow and patch the TestRun.status when completed.
+This is a lightweight helper for QA orchestration; in production use a robust operator framework.
+"""
+import os, time, subprocess, json
+from kubernetes import client, config, watch
+
+NAMESPACE = os.environ.get("WATCH_NAMESPACE","aegis-test")
+QA_WORKFLOW = os.environ.get("QA_WORKFLOW","qa/argo/qa_e2e.yaml")
+
+def submit_argo_for_test(name):
+    try:
+        cmd = ["argo","submit", QA_WORKFLOW, "--watch", "--parameter", f"IMAGE_TAG=qa"]
+        print("Submitting Argo workflow for TestRun", name)
+        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode()
+        # parse workflow name from output (best-effort)
+        wf_name = None
+        for line in out.splitlines():
+            if line.strip().startswith("workflow"):
+                parts = line.split()
+                if len(parts) >= 2:
+                    wf_name = parts[-1]
+        return wf_name, out
+    except Exception as e:
+        print("Failed to submit argo:", e)
+        return None, str(e)
+
+def patch_testrun_status(api, name, state, result=None, artifact=None):
+    body = {"status": {"state": state, "result": result or "", "artifact": artifact or ""}}
+    try:
+        api.patch_namespaced_custom_object_status("aegis.ai", "v1", NAMESPACE, "testruns", name, body)
+    except Exception as e:
+        print("Failed to patch status:", e)
+
+def main():
+    if os.environ.get("KUBERNETES_SERVICE_HOST"):
+        config.load_incluster_config()
+    else:
+        config.load_kube_config()
+    api = client.CustomObjectsApi()
+    w = watch.Watch()
+    print("Starting TestRun controller in namespace", NAMESPACE)
+    for event in w.stream(api.list_namespaced_custom_object, "aegis.ai", "v1", NAMESPACE, "testruns", timeout_seconds=0):
+        typ = event['type']
+        obj = event['object']
+        name = obj['metadata']['name']
+        spec = obj.get('spec', {})
+        print("Event", typ, "TestRun", name)
+        try:
+            if typ == "ADDED":
+                patch_testrun_status(api, name, "Running")
+                wf_name, out = submit_argo_for_test(name)
+                if wf_name:
+                    patch_testrun_status(api, name, "Completed", "Succeeded", f"workflow:{wf_name}")
+                else:
+                    patch_testrun_status(api, name, "Failed", "Argo submission failed", "")
+        except Exception as e:
+            print("Controller error:", e)
+            patch_testrun_status(api, name, "Failed", str(e))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:controllers/testrun_Dockerfile
+FROM python:3.10-slim
+RUN pip install kubernetes
+WORKDIR /app
+COPY controllers/testrun_controller.py /app/
+CMD ["python3","/app/testrun_controller.py"]
+
*** End Patch
*** Begin Patch
*** Add File:qa/ci/qa_workflow.yml
+name: QA End-to-End
+on:
+  workflow_dispatch:
+jobs:
+  run-qa:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.27.0'
+      - name: Create TestRun CR
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          cat <<EOF | kubectl apply -f -
+          apiVersion: aegis.ai/v1
+          kind: TestRun
+          metadata:
+            name: qa-testrun-example
+            namespace: aegis-test
+          spec:
+            test_type: "e2e"
+            parameters:
+              image_tag: "qa"
+          EOF
+      - name: Wait for TestRun completion
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          for i in {1..60}; do
+            status=$(kubectl get testrun -n aegis-test qa-testrun-example -o jsonpath='{.status.state}' 2>/dev/null || echo "Pending")
+            echo "Status: $status"
+            if [ "$status" = "Completed" ] || [ "$status" = "Failed" ]; then
+              kubectl get testrun -n aegis-test qa-testrun-example -o yaml
+              exit 0
+            fi
+            sleep 10
+          done
+          echo "Timed out waiting for TestRun"
+          exit 1
+
*** End Patch
*** Begin Patch
*** Add File:qa/README_QA_SETUP.md
+# Aegis QA Setup (All-in-one)
+
+This patch adds QA tooling:
+- A QA Argo workflow: qa/argo/qa_e2e.yaml — runs a lightweight train/validate/sign/deploy/monitor/evidence flow using mock services.
+- Mock services:
+  - qa/mocks/mock_hsm.py — simple HTTP sign service for QA.
+  - qa/mocks/mock_qpu.py — simulate QPU responses.
+  - qa/mocks/mock_sign_client.py — client used by the Argo workflow to call mock HSM.
+- Vault test wiring script: qa/vault/write_test_creds.sh — writes mock provider creds to Vault for QA.
+- TestRun CRD & controller:
+  - k8s/crd/testrun_crd.yaml — TestRun CRD definition.
+  - controllers/testrun_controller.py + controllers/testrun_Dockerfile — minimal controller that submits the QA Argo workflow on TestRun creation.
+  - k8s/testrun_rbac.yaml — RBAC & ServiceAccount for controller in namespace aegis-test.
+- CI action: qa/ci/qa_workflow.yml — GitHub Action to create a TestRun CR and wait for completion.
+
+Quick start (local / dev)
+1. Start mock services (locally or in-cluster):
+   - python3 qa/mocks/mock_hsm.py &
+   - python3 qa/mocks/mock_qpu.py &
+2. Prepare Vault test creds (optional):
+   - export VAULT_ADDR=... VAULT_TOKEN=...
+   - bash qa/vault/write_test_creds.sh
+3. Create a namespace and apply CRD & RBAC:
+   - kubectl apply -f k8s/crd/testrun_crd.yaml
+   - kubectl create ns aegis-test || true
+   - kubectl apply -f k8s/testrun_rbac.yaml -n aegis-test
+4. Deploy controller (build from controllers/testrun_Dockerfile) and give it access to cluster in aegis-test.
+5. Submit a TestRun CR (or use the GitHub Action):
+   - kubectl apply -f - <<EOF
+     apiVersion: aegis.ai/v1
+     kind: TestRun
+     metadata:
+       name: qa-testrun-example
+       namespace: aegis-test
+     spec:
+       test_type: "e2e"
+       parameters:
+         image_tag: "qa"
+     EOF
+
+6. Controller will submit the Argo workflow and update the TestRun.status when complete.
+
+Notes & rationale
+- Mock HSM/QPU let CI run quickly without vendor hardware while still exercising the full pipeline.
+- TestRun CRD provides a lightweight way for teams to request tests and view results via kubectl.
+- The controller is intentionally simple; consider replacing with Kopf/operator-sdk for production.
+
*** End Patch
*** End Patch
