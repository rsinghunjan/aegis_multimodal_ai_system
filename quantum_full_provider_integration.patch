*** Begin Patch
*** Add File:iam/rigetti_policy.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowRigettiAccessPlaceholder",
+      "Effect": "Allow",
+      "Action": [
+        "sts:AssumeRole"
+      ],
+      "Resource": "*"
+    },
+    {
+      "Sid": "AllowS3ForQuantum",
+      "Effect": "Allow",
+      "Action": [
+        "s3:PutObject",
+        "s3:GetObject",
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET",
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/irsa/irsa_braket_sa.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: braket-sa
+  namespace: aegis
+  annotations:
+    # Replace with the real IRSA role ARN created for Braket access
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-braket-irsa-role
+
*** End Patch
*** Begin Patch
*** Add File:k8s/irsa/irsa_ibm_sa.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: ibm-quantum-sa
+  namespace: aegis
+  annotations:
+    # If using IRSA-to-IBM-proxy or similar, replace ARN below with the role that can access IBM creds in SecretsManager
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-ibm-irsa-role
+
*** End Patch
*** Begin Patch
*** Add File:k8s/irsa/irsa_azure_sa.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: azure-quantum-sa
+  namespace: aegis
+  annotations:
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-azure-irsa-role
+
*** End Patch
*** Begin Patch
*** Add File:k8s/irsa/irsa_rigetti_sa.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: rigetti-quantum-sa
+  namespace: aegis
+  annotations:
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-rigetti-irsa-role
+
*** End Patch
*** Begin Patch
*** Add File:k8s/externalsecrets/rigetti_secret.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: rigetti-credentials
+  namespace: security
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: vault-secret-store
+    kind: SecretStore
+  target:
+    name: rigetti-creds
+    creationPolicy: Owner
+  data:
+    - secretKey: RIGETTI_API_KEY
+      remoteRef:
+        key: "aegis/rigetti/credentials"
+        property: api_key
+
*** End Patch
*** Begin Patch
*** Add File:ops/qpu_cost_estimator.py
+#!/usr/bin/env python3
+"""
+Cost estimator for QPU jobs. Heuristic provider-agnostic estimator used by CI and Argo submission gates.
+
+Usage:
+  python ops/qpu_cost_estimator.py --provider braket --shots 1000 --device arn:aws:braket:::device/quantum-simulator/amazon/sv1
+
+Notes:
+ - This is a best-effort estimator. Replace pricing logic with real provider price APIs for production.
+ - Returns JSON: {"estimated_usd": <float>, "details": {...}}
+"""
+import argparse, json, sys
+
+def estimate_braket(shots, device, circuit_complexity):
+    # Example heuristic:
+    base_per_shot = 0.0005  # USD per shot for simulator baseline
+    # If device is physical QPU, multiplier
+    if "device/quantum-simulator" in device:
+        device_mult = 1.0
+    else:
+        device_mult = 10.0
+    complexity_mult = 1.0 + (circuit_complexity or 1) * 0.1
+    estimated = shots * base_per_shot * device_mult * complexity_mult
+    return {"estimated_usd": round(estimated, 4), "provider": "braket", "shots": shots, "device": device}
+
+def estimate_ibm(shots, device, circuit_complexity):
+    base = 0.001
+    dev_mult = 5.0 if "ibm" in device.lower() else 1.0
+    est = shots * base * dev_mult * (1.0 + (circuit_complexity or 1) * 0.1)
+    return {"estimated_usd": round(est,4), "provider":"ibm", "shots":shots, "device":device}
+
+def estimate_azure(shots, device, circuit_complexity):
+    base = 0.0008
+    dev_mult = 8.0 if device and device.lower() != "simulator" else 1.0
+    est = shots * base * dev_mult * (1.0 + (circuit_complexity or 1)*0.1)
+    return {"estimated_usd": round(est,4), "provider":"azure", "shots":shots, "device":device}
+
+def estimate_rigetti(shots, device, circuit_complexity):
+    base = 0.0012
+    est = shots * base * (1.0 + (circuit_complexity or 1)*0.1)
+    return {"estimated_usd": round(est,4), "provider":"rigetti", "shots":shots, "device":device}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--provider", choices=["braket","ibm","azure","rigetti"], required=True)
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="simulator")
+    p.add_argument("--circuit-complexity", type=float, default=1.0, help="User-supplied complexity heuristic (1.0 = baseline)")
+    args = p.parse_args()
+    if args.provider == "braket":
+        out = estimate_braket(args.shots, args.device, args.circuit_complexity)
+    elif args.provider == "ibm":
+        out = estimate_ibm(args.shots, args.device, args.circuit_complexity)
+    elif args.provider == "azure":
+        out = estimate_azure(args.shots, args.device, args.circuit_complexity)
+    else:
+        out = estimate_rigetti(args.shots, args.device, args.circuit_complexity)
+    print(json.dumps(out))
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/quantum_schema.json
+{
+  "$schema": "http://json-schema.org/draft-07/schema#",
+  "title": "Aegis Quantum MLflow Run Schema",
+  "type": "object",
+  "required": [
+    "circuit_spec",
+    "provider",
+    "provider_job_id",
+    "shots",
+    "device",
+    "seed",
+    "result_s3_uri",
+    "signed_receipt_s3_uri"
+  ],
+  "properties": {
+    "circuit_spec": {
+      "type": "object",
+      "description": "Circuit source, encoding and parameters (JSON serializable)"
+    },
+    "provider": {
+      "type": "string",
+      "enum": ["braket","ibm","azure","rigetti","simulator"]
+    },
+    "provider_job_id": {
+      "type": "string",
+      "description": "Provider-assigned job ID (or simulator marker)"
+    },
+    "shots": { "type": "integer" },
+    "device": { "type": "string" },
+    "seed": { "type": "integer" },
+    "transpilation_options": { "type": "object" },
+    "noise_model": { "type": ["object", "null"] },
+    "result_s3_uri": { "type": "string", "format": "uri" },
+    "signed_receipt_s3_uri": { "type": "string", "format": "uri" },
+    "estimated_cost_usd": { "type": "number" },
+    "owner": { "type": "string" }
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/validate_quantum_mlflow.py
+#!/usr/bin/env python3
+"""
+Validate that a given MLflow run contains required quantum metadata per mlflow/quantum_schema.json.
+Usage:
+  python mlflow/validate_quantum_mlflow.py --run-id <run_id> --tracking-uri <uri>
+"""
+import json, argparse, sys
+from jsonschema import validate, ValidationError
+import mlflow
+
+def load_schema():
+    import pkgutil, os
+    schema_path = os.path.join(os.path.dirname(__file__), "quantum_schema.json")
+    with open(schema_path) as f:
+        return json.load(f)
+
+def run_validate(run_id, tracking_uri):
+    mlflow.set_tracking_uri(tracking_uri)
+    client = mlflow.tracking.MlflowClient()
+    run = client.get_run(run_id)
+    # assemble metadata: prefer tags and params in run.data
+    meta = {}
+    # try to load JSON artifact if present
+    try:
+        # look for artifact file quantum/metadata.json
+        local_tmp = "/tmp/mlflow_quantum_meta.json"
+        client.download_artifacts(run_id, "quantum/metadata.json", "/tmp") 
+        with open(local_tmp) as f:
+            meta = json.load(f)
+    except Exception:
+        # fallback to tags/params
+        for k,v in run.data.tags.items():
+            try:
+                meta[k] = json.loads(v)
+            except Exception:
+                meta[k] = v
+        for k,v in run.data.params.items():
+            meta[k] = v
+    schema = load_schema()
+    try:
+        validate(instance=meta, schema=schema)
+        print("MLflow run satisfies quantum schema")
+        return 0
+    except ValidationError as e:
+        print("MLflow run validation FAILED:", e)
+        return 2
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--run-id", required=True)
+    p.add_argument("--tracking-uri", required=True)
+    args = p.parse_args()
+    rc = run_validate(args.run_id, args.tracking_uri)
+    sys.exit(rc)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:tests/mlflow_quantum_qa.py
+#!/usr/bin/env python3
+"""
+Simple QA script that scans MLflow for recent quantum runs and validates schema compliance.
+Intended to be run in CI as a dbt-style QA test.
+"""
+import mlflow, argparse, time, json, os, sys
+from mlflow.tracking import MlflowClient
+from mlflow.validate_quantum_mlflow import load_schema
+from jsonschema import validate, ValidationError
+
+def main(tracking_uri, lookback_seconds=86400):
+    mlflow.set_tracking_uri(tracking_uri)
+    client = MlflowClient()
+    now = int(time.time() * 1000)
+    earliest = now - lookback_seconds * 1000
+    runs = client.search_runs(experiment_ids=None, filter_string=f"attributes.start_time >= {earliest}", max_results=100)
+    schema = load_schema()
+    failures = []
+    for r in runs:
+        # only consider runs tagged quantum
+        if r.data.tags.get("quantum_provider") or r.data.tags.get("quantum"):
+            # attempt to download metadata artifact
+            meta = {}
+            try:
+                tmpdir = "/tmp/mlflow_meta_" + r.info.run_id
+                client.download_artifacts(r.info.run_id, "quantum/metadata.json", tmpdir)
+                with open(os.path.join(tmpdir, "metadata.json")) as f:
+                    meta = json.load(f)
+            except Exception:
+                # fallback to tags
+                for k,v in r.data.tags.items():
+                    try:
+                        meta[k] = json.loads(v)
+                    except:
+                        meta[k] = v
+                for k,v in r.data.params.items():
+                    meta[k] = v
+            try:
+                validate(instance=meta, schema=schema)
+            except ValidationError as e:
+                failures.append((r.info.run_id, str(e.message)))
+    if failures:
+        print("QA Failures:", failures)
+        sys.exit(2)
+    print("All recent quantum MLflow runs comply with schema")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--tracking-uri", required=True)
+    p.add_argument("--lookback-seconds", type=int, default=86400)
+    args = p.parse_args()
+    main(args.tracking_uri, args.lookback_seconds)
+
*** End Patch
*** Begin Patch
*** Add File:ops/argo_submit_guard.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Guarded Argo submit script: checks cost estimate and qpu-approved annotation/label before submitting.
+#
+# Usage:
+#  ./ops/argo_submit_guard.sh --provider braket --shots 1000 --device arn:... --budget 10 --workflow argo/quantum_braket_workflow.yaml --circuit s3://bucket/circuit.py
+#
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --provider) PROVIDER="$2"; shift 2;;
+    --shots) SHOTS="$2"; shift 2;;
+    --device) DEVICE="$2"; shift 2;;
+    --budget) BUDGET="$2"; shift 2;;
+    --workflow) WORKFLOW="$2"; shift 2;;
+    --circuit) CIRCUIT="$2"; shift 2;;
+    --image) IMAGE="$2"; shift 2;;
+    --kubecontext) KUBECTX="--kubeconfig $2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+: "${PROVIDER:?--provider required}"
+: "${SHOTS:=1000}"
+: "${BUDGET:=10}"
+: "${WORKFLOW:?--workflow required}"
+: "${CIRCUIT:?--circuit required}"
+
+echo "Estimating cost for provider=$PROVIDER shots=$SHOTS device=$DEVICE"
+EST=$(python3 ops/qpu_cost_estimator.py --provider "$PROVIDER" --shots "$SHOTS" --device "${DEVICE:-simulator}" --circuit-complexity 1.0)
+EST_USD=$(echo "$EST" | python3 -c "import sys, json; print(json.load(sys.stdin)['estimated_usd'])")
+echo "Estimated cost: $${EST_USD} USD (budget: $${BUDGET})"
+awk -v e="$EST_USD" -v b="$BUDGET" 'BEGIN{ if (e+0 > b+0) exit 2; else exit 0 }'
+echo "Cost within budget."
+
+# Require qpu-approved label on PR when applicable (best enforced by GitHub Action on PR)
+echo "Submitting Argo workflow..."
+argo submit -n aegis "$WORKFLOW" -p circuit-file-s3="$CIRCUIT" -p run-type="$PROVIDER" -p device="$DEVICE" -p image="${IMAGE:-ghcr.io/yourorg/aegis-quantum:latest}"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_pre_submit_check.yml
+name: QPU Pre-submit & Budget Check
+
+on:
+  workflow_dispatch:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  preflight:
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+    steps:
+      - uses: actions/checkout@v4
+      - name: Scan PR for quantum artifacts
+        id: scan
+        run: |
+          Q=$(git diff --name-only origin/main...HEAD | xargs grep -IEl "quantum_braket_workflow|quantum_ibm_workflow|quantum_azure_workflow|quantum_rigetti_workflow|quantum/|bra ket|ibm|azure" || true)
+          echo "found<<EOF" >> $GITHUB_OUTPUT
+          echo "$Q" >> $GITHUB_OUTPUT
+          echo "EOF" >> $GITHUB_OUTPUT
+      - name: Estimate cost if circuit metadata present
+        if: steps.scan.outputs.found != ''
+        run: |
+          echo "Found quantum artifacts; running estimator stub (requires PR to include a simple metadata file at .qpu/metadata.json)"
+          if [ -f ".qpu/metadata.json" ]; then
+            cat .qpu/metadata.json
+            python3 ops/qpu_cost_estimator.py --provider $(jq -r .provider .qpu/metadata.json) --shots $(jq -r .shots .qpu/metadata.json) --device $(jq -r .device .qpu/metadata.json) > /tmp/qpu_est.json
+            echo "est<<EOF" >> $GITHUB_OUTPUT
+            cat /tmp/qpu_est.json >> $GITHUB_OUTPUT
+            echo "EOF" >> $GITHUB_OUTPUT
+          else
+            echo "No .qpu/metadata.json found; failing safe."
+            exit 2
+          fi
+      - name: Require qpu-approved label and approval
+        if: steps.scan.outputs.found != ''
+        uses: actions/github-script@v6
+        with:
+          script: |
+            const labels = await github.rest.issues.listLabelsOnIssue({
+              owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number
+            });
+            const labelNames = labels.data.map(l=>l.name);
+            if (!labelNames.includes('qpu-approved')) {
+              core.setFailed("PR includes quantum artifacts; please add the 'qpu-approved' label after review.");
+            }
+            const reviews = await github.rest.pulls.listReviews({owner: context.repo.owner, repo: context.repo.repo, pull_number: context.issue.number});
+            const approvals = reviews.data.filter(r => r.state.toLowerCase() === 'approved');
+            if (approvals.length < 1) {
+              core.setFailed("PR includes quantum artifacts; at least one approving review required.");
+            }
+
*** End Patch
*** Begin Patch
*** Add File:ops/legal_safe_sanitizer.py
+#!/usr/bin/env python3
+"""
+Legal-safe data sanitizer for QPU submissions.
+Scans a directory or file for obvious PII patterns and either redacts or fails.
+Patterns: emails, phone numbers, SSN-like, credit-cards (Luhn light), IPv4 addresses.
+
+Usage:
+  python ops/legal_safe_sanitizer.py --input path/to/data.json --mode redact --out sanitized.json
+
+Modes:
+  redact - replace detected PII with placeholders and write sanitized file
+  fail - abort if any PII detected
+"""
+import argparse, re, json, sys
+from pathlib import Path
+
+PATTERNS = {
+    "email": re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+"),
+    "phone": re.compile(r"(\+?\d{1,3}[-.\s]?)?(\(?\d{3}\)?[-.\s]?)?\d{3}[-.\s]?\d{4}"),
+    "ssn": re.compile(r"\b\d{3}-\d{2}-\d{4}\b"),
+    "ipv4": re.compile(r"\b(?:\d{1,3}\.){3}\d{1,3}\b"),
+    "credit_card": re.compile(r"\b(?:\d[ -]*?){13,16}\b")
+}
+
+def scan_text(s):
+    hits = {}
+    for k,pat in PATTERNS.items():
+        m = pat.findall(s)
+        if m:
+            hits[k] = m
+    return hits
+
+def sanitize_text(s):
+    out = s
+    for k,pat in PATTERNS.items():
+        out = pat.sub(f"<REDACTED_{k.upper()}>", out)
+    return out
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--mode", choices=["redact","fail"], default="fail")
+    p.add_argument("--out", default="")
+    args = p.parse_args()
+    path = Path(args.input)
+    if not path.exists():
+        print("Input not found", args.input); sys.exit(2)
+    text = path.read_text()
+    hits = scan_text(text)
+    if hits:
+        print("PII detected:", json.dumps(hits, indent=2))
+        if args.mode == "fail":
+            print("Failing due to PII presence. Use --mode redact to produce sanitized output.")
+            sys.exit(3)
+        else:
+            sanitized = sanitize_text(text)
+            outp = args.out or (str(path) + ".sanitized")
+            Path(outp).write_text(sanitized)
+            print("Sanitized output written to", outp)
+            sys.exit(0)
+    else:
+        print("No PII detected")
+        if args.out:
+            Path(args.out).write_text(text)
+        sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/nightly_qpu_sandbox.yml
+name: Nightly QPU Sandbox Orchestration
+
+on:
+  schedule:
+    - cron: "0 3 * * *" # daily at 03:00 UTC
+  workflow_dispatch:
+
+jobs:
+  qpu-sandbox:
+    runs-on: ubuntu-latest
+    env:
+      EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+      SANDBOX_BUDGET_USD: ${{ secrets.SANDBOX_QPU_BUDGET_USD }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install boto3 mlflow
+      - name: Run cost estimator for providers
+        id: estimate
+        run: |
+          echo "Estimating costs..."
+          python3 ops/qpu_cost_estimator.py --provider braket --shots 1000 --device arn:aws:braket:::device/quantum-simulator/amazon/sv1 > /tmp/braket_est.json
+          python3 ops/qpu_cost_estimator.py --provider ibm --shots 500 --device ibm_washington > /tmp/ibm_est.json
+          python3 ops/qpu_cost_estimator.py --provider azure --shots 500 --device ionq > /tmp/azure_est.json
+          python3 ops/qpu_cost_estimator.py --provider rigetti --shots 500 --device rigetti_qpu > /tmp/rigetti_est.json
+          cat /tmp/*_est.json
+          jq -s '.' /tmp/*_est.json > /tmp/all_est.json
+          echo "estimates<<EOF" >> $GITHUB_OUTPUT
+          cat /tmp/all_est.json >> $GITHUB_OUTPUT
+          echo "EOF" >> $GITHUB_OUTPUT
+      - name: Budget enforcement
+        run: |
+          BUD=${{ secrets.SANDBOX_QPU_BUDGET_USD }}
+          if [ -z "$BUD" ]; then echo "No budget configured; aborting sandbox run"; exit 2; fi
+          python3 - <<'PY'
+import json
+all_est = json.load(open('/tmp/all_est.json'))
+total = sum(e['estimated_usd'] for e in all_est)
+print("Total estimated USD:", total)
+import os,sys
+if total > float(os.environ.get('SANDBOX_BUDGET_USD', '0')): 
+    print("Estimated cost exceeds budget; aborting sandbox run"); sys.exit(3)
+print("Within budget; proceeding")
+PY
+      - name: Submit Argo workflows (operator must run in cluster or use remote argo CLI)
+        run: |
+          echo "Argo submission is operator-controlled. Example commands:"
+          echo "argo submit -n aegis argo/quantum_braket_workflow.yaml -p run-type=braket -p circuit-file-s3=s3://REPLACE_BUCKET/quantum/circuits/sandbox_bell.py -p image=ghcr.io/yourorg/aegis-quantum:latest"
+      - name: Generate report
+        run: |
+          python3 ops/generate_qpu_report.py --estimates /tmp/all_est.json --out /tmp/qpu_sandbox_report.json
+          aws s3 cp /tmp/qpu_sandbox_report.json s3://${{ secrets.EVIDENCE_BUCKET }}/monitoring/qpu_sandbox_reports/${{ github.run_id }}.json
+
*** End Patch
*** Begin Patch
*** Add File:ops/generate_qpu_report.py
+#!/usr/bin/env python3
+"""
+Aggregate estimator results and (optionally) Argo-run outputs to a JSON report uploaded to S3.
+Usage:
+  python ops/generate_qpu_report.py --estimates /tmp/all_est.json --out /tmp/report.json
+"""
+import argparse, json, os
+from datetime import datetime
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--estimates", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    est = json.load(open(args.estimates))
+    total = sum(e.get("estimated_usd",0) for e in est)
+    report = {
+        "timestamp": datetime.utcnow().isoformat() + "Z",
+        "estimates": est,
+        "total_estimated_usd": total,
+        "notes": "This report was generated by ops/generate_qpu_report.py. For actual billing, query provider APIs."
+    }
+    json.dump(report, open(args.out, "w"), indent=2)
+    print("Report written to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/qpu_ci_dispatcher_check.py
+#!/usr/bin/env python3
+"""
+Small helper for CI that ensures:
+ - PR has qpu-approved label (if it touches quantum artifacts)
+ - estimated cost < provided budget
+ Returns non-zero exit code to block CI if checks fail.
+"""
+import os, sys, json, subprocess, argparse
+
+def run_estimator(provider, shots, device, budget):
+    out = subprocess.check_output(["python3","ops/qpu_cost_estimator.py","--provider",provider,"--shots",str(shots),"--device",device])
+    j = json.loads(out)
+    est = j.get("estimated_usd", 0.0)
+    print("Estimated cost:", est)
+    return est <= float(budget), j
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--provider", required=True)
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="simulator")
+    p.add_argument("--budget", required=True)
+    args = p.parse_args()
+    ok, details = run_estimator(args.provider, args.shots, args.device, args.budget)
+    if not ok:
+        print("Estimated cost exceeds budget:", details)
+        sys.exit(2)
+    print("Cost check OK:", details)
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/quantum_rigetti_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: quantum-rigetti-
+  namespace: aegis
+spec:
+  entrypoint: rigetti-job
+  arguments:
+    parameters:
+      - name: circuit-file-s3
+        value: "s3://REPLACE_BUCKET/quantum/circuits/example_rigetti_circuit.py"
+      - name: run-type
+        value: "rigetti"
+      - name: device
+        value: "rigetti_qpu"
+      - name: s3-output
+        value: "s3://REPLACE_BUCKET/quantum/results/{{workflow.uid}}"
+      - name: image
+        value: "ghcr.io/yourorg/aegis-quantum:latest"
+  templates:
+    - name: rigetti-job
+      inputs:
+        parameters:
+          - name: circuit-file-s3
+          - name: run-type
+          - name: device
+          - name: s3-output
+          - name: image
+      container:
+        image: "{{inputs.parameters.image}}"
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            aws s3 cp "{{inputs.parameters.circuit-file-s3}}" /tmp/circuit.py
+            # Placeholder: integrator must implement submit_rigetti_task in circuit file
+            python - <<'PY'
+import importlib.util, subprocess, json
+spec = importlib.util.spec_from_file_location("cmod","/tmp/circuit.py")
+mod = importlib.util.module_from_spec(spec)
+spec.loader.exec_module(mod)
+if hasattr(mod,"submit_rigetti_task"):
+    mod.submit_rigetti_task("{{inputs.parameters.device}}","{{inputs.parameters.s3-output}}")
+else:
+    # fallback to local simulation
+    if hasattr(mod,"build_circuit"):
+        from qiskit import Aer, execute
+        qc = mod.build_circuit()
+        job = execute(qc, backend=Aer.get_backend('aer_simulator'), shots=256)
+        res = job.result().get_counts()
+        open('/tmp/result.json','w').write(json.dumps(res))
+        subprocess.check_call(['aws','s3','cp','/tmp/result.json','{{inputs.parameters.s3-output}}/result.json'])
+PY
+      resources:
+        requests:
+          cpu: "1000m"
+          memory: "2Gi"
+        limits:
+          cpu: "2000m"
+          memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:docs/quantum_mlflow_schema.md
+# Aegis Quantum MLflow Schema
+
+This document describes the minimal MLflow metadata required for every quantum experiment run recorded in Aegis.
+
+See mlflow/quantum_schema.json for the machine-readable JSON Schema.
+
+Required fields (summary)
+- circuit_spec: JSON object with source (git path or S3), parameters and encoding description.
+- provider: braket | ibm | azure | rigetti | simulator
+- provider_job_id: provider-assigned job id or simulator marker
+- shots: integer number of shots executed
+- device: device name or ARN
+- seed: integer RNG seed used for sampling/transpilation
+- result_s3_uri: S3 path to archived result bundle (.tgz)
+- signed_receipt_s3_uri: S3 path to cosign signature or signed receipt
+
+Best practice
+- Log a single artifact: quantum/metadata.json with the above structure.
+- Add MLflow tags: quantum_provider, quantum_owner, quantum_experiment
+- Cosign-sign the archived result bundle and upload signature alongside the archive; log signed_receipt_s3_uri to metadata.json.
+
*** End Patch
*** Begin Patch
*** Add File:examples/quantum/sample_circuits/metadata_example.json
+{
+  "provider": "braket",
+  "shots": 1024,
+  "device": "arn:aws:braket:::device/quantum-simulator/amazon/sv1",
+  "circuit_complexity": 1.2,
+  "owner": "data-science-team@example.com"
+}
+
*** End Patch
*** Begin Patch
*** Add File:README_QUANTUM_PROVIDER_INTEGRATION.md
+# Quantum Provider Integration — What this patch provides
+
+This patch adds provider-specific integration scaffolds for AWS Braket, IBM Quantum, Azure Quantum and Rigetti:
+
+- IRSA ServiceAccount templates (k8s/irsa/*) to annotate ServiceAccounts with role ARNs.
+- ExternalSecrets templates for provider credentials (k8s/externalsecrets/*.yaml).
+- IAM policy snippets for providers (iam/*_policy.json) — review and attach to IRSA roles.
+- Argo workflow templates for each provider (argo/*_workflow.yaml) that call the generic quantum/braket_job.py or provider-specific submitter logic.
+- A cost estimator (ops/qpu_cost_estimator.py) used by CI and submission gates.
+- MLflow quantum metadata schema (mlflow/quantum_schema.json) and validator (mlflow/validate_quantum_mlflow.py).
+- A QA test (tests/mlflow_quantum_qa.py) that scans recent runs for schema compliance (use in CI).
+- A guarded Argo submit helper (ops/argo_submit_guard.sh) that enforces cost budget before submission.
+- A sanitizer to prevent sending PII to QPUs (ops/legal_safe_sanitizer.py).
+- CI integrations:
+  - qpu_pre_submit_check.yml: PR-time pre-check requiring .qpu/metadata.json and qpu-approved label.
+  - qpu_pre_submit_check & qpu_pre_submit_dispatcher guard cost and approval.
+  - nightly_qpu_sandbox.yml: nightly sandbox orchestration with cost enforcement and report upload.
+- Utilities: ops/generate_qpu_report.py and cost estimator integration for nightly runs.
+
+Operator notes & next steps
+- MUST review and adapt all IAM policies (replace REPLACE_EVIDENCE_BUCKET, REPLACE_ACCOUNT, and KMS ARNs) before attaching.
+- Create IRSA roles (via eksctl or terraform) and set the role ARNs in the k8s/irsa/* ServiceAccount manifests (or use ops/annotate_serviceaccounts.sh).
+- Prefer IRSA over static secrets. If using ExternalSecrets, configure your SecretStore (Vault/SecretsManager/KeyVault) accordingly.
+- Update Argo workflows with real image names and S3 prefixes before submitting in staging.
+- For production QPU runs, obtain legal sign-off for data being sent to external providers and ensure PII is sanitized.
+
*** End Patch
*** End Patch
