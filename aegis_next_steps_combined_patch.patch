*** Begin Patch
*** Add File:issues/ISSUES_BACKLOG.md
+## Aegis Backlog — Prioritized Remaining Work (create GitHub issues from these)
+
+Each block below is a suggested GitHub issue. Copy into Issues UI or import via API. Fields: title, description, estimate (days), priority, suggested assignee.
+
+- Title: "Collect sustained telemetry for representative node types"
+  Description: |
+    Deploy node-power-exporter DaemonSet to a representative cohort of nodes.
+    Collect wall/power telemetry and job runtime features for at least 7–14 days.
+    Ensure telemetry files are written to PVC `/data/telemetry` and ingested by calibration job.
+    Deliverable: dataset in S3 or PVC, Prometheus targets healthy, sample CSVs.
+  Estimate: 3
+  Priority: high
+  Assignee: @ml-platform
+
+- Title: "Iterative calibration & CI gating (holdout validation)"
+  Description: |
+    Wire CI job to run iterative calibration: run calibration Job, train runtime predictor, compute validation median relative error on holdout reconciliations.
+    Fail promotion if median error > 10%.
+    Deliverable: GitHub Action + validation script (already added) and documented manual approval step.
+  Estimate: 3
+  Priority: high
+  Assignee: @ml-platform
+
+- Title: "Scheduler extender & Argo admission enforcer rollout (staging → prod)"
+  Description: |
+    Deploy kube-scheduler extender in staging and configure Argo admission enforcer to mutate workflows with nodeSelector or deny per policy.
+    Create test harness that submits synthetic workflows and measures enforcement coverage; iterate until ≥95% coverage.
+    Deliverable: extender deployment, webhook config, enforcement test results.
+  Estimate: 10
+  Priority: high
+  Assignee: @k8s-sre
+
+- Title: "Implement bulk parquet pipeline → DW loader and IAM role"
+  Description: |
+    Run s3_parquet_pipeline_v2 on representative dataset. Create COPY manifest and bulk-load into Redshift/BigQuery/Snowflake.
+    Provision minimal IAM role with s3:GetObject/ListBucket for the DW. Implement idempotent merge/upsert (MERGE/DELETE+INSERT).
+    Deliverable: successful bulk load into dev DW with MERGE, and Terraform snippets for IAM.
+  Estimate: 7
+  Priority: high
+  Assignee: @data-eng
+
+- Title: "Great Expectations productionization & alerts"
+  Description: |
+    Run GE validations in CI and in Airflow pipeline. Publish GE success gauge to Prometheus via exporter. Create Alertmanager rules that route failures to Slack/PagerDuty.
+    Deliverable: GE runs every DAG, Prometheus metric, alerting configured.
+  Estimate: 4
+  Priority: medium
+  Assignee: @data-quality
+
+- Title: "Security hardening: PKI, KMS, cosign key governance, RBAC"
+  Description: |
+    Replace self-signed webhook certs with corporate PKI / cert-manager Vault Issuer. Provision KMS key for cosign with tight key policy, rotate key and update CI. Audit service accounts and tighten RBAC rules.
+    Deliverable: Vault/corporate issuer configured, KMS key policy in Terraform, updated RBAC manifests.
+  Estimate: 8
+  Priority: high
+  Assignee: @security
+
+- Title: "Operational runbooks & DR drills"
+  Description: |
+    Create concise incident playbooks and schedule DR/resume drills for suspend/resume and bulk-load recovery. Validate runbooks with an on-call drill.
+    Deliverable: executed drill, updated runbooks, issues closed from drill findings.
+  Estimate: 5
+  Priority: medium
+  Assignee: @sre
+
+- Title: "BI SSO & Row-Level Security"
+  Description: |
+    Configure Superset SSO (OIDC with Keycloak/Okta), configure dataset-level RBAC and implement DB session pre-query to set `app.team` for RLS. Validate with two teams.
+    Deliverable: SSO working, RLS enforced in Superset and DB policies.
+  Estimate: 4
+  Priority: medium
+  Assignee: @analytics
+
+---
+How to use:
+- Create issues from each YAML block or paste directly into new issue UI.
+- Add links to runbooks and files referenced in descriptions.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/telemetry_retrain_pipeline.yml
+name: Telemetry → Calibration → Retrain Pipeline (with manual promote)
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 7 * * 1' # weekly Monday 07:00 UTC
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  KUBECONFIG: ${{ secrets.KUBECONFIG }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+  MEDIAN_THRESHOLD: "0.10"
+
+jobs:
+  calibrate-and-train:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.28.0'
+
+      - name: Configure kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+
+      - name: Trigger calibration Job (staging)
+        run: |
+          kubectl -n aegis apply -f ci/jobs/calibration_job.yaml
+          kubectl -n aegis wait --for=condition=complete job/aegis-calibration-manual --timeout=1800s
+
+      - name: Trigger runtime training Job
+        run: |
+          kubectl -n aegis apply -f ci/jobs/runtime_train_job.yaml
+          kubectl -n aegis wait --for=condition=complete job/aegis-runtime-train --timeout=3600s
+
+      - name: Run validation gating (compute median error)
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          pip install boto3
+          python3 ci/validate_model_gating.py --bucket "${EVIDENCE_BUCKET}" --prefix "reconciliations/" --threshold "${{ env.MEDIAN_THRESHOLD }}" --upload-report "true"
+
+  promote:
+    needs: calibrate-and-train
+    if: success()
+    runs-on: ubuntu-latest
+    environment:
+      name: production
+      url: https://kubernetes.example.com
+    steps:
+      - name: Manual approval required
+        run: |
+          echo "This job requires a manual approval in GitHub environment protections before continuing."
+      - name: Apply to production (manual step)
+        run: |
+          echo "Operator must manually apply the production ConfigMap from the approved artifacts in S3."
+        # NOTE: Intentionally no automated apply to production. Operator should review evidence in S3 and apply using kubectl.
+
*** End Patch
*** Begin Patch
*** Add File:ci/PR_TELEMETRY_RETRAIN.md
+Telemetry → Calibration → Retrain PR checklist (for reviewers / operators)
+
+What this PR / workflow does:
+- Runs calibration job in-staging (calculation of power profiles).
+- Runs runtime predictor training on collected telemetry.
+- Runs validation gate computing median relative error from reconciliations.
+- If validation passes, creates a validation report in S3. Promotion to production requires manual approval.
+
+Review steps:
+1. Confirm kubeconfig secret is correct and scoped to staging cluster.
+2. Confirm EVIDENCE_BUCKET secret exists and is writable by CI.
+3. Confirm COSIGN_KMS_KEY_ARN in secrets for signing artifacts if desired.
+4. Approve the workflow result after manual inspection of validation report in S3:
+   - Inspect S3 path `s3://<EVIDENCE_BUCKET>/calibration/` and `s3://<EVIDENCE_BUCKET>/calibration/drafts/`
+   - Open validation report uploaded by the workflow (JSON) and verify median_relative_error <= 0.10 for targeted node types.
+5. If acceptable, operator runs (manual):
+   kubectl -n aegis create configmap aegis-power-profiles --from-file=power_profiles.yaml=/path/to/approved_power_profiles.yaml --dry-run=client -o yaml | kubectl apply -f -
+
+Notes:
+- Promotion is intentionally manual to satisfy audit: operator records evidence and signs promotion with cosign if required.
+- Use GitHub environment protections to require a human approver for the `promote` job.
+
*** End Patch
*** Begin Patch
*** Add File:extender/README_EXTENDER_ROLLOUT.md
+# Scheduler Extender & Argo Enforcer Rollout Runbook (staged)
+
+Goal
+- Safely roll out kube-scheduler extender and Argo admission enforcer to enforce Aegis placement hints at submit time.
+
+Prereqs
+- Staging Kubernetes cluster with kube-scheduler configurable (or managed cluster with extender support).
+- cert-manager installed and a working Issuer (corporate PKI or Vault).
+- Service accounts and minimal RBAC (provided).
+- Redis and token‑budget accessible from extender/enforcer.
+
+Steps (staging)
+1. Deploy the extender service:
+   kubectl apply -f extender/deploy_extender.yaml
+2. Expose extender via Service (done in manifest). Lock network access via NetworkPolicy if available.
+3. Configure scheduler policy to call extender (edit kube-scheduler config):
+   extenders:
+     - urlPrefix: "http://aegis-scheduler-extender.kube-system.svc"
+       filterVerb: "filter"
+       prioritizeVerb: "prioritize"
+       weight: 1
+4. Deploy Argo admission enforcer (mutating webhook):
+   - Create TLS certificate via cert-manager for `aegis-argo-enforcer.aegis.svc`
+   - Deploy service + deployment: (use enforcement/enforcer_admission.py packaged)
+   - Create MutatingWebhookConfiguration pointing to the service and patch caBundle (cert-manager/patch script helps)
+5. Run enforcement enforcer E2E test:
+   python3 enforcement/enforcer_e2e_test.py
+6. Collect metrics (use enforcement_coverage_exporter) and compute coverage: enforced / triggered.
+
+Staged rollout to prod
+1. Start with a single team/namespace in prod and monitor metrics for 48–72 hours.
+2. Increase scope gradually (50% of policy-driven jobs, then 100%).
+3. Run DR/resume drills during low traffic windows.
+
+Rollback
+- Remove extender entry from kube-scheduler config and restart scheduler. Remove MutatingWebhookConfiguration.
+
+Security
+- Restrict extender Service via NetworkPolicy. Ensure webhook TLS is signed by corporate PKI and CA bundle is correct.
+
*** End Patch
*** Begin Patch
*** Add File:extender/manifests/enforcer-webhook-manifests.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-enforcer-sa
+  namespace: aegis
+
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-argo-enforcer
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-argo-enforcer
+  template:
+    metadata:
+      labels:
+        app: aegis-argo-enforcer
+    spec:
+      serviceAccountName: aegis-enforcer-sa
+      containers:
+        - name: enforcer
+          image: ghcr.io/yourorg/aegis-argo-enforcer:latest
+          ports:
+            - containerPort: 9443
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: aegis-argo-enforcer-tls
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-argo-enforcer
+  namespace: aegis
+spec:
+  selector:
+    app: aegis-argo-enforcer
+  ports:
+    - port: 443
+      targetPort: 9443
+
+---
+# MutatingWebhookConfiguration (caBundle to be patched by cert-manager helper)
+apiVersion: admissionregistration.k8s.io/v1
+kind: MutatingWebhookConfiguration
+metadata:
+  name: aegis-argo-enforcer-webhook
+webhooks:
+  - name: enforcer.aegis.example.com
+    admissionReviewVersions: ["v1"]
+    clientConfig:
+      service:
+        name: aegis-argo-enforcer
+        namespace: aegis
+        path: "/mutate"
+      caBundle: "REPLACE_WITH_CA_BUNDLE"
+    failurePolicy: Fail
+    sideEffects: None
+    timeoutSeconds: 10
+    rules:
+      - apiGroups: ["argoproj.io"]
+        apiVersions: ["v1alpha1"]
+        operations: ["CREATE"]
+        resources: ["workflows"]
+
*** End Patch
*** Begin Patch
*** Add File:handoff/HANDOFF_CHECKLIST.md
+# One-page Handoff Checklist — Delegate this to an operator
+
+Goal: Give a colleague the minimal steps to run a safe weekly telemetry→calibration→retrain cycle, test enforcement in staging, and check ETL health.
+
+Pre-reqs (operator)
+- Access to GitHub repo & Actions, kubeconfig for staging cluster, AWS creds with S3 access, Slack webhook if configured.
+
+Checklist (30–120 minutes)
+1. Telemetry check (10–20m)
+   - kubectl -n observability get pods -l app=node-power-exporter
+   - In Prometheus: check target `node-power-exporter` and that `aegis_node_power_*` metrics show.
+
+2. Run calibration & train (25–60m)
+   - Trigger CI workflow: GitHub -> Actions -> Telemetry → Calibration (workflow_dispatch)
+   - Wait for `calibrate-and-train` job to complete.
+   - Inspect S3 under `s3://<EVIDENCE_BUCKET>/calibration/` for outputs and `calibration/drafts/` for suggestions.
+
+3. Validate gate (10–20m)
+   - Review validation report in S3: `s3://<EVIDENCE_BUCKET>/calibration/validation_reports/`
+   - Confirm median_relative_error ≤ 0.10 for key node types.
+   - If pass, proceed to manual promotion step (see PR checklist). If fail, document and escalate to @ml-platform.
+
+4. Enforcement smoke test (20–40m)
+   - Deploy extender/enforcer in staging if not already (extender/deploy_extender.yaml + extender/manifests/enforcer-webhook-manifests.yaml).
+   - Run E2E test: `python3 enforcement/enforcer_e2e_test.py`
+   - Check Redis counters or Prometheus metrics for enforcement coverage.
+
+5. ETL health check (10–30m)
+   - Verify Airflow DAG `aegis_etl_robust` last run success.
+   - Check Prometheus metrics: `aegis_etl_parquet_failed`, `aegis_etl_bulk_load_failed`.
+   - If any alert, follow incident_playbooks.md steps.
+
+6. Report & sign (10m)
+   - If promotion done: operator documents action and signs the evidence artifact (cosign) and uploads to evidence S3.
+   - Post a short note in Slack channel #aegis-ops with links to reports.
+
+Contacts
+- ML Platform: ml-platform@example.com
+- Data Eng: data-eng@example.com
+- SRE: oncall@sre.example.com
+- Security: security@example.com
+
*** End Patch
*** End Patch
