*** Begin Patch
*** Add File: policies/opa/edge_deploy.rego
+package aegis.policies.edge_deploy
+
+default allow = false
+
+# Require presence of model_card and attestation in the manifest before rollout
+allow {
+  input.manifest.model_card != ""
+  input.manifest.attestation != ""
+  not failed_checks(input)
+}
+
+failed_checks(input) {
+  # fairness check: if provided, fail when group_gap > threshold (0.1 default)
+  some g
+  vals := input.fairness
+  vals != null
+  gap := vals.group_gap
+  gap > (input.fairness_threshold | 0.1)
+}
+
+# Helper: require model_card path exists (caller must populate model_card_exists)
+model_card_required {
+  input.manifest.model_card != ""
+  input.model_card_exists == true
+}
+
*** End Patch
*** Begin Patch
*** Add File: tools/check_model_card.py
+#!/usr/bin/env python3
+"""
+Check that model_card.md exists in an MLflow run's artifacts.
+Usage:
+  python tools/check_model_card.py --run-id <mlflow-run-id> --mlflow-uri <uri>
+Exits 0 if present, non-zero otherwise.
+"""
+import argparse
+import mlflow
+import sys
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--run-id", required=True)
+    p.add_argument("--mlflow-uri", default=None)
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    if args.mlflow_uri:
+        mlflow.set_tracking_uri(args.mlflow_uri)
+    client = mlflow.tracking.MlflowClient()
+    try:
+        artifacts = client.list_artifacts(args.run_id, path=".")
+    except Exception as e:
+        print("ERROR listing artifacts:", e, file=sys.stderr)
+        sys.exit(2)
+    names = [a.path for a in artifacts]
+    # check for model_card.md or model_card.json
+    ok = any(n.lower().endswith("model_card.md") or n.lower().endswith("model_card.json") for n in names)
+    if ok:
+        print("model_card found")
+        sys.exit(0)
+    print("model_card missing", file=sys.stderr)
+    sys.exit(1)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: tools/verify_rekor_entry.py
+#!/usr/bin/env python3
+"""
+Verify that a manifest (by digest) has an entry in Rekor transparency log.
+Requires 'rekor-cli' in PATH and REKOR_SERVER env var (optional).
+Usage:
+  python tools/verify_rekor_entry.py --manifest manifest.json
+Exits 0 if found, non-zero otherwise.
+"""
+import argparse
+import subprocess
+import json
+import hashlib
+import sys
+from pathlib import Path
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    return p.parse_args()
+
+def rekor_search(digest):
+    # Uses rekor-cli to search by artifact hash
+    try:
+        out = subprocess.check_output(["rekor-cli", "search", "hash", digest], stderr=subprocess.STDOUT)
+        return out.decode()
+    except subprocess.CalledProcessError as e:
+        return ""
+
+def main():
+    args = parse_args()
+    manifest = Path(args.manifest)
+    if not manifest.exists():
+        print("manifest missing", file=sys.stderr)
+        sys.exit(2)
+    digest = hashlib.sha256(manifest.read_bytes()).hexdigest()
+    print("manifest sha256:", digest)
+    res = rekor_search(digest)
+    if res and "UUID" in res:
+        print("rekor entry found")
+        sys.exit(0)
+    print("rekor entry not found", file=sys.stderr)
+    sys.exit(1)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: tools/opa_eval.py
+#!/usr/bin/env python3
+"""
+Evaluate an OPA policy against an input JSON.
+Requires `opa` binary in PATH.
+Usage:
+  python tools/opa_eval.py --policy policies/opa/edge_deploy.rego --input payload.json
+"""
+import argparse
+import subprocess
+import sys
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--policy", required=True)
+    p.add_argument("--input", required=True)
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    cmd = ["opa", "eval", "-d", args.policy, "-i", args.input, "data.aegis.policies.edge_deploy.allow", "-f", "json"]
+    try:
+        out = subprocess.check_output(cmd)
+        print(out.decode())
+        # parse JSON to check if result is true
+        import json
+        j = json.loads(out.decode())
+        # OPA returns result[].expressions[0].value
+        val = j.get("result", [])[0].get("expressions", [])[0].get("value", False)
+        if val is True:
+            print("OPA allow == true")
+            sys.exit(0)
+        else:
+            print("OPA denied")
+            sys.exit(1)
+    except subprocess.CalledProcessError as e:
+        print("OPA eval failed", e.output.decode(), file=sys.stderr)
+        sys.exit(2)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/hardware_validation_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run hardware validation on a device (jetson/arm) and produce JSON report.
+# Example checks: inference latency, memory use, power sampling (if available), correctness checks.
+OUT="${1:-./hardware_validation.json}"
+URL="${2:-http://localhost:8080/infer}"
+N="${3:-10}"
+
+python - <<PY
+import time, json, requests, statistics
+lat=[]
+for i in range(${N}):
+    t0=time.time()
+    try:
+        r=requests.post("${URL}", json={"tensor":[[[0.0]*224]*224]}, timeout=30)
+        ok=r.status_code==200
+    except Exception as e:
+        ok=False
+    t=time.time()-t0
+    lat.append(t)
+report={"n": len(lat), "p50": statistics.median(lat) if lat else None, "p95": sorted(lat)[int(0.95*len(lat))-1] if len(lat)>0 else None, "success": all(True for _ in lat)}
+open("${OUT}","w").write(json.dumps(report, indent=2))
+print("wrote", "${OUT}")
+PY
+
*** End Patch
*** Begin Patch
*** Add File: scripts/upload_validation.py
+#!/usr/bin/env python3
+"""
+Upload hardware validation report to MLflow as an artifact and optionally to controller.
+Usage: python scripts/upload_validation.py --run-id <mlflow-run-id> --file hardware_validation.json --mlflow-uri ...
+"""
+import argparse
+import mlflow
+from pathlib import Path
+import requests
+
+def parse_args():
+    p=argparse.ArgumentParser()
+    p.add_argument("--run-id", required=True)
+    p.add_argument("--file", required=True)
+    p.add_argument("--mlflow-uri", default=None)
+    p.add_argument("--controller-url", default=None)
+    p.add_argument("--device-id", default=None)
+    return p.parse_args()
+
+def main():
+    args=parse_args()
+    if args.mlflow_uri:
+        mlflow.set_tracking_uri(args.mlflow_uri)
+    with mlflow.start_run(run_id=args.run_id) as run:
+        mlflow.log_artifact(args.file, artifact_path="hardware_validation")
+    if args.controller_url and args.device_id:
+        try:
+            requests.post(f"{args.controller_url}/api/v1/devices/{args.device_id}/telemetry", json={"hardware_validation": Path(args.file).read_text()})
+        except Exception:
+            pass
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: tools/create_release_with_checks.py
+#!/usr/bin/env python3
+"""
+CI helper to run the promotion checks and create a release in the fleet controller only if checks pass.
+Steps:
+ - Validate model_card existence in MLflow
+ - Verify Rekor entry for manifest
+ - Run OPA policy evaluation
+ - On success, POST to controller /api/v1/releases
+
+Usage:
+  python tools/create_release_with_checks.py --run-id <mlflow-run-id> --manifest manifest.json --controller-url ...
+"""
+import argparse
+import subprocess
+import requests
+import json
+import sys
+from pathlib import Path
+
+def parse_args():
+    p=argparse.ArgumentParser()
+    p.add_argument("--run-id", required=True)
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--mlflow-uri", default=None)
+    p.add_argument("--controller-url", required=True)
+    p.add_argument("--api-key", required=True)
+    return p.parse_args()
+
+def run(cmd):
+    print(">", " ".join(cmd))
+    subprocess.check_call(cmd)
+
+def main():
+    args=parse_args()
+    # check model_card
+    cmd = ["python", "tools/check_model_card.py", "--run-id", args.run_id]
+    if args.mlflow_uri:
+        cmd += ["--mlflow-uri", args.mlflow_uri]
+    try:
+        run(cmd)
+    except Exception:
+        print("model_card check failed", file=sys.stderr)
+        sys.exit(2)
+
+    # verify rekor
+    try:
+        run(["python", "tools/verify_rekor_entry.py", "--manifest", args.manifest])
+    except Exception:
+        print("rekor verification failed", file=sys.stderr)
+        sys.exit(3)
+
+    # build OPA input payload
+    manifest = json.loads(Path(args.manifest).read_text())
+    payload = {
+        "manifest": manifest,
+        "model_card_exists": True,
+        "fairness": {"group_gap": 0.05},
+        "fairness_threshold": 0.1
+    }
+    Path("/tmp/opa_input.json").write_text(json.dumps(payload))
+    try:
+        run(["python", "tools/opa_eval.py", "--policy", "policies/opa/edge_deploy.rego", "--input", "/tmp/opa_input.json"])
+    except Exception:
+        print("OPA policy check failed", file=sys.stderr)
+        sys.exit(4)
+
+    # all checks passed -> create release on controller
+    headers = {"X-API-Key": args.api_key, "Content-Type": "application/json"}
+    with open(args.manifest) as fh:
+        manifest_obj = json.load(fh)
+    release = {"bundle_url": manifest_obj.get("bundle_url") or manifest_obj.get("bundle_uri") or "", "manifest": manifest_obj}
+    r = requests.post(f"{args.controller_url}/api/v1/releases", json=release, headers=headers, timeout=15)
+    if r.status_code != 200:
+        print("controller rejected release", r.status_code, r.text, file=sys.stderr)
+        sys.exit(5)
+    print("release created:", r.json())
+    sys.exit(0)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: ci/.github/workflows/edge_promote.yml
+name: Edge Promotion (gated)
+on:
+  workflow_dispatch:
+
+jobs:
+  promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+
+      - name: Run fairness check
+        run: |
+          python pipeline/checks/fairness_check.py --preds-csv preds.csv --threshold 0.15
+
+      - name: Run explainability sample (logs SHAP to MLflow)
+        run: |
+          python pipeline/explainability/generate_sample_shap.py --run-id ${{ secrets.MLFLOW_RUN_ID }} --mlflow-uri ${{ secrets.MLFLOW_URI }}
+
+      - name: Verify model_card
+        run: |
+          python tools/check_model_card.py --run-id ${{ secrets.MLFLOW_RUN_ID }} --mlflow-uri ${{ secrets.MLFLOW_URI }}
+
+      - name: Verify Rekor entry
+        run: |
+          python tools/verify_rekor_entry.py --manifest manifest.json
+
+      - name: Run OPA policy
+        run: |
+          python tools/opa_eval.py --policy policies/opa/edge_deploy.rego --input opa_input.json
+
+      - name: Hardware validation (Jetson) - self-hosted runner required
+        runs-on: [self-hosted, jetson]
+        steps:
+          - uses: actions/checkout@v4
+          - name: Run hardware validation script on device
+            run: |
+              ./scripts/hardware_validation_runner.sh ./hardware_validation.json ${{ secrets.EDGE_RUNTIME_URL }} 10
+              python scripts/upload_validation.py --run-id ${{ secrets.MLFLOW_RUN_ID }} --file ./hardware_validation.json --mlflow-uri ${{ secrets.MLFLOW_URI }} --controller-url ${{ secrets.CONTROLLER_URL }} --device-id ${{ secrets.DEVICE_ID }}
+
+      - name: Create release (controller) after checks
+        run: |
+          python tools/create_release_with_checks.py --run-id ${{ secrets.MLFLOW_RUN_ID }} --manifest manifest.json --controller-url ${{ secrets.CONTROLLER_URL }} --api-key ${{ secrets.CONTROLLER_API_KEY }} --mlflow-uri ${{ secrets.MLFLOW_URI }}
+
*** End Patch
*** Begin Patch
*** Add File: docs/edge_deployment_process.md
+Edge deployment process (high level)
+
+1) ML team: produces model run in MLflow, includes model_card.md and evaluation artifacts (fairness, explainability samples).
+2) CI pipeline: runs automated checks:
+   - fairness_check, robustness_check, explainability sample capture
+   - verify model_card presence
+   - sign manifest and submit manifest to Rekor (cosign/cosign KMS)
+   - run OPA policy (edge_deploy.rego) against manifest + metrics
+   - run hardware validation on target hardware (self-hosted runner)
+3) Only when all checks pass will the CI helper call the Fleet Controller to create a release and start a staged rollout.
+4) Fleet Controller assigns bundles to devices based on rollout percentage. Agents will verify bundle signatures and apply them.
+5) Telemetry & validation results are sent back to MLflow/Controller and used to evaluate health and (if needed) trigger automated rollback.
+
+Roles
+- ML teams: model development, model cards, evaluation metrics, explainability artifacts.
+- Infra/SRE: building optimized artifacts, KMS/HSM key management, self-hosted hardware runners, monitoring & CI infra.
+- Fleet Ops: release orchestration, device provisioning, rollout policies and runbooks.
+
*** End Patch
*** Begin Patch
*** Add File: CODEOWNERS
+# CODEOWNERS to map responsibilities
+/.github/workflows/ @aegis-infra
+/fleet/ @aegis-fleet-ops
+/policies/opa/ @aegis-ml-team
+/pipeline/ @aegis-ml-team
+/scripts/ @aegis-infra
+/edge/ @aegis-fleet-ops
+docs/ @aegis-product
+
*** End Patch
*** Begin Patch
*** Add File: docs/roles.md
+Team responsibilities (recommended)
+
+- ML Team (@aegis-ml-team)
+  - Own model training, MLflow runs, model_card contents, fairness & explainability tests.
+  - Provide acceptance criteria and test datasets for hardware validation.
+
+- Infra / SRE (@aegis-infra)
+  - Manage KMS/HSM keys, Rekor integration, CI runners, build pipelines for target hardware.
+  - Operate Postgres, Prometheus, and central telemetry ingestion.
+
+- Fleet Ops (@aegis-fleet-ops)
+  - Manage fleet controller, device provisioning (Vault PKI), rollout policies and canary strategies.
+  - Operate agent fleet, system updates and runbooks.
+
+Ensure changes to the deployment pipeline and policies are reviewed by the corresponding owners above.
+
*** End Patch
*** End Patch
