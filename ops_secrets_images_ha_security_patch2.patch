*** Begin Patch
*** Add File:ops/populate_secrets_gh_and_aws.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Populate GitHub secrets, AWS SecretsManager and SSM from terraform output.
+# Usage: ./ops/populate_secrets_gh_and_aws.sh /tmp/aegis_tf_output.json
+#
+TF_OUT=${1:-/tmp/aegis_tf_output.json}
+REPO=${GITHUB_REPOSITORY:-owner/repo}
+
+if [ ! -f "$TF_OUT" ]; then
+  echo "Terraform output JSON required: $TF_OUT"
+  exit 2
+fi
+
+: "${GITHUB_TOKEN:?set GITHUB_TOKEN env var or login with gh}"
+
+COSIGN_KMS_ARN=$(jq -r '.cosign_kms_arn.value // .cosign_kms_arn' "$TF_OUT")
+EVIDENCE_BUCKET=$(jq -r '.evidence_bucket.value // .evidence_bucket' "$TF_OUT")
+REKOR_SECRET_ARN=$(jq -r '.rekor_secret_arn.value // .rekor_secret_arn' "$TF_OUT")
+REGISTRY_DB=$(jq -r '.registry_database_url.value // .registry_database_url' "$TF_OUT")
+
+echo "Setting GitHub secrets for $REPO..."
+if command -v gh >/dev/null 2>&1; then
+  echo -n "$COSIGN_KMS_ARN" | gh secret set COSIGN_KMS_ARN --repo "$REPO" --body -
+  echo -n "$EVIDENCE_BUCKET" | gh secret set EVIDENCE_BUCKET --repo "$REPO" --body -
+  echo -n "$REKOR_SECRET_ARN" | gh secret set REKOR_SECRET_ARN --repo "$REPO" --body -
+  echo -n "$REGISTRY_DB" | gh secret set REGISTRY_DATABASE_URL --repo "$REPO" --body -
+else
+  echo "gh CLI not found; please set COSIGN_KMS_ARN, EVIDENCE_BUCKET, REKOR_SECRET_ARN and REGISTRY_DATABASE_URL manually in GitHub repo settings."
+fi
+
+echo "Populating AWS SecretsManager placeholders..."
+aws secretsmanager create-secret --name "aegis/cosign" --description "Cosign KMS mapping" --secret-string "{\"kms_arn\":\"$COSIGN_KMS_ARN\"}" >/dev/null 2>&1 || \
+  aws secretsmanager put-secret-value --secret-id "aegis/cosign" --secret-string "{\"kms_arn\":\"$COSIGN_KMS_ARN\"}" >/dev/null 2>&1 || true
+
+aws ssm put-parameter --name "/aegis/evidence_bucket" --value "$EVIDENCE_BUCKET" --type String --overwrite || true
+
+echo "Done. Verify Kubernetes ExternalSecrets sync and GitHub secrets visibility."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/build_and_push_images.yml
+name: Build & Push Pipeline Images
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: read
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to GHCR
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build & push aegis-tools
+        run: |
+          docker build -t ghcr.io/${{ github.repository_owner }}/aegis-tools:latest -f tools/Dockerfile .
+          docker push ghcr.io/${{ github.repository_owner }}/aegis-tools:latest
+      - name: Build & push aegis-train
+        run: |
+          docker build -t ghcr.io/${{ github.repository_owner }}/aegis-train:latest -f train/Dockerfile .
+          docker push ghcr.io/${{ github.repository_owner }}/aegis-train:latest
+      - name: Build & push aegis-transformer
+        run: |
+          docker build -t ghcr.io/${{ github.repository_owner }}/aegis-transformer:latest -f transformer/Dockerfile .
+          docker push ghcr.io/${{ github.repository_owner }}/aegis-transformer:latest
+
*** End Patch
*** Begin Patch
*** Add File:k8s/external-secrets/externalsecret_db.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: aegis-db-creds
+  namespace: aegis
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: aws-secretsmanager
+    kind: SecretStore
+  target:
+    name: aegis-db-creds
+    creationPolicy: Owner
+  data:
+    - secretKey: username
+      remoteRef:
+        key: "aegis/db"
+        property: "username"
+    - secretKey: password
+      remoteRef:
+        key: "aegis/db"
+        property: "password"
+    - secretKey: host
+      remoteRef:
+        key: "aegis/db"
+        property: "host"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/irsa/annotate_serviceaccounts.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Annotate serviceaccounts with IAM role for IRSA.
+# Usage: ./k8s/irsa/annotate_serviceaccounts.sh <serviceaccount> <namespace> <role_arn>
+#
+SA=${1:-cosign-signer}
+NS=${2:-security}
+ROLE_ARN=${3:-}
+
+if [ -z "$ROLE_ARN" ]; then
+  echo "Usage: $0 <serviceaccount> <namespace> <role_arn>"
+  exit 2
+fi
+
+kubectl -n "$NS" annotate serviceaccount "$SA" eks.amazonaws.com/role-arn="$ROLE_ARN" --overwrite
+echo "Annotated serviceaccount $NS/$SA with role $ROLE_ARN"
+
*** End Patch
*** Begin Patch
*** Add File:ops/setup_pvc_and_backups.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Configure storageClass and install Velero + Postgres pg_dump CronJob for backups.
+#
+STORAGE_CLASS=${1:-gp2}
+BUCKET=${2:-REPLACE_EVIDENCE_BUCKET}
+REGION=${3:-us-west-2}
+
+echo "Applying Postgres values with storageClass ${STORAGE_CLASS}"
+mkdir -p registry/postgres
+cat > registry/postgres/values.prod.yaml <<EOF
+primary:
+  persistence:
+    storageClass: "${STORAGE_CLASS}"
+    size: 200Gi
+replica:
+  replicaCount: 2
+  persistence:
+    storageClass: "${STORAGE_CLASS}"
+    size: 200Gi
+resources:
+  requests:
+    cpu: "500m"
+    memory: "1Gi"
+  limits:
+    cpu: "2"
+    memory: "4Gi"
+EOF
+
+echo "Installing Velero (assumes helm repo added) with S3 bucket ${BUCKET}"
+sed "s/REPLACE_EVIDENCE_BUCKET/${BUCKET}/g" velero/values.yaml > /tmp/velero_values.yaml
+helm upgrade --install velero vmware-tanzu/velero -n velero --create-namespace -f /tmp/velero_values.yaml || true
+
+echo "Applying pgdump cronjob for Postgres backups"
+kubectl -n aegis apply -f k8s/postgres/pgdump-to-s3-cronjob.yaml
+
+echo "PVC/storageClass and backup setup applied. Verify Velero & CronJob status."
+
*** End Patch
*** Begin Patch
*** Add File:registry/postgres/values.prod.yaml
+global:
+  postgresql:
+    postgresqlDatabase: aegis_registry
+    postgresqlUsername: aegis
+    postgresqlPassword: "REPLACE_POSTGRES_PASSWORD"
+primary:
+  persistence:
+    storageClass: "REPLACE_STORAGE_CLASS"
+    size: 200Gi
+resources:
+  requests:
+    cpu: "500m"
+    memory: "1Gi"
+  limits:
+    cpu: "2"
+    memory: "4Gi"
+replica:
+  replicaCount: 2
+  persistence:
+    storageClass: "REPLACE_STORAGE_CLASS"
+    size: 200Gi
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kserve/certificate.yaml
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: kserve-tls
+  namespace: kserve
+spec:
+  secretName: kserve-tls-secret
+  dnsNames:
+    - "kserve.aegis.example.com"
+  issuerRef:
+    name: letsencrypt-prod
+    kind: ClusterIssuer
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kserve/tls_ingress.yaml
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: kserve-ingress
+  namespace: aegis
+  annotations:
+    kubernetes.io/ingress.class: "nginx"
+    cert-manager.io/cluster-issuer: "letsencrypt-prod"
+spec:
+  tls:
+    - hosts:
+        - kserve.aegis.example.com
+      secretName: kserve-tls-secret
+  rules:
+    - host: kserve.aegis.example.com
+      http:
+        paths:
+          - path: /
+            pathType: Prefix
+            backend:
+              service:
+                name: ambassador # adapt to your ingress controller e.g., ambassador/istio/traefik
+                port:
+                  number: 80
+
*** End Patch
*** Begin Patch
*** Add File:services/mcpx_logger_auth.py
+#!/usr/bin/env python3
+"""
+MCPx logger with simple token auth. Expects header: Authorization: Bearer <TOKEN>
+TOKEN is stored in k8s secret 'mcpx-auth' key 'token'.
+"""
+from http.server import BaseHTTPRequestHandler, HTTPServer
+import json, os, boto3, time, uuid
+
+S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "aegis-evidence-bucket")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+PORT = int(os.environ.get("PORT", "8080"))
+AUTH_TOKEN = os.environ.get("MCPX_AUTH_TOKEN", "")
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+class Handler(BaseHTTPRequestHandler):
+    def _unauthorized(self):
+        self.send_response(401)
+        self.end_headers()
+        self.wfile.write(b"unauthorized")
+
+    def do_POST(self):
+        auth = self.headers.get("Authorization","")
+        if not AUTH_TOKEN:
+            return self._unauthorized()
+        if not auth.startswith("Bearer ") or auth.split(" ",1)[1] != AUTH_TOKEN:
+            return self._unauthorized()
+        length = int(self.headers.get('content-length', 0))
+        body = self.rfile.read(length)
+        try:
+            payload = json.loads(body)
+        except Exception:
+            payload = {"raw": body.decode('utf-8', errors='ignore')}
+        key = f"aegis/mcpx/{int(time.time())}-{uuid.uuid4().hex}.json"
+        s3.put_object(Bucket=S3_BUCKET, Key=key, Body=json.dumps(payload).encode('utf-8'))
+        print(f"mcpx_logged s3://{S3_BUCKET}/{key}")
+        self.send_response(200)
+        self.end_headers()
+        self.wfile.write(b"ok")
+
+def run():
+    server = HTTPServer(('', PORT), Handler)
+    print(f"Starting MCPx logger (auth enabled) on port {PORT}")
+    server.serve_forever()
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:ops/generate_baseline.py
+#!/usr/bin/env python3
+"""
+Create baseline feature distributions to S3 for drift detectors.
+Reads a sample of historical production features and writes JSON baseline to S3.
+"""
+import os, json, boto3
+import pandas as pd
+
+BUCKET = os.environ.get("EVIDENCE_BUCKET")
+PREFIX = os.environ.get("BASELINE_PREFIX","aegis/baseline")
+SAMPLE_PATH = os.environ.get("BASELINE_SAMPLE_PATH","s3://path/to/historical/features.parquet")
+
+def read_parquet_s3(s3uri):
+    import tempfile
+    parts = s3uri[5:].split("/",1)
+    bucket, key = parts[0], parts[1]
+    s3 = boto3.client("s3")
+    tmp = tempfile.mktemp(suffix=".parquet")
+    s3.download_file(bucket, key, tmp)
+    return pd.read_parquet(tmp)
+
+def compute_baseline(df):
+    baseline = {}
+    for c in df.select_dtypes(include=["number"]).columns:
+        baseline[c] = {"mean": float(df[c].mean()), "std": float(df[c].std()), "hist": df[c].dropna().tolist()[:1000]}
+    return baseline
+
+def main():
+    if not BUCKET:
+        print("EVIDENCE_BUCKET not set; abort")
+        return
+    df = read_parquet_s3(SAMPLE_PATH)
+    baseline = compute_baseline(df)
+    s3 = boto3.client("s3")
+    key = f"{PREFIX}/baseline.json"
+    s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(baseline).encode('utf-8'))
+    print("Wrote baseline to s3://%s/%s" % (BUCKET,key))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/alertmanager_pagerduty.yml.tpl
+global:
+  resolve_timeout: 5m
+route:
+  receiver: 'pagerduty-main'
+  group_by: ['alertname','severity']
+  group_wait: 30s
+  group_interval: 5m
+  repeat_interval: 2h
+receivers:
+  - name: 'pagerduty-main'
+    pagerduty_configs:
+      - service_key: 'REPLACE_PAGERDUTY_SERVICE_KEY'
+        severity: '{{ .CommonLabels.severity }}'
+inhibit_rules:
+  - source_match:
+      severity: 'critical'
+    target_match:
+      severity: 'warning'
+    equal:
+      - alertname
+
*** End Patch
*** Begin Patch
*** Add File:ops/apply_verifier_thresholds.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run eval harness, create PR with thresholds, and post a message to Slack/GitHub.
+# Usage: ops/apply_verifier_thresholds.sh
+#
+: "${GITHUB_TOKEN:?}"
+python3 ops/verifier_model_selector.py --runs 5 --out ops/verifier_recommendation.json
+python3 ops/eval_harness/run_eval.py --model $(jq -r .ci_model ops/verifier_recommendation.json) --dev ops/eval_harness/eval_dataset_template/nli/dev.jsonl --out eval_results.json
+python3 ops/verifier/apply_threshold_pr.py
+echo "Verifier threshold PR proposed. Review and merge to apply thresholds."
+
*** End Patch
*** Begin Patch
*** Add File:dsar/sign_audit_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Sign audit bundle (S3 key) with cosign (KMS) and post Rekor entry.
+# Usage: dsar/sign_audit_bundle.sh s3://bucket/path/to/audit_bundle.tar.gz
+BUNDLE=${1:-}
+if [ -z "$BUNDLE" ]; then
+  echo "Usage: $0 s3://bucket/key"
+  exit 2
+fi
+
+# download
+tmpf=/tmp/audit_bundle.$$.tar.gz
+aws s3 cp "$BUNDLE" "$tmpf"
+
+# sign using cosign KMS (COSIGN_KMS_ARN must be configured in environment or gh secret)
+: "${COSIGN_KMS_ARN:?set COSIGN_KMS_ARN env var}"
+cosign sign --key "$COSIGN_KMS_ARN" "$tmpf"
+cosign verify --key "$COSIGN_KMS_ARN" "$tmpf"
+
+# push signed artifact back to S3 under signed/
+dest=$(echo "$BUNDLE" | sed 's|/audit/|/audit/signed/|')
+aws s3 cp "$tmpf" "$dest"
+echo "Signed bundle uploaded to $dest"
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/legal_dsar_playbook.md
+# Legal & DSAR Playbook (Operators + Legal)
+
+1. When an automated DSAR run creates an Approval Service request:
+   - Legal reviews request and determines action (redact, legal_hold, full review).
+2. For legal_hold:
+   - Insert entry into legal_holds table (see dsar/legal_hold_registry.sql).
+   - Ensure objects marked `legal_hold=true` are preserved and not auto-deleted by retention jobs.
+3. If approval to redact/delete:
+   - Execute dsar/advanced_dsar.py with action=redact/delete.
+   - Run dsar/sign_audit_bundle.sh on the produced audit bundle and store signed bundle in evidence bucket.
+4. Record all steps in audit S3 prefix `audit/dsar/` and link to Approval Service approval id.
+
*** End Patch
*** Begin Patch
*** Add File:pipelines/ssl/train_base_prod.py
+#!/usr/bin/env python3
+"""
+Production-ready train_base: read labeled dataset from S3/lakeFS (parquet),
+train and log model to MLflow.
+"""
+import os, mlflow, pandas as pd
+from sklearn.ensemble import RandomForestClassifier
+import pickle
+import boto3
+
+MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI")
+mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
+mlflow.set_experiment("ssl-demo")
+
+LABELED_S3_PATH = os.environ.get("LABELED_S3_PATH","s3://REPLACE_EVIDENCE_BUCKET/datasets/labeled/part-000.parquet")
+
+def download_s3_to_local(s3uri, dst="/tmp/labeled.parquet"):
+    s3 = boto3.client("s3")
+    parts = s3uri[5:].split("/",1)
+    s3.download_file(parts[0], parts[1], dst)
+    return dst
+
+def main():
+    local = download_s3_to_local(LABELED_S3_PATH)
+    df = pd.read_parquet(local)
+    X = df[[c for c in df.columns if c.startswith("x")]]
+    y = df["label"]
+    with mlflow.start_run() as run:
+        model = RandomForestClassifier(n_estimators=100)
+        model.fit(X,y)
+        mlflow.log_metric("train_rows", len(df))
+        path = "/tmp/base_model.pkl"
+        with open(path,"wb") as f:
+            pickle.dump(model,f)
+        mlflow.log_artifact(path, artifact_path="model")
+        with open("/tmp/base_run_id.txt","w") as f:
+            f.write(run.info.run_id)
+    print("Training complete, run id:", run.info.run_id)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:pipelines/ssl/merge_datasets_prod.py
+#!/usr/bin/env python3
+"""
+Production merge: combine labeled dataset (lakeFS snapshot) with filtered pseudo-labels (S3 artifact).
+Uploads merged parquet to lakeFS (via HTTP API) or S3 and writes snapshot ref to k8s secret.
+"""
+import os, pandas as pd, tempfile, subprocess, json
+import boto3
+
+LAKEFS_URL = os.environ.get("LAKEFS_URL")
+REPO = os.environ.get("LAKEFS_REPO","aegis")
+BUCKET = os.environ.get("EVIDENCE_BUCKET")
+FILTERED_PSEUDO_S3 = os.environ.get("FILTERED_PSEUDO_S3","s3://REPLACE_EVIDENCE_BUCKET/pseudo/filtered_pseudo.csv")
+
+def download_s3(s3uri, dst):
+    s3 = boto3.client("s3")
+    parts = s3uri[5:].split("/",1)
+    s3.download_file(parts[0], parts[1], dst)
+    return dst
+
+def upload_to_lakefs(branch, local_path):
+    # simplistic: use lakeFS PUT objects API (operator must ensure lakeFS credentials available)
+    url = f"{LAKEFS_URL}/api/v1/repositories/{REPO}/objects/put?path=/dataset/merged_{os.urandom(4).hex()}.parquet&branch={branch}"
+    with open(local_path,"rb") as f:
+        r = requests.put(url, data=f, auth=(os.environ.get("LAKEFS_USER"), os.environ.get("LAKEFS_PASS")), timeout=60)
+        r.raise_for_status()
+    return True
+
+def main():
+    tmp_labeled = "/tmp/labeled.parquet"
+    tmp_pseudo = "/tmp/filtered_pseudo.csv"
+    # For demo we assume labeled exists in S3; in prod replace with lakeFS snapshot ref handling
+    # Operator must set LABELED_S3_PATH env var when invoking
+    labeled_path = os.environ.get("LABELED_S3_PATH")
+    if labeled_path:
+        download_s3(labeled_path, tmp_labeled)
+    else:
+        print("LABELED_S3_PATH not provided; abort")
+        return
+    download_s3(FILTERED_PSEUDO_S3, tmp_pseudo)
+    df_lab = pd.read_parquet(tmp_labeled)
+    df_pseudo = pd.read_csv(tmp_pseudo)
+    if "confidence" in df_pseudo.columns:
+        df_pseudo = df_pseudo.drop(columns=["confidence"])
+    df_pseudo = df_pseudo.rename(columns={"pseudo_label":"label"})
+    combined = pd.concat([df_lab, df_pseudo], ignore_index=True)
+    out = "/tmp/combined_dataset.parquet"
+    combined.to_parquet(out, index=False)
+    # upload combined back to S3 as fallback for lakeFS
+    s3 = boto3.client("s3")
+    key = f"datasets/merged/combined_{os.urandom(4).hex()}.parquet"
+    s3.upload_file(out, BUCKET, key)
+    print("Uploaded merged dataset to s3://%s/%s" % (BUCKET, key))
+    # write last-ssl-snapshot secret in k8s
+    snapshot_ref = f"s3://{BUCKET}/{key}"
+    secret_yaml = {
+      "apiVersion":"v1","kind":"Secret","metadata":{"name":"last-ssl-snapshot","namespace":"aegis"},
+      "type":"Opaque","data":{"snapshot_ref": __import__("base64").b64encode(snapshot_ref.encode()).decode()}
+    }
+    proc = subprocess.run(["kubectl","apply","-f","-"], input=json.dumps(secret_yaml), text=True, capture_output=True)
+    print(proc.stdout, proc.stderr)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_kserve_tls.py
+import os, requests
+
+def test_kserve_ingress_tls():
+    url = os.environ.get("KSERVE_INGRESS_URL", "https://kserve.aegis.example.com/health")
+    try:
+        r = requests.get(url, timeout=5, verify=False)
+        assert r.status_code in (200,204)
+    except Exception as e:
+        raise AssertionError("KServe TLS ingress check failed: " + str(e))
+
*** End Patch
*** Begin Patch
*** Add File:README_OPERATORS_NEXT_STEPS.md
+# Operator Next Steps (secrets, images, HA, security, monitoring, CI and testing)
+
+This patch adds a collection of scripts, manifests and runbooks to complete critical operational work:
+
+- ops/populate_secrets_gh_and_aws.sh: populate GitHub secrets and AWS SecretsManager/SSM from terraform outputs.
+- .github/workflows/build_and_push_images.yml: build and push pipeline images to GHCR.
+- k8s/external-secrets/externalsecret_db.yaml: ExternalSecret to inject DB credentials into k8s.
+- k8s/irsa/annotate_serviceaccounts.sh: annotate SA with IRSA role.
+- ops/setup_pvc_and_backups.sh + registry/postgres/values.prod.yaml: tune PVC/storageClass and install Velero + Postgres backup CronJob.
+- k8s/kserve/*: cert-manager Certificate and Ingress manifest for TLS on KServe endpoints.
+- services/mcpx_logger_auth.py: MCPx logger requiring token auth (mirror in k8s secret).
+- ops/generate_baseline.py + monitoring/*: create baseline distributions and Alertmanager/PagerDuty templates.
+- ops/apply_verifier_thresholds.sh: run eval harness and create threshold PR.
+- dsar/sign_audit_bundle.sh and legal playbook for DSAR/legal hold process.
+- pipelines/ssl/*_prod.py: production-ready train and merge pipeline helpers (S3/lakeFS readers).
+- tests/integration/test_kserve_tls.py: smoke test for TLS ingress.
+
+Operator checklist (recommended order)
+1. Run terraform and save output: terraform output -json > /tmp/aegis_tf_output.json
+2. Populate secrets: ./ops/populate_secrets_gh_and_aws.sh /tmp/aegis_tf_output.json
+3. Build & push images via GitHub workflow or local Docker; confirm GHCR images are visible.
+4. Annotate serviceaccounts with IRSA roles: ./k8s/irsa/annotate_serviceaccounts.sh cosign-signer security <ROLE_ARN>
+5. Tune storage & backups: ./ops/setup_pvc_and_backups.sh <storageClass> <evidence-bucket>
+6. Deploy cert-manager + create Certificate and apply k8s/kserve/tls_ingress.yaml to enable TLS for KServe.
+7. Deploy MCPx logger with auth token in k8s secret `mcpx-auth` and supply MCPX_AUTH_TOKEN env var.
+8. Generate baseline distributions: run ops/generate_baseline.py (set BASELINE_SAMPLE_PATH and EVIDENCE_BUCKET).
+9. Run verifier eval and propose thresholds: ./ops/apply_verifier_thresholds.sh
+10. Replace demo dataset env vars with real LABELED_S3_PATH / UNLABELED_S3_PATH and run SSL pipeline in staging.
+11. Run tests: pytest tests/integration -q and iterate on failures.
+
+If you want I can now:
+- open PR(s) modifying your repository to wire GHCR image names and configure CI secrets (provide repo + branch), or
+- generate Kubernetes manifests for Deployment/Service for MCPx logger with secret mounting and a Helm values snippet, or
+- produce an end-to-end operator playbook with exact CLI commands and values for a specific AWS account (provide account details & region).
+
*** End Patch
*** End Patch
