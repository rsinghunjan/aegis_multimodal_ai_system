*** Begin Patch
*** Add File:ci/telemetry_14d_pipeline.yml
+name: Telemetry 14-day Collection & Health Monitor
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 4 * * *' # daily
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+
+jobs:
+  telemetry-health:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install boto3
+      - name: Run telemetry healthcheck
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          python3 telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 14
+      - name: Verify 14-day window report exists (best-effort)
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          LATEST=$(aws s3 ls s3://${EVIDENCE_BUCKET}/telemetry/health/ | tail -n1 | awk '{print $4}')
+          if [ -z "$LATEST" ]; then
+            echo "No telemetry health reports found"; exit 1
+          fi
+          echo "Latest telemetry health report: $LATEST"
+
+  telemetry-alert:
+    needs: telemetry-health
+    runs-on: ubuntu-latest
+    if: failure()
+    steps:
+      - name: Notify on failure (Slack/PagerDuty)
+        run: |
+          echo "Telemetry healthcheck failed. Please review S3 and telemetry agents."
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/feast_ingest_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: feast-feature-ingest
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-ci-sa
+      containers:
+        - name: feast-ingest
+          image: ghcr.io/yourorg/aegis-feast:latest
+          env:
+            - name: EVIDENCE_BUCKET
+              value: "REPLACE_WITH_EVIDENCE_BUCKET"
+            - name: FEAST_REDIS_HOST
+              value: "redis.aegis.svc.cluster.local"
+            - name: FEAST_OFFLINE_STORE_PATH
+              value: "s3://REPLACE_WITH_EVIDENCE_BUCKET/feature_snapshots/"
+          command: ["/bin/sh","-c"]
+          args:
+            - |
+              set -e
+              # Example: run job to ingest features and materialize to online store
+              feast apply --repo /opt/feature_repo
+              feast materialize-incremental job_run_features 2020-01-01T00:00:00Z
+      restartPolicy: Never
+  backoffLimit: 2
+
*** End Patch
*** Begin Patch
*** Add File:feature_store/ci/feast_parity_test.py
+#!/usr/bin/env python3
+"""
+Simple parity test: verify feature vectors in offline snapshots and online store have matching keys/counts.
+Intended to run in CI on a nightly basis to ensure feature parity for a recent time window.
+"""
+import os
+import boto3
+import redis
+import pyarrow.parquet as pq
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+S3_PREFIX = os.environ.get("FEAST_OFFLINE_PREFIX", "feature_snapshots/")
+REDIS_HOST = os.environ.get("FEAST_REDIS_HOST", "localhost")
+
+def sample_offline_count(s3, bucket, prefix):
+    # sample latest parquet file and count keys
+    paginator = s3.get_paginator("list_objects_v2")
+    latest = None
+    for p in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for o in p.get("Contents", []):
+            latest = o["Key"]
+    if not latest:
+        return 0
+    tmp="/tmp/offline.parquet"
+    s3.download_file(bucket, latest, tmp)
+    tab = pq.read_table(tmp)
+    return tab.num_rows
+
+def online_count(r):
+    # rough approximation: count keys with prefix "feast:job_run_features:"
+    keys = r.scan_iter(match="feast:job_run_features:*", count=1000)
+    return sum(1 for _ in keys)
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET env required")
+    s3 = boto3.client("s3")
+    offline = sample_offline_count(s3, EVIDENCE_BUCKET, S3_PREFIX)
+    r = redis.Redis(host=REDIS_HOST, port=6379, db=0)
+    online = online_count(r)
+    print("offline_rows:", offline, "online_keys:", online)
+    if online == 0:
+        raise SystemExit("Online store empty; parity failing")
+    # basic threshold: online should be at least 70% of offline for incremental runs
+    if online < offline * 0.7:
+        raise SystemExit("Parity below threshold")
+    print("Parity check passed")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:serving/argo_rollout_canary.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: model-runtime-rollout
+  namespace: aegis
+spec:
+  replicas: 3
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+        - pause: {duration: 60}
+        - setWeight: 50
+        - pause: {duration: 300}
+      trafficRouting:
+        istio:
+          virtualService:
+            name: model-runtime-vs
+  selector:
+    matchLabels:
+      app: model-runtime
+  template:
+    metadata:
+      labels:
+        app: model-runtime
+    spec:
+      containers:
+        - name: runtime
+          image: ghcr.io/yourorg/aegis-runtime:stable
+          ports:
+            - containerPort: 8080
+          resources:
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+
*** End Patch
*** Begin Patch
*** Add File:serving/seldon_deployment_example.yaml
+apiVersion: machinelearning.seldon.io/v1
+kind: SeldonDeployment
+metadata:
+  name: runtime-model
+  namespace: aegis
+spec:
+  annotations:
+    seldon.io/engine-log-request: "true"
+  name: runtime-model
+  predictors:
+  - graph:
+      name: runtime
+      implementation: MLFLOW_SERVER
+      modelUri: s3://REPLACE_WITH_EVIDENCE_BUCKET/models/runtime-model/
+    name: default
+    replicas: 2
+
*** End Patch
*** Begin Patch
*** Add File:terraform/aws/redshift_full.tf
+/*
+Complete Redshift provisioning scaffold (AWS). Review, add vars and run in your infra environment.
+This resource creates a Redshift cluster, security group, and parameter group. Customize to taste.
+*/
+variable "aws_region" { default = "us-west-2" }
+variable "cluster_identifier" { default = "aegis-redshift" }
+variable "master_username" {}
+variable "master_password" { sensitive = true }
+variable "vpc_id" {}
+variable "subnet_ids" { type = list(string) }
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_redshift_subnet_group" "aegis" {
+  name       = "${var.cluster_identifier}-subnetgroup"
+  subnet_ids = var.subnet_ids
+}
+
+resource "aws_redshift_cluster" "aegis" {
+  cluster_identifier = var.cluster_identifier
+  database_name      = "aegis_dw"
+  master_username    = var.master_username
+  master_password    = var.master_password
+  node_type          = "ra3.xlplus"
+  cluster_type       = "multi-node"
+  number_of_nodes    = 2
+  iam_roles          = [aws_iam_role.redshift_s3_access.arn]
+  skip_final_snapshot = true
+  encrypted = true
+  publicly_accessible = false
+  iam_roles = [aws_iam_role.redshift_s3_access.arn]
+  depends_on = [aws_redshift_subnet_group.aegis]
+}
+
+resource "aws_security_group" "redshift" {
+  name   = "${var.cluster_identifier}-sg"
+  vpc_id = var.vpc_id
+  ingress {
+    from_port   = 5439
+    to_port     = 5439
+    protocol    = "tcp"
+    cidr_blocks = ["10.0.0.0/16"]
+  }
+  egress {
+    from_port   = 0
+    to_port     = 0
+    protocol    = "-1"
+    cidr_blocks = ["0.0.0.0/0"]
+  }
+}
+
+resource "aws_iam_role" "redshift_s3_access" {
+  name = "aegis-redshift-s3-access-prod"
+  assume_role_policy = jsonencode({
+    Version = "2012-10-17",
+    Statement = [{
+      Effect = "Allow",
+      Principal = { Service = "redshift.amazonaws.com" },
+      Action = "sts:AssumeRole"
+    }]
+  })
+}
+
+resource "aws_iam_policy" "redshift_s3_policy" {
+  name = "aegis-redshift-s3-policy"
+  policy = jsonencode({
+    Version = "2012-10-17",
+    Statement = [
+      {
+        Sid = "ListAndGetParquet",
+        Effect = "Allow",
+        Action = ["s3:ListBucket", "s3:GetObject"],
+        Resource = ["arn:aws:s3:::${var.evidence_bucket}", "arn:aws:s3:::${var.evidence_bucket}/*"]
+      }
+    ]
+  })
+}
+
+resource "aws_iam_role_policy_attachment" "attach_redshift" {
+  role       = aws_iam_role.redshift_s3_access.name
+  policy_arn = aws_iam_policy.redshift_s3_policy.arn
+}
+
+output "redshift_endpoint" {
+  value = aws_redshift_cluster.aegis.endpoint
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/aws/kms_strict_policy.tf
+variable "allowed_principals" { type = list(string) }
+
+resource "aws_kms_key" "cosign" {
+  description = "Aegis cosign signing key (production, restricted)"
+  policy = jsonencode({
+    Version = "2012-10-17",
+    Statement = [
+      {
+        Sid = "AllowUseKeyForSpecificPrincipals",
+        Effect = "Allow",
+        Principal = { AWS = var.allowed_principals },
+        Action = ["kms:Sign","kms:GetPublicKey","kms:DescribeKey"],
+        Resource = "*"
+      },
+      {
+        Sid = "AllowAdminForSecurityTeam",
+        Effect = "Allow",
+        Principal = { AWS = var.allowed_principals },
+        Action = ["kms:*"],
+        Resource = "*",
+        Condition = {
+          StringEquals = {
+            "aws:PrincipalTag/role": "security"
+          }
+        }
+      }
+    ]
+  })
+  deletion_window_in_days = 30
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign"
+  target_key_id = aws_kms_key.cosign.key_id
+}
+
+output "cosign_kms_key_arn" {
+  value = aws_kms_key.cosign.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:connectors/slurm_adapter.py
+#!/usr/bin/env python3
+"""
+Adapter to submit Aegis jobs to an HPC Slurm cluster.
+ - Translates Aegis job hints into SBATCH directives.
+ - Uploads evidence outputs to S3 after job completion.
+"""
+import os, subprocess, tempfile, boto3, json
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+def submit_slurm_job(command, cpu=4, mem_gb=16, gpus=0, partition="gpu"):
+    sbatch = f"""#!/bin/bash
+#SBATCH --cpus-per-task={cpu}
+#SBATCH --mem={mem_gb}G
+#SBATCH --gres=gpu:{gpus}
+#SBATCH --partition={partition}
+{command}
+"""
+    fd, path = tempfile.mkstemp(suffix=".sh")
+    os.write(fd, sbatch.encode())
+    os.close(fd)
+    try:
+        out = subprocess.check_output(["sbatch", path]).decode()
+        return out.strip()
+    finally:
+        os.remove(path)
+
+def upload_artifact(local_path, s3_key):
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    s3.upload_file(local_path, EVIDENCE_BUCKET, s3_key)
+
+if __name__ == "__main__":
+    # Example usage: submit simple job
+    print(submit_slurm_job("echo hello; sleep 10", cpu=2, mem_gb=4, gpus=0, partition="short"))
+
*** End Patch
*** Begin Patch
*** Add File:connectors/mqtt_ingest.py
+#!/usr/bin/env python3
+"""
+Simple MQTT to S3 ingestion bridge (for IoT devices).
+ - Subscribes to device topics and writes JSON messages to S3 partitioned by date.
+Requires paho-mqtt and boto3.
+"""
+import os, json, datetime
+import boto3
+import paho.mqtt.client as mqtt
+
+MQTT_BROKER = os.environ.get("MQTT_BROKER", "mqtt.example.com")
+MQTT_TOPIC = os.environ.get("MQTT_TOPIC", "aegis/devices/#")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def on_connect(client, userdata, flags, rc):
+    print("Connected to MQTT", rc)
+    client.subscribe(MQTT_TOPIC)
+
+def on_message(client, userdata, msg):
+    now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H%M%SZ")
+    key = f"iot/{datetime.datetime.utcnow().date()}/{now}_{msg.topic.replace('/','_')}.json"
+    payload = msg.payload.decode()
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=payload.encode())
+
+def main():
+    client = mqtt.Client()
+    client.on_connect = on_connect
+    client.on_message = on_message
+    client.connect(MQTT_BROKER, 1883, 60)
+    client.loop_forever()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:connectors/quantum/backend_adapter.py
+"""
+Stub adapter to integrate quantum backend calls into Aegis workflows.
+Provide a consistent interface so training or calibration jobs can request quantum evaluation.
+"""
+import os, json
+
+def submit_quantum_job(circuit_spec, backend="ibmq"):
+    # This is a stub. Implement provider-specific API calls (IBM Q, AWS Braket, etc)
+    job_id = "quantum-job-" + str(hash(json.dumps(circuit_spec)) % 100000)
+    # In production: use provider SDK to submit job and poll for results, then store outputs to evidence S3
+    return {"job_id": job_id, "status": "queued", "backend": backend}
+
*** End Patch
*** Begin Patch
*** Add File:carbon/auto_calibrator_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-auto-calibrator
+  namespace: aegis
+spec:
+  schedule: "0 2 * * *" # daily
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-ci-sa
+          containers:
+            - name: calibrator
+              image: ghcr.io/yourorg/aegis-calibrator:latest
+              env:
+                - name: EVIDENCE_BUCKET
+                  value: "REPLACE_WITH_EVIDENCE_BUCKET"
+                - name: AWS_REGION
+                  value: "REPLACE_WITH_AWS_REGION"
+                - name: MEDIAN_THRESHOLD
+                  value: "0.10"
+              command: ["/bin/sh","-c"]
+              args:
+                - python3 /opt/calibration/iterative_convergence_runner.py || true
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign_rotate_rekor_automate.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Operator helper to create a new KMS key for cosign, rotate the alias, and append a Rekor note.
+# Usage: ./cosign_rotate_rekor_automate.sh <new-alias> <rekor-server>
+ALIAS=${1:-aegis-cosign-rot-$(date -u +%Y%m%d)}
+REKOR=${2:-http://rekor.example.com:3000}
+echo "Creating new KMS key and alias ${ALIAS}..."
+KEY_ID=$(aws kms create-key --description "Rotate cosign key $(date -u +%Y%m%d)" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name "alias/${ALIAS}" --target-key-id "${KEY_ID}"
+ARN=$(aws kms describe-key --key-id "${KEY_ID}" --query KeyMetadata.Arn --output text)
+echo "New key ARN: ${ARN}"
+echo "Update CI secret COSIGN_KMS_KEY_ARN to awskms://${ARN}"
+
+# Create Rekor note (best-effort)
+cat > /tmp/rotation_note.json <<EOF
+{
+  "rotated_at": "$(date -u --iso-8601=seconds)",
+  "kms_alias": "${ALIAS}",
+  "kms_key_arn": "${ARN}",
+  "notes": "Automated cosign KMS rotation (operator must update CI secrets)"
+}
+EOF
+curl -s -X POST -H "Content-Type: application/json" -d @/tmp/rotation_note.json ${REKOR}/api/v1/log/entries || echo "Rekor log failed (best-effort)"
+echo "Rotation documented (best-effort)."
+
*** End Patch
*** Begin Patch
*** Add File:security/rbac_remediate_suggester.py
+#!/usr/bin/env python3
+"""
+Scan cluster RBAC, detect overly-broad ClusterRoleBindings (e.g., cluster-admin), and produce remediation suggestions.
+Writes a suggested YAML patch file per subject for operator review.
+"""
+import subprocess, json, os, time
+
+def kubectl_json(args):
+    return json.loads(subprocess.check_output(["kubectl"] + args, text=True))
+
+def main():
+    crbs = kubectl_json(["get","clusterrolebinding","-o","json"])
+    findings = []
+    for cr in crbs.get("items",[]):
+        role = cr.get("roleRef",{}).get("name")
+        if role in ("cluster-admin","admin"):
+            name = cr["metadata"]["name"]
+            for subj in cr.get("subjects",[]):
+                findings.append({"binding": name, "subject": subj, "role": role})
+    out = {"timestamp": int(time.time()), "findings": findings}
+    fname = f"security/rbac_remediation_suggestions_{int(time.time())}.json"
+    with open(fname,"w") as f:
+        json.dump(out, f, indent=2)
+    print("Wrote remediation suggestions to", fname)
+    if findings:
+        print("Sample remediation (operator must review):")
+        for fnd in findings:
+            print(f"- consider replacing ClusterRoleBinding {fnd['binding']} for subject {fnd['subject']} with a namespaced RoleBinding or least-privilege role")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ui/superset_superset_config.py
+# Superset config snippet for OIDC + RLS enforcement (to be copy/paste into superset_config.py)
+from flask_appbuilder.security.manager import AUTH_OID
+AUTH_TYPE = AUTH_OID
+OPENID_PROVIDERS = [
+    {
+      'name': 'keycloak',
+      'icon': 'fa-address-card',
+      'token_key': 'access_token',
+      'remote_app': {
+          'client_id': 'superset-client',
+          'client_secret': 'SUPERSECRET',
+          'api_base_url': 'https://keycloak.example.com/auth/realms/aegis',
+          'access_token_url': 'https://keycloak.example.com/auth/realms/aegis/protocol/openid-connect/token',
+          'authorize_url': 'https://keycloak.example.com/auth/realms/aegis/protocol/openid-connect/auth',
+          'client_kwargs': {'scope': 'openid email profile groups'}
+      }
+    }
+]
+
+# Example: enforce DB pre-query to set app.team for RLS
+# DATABASES['your_dw']['extra'] = json.dumps({"metadata_params": {"pre_query": "SET app.team = '{{ current_user.extra.team }}';"}})
+
*** End Patch
*** Begin Patch
*** Add File:datahub/datahub_owner_mapper.py
+#!/usr/bin/env python3
+"""
+Map dbt model meta.owner to DataHub owner via DataHub REST API for improved UX.
+Run after dbt run; reads target/manifest.json and updates DataHub entity owners.
+"""
+import os, json, requests
+
+DATAHUB_SERVER = os.environ.get("DATAHUB_SERVER")
+API_KEY = os.environ.get("DATAHUB_API_KEY")
+MANIFEST = os.environ.get("DBT_MANIFEST_PATH", "dbt/target/manifest.json")
+
+def load_manifest(path):
+    with open(path) as f:
+        return json.load(f)
+
+def create_owner_patch(entity_urn, owner):
+    return {
+        "entityUrn": entity_urn,
+        "patch": {
+            "op": "add",
+            "path": "/owners",
+            "value": [{"owner": owner, "type": "USER"}]
+        }
+    }
+
+def main():
+    if not DATAHUB_SERVER or not API_KEY:
+        raise SystemExit("DATAHUB_SERVER and DATAHUB_API_KEY required")
+    mf = load_manifest(MANIFEST)
+    for node_id, node in mf.get("nodes", {}).items():
+        meta = node.get("meta", {})
+        owner = meta.get("owner")
+        if owner:
+            entity_urn = "urn:li:dataset:{}".format(node.get("metadata", {}).get("qualified_name", node.get("unique_id")))
+            patch = create_owner_patch(entity_urn, owner)
+            resp = requests.post(f"{DATAHUB_SERVER}/entities?action=update", json=patch, headers={"X-API-KEY": API_KEY})
+            print(node.get("unique_id"), resp.status_code)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:notebooks/onboarding_ds_enhanced.py
+"""
+Starter notebook script for data scientists:
+ - shows how to query dbt marts, pull evidence links and load feature snapshots
+ - demonstrates logging a run to MLflow with evidence link
+"""
+import os
+import pandas as pd
+import sqlalchemy
+import mlflow
+
+DW_CONN = os.environ.get("DW_CONN", "postgresql://user:pass@host:5432/aegis_dw")
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow.example.com")
+
+def query_mart(sql):
+    engine = sqlalchemy.create_engine(DW_CONN)
+    return pd.read_sql(sql, engine)
+
+def example():
+    sql = "select team, day, total_estimated_kg, total_measured_kg from marts.daily_team_rollup order by day desc limit 50"
+    df = query_mart(sql)
+    print(df.head())
+    # log example
+    mlflow.set_tracking_uri(MLFLOW_URI)
+    with mlflow.start_run(run_name="example-query"):
+        mlflow.log_param("query", "daily_team_rollup sample")
+        # attach evidence sample as artifact
+        df.head(10).to_csv("/tmp/sample_ct.csv", index=False)
+        mlflow.log_artifact("/tmp/sample_ct.csv", artifact_path="evidence_samples")
+
+if __name__ == "__main__":
+    example()
+
*** End Patch
*** Begin Patch
*** Add File:docs/operational_finalize_playbook.md
+# Operational Finalize Playbook â€” Automated & actionable steps to close weaknesses
+
+This document consolidates scripts and manifests added to remediate previous weaknesses and contains the sequence operators should follow.
+
+Steps:
+1) Telemetry collection
+ - Deploy feature telemetry CronJob / DaemonSet and verify daily reports:
+   kubectl apply -f ci/telemetry_14d_pipeline.yml (CI) and telemetry/cron_telemetry_collector.yaml (CronJob)
+ - Confirm 14-day coverage with CI job telemetry_14d_pipeline
+
+2) Feature store parity
+ - Apply Feast ingest job:
+   kubectl -n aegis apply -f feature_store/feast_ingest_job.yaml
+ - Run CI parity test nightly: feature_store/ci/feast_parity_test.py
+
+3) Canary & serving infra
+ - Install Argo Rollouts & Istio (or your mesh)
+ - Apply canary rollout for runtime:
+   kubectl -n aegis apply -f serving/argo_rollout_canary.yaml
+ - Optionally deploy Seldon for model serving:
+   kubectl -n aegis apply -f serving/seldon_deployment_example.yaml
+
+4) DW provisioning & tuning
+ - Use terraform/aws/redshift_full.tf and terraform/aws/kms_strict_policy.tf to provision infra.
+ - Sample tuning: run etl/tune_parquet.py against sample files to determine optimal batch size.
+ - Run parquet->manifest->loader flow in dev and validate MERGE/upsert idempotency.
+
+5) IoT/HPC/Quantum connectors
+ - For IoT: deploy connectors/mqtt_ingest.py as a Deployment or Lambda.
+ - For HPC: use connectors/slurm_adapter.py in your job submission tooling.
+ - Quantum: use connectors/quantum/backend_adapter.py as an integration stub with provider SDKs.
+
+6) Carbon calibration
+ - Deploy CronJob carbon/auto_calibrator_cronjob.yaml in staging.
+ - Monitor calibration reports in s3://<EVIDENCE_BUCKET>/calibration/validation_reports/ until median <= 0.10.
+
+7) Security finalization
+ - Create cert-manager secret using security/create_cert_secret.sh and apply certmanager ClusterIssuer (security/certmanager_clusterissuer_prod.yaml).
+ - Provision KMS with terraform and rotate cosign using security/cosign_rotate_rekor_automate.sh.
+ - Run RBAC remediation suggestions: python3 security/rbac_remediate_suggester.py and act on suggested least-privilege changes.
+
+8) UX & self-service polish
+ - Configure Superset using ui/superset_superset_config.py and enable RLS policies per dataset.
+ - Run datahub/datahub_owner_mapper.py after dbt runs to populate owners.
+ - Distribute notebooks/onboarding_ds_enhanced.py to new hires.
+
+9) Operationalize alerts & drills
+ - Deploy operational/alertmanager_slack_pagerduty.yaml with operator webhook/PD secrets.
+ - Enable monthly_dr_resume_drill DAG and schedule initial drill; archive artifacts via audit/upload workflow.
+
+Acceptance:
+- All steps recorded, scripts executed with evidence uploaded to S3 and audit package created via audit/audit_package_collector.py
+
*** End Patch
*** End Patch
