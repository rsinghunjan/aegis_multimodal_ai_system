*** Begin Patch
*** Add File:deploy/vault/values-vault.yaml
+replicaCount: 1
+server:
+  enabled: true
+  standalone:
+    enabled: true
+  dataStorage:
+    enabled: false
+ui:
+  enabled: true
+injector:
+  enabled: true
+extraEnvironmentVars:
+  - name: VAULT_LOG_LEVEL
+    value: "info"
+serviceAccount:
+  create: true
+  name: vault-sa
+resources:
+  requests:
+    cpu: 100m
+    memory: 256Mi
+  limits:
+    cpu: 500m
+    memory: 1Gi
+
+# NOTE: Replace with production HA values, storage backend and TLS secrets before deploy.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/secretproviderclass.yaml
+apiVersion: secrets-store.csi.x-k8s.io/v1
+kind: SecretProviderClass
+metadata:
+  name: vault-secret-provider
+  namespace: aegis
+spec:
+  provider: "vault"
+  parameters:
+    vaultAddress: "https://vault.aegis.svc:8200"
+    roleName: "aegis-role"
+    objects: |
+      - objectName: "db-creds"
+        secretPath: "secret/data/aegis/db"
+        objectType: "secret"
+
+# Requires HashiCorp Vault Helm chart + CSI provider installed.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/agent_injector_configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: vault-agent-injector-config
+  namespace: kube-system
+data:
+  config: |
+    exit_after_auth: false
+    auto_auth:
+      method:
+        type: kubernetes
+        mount_path: auth/kubernetes
+        config:
+          role: aegis-role
+    sink:
+      type: file
+      config:
+        path: /vault/secrets
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/policies/aegis-policy.hcl
+path "secret/data/aegis/*" {
+  capabilities = ["read","list"]
+}
+path "secret/data/aegis/ci/*" {
+  capabilities = ["read","list"]
+}
+path "auth/token/create" {
+  capabilities = ["create","update"]
+}
+
+# Load into Vault with: vault policy write aegis-policy aegis-policy.hcl
+
*** End Patch
*** Begin Patch
*** Add File:deploy/signing-proxy/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: signing-proxy
+  template:
+    metadata:
+      labels:
+        app: signing-proxy
+    spec:
+      serviceAccountName: signing-proxy-sa
+      containers:
+        - name: signing-proxy
+          image: registry.example.com/aegis/signing-proxy:latest
+          ports:
+            - containerPort: 8080
+          env:
+            - name: HSM_ENDPOINT
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: hsm_endpoint
+            - name: HSM_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: hsm_token
+
*** End Patch
*** Begin Patch
*** Add File:ops/rbac/serviceaccounts_roles.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: signing-proxy-sa
+  namespace: aegis
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: signing-proxy-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get","list"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: signing-proxy-rb
+  namespace: aegis
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: signing-proxy-role
+subjects:
+  - kind: ServiceAccount
+    name: signing-proxy-sa
+    namespace: aegis
+
*** End Patch
*** Begin Patch
*** Add File:governance/gatekeeper/require_signed_image.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: requirecosignattestation
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireCosignAttestation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package requirecosignattestation
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Pod"
+          not input.review.object.metadata.annotations["cosign.sig"]
+          msg := "Pod must have cosign.sig annotation (image must be attested)"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireCosignAttestation
+metadata:
+  name: cosign-required
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
*** End Patch
*** Begin Patch
*** Add File:governance/gatekeeper/no_privileged_pods.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: denyprivilegedpods
+spec:
+  crd:
+    spec:
+      names:
+        kind: DenyPrivilegedPods
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package denyprivilegedpods
+        violation[{"msg": msg}] {
+          input.review.object.spec.containers[_].securityContext.privileged == true
+          msg := "Privileged containers are not allowed"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: DenyPrivilegedPods
+metadata:
+  name: no-privileged
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
*** End Patch
*** Begin Patch
*** Add File:governance/opa/runtime_deny.rego
+package aegis.runtime
+
+deny[msg] {
+  input.proc and sys := input.proc.syscall
+  sys == "ptrace"
+  msg = sprintf("suspicious syscall ptrace detected pid=%v exe=%v", [input.proc.pid, input.proc.exe])
+}
+
+# Runtime agent must feed process syscalls to OPA for enforcement or alerting.
+
*** End Patch
*** Begin Patch
*** Add File:security/falco/falco_rules.yaml
+rules:
+  - rule: Write to /etc/passwd
+    desc: Detect writes to /etc/passwd
+    condition: (evt.type = open) and fd.name = "/etc/passwd"
+    output: "etc/passwd modified (user=%user.name command=%proc.cmdline file=%fd.name)"
+    priority: WARNING
+
+  - rule: Container shell
+    desc: Detect shell spawn in container
+    condition: container and evt.type = execve and proc.name in (bash, sh)
+    output: "Shell in container (container=%container.id user=%user.name proc=%proc.name cmd=%proc.cmdline)"
+    priority: WARNING
+
+  - rule: Falco write to sensitive mount
+    desc: A process wrote to /var/run/secrets
+    condition: open and fd.name startswith "/var/run/secrets"
+    output: "sensitive secrets access (proc=%proc.name file=%fd.name user=%user.name)"
+    priority: NOTICE
+
*** End Patch
*** Begin Patch
*** Add File:security/falco/daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: falco
+  namespace: kube-system
+spec:
+  selector:
+    matchLabels:
+      app: falco
+  template:
+    metadata:
+      labels:
+        app: falco
+    spec:
+      hostNetwork: true
+      containers:
+        - name: falco
+          image: falcosecurity/falco:latest
+          securityContext:
+            privileged: true
+          volumeMounts:
+            - name: dev
+              mountPath: /host/dev
+            - name: proc
+              mountPath: /host/proc
+            - name: varlog
+              mountPath: /host/var/log
+      volumes:
+        - name: dev
+          hostPath:
+            path: /dev
+        - name: proc
+          hostPath:
+            path: /proc
+        - name: varlog
+          hostPath:
+            path: /var/log
+
+# Note: tune rules to reduce false positives and configure alerting webhook.
+
*** End Patch
*** Begin Patch
*** Add File:logging/fluentbit/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluent-bit-config
+  namespace: logging
+data:
+  fluent-bit.conf: |
+    [SERVICE]
+        Flush        5
+        Log_Level    info
+    [INPUT]
+        Name    tail
+        Path    /var/log/containers/*.log
+        Parser  docker
+    [OUTPUT]
+        Name  s3
+        Match *
+        bucket ${EVIDENCE_BUCKET}
+        region us-west-2
+  parsers.conf: |
+    [PARSER]
+        Name        docker
+        Format      json
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-security-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: aegis-security
+      rules:
+        - alert: FalcoAlertFiring
+          expr: sum(falco_alerts_total) > 0
+          for: 1m
+          labels:
+            severity: page
+          annotations:
+            summary: "Falco security alert(s) firing"
+        - alert: BackupFailed
+          expr: increase(velero_backup_failed_total[1h]) > 0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Velero backup failures detected"
+
*** End Patch
*** Begin Patch
*** Add File:backups/velero/velero-credentials-secret.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: velero-creds
+  namespace: velero
+type: Opaque
+data:
+  # base64 AWS credentials placeholder
+  cloud: ""
+
+# Populate with base64-encoded credentials for the cloud backup provider.
+
*** End Patch
*** Begin Patch
*** Add File:backups/velero/backup-schedule.yaml
+apiVersion: velero.io/v1
+kind: Schedule
+metadata:
+  name: daily-backup
+  namespace: velero
+spec:
+  schedule: "0 2 * * *" # daily 02:00 UTC
+  template:
+    ttl: 720h0m0s
+    includedNamespaces:
+      - aegis
+    includedResources:
+      - pods
+      - deployments
+      - persistentvolumeclaims
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/compliance_scan.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: compliance-scan-
+  namespace: aegis
+spec:
+  entrypoint: compliance-scan
+  templates:
+    - name: compliance-scan
+      steps:
+        - - name: run-inspec
+            template: run-inspec
+        - - name: sign-report
+            template: sign-report
+
+    - name: run-inspec
+      container:
+        image: inspec/inspec:latest
+        command: [sh, -c]
+        args:
+          - echo "Running compliance checks (InSpec)"; sleep 2
+            # Add real InSpec profiles in production
+
+    - name: sign-report
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/ci/assemble_evidence.py --sbom sbom.spdx.json --trivy trivy_report.json --out /tmp/compliance_manifest.json || true
+            python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/compliance_manifest.json --proxy ${SIGNING_PROXY_URL:-http://signing-proxy:8080} --out /tmp/compliance_signed.json || true
+            cat /tmp/compliance_signed.json
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/backup_and_restore_test.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: backup-restore-test-
+  namespace: aegis
+spec:
+  entrypoint: backup-restore
+  templates:
+    - name: backup-restore
+      steps:
+        - - name: trigger-backup
+            template: trigger-backup
+        - - name: verify-backup
+            template: verify-backup
+        - - name: test-restore
+            template: test-restore
+
+    - name: trigger-backup
+      container:
+        image: bitnami/velero:latest
+        command: [sh, -c]
+        args:
+          - echo "Triggering on-demand Velero backup (stub)"; sleep 2
+
+    - name: verify-backup
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/backup/verify_backup.sh || true
+
+    - name: test-restore
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - echo "Running restore drill (operator-controlled, stub)"; sleep 2
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/falco_remediate.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: falco-remediate-
+  namespace: aegis
+spec:
+  entrypoint: falco-remediate
+  templates:
+    - name: falco-remediate
+      inputs:
+        parameters:
+          - name: event
+      steps:
+        - - name: collect-evidence
+            template: collect-evidence
+        - - name: quarantine
+            template: quarantine
+        - - name: open-ticket
+            template: open-ticket
+
+    - name: collect-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/forensics/collect_logs_and_sign.sh --event "{{inputs.parameters.event}}" || true
+
+    - name: quarantine
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - echo "Quarantining offending pod (operator: implement selector mapping)" && sleep 1
+
+    - name: open-ticket
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/ci/report_vuln.py --title "Falco Alert" --body "Investigation: see evidence bundle" || true
+
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/assemble_audit_evidence.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: assemble-evidence-
+  namespace: aegis
+spec:
+  entrypoint: assemble-evidence
+  templates:
+    - name: assemble-evidence
+      inputs:
+        parameters:
+          - name: topic
+      steps:
+        - - name: collect-sbom
+            template: collect-sbom
+        - - name: collect-trivy
+            template: collect-trivy
+        - - name: assemble
+            template: assemble
+
+    - name: collect-sbom
+      container:
+        image: alpine
+        command: [sh, -c]
+        args:
+          - echo "Collecting SBOM artifacts (operator: configure artifact locations)" && sleep 1
+
+    - name: collect-trivy
+      container:
+        image: alpine
+        command: [sh, -c]
+        args:
+          - echo "Collecting Trivy reports" && sleep 1
+
+    - name: assemble
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 evidence/mission_evidence_advanced.py --sbom sbom.spdx.json --checkpoint-sign /tmp/check_sign.json --sim-coverage /tmp/sim_coverage.json --pil-log /tmp/pil_result.json --hil-log /tmp/hil_result.json --uplink-manifest /tmp/signed_manifest.json --out /tmp/mission_evidence.json --evidence-bucket ${EVIDENCE_BUCKET:-} || true
+            cat /tmp/mission_evidence.json || true
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cicd_security.yml
+name: CI Security (SBOM, SCA, Compliance)
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+jobs:
+  sbom-sca:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install Syft
+        run: curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+      - name: Generate SBOM
+        run: syft packages dir:. -o spdx-json > sbom.spdx.json || true
+      - name: Upload SBOM artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom
+          path: sbom.spdx.json
+      - name: Install Trivy
+        run: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+      - name: Run Trivy
+        run: trivy fs --format json --output trivy_report.json --severity HIGH,CRITICAL . || true
+      - name: Upload Trivy report
+        uses: actions/upload-artifact@v4
+        with:
+          name: trivy-report
+          path: trivy_report.json
+  compliance:
+    runs-on: ubuntu-latest
+    needs: sbom-sca
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run InSpec (placeholder)
+        run: echo "Run InSpec profiles (operator configure profiles)" && sleep 2
+      - name: Assemble evidence & sign
+        run: |
+          python3 scripts/ci/assemble_evidence.py --sbom sbom.spdx.json --trivy trivy_report.json --out /tmp/evidence.json
+          python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/evidence.json --proxy ${SIGNING_PROXY_URL:-http://signing-proxy:8080} --out /tmp/evidence_sign.json || true
+      - name: Upload evidence artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: evidence
+          path: /tmp/evidence.json
+
*** End Patch
*** Begin Patch
*** Add File:scripts/kms/encrypt_decrypt_kms.sh
+#!/usr/bin/env bash
+set -euo pipefail
+CMD=${1:-}
+FILE=${2:-}
+if [ -z "$CMD" ] || [ -z "$FILE" ]; then
+  echo "Usage: $0 encrypt|decrypt <file>"
+  exit 2
+fi
+if [ "$CMD" = "encrypt" ]; then
+  aws kms encrypt --key-id $KMS_KEY_ID --plaintext fileb://"$FILE" --output text --query CiphertextBlob | base64 --decode > "${FILE}.enc"
+  echo "Wrote ${FILE}.enc"
+else
+  aws kms decrypt --ciphertext-blob fileb://"$FILE" --output text --query Plaintext | base64 --decode > "${FILE}.dec"
+  echo "Wrote ${FILE}.dec"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:scripts/backup/verify_backup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Verifying Velero backups (stub)."
+velero backup get -o json | jq '.' > /tmp/velero_backups.json || true
+if [ -s /tmp/velero_backups.json ]; then
+  echo "Backups present. Please inspect /tmp/velero_backups.json"
+else
+  echo "No backups detected or Velero not configured"
+  exit 1
+fi
+
*** End Patch
*** Begin Patch
*** Add File:scripts/forensics/collect_logs_and_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+EVENT=${1:-"unknown"}
+OUTDIR=${2:-/tmp/aegis_forensics}
+mkdir -p "$OUTDIR"
+kubectl get pods --all-namespaces -o json > "$OUTDIR/pods.json" || true
+kubectl get events --all-namespaces > "$OUTDIR/events.txt" || true
+tar -czf /tmp/aegis_forensics.tar.gz -C "$OUTDIR" .
+echo "Collected forensic bundle /tmp/aegis_forensics.tar.gz"
+if [ -n "${SIGNING_PROXY_URL:-}" ]; then
+  python3 scripts/ci/cosign_sign_via_proxy.py --image /tmp/aegis_forensics.tar.gz --proxy ${SIGNING_PROXY_URL} --out /tmp/forensics_sign.json || true
+  echo "Signed forensic bundle /tmp/forensics_sign.json"
+fi
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  aws s3 cp /tmp/aegis_forensics.tar.gz s3://${EVIDENCE_BUCKET}/forensics/ || true
+  aws s3 cp /tmp/forensics_sign.json s3://${EVIDENCE_BUCKET}/forensics/ || true
+fi
+
*** End Patch
*** Begin Patch
*** Add File:docs/SECURITY_RUNBOOK.md
+# Aegis Security & Compliance Runbook (Defaults)
+
+This runbook explains the new IAM, auditing, compliance, threat detection, encryption and backup scaffolding added to Aegis.
+
+Quick reference
+- Vault: deploy via Helm using deploy/vault/values-vault.yaml. Create policies in deploy/vault/policies.
+- Secrets injection: use SecretProviderClass deploy/vault/secretproviderclass.yaml and the Vault Agent Injector configmap.
+- Signing proxy: deploy deploy/signing-proxy/deployment.yaml and provide HSM credentials in aegis-secrets.
+- Gatekeeper: apply governance/gatekeeper/*.yaml to enforce signed images and prevent privileged pods.
+- Falco: apply security/falco/daemonset.yaml and tune rules in security/falco/falco_rules.yaml.
+- Fluent Bit: logging/fluentbit/configmap.yaml to archive logs to ${EVIDENCE_BUCKET}.
+- Velero: configure backups and schedule in backups/velero.
+- Argo workflows: compliance, backup tests, falco remediation, assemble evidence.
+- CI: .github/workflows/cicd_security.yml runs SBOM/trivy and assembles signed evidence.
+
+Operational steps
+1. Vault
+   - Helm install hashicorp/vault with values-vault.yaml.
+   - Create Vault policies (deploy/vault/policies/aegis-policy.hcl).
+   - Configure Kubernetes auth role mapping and test Vault Agent injection on a demo pod.
+
+2. Signing & provenance
+   - Deploy signing-proxy and secure it (mTLS + network policy).
+   - Use COSIGN + Rekor in CI where possible; CI configured to call signing-proxy.
+   - Gatekeeper ensures images have cosign.sig annotation before production schedule.
+
+3. Detection & response
+   - Deploy Falco daemonset and connect webhook to Argo remediation (argo/workflows/falco_remediate.yaml).
+   - Configure Prometheus alerts in monitoring/prometheus/rules.yaml and connect Alertmanager receivers.
+
+4. Backups & restores
+   - Install Velero with provider credentials (backups/velero/*) and create daily schedule.
+   - Use argo/workflows/backup_and_restore_test.yaml to run backup/restore drills.
+
+5. Compliance automation
+   - Run argo/workflows/compliance_scan.yaml regularly; CI runs InSpec placeholder.
+   - Archive signed compliance reports with evidence workflow.
+
+6. Encryption & KMS
+   - Use scripts/kms/encrypt_decrypt_kms.sh for envelope operations in CI/ops.
+   - Ensure S3 buckets use SSE-KMS and enforce IAM policies to limit access.
+
+Incident response
+- On Falco alert: workflow falco_remediate will collect evidence, optionally quarantine pod, and create a ticket.
+- For backups failing: backup_and_restore_test will generate artifacts and alert on Prometheus.
+- For compliance drift: compliance_scan will sign and produce evidence; open a ticket via CI job.
+
+Notes & next steps
+- Replace scaffolds with vendor-specific connectors (HSM SDK, SIEM endpoints, ground station details).
+- Harden signing-proxy: mTLS, RBAC, audit logging, and rate-limiting.
+- Tune Falco rules & Prometheus queries to reduce false positives.
+- Make EVIDENCE_BUCKET immutable (S3 object lock) for auditability.
+
*** End Patch
*** End Patch
