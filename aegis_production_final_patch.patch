*** Begin Patch
*** Add File:tenant-finalization/aws/finalize_aws_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_aws_tenant.sh
+# - Create IAM role for GitHub OIDC (trust policy), minimal policies to call KMS (kms:Sign) and access S3 (artifact store)
+# - Create KMS key or print guidance to use existing key
+# - Output values to store as GitHub secrets (AWS_ROLE_ARN, AWS_REGION, KMS_KEY_ARN, S3_BUCKET)
+#
+usage(){ cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --region <aws-region> --s3-bucket <bucket>
+Creates (or recommends) IAM role and KMS key and prints secrets to save in GitHub:
+  - AWS_ROLE_ARN
+  - AWS_REGION
+  - KMS_KEY_ARN
+  - S3_BUCKET
+EOF
+exit 1; }
+
+[ -n "${AWS_PROFILE:-}" ] || echo "Note: using default AWS CLI profile; set AWS_PROFILE if needed"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${REGION:-}" ] || usage
+[ -n "${S3_BUCKET:-}" ] || usage
+
+ROLE_NAME="AegisGitHubOIDCRole-${ORG}-${REPO}"
+OIDC_PROVIDER_URL="https://token.actions.githubusercontent.com"
+
+echo "1) Create IAM OIDC provider (if not exists):"
+echo "   aws iam list-open-id-connect-providers | jq -r .Arn"
+# Operator: check console to create OIDC provider if not present (requires thumbprint)
+
+echo "2) Create role with trust policy for GitHub Actions OIDC"
+TRUST_POLICY=$(cat <<EOF
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Principal": {"Federated": "arn:aws:iam::$(aws sts get-caller-identity --query Account -o text):oidc-provider/token.actions.githubusercontent.com"},
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Condition": {
+        "StringLike": {
+          "token.actions.githubusercontent.com:sub": "repo:${ORG}/${REPO}:ref:refs/heads/main"
+        }
+      }
+    }
+  ]
+}
+EOF
+)
+
+echo "$TRUST_POLICY" > /tmp/aegis-trust.json
+ROLE_ARN=$(aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file:///tmp/aegis-trust.json --query Role.Arn -o text || true)
+if [ -z "$ROLE_ARN" ]; then
+  ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query Role.Arn -o text)
+fi
+echo "Role ARN: $ROLE_ARN"
+
+echo "3) Attach minimal inline policy to allow KMS sign and S3 read/write (operator: tighten resource ARNs)"
+POLICY=$(cat <<EOF
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {"Effect":"Allow","Action":["kms:Sign","kms:GetPublicKey"],"Resource":"*"},
+    {"Effect":"Allow","Action":["s3:GetObject","s3:PutObject","s3:ListBucket"],"Resource":["arn:aws:s3:::${S3_BUCKET}","arn:aws:s3:::${S3_BUCKET}/*"]}
+  ]
+}
+EOF
+)
+echo "$POLICY" > /tmp/aegis-inline-policy.json
+aws iam put-role-policy --role-name "$ROLE_NAME" --policy-name "AegisOIDCPolicy" --policy-document file:///tmp/aegis-inline-policy.json
+
+echo "4) Create KMS key (optional):"
+KMS_KEY_ARN=$(aws kms create-key --description "Aegis signing key" --query KeyMetadata.Arn -o text || true)
+if [ -z "$KMS_KEY_ARN" ]; then
+  echo "KMS key not created (maybe exists); operator to provide existing KMS key ARN"
+  KMS_KEY_ARN="<REPLACE_KMS_KEY_ARN>"
+fi
+echo "KMS_KEY_ARN=$KMS_KEY_ARN"
+
+echo
+cat <<EOF
+Save the following as GitHub repository secrets:
+  AWS_ROLE_ARN=$ROLE_ARN
+  AWS_REGION=$REGION
+  KMS_KEY_ARN=$KMS_KEY_ARN
+  S3_BUCKET=$S3_BUCKET
+
+Operator: tighten the IAM policy to specific KMS key ARN and S3 bucket ARNs for least privilege.
+EOF
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/gcp/finalize_gcp_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_gcp_tenant.sh
+# - Creates IAM service account for GitHub OIDC (workload identity pool), binds minimal roles for KMS and storage
+# - Outputs values to store as GitHub secrets (GCP_WORKLOAD_POOL, GCP_PROVIDER_ID, SA_EMAIL, KMS_KEY_RING/KEY)
+#
+usage(){ cat <<EOF
+Usage: $0 --project <gcp-project> --pool-id <workload-pool-id> --bucket <gcs-bucket>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --project) PROJECT="$2"; shift 2;;
+    --pool-id) POOL="$2"; shift 2;;
+    --bucket) BUCKET="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${PROJECT:-}" ] || usage
+[ -n "${POOL:-}" ] || usage
+[ -n "${BUCKET:-}" ] || usage
+
+echo "1) Ensure Workload Identity Pool exists (GCP Console preferred for initial setup)"
+echo "Pool ID: $POOL; Project: $PROJECT"
+
+SA_NAME="aegis-github-oidc-sa"
+SA_EMAIL="${SA_NAME}@${PROJECT}.iam.gserviceaccount.com"
+gcloud iam service-accounts create "$SA_NAME" --project="$PROJECT" || true
+
+echo "2) Grant kms.cryptoSign and storage.objectAdmin to the service account (least-privilege recommended)"
+gcloud projects add-iam-policy-binding "$PROJECT" --member="serviceAccount:${SA_EMAIL}" --role="roles/cloudkms.signerVerifier" || true
+gcloud projects add-iam-policy-binding "$PROJECT" --member="serviceAccount:${SA_EMAIL}" --role="roles/storage.objectAdmin" || true
+
+echo
+cat <<EOF
+Set these repo secrets / values:
+GCP_PROJECT=$PROJECT
+GCP_SA_EMAIL=$SA_EMAIL
+GCP_WORKLOAD_POOL=$POOL
+GCP_BUCKET=$BUCKET
+
+Operator: create a Workload Identity Pool & Provider to trust token.actions.githubusercontent.com and allow GitHub repo to impersonate this service account (follow GCP docs).
+EOF
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/ibm/finalize_ibm_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_ibm_tenant.sh
+# - Create service ID and grant minimal policies for Key Protect and COS
+# - Output values to store as GitHub secrets
+#
+usage(){ cat <<EOF
+Usage: $0 --resource-group <rg> --region <region> --cos-bucket <bucket>
+EOF
+exit 1; }
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --resource-group) RG="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --cos-bucket) BUCKET="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${RG:-}" ] || usage
+[ -n "${REGION:-}" ] || usage
+[ -n "${BUCKET:-}" ] || usage
+
+echo "1) Create Service ID"
+SERVICE_ID_NAME="aegis-ci-serviceid"
+ibmcloud iam service-id-create "$SERVICE_ID_NAME" -d "Aegis CI" || true
+SID=$(ibmcloud iam service-id "$SERVICE_ID_NAME" --output json | jq -r .guid)
+echo "Service ID GUID: $SID"
+
+echo "2) Create API key for service id (store securely)"
+API_KEY_JSON=$(ibmcloud iam service-id-api-key-create "$SERVICE_ID_NAME" aegis-ci-key -d "CI key" -f || true)
+API_KEY=$(echo "$API_KEY_JSON" | jq -r .apikey)
+echo "Generated API key (store in GitHub secret IBM_CLOUD_API_KEY)"
+
+echo
+cat <<EOF
+Save these as GitHub secrets:
+  IBM_CLOUD_API_KEY=<the API key printed above>
+  IBM_COS_BUCKET=$BUCKET
+  IBM_REGION=$REGION
+
+Operator: attach minimal policies to the Service ID to access Key Protect (key actions) and COS bucket operations.
+EOF
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/gcp/bigtable/README.md
+# Terraform scaffold for creating a Bigtable instance for Feast offline store (example)
+
+Adapt and apply with proper billing & quotas. Use provider=google and provide project, region and instance type.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/gcp/bigtable/main.tf
+resource "google_bigtable_instance" "feast" {
+  project      = var.project
+  name         = "${var.stage}-feast-instance"
+  cluster {
+    cluster_id   = "${var.stage}-feast-cluster"
+    zone         = var.zone
+    num_nodes    = 3
+    storage_type = "SSD"
+  }
+  instance_type = "PRODUCTION"
+}
+
+output "bigtable_instance_id" { value = google_bigtable_instance.feast.name }
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/pentest/orchestrate_pentest.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate a pre-pentest scan (trivy/kube-bench), package artifacts, and produce a ticket CSV for external pentest vendor.
+OUTDIR=${OUTDIR:-/tmp/aegis-pentest}
+mkdir -p "$OUTDIR"
+
+echo "1) Run lightweight scans"
+if command -v trivy >/dev/null 2>&1; then
+  kubectl get pods --all-namespaces -o jsonpath='{..image}' | tr -s '[[:space:]]' '\n' | sort -u > "$OUTDIR/images.txt"
+  while read -r img; do
+    trivy image --quiet --severity HIGH,CRITICAL -o "$OUTDIR/$(echo $img | tr '/:' '_').json" "$img" || true
+  done < "$OUTDIR/images.txt"
+fi
+if command -v kube-bench >/dev/null 2>&1; then
+  kube-bench run --json > "$OUTDIR/kube-bench.json" || true
+fi
+
+echo "2) Collect manifests & configs"
+kubectl get all --all-namespaces -o yaml > "$OUTDIR/k8s_all.yaml"
+helm list -A -o yaml > "$OUTDIR/helm_list.yaml"
+vault audit list -format=json > "$OUTDIR/vault_audit.json" || true
+
+echo "3) Produce vendor intake file (CSV)"
+cat > "$OUTDIR/vendor_intake.csv" <<CSV
+area,contact,notes
+Kubernetes,ops@example.com,K8s cluster access and kubeconfig required
+Vault,security@example.com,VAULT_ADDR and admin token for auditing
+Object Storage,storage@example.com,S3 bucket access and sample artifacts
+CSV
+
+echo "Pre-pentest artifacts in $OUTDIR. Provide these to vendor and open a pentest ticket with scope & timeframe."
+
*** End Patch
*** Begin Patch
*** Add File:prod/vault/fluentd/forwarder-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: vault-fluentd-config
+  namespace: ops
+data:
+  fluent.conf: |
+    <source>
+      @type tail
+      path /var/log/vault_audit.log
+      pos_file /var/log/fluentd-vault.pos
+      tag vault.audit
+      <parse>
+        @type none
+      </parse>
+    </source>
+    <match vault.audit>
+      @type forward
+      <server>
+        host syslog.collector.local
+        port 24224
+      </server>
+    </match>
+
+---
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: fluentd-vault-forwarder
+  namespace: ops
+spec:
+  selector:
+    matchLabels:
+      app: fluentd-vault
+  template:
+    metadata:
+      labels: { app: fluentd-vault }
+    spec:
+      containers:
+      - name: fluentd
+        image: fluent/fluentd:v1.14
+        volumeMounts:
+        - name: config
+          mountPath: /fluentd/etc
+        - name: vault-logs
+          mountPath: /var/log
+      volumes:
+      - name: config
+        configMap:
+          name: vault-fluentd-config
+      - name: vault-logs
+        hostPath:
+          path: /var/log
+
*** End Patch
*** Begin Patch
*** Add File:prod/reliability/cron_vault_snapshot.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: vault-snapshot
+  namespace: ops
+spec:
+  schedule: "0 3 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+          - name: snapshot
+            image: hashicorp/vault:1.14.3
+            command: ["/bin/sh","-c"]
+            env:
+            - name: VAULT_ADDR
+              value: "https://vault.ops.svc.cluster.local:8200"
+            - name: VAULT_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: vault-root
+                  key: root-token
+            volumeMounts:
+            - name: backup
+              mountPath: /backup
+            args:
+            - |
+              vault operator raft snapshot save /backup/vault-$(date -u +%Y%m%dT%H%M%SZ).snap
+          restartPolicy: OnFailure
+          volumes:
+          - name: backup
+            persistentVolumeClaim:
+              claimName: vault-backup-pvc
+
*** End Patch
*** Begin Patch
*** Add File:prod/serving/perf/sweep_batch.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Sweep batch sizes & concurrency using Triton perf_analyzer and record best points
+HOST="$1"
+MODEL="$2"
+OUTDIR="${3:-/tmp/aegis-perf}"
+mkdir -p "$OUTDIR"
+for B in 1 2 4 8 16 32 64; do
+  for C in 1 2 4 8 16; do
+    echo "Testing batch=$B conc=$C"
+    perf_analyzer -m "$MODEL" -u "$HOST" -b "$B" -n 200 --concurrency-range $C:$C --measure-performance > "$OUTDIR/perf_b${B}_c${C}.txt" 2>&1 || true
+  done
+done
+echo "Perf sweep complete. Review $OUTDIR for tradeoffs between latency and throughput."
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/volcano/resubmit_on_preempt.py
+#!/usr/bin/env python3
+"""
+Simple controller script (cron-run) that watches for preempted Volcano jobs and resubmits with checkpoint resume.
+In production use a controller/operator; this script is a scaffold for resubmission logic.
+"""
+import subprocess, json
+def list_failed_jobs():
+    out = subprocess.check_output(["kubectl","get","jobs.batch","-n","ml","-o","json"])
+    jobs = json.loads(out)
+    for j in jobs.get("items",[]):
+        if j.get("status",{}).get("failed",0) > 0:
+            print("Failed job:", j["metadata"]["name"])
+            # Implement resubmit logic: read checkpoint location from annotations and resubmit with same args
+
+if __name__=="__main__":
+    list_failed_jobs()
+
*** End Patch
*** Begin Patch
*** Add File:prod/cost/kubecost/deploy_kubecost.sh
+#!/usr/bin/env bash
+set -euo pipefail
+kubectl create ns kube-cost || true
+helm repo add kubecost https://kubecost.github.io/cost-analyzer/ || true
+helm repo update
+helm upgrade --install kubecost kubecost/cost-analyzer -n kube-cost -f prod/cost/kubecost/helm-values.yaml --wait || true
+echo "Kubecost installed (configure cloud billing export connectors per provider)"
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/tpm/attest_service.py
+#!/usr/bin/env python3
+"""
+TPM-backed device attestation reference (server side)
+This demo expects devices to POST TPM attestation quote & certificate. Operator must integrate with TPM tooling and CA.
+"""
+from flask import Flask, request, jsonify
+import subprocess, json
+app = Flask(__name__)
+
+@app.post("/attest")
+def attest():
+    payload = request.get_json(force=True)
+    # payload expected: {device_id, aik_pub, quote, ak_cert}
+    # Validate quote using verifier (platform-specific); placeholder below
+    device_id = payload.get("device_id")
+    # In production: verify AK/AIK, check nonce, validate quote PCRs against known good
+    return jsonify({"device_id": device_id, "status":"ok"})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/tpm/device_demo.sh
+#!/usr/bin/env bash
+#
+# Demo device-side TPM attestation using tpm2-tools (operator: install tpm2-tools)
+DEVICE_ID=${DEVICE_ID:-device-`hostname`}
+# Generate AK/AIK and quote (this is simplified)
+tpm2_createek -G rsa -c ek.ctx || true
+tpm2_createak -C ek.ctx -c ak.ctx || true
+tpm2_quote -c ak.ctx -l sha256:0,1,2,3 -q "randomnonce" -m quote.out || true
+echo "Send quote to server for attestation (manual step)"
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/aws_braket_connector.py
+#!/usr/bin/env python3
+"""
+Minimal AWS Braket connector example to submit a circuit or hybrid step.
+Operator: configure AWS credentials and Braket resources and adapt to your use-case.
+"""
+from braket.circuits import Circuit
+from braket.aws import AwsDevice, AwsQuantumTask
+def run_braket():
+    device = AwsDevice("arn:aws:braket:::device/quantum-simulator/amazon/sv1")
+    circuit = Circuit().h(0).cnot(0,1).measure(0,0)
+    task = device.run(circuit, shots=100)
+    print("Task:", task.result())
+
+if __name__=="__main__":
+    run_braket()
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/ibm_connector.py
+#!/usr/bin/env python3
+"""
+IBM Quantum connector example (qiskit)
+Install qiskit-ibmq-provider and configure IBMQ account token
+"""
+from qiskit import IBMQ, QuantumCircuit, execute
+def run_ibm():
+    IBMQ.load_account()
+    provider = IBMQ.get_provider(hub='ibm-q')
+    backend = provider.get_backend('ibmq_qasm_simulator')
+    qc = QuantumCircuit(1,1)
+    qc.h(0)
+    qc.measure(0,0)
+    job = execute(qc, backend=backend, shots=1024)
+    print(job.result().get_counts())
+
+if __name__=="__main__":
+    run_ibm()
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/token_refresh.py
+import os, json, time, requests
+from prod.devtools.aegis_sdk.auth import TokenStore
+
+def refresh_if_needed():
+    data = TokenStore.load()
+    if not data:
+        return False
+    if data.get("expires_at",0) - time.time() < 300:
+        refresh_token = data.get("refresh_token")
+        if not refresh_token:
+            return False
+        token_url = data.get("token_url")
+        client_id = data.get("client_id")
+        resp = requests.post(token_url, data={"grant_type":"refresh_token","refresh_token":refresh_token,"client_id":client_id})
+        if resp.status_code==200:
+            tok = resp.json()
+            tok["expires_at"] = time.time() + tok.get("expires_in",3600)
+            TokenStore.save(tok)
+            return True
+    return False
+
+if __name__=="__main__":
+    print("Refreshing token if needed:", refresh_if_needed())
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/aegis_sdk_ci.yml
+name: Aegis SDK CI
+on:
+  push:
+    paths:
+      - "prod/devtools/aegis_sdk/**"
+  workflow_dispatch:
+jobs:
+  test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install SDK
+        run: |
+          pip install -e prod/devtools/aegis_sdk
+      - name: Run unit checks (smoke)
+        run: |
+          python -c "from aegis.client import AegisClient; print('ok', hasattr(AegisClient,'list_models'))"
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/finish_prod_checklist.md
+# Finish-to-production checklist
+
+This checklist collates tasks required to call Aegis production-ready:
+1. Tenant & IAM finalization for each cloud (run tenant-finalization scripts and create Github secrets).
+2. Provision managed services (Terraform modules under terraform/managed/*). Validate connectivity and credentials.
+3. Vault HA backend & auto-unseal configured with provider KMS/HSM and tested backup/restore.
+4. Enable Vault audit forwarding to SIEM (deploy fluentd/log forwarder).
+5. Enforce network policies, mTLS and RBAC; run initial kube-bench and trivy scans; remediate critical issues.
+6. Deploy KServe/Triton with production values; run perf sweeps; tune KEDA thresholds; configure Volcano for gang scheduling.
+7. Replace POC Feast/lakeFS with managed/HA alternatives and validate dataset snapshot reproduction.
+8. Execute DR drills and incident exercises; verify SLO monitoring & alert playbooks.
+9. Replace device attestation demo with TPM/TEE-based flow for Edge devices; test OTA signing & rollout.
+10. Add quantum provider connectors and run pilot hybrid workflow if required.
+11. Harden aegisctl & SDK auth flows and add CI tests for onboarding templates.
+
*** End Patch
*** End Patch
