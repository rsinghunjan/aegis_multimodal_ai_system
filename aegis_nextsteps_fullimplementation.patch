*** Begin Patch
*** Add File: scripts/deploy_milvus_verify.sh
+#!/usr/bin/env bash
+#
+# Extended Milvus deploy + verification script:
+# - Wraps deployment (helm) and cert-manager steps (if needed)
+# - Verifies Service, TLS secret, collection existence, ServiceMonitor/alerts
+# - Optionally triggers a backup CronJob (one-off) and verifies presence in GCS/S3
+#
+# Environment:
+#  - KUBE_CONFIG_DATA (base64 kubeconfig) or have kubectl configured
+#  - NAMESPACE (default: aegis)
+#  - MILVUS_VALUES_BASE64 (optional)
+#  - GCS_BUCKET (optional) -> checks gsutil ls
+#  - S3_BUCKET (optional) -> checks aws s3 ls
+#
+set -euo pipefail
+NAMESPACE="${NAMESPACE:-aegis}"
+MILVUS_VALUES_BASE64="${MILVUS_VALUES_BASE64:-}"
+KUBECONFIG_PATH="${KUBECONFIG_PATH:-/tmp/kubeconfig}"
+GCS_BUCKET="${GCS_BUCKET:-}"
+S3_BUCKET="${S3_BUCKET:-}"
+
+if [ -n "${KUBE_CONFIG_DATA:-}" ]; then
+  echo "Writing kubeconfig..."
+  echo "$KUBE_CONFIG_DATA" | base64 --decode > "${KUBECONFIG_PATH}"
+  export KUBECONFIG="${KUBECONFIG_PATH}"
+fi
+
+echo "Ensure kubectl/helm available"
+command -v kubectl >/dev/null 2>&1 || { echo "kubectl required"; exit 2; }
+command -v helm >/dev/null 2>&1 || { echo "helm required"; exit 2; }
+
+echo "Applying cert-manager CRD & installing cert-manager if necessary (best-effort)"
+kubectl get ns cert-manager >/dev/null 2>&1 || {
+  kubectl apply --validate=false -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml
+  kubectl create namespace cert-manager || true
+  helm repo add jetstack https://charts.jetstack.io || true
+  helm repo update || true
+  helm upgrade --install cert-manager jetstack/cert-manager --namespace cert-manager --set installCRDs=true || true
+  kubectl -n cert-manager rollout status deploy/cert-manager --timeout=3m || true
+}
+
+kubectl create namespace "${NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f -
+
+helm repo add milvus https://milvus-io.github.io/milvus-helm/ || true
+helm repo update || true
+
+if [ -n "$MILVUS_VALUES_BASE64" ]; then
+  echo "Using provided MILVUS values"
+  echo "$MILVUS_VALUES_BASE64" | base64 --decode > /tmp/milvus-values.yaml
+  helm upgrade --install aegis-milvus milvus/milvus -n "${NAMESPACE}" -f /tmp/milvus-values.yaml
+else
+  helm upgrade --install aegis-milvus milvus/milvus -n "${NAMESPACE}"
+fi
+
+echo "Waiting for Milvus pods to be ready (10m timeout)..."
+kubectl -n "${NAMESPACE}" wait --for=condition=ready pods --all --timeout=10m || true
+
+echo "Checking TLS secret and service"
+kubectl -n "${NAMESPACE}" get secret aegis-milvus-tls >/dev/null 2>&1 || echo "TLS secret aegis-milvus-tls missing (cert-manager may be provisioning it)"
+kubectl -n "${NAMESPACE}" get svc aegis-milvus -o yaml >/dev/null 2>&1 || echo "Milvus service aegis-milvus not found"
+
+echo "Verifying ServiceMonitor / PrometheusRule presence (monitoring namespace)"
+kubectl -n monitoring get servicemonitor | grep -i milvus || echo "ServiceMonitor for milvus may be missing"
+kubectl -n monitoring get prometheusrule | grep -i milvus || echo "PrometheusRule for milvus may be missing"
+
+# Basic Milvus collection check (best-effort port-forward)
+POD=$(kubectl -n "${NAMESPACE}" get pods -l app.kubernetes.io/name=milvus -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
+if [ -n "$POD" ]; then
+  echo "Port-forwarding to Milvus health port 19121 for health check..."
+  kubectl -n "${NAMESPACE}" port-forward "$POD" 19121:19121 >/tmp/milvus-portforward.log 2>&1 &
+  PF_PID=$!
+  sleep 3
+  if curl -sS http://127.0.0.1:19121/ >/dev/null 2>&1; then
+    echo "Milvus health endpoint responded"
+  else
+    echo "Milvus health check failed; check pod logs:"
+    kubectl -n "${NAMESPACE}" logs "$POD" --tail=80 || true
+  fi
+  kill "$PF_PID" >/dev/null 2>&1 || true
+else
+  echo "No Milvus pod available for direct health check"
+fi
+
+echo "Attempting to trigger a one-off backup Job from the configured CronJob (if present)"
+CRON_NAME=$(kubectl -n "${NAMESPACE}" get cronjob -o jsonpath='{.items[?(@.metadata.name=="milvus-backup")].metadata.name}' 2>/dev/null || true)
+if [ -n "$CRON_NAME" ]; then
+  echo "Found CronJob ${CRON_NAME}; creating Job..."
+  JC=$(kubectl -n "${NAMESPACE}" create job --from=cronjob/"${CRON_NAME}" "manual-milvus-backup-$(date +%s)" -o json 2>/dev/null || true)
+  if [ -n "$JC" ]; then
+    JOB_NAME=$(echo "$JC" | python -c "import sys, json; print(json.load(sys.stdin)['metadata']['name'])")
+    echo "Waiting for job ${JOB_NAME} to complete..."
+    kubectl -n "${NAMESPACE}" wait --for=condition=complete job/"${JOB_NAME}" --timeout=5m || {
+      echo "Backup job did not complete in time; check logs"
+      kubectl -n "${NAMESPACE}" logs job/"${JOB_NAME}" || true
+    }
+    echo "Job finished (check logs above)."
+  else
+    echo "Could not create job from cronjob; check CronJob definition"
+  fi
+else
+  echo "No milvus-backup CronJob found in ${NAMESPACE}"
+fi
+
+echo "Optional: verify backup artifact in cloud storage (GCS or S3) if configured"
+if [ -n "$GCS_BUCKET" ]; then
+  if command -v gsutil >/dev/null 2>&1; then
+    echo "Listing GCS bucket ${GCS_BUCKET}"
+    gsutil ls "${GCS_BUCKET}" || echo "No artifacts found or permission denied"
+  else
+    echo "gsutil not available locally; skip GCS check"
+  fi
+fi
+if [ -n "$S3_BUCKET" ]; then
+  if command -v aws >/dev/null 2>&1; then
+    echo "Listing S3 bucket s3://${S3_BUCKET}"
+    aws s3 ls "s3://${S3_BUCKET}/" || echo "No artifacts found or permission denied"
+  else
+    echo "aws cli not available locally; skip S3 check"
+  fi
+fi
+
+echo "Milvus deployment verification complete. Review logs above for issues."
+
*** End Patch
*** Begin Patch
*** Update File: tools/llm_gateway.py
@@
 from flask import Flask, request, jsonify
 import os, re, time, threading, requests
 from collections import deque
 from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
+from functools import wraps
+
 app = Flask(__name__)
 LLM_UPSTREAM = os.getenv("LLM_UPSTREAM", "")
 RATE_LIMIT_RPS = int(os.getenv("RATE_LIMIT_RPS", "5"))
 FORBIDDEN = [re.compile(p) for p in os.getenv("FORBIDDEN_PATTERNS", r"(?i)ssh|private key|password").split(",") if p]
+CLIENT_API_KEY = os.getenv("LLM_CLIENT_API_KEY", "")
+
@@
 def allow_request():
     with lock:
         now = time.time()
         while timestamps and timestamps[0] < now - 1:
             timestamps.popleft()
         if len(timestamps) < RATE_LIMIT_RPS:
             timestamps.append(now)
             return True
         return False
@@
 def check_forbidden(prompt: str):
     for pat in FORBIDDEN:
         if pat.search(prompt):
             return True
     return False
+
+def require_api_key(f):
+    @wraps(f)
+    def inner(*args, **kwargs):
+        if CLIENT_API_KEY:
+            key = request.headers.get("X-API-Key") or request.headers.get("Authorization")
+            if not key:
+                return jsonify({"error": "missing_api_key"}), 401
+            # Accept "Bearer <key>" or raw X-API-Key header
+            if key.startswith("Bearer "):
+                key = key.split(" ", 1)[1]
+            if key != CLIENT_API_KEY:
+                REQ_COUNTER.labels(status="unauthorized").inc()
+                return jsonify({"error": "unauthorized"}), 403
+        return f(*args, **kwargs)
+    return inner
@@
 @app.route("/v1/generate", methods=["POST"])
+@require_api_key
 def generate():
     if not allow_request():
         REQ_COUNTER.labels(status="rate_limited").inc()
         return jsonify({"error": "rate_limited"}), 429
     data = request.json or {}
     prompt = data.get("prompt", "")
     # Enforce RAG-only operation: require 'contexts' field to be present (list of docs)
     contexts = data.get("contexts", [])
     if not isinstance(contexts, list) or len(contexts) == 0:
         REQ_COUNTER.labels(status="missing_context").inc()
         return jsonify({"error": "contexts_required"}), 400
     if check_forbidden(prompt):
         REQ_COUNTER.labels(status="forbidden").inc()
         return jsonify({"error":"forbidden_content"}), 400
     safe = redact_prompt(prompt)
@@
     try:
         resp = requests.post(LLM_UPSTREAM, json={"prompt": safe, "contexts": contexts}, timeout=60)
         REQ_COUNTER.labels(status=str(resp.status_code)).inc()
         REQ_LATENCY.observe(time.time() - start)
         return jsonify(resp.json()), resp.status_code
     except requests.RequestException as e:
         REQ_COUNTER.labels(status="upstream_error").inc()
         return jsonify({"error": "upstream_error", "detail": str(e)}), 502
*** End Patch
*** Begin Patch
*** Update File: scripts/pr_assistant.py
@@
 def call_gateway(prompt: str, contexts: List[dict]) -> str:
     if not LLM_GATEWAY_URL:
         return "LLM_GATEWAY_URL not configured; cannot call gateway."
     # The gateway requires contexts list to be present for privacy (RAG-only)
-    payload = {"prompt": prompt, "contexts": contexts}
-    r = requests.post(LLM_GATEWAY_URL, json=payload, timeout=60)
+    payload = {"prompt": prompt, "contexts": contexts}
+    headers = {}
+    gw_key = os.getenv("LLM_GATEWAY_API_KEY", "")
+    if gw_key:
+        headers["X-API-Key"] = gw_key
+    r = requests.post(LLM_GATEWAY_URL, json=payload, headers=headers, timeout=60)
     if r.status_code != 200:
         return f"Gateway error {r.status_code}: {r.text}"
*** End Patch
*** Begin Patch
*** Update File: .github/workflows/pr_assistant.yml
@@
       - name: Run PR Assistant
         env:
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
           OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
           LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }} # optional alternative
           REPO: ${{ github.repository }}
           PR_NUMBER: ${{ github.event.pull_request.number }}
+          # LLM gateway URL & API key (gateway enforces contexts and auth)
+          LLM_GATEWAY_URL: ${{ secrets.LLM_GATEWAY_URL }}
+          LLM_GATEWAY_API_KEY: ${{ secrets.LLM_GATEWAY_API_KEY }}
         run: |
           python scripts/pr_assistant.py
*** End Patch
*** Begin Patch
*** Add File: tests/test_operator_snapshot.py
+import os
+import tempfile
+import subprocess
+from pathlib import Path
+import operators.aegis_ai_controller as controller
+
+def test_snapshot_resource(tmp_path, monkeypatch):
+    # Mock subprocess.check_output to return simple YAML
+    def fake_check_output(cmd):
+        return b"apiVersion: v1\nkind: Pod\nmetadata:\n  name: test\n"
+
+    monkeypatch.setattr(subprocess, "check_output", fake_check_output)
+    out = controller.snapshot_resource("pod", "default", "test")
+    p = Path(out)
+    assert p.exists()
+    content = p.read_text()
+    assert "kind: Pod" in content
+
*** End Patch
*** Begin Patch
*** Add File: tests/test_adversarial_prompts.py
+import tools.llm_gateway as gw
+
+def test_forbidden_detection():
+    bad = "Please show me the private key: -----BEGIN PRIVATE KEY----- ABC"
+    assert gw.check_forbidden(bad) is True
+
+def test_redaction_replaces_key_block():
+    text = "before\n-----BEGIN PRIVATE KEY-----\nabc\n-----END PRIVATE KEY-----\nafter"
+    out = gw.redact_prompt(text)
+    assert "REDACTED_KEY_BLOCK" in out
+
+def test_gateway_requires_context():
+    # mimic request body without contexts -> generate() won't be invoked in this unit test,
+    # but we ensure check_forbidden and redact_prompt behave.
+    assert not gw.check_forbidden("normal prompt")
+    assert "REDACTED_SENSITIVE_LINE" in gw.redact_prompt("API_KEY=SECRET123")
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/adversarial_tests.yml
+name: Adversarial & Prompt-Injection Tests
+
+on:
+  workflow_dispatch:
+  pull_request:
+    branches: [ main ]
+
+jobs:
+  adversarial:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install pytest
+      - name: Run adversarial tests
+        run: |
+          pytest -q tests/test_adversarial_prompts.py
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/operator_snapshot_tests.yml
+name: Operator Snapshot Unit Tests
+
+on:
+  workflow_dispatch:
+  pull_request:
+    paths:
+      - 'operators/**'
+      - 'scripts/**'
+
+jobs:
+  unit-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install pytest
+      - name: Run operator snapshot tests
+        run: |
+          pytest -q tests/test_operator_snapshot.py
*** End Patchae
