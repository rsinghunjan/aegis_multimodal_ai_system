*** Begin Patch
*** Add File:deepspeed/mpi/deepspeed_mpi_job.yaml
+apiVersion: kubeflow.org/v1
+kind: MPIJob
+metadata:
+  name: deepspeed-multi-node-test
+  namespace: aegis
+spec:
+  slotsPerWorker: 4
+  replicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+        spec:
+          containers:
+            - name: launcher
+              image: registry.example.com/aegis/deepspeed-trainer:latest
+              command:
+                - /bin/bash
+                - -lc
+                - |
+                  set -euo pipefail
+                  echo "Launcher starting"
+                  # Example: run deepspeed across workers. MPI operator will set env like OMPI_COMM_WORLD_SIZE
+                  # Replace training/world_model/deepspeed_entrypoint.py with your entrypoint.
+                  deepspeed --num_nodes=${MPI_NUM_WORKER_REPLICAS:-2} --num_gpus=4 training/world_model/deepspeed_entrypoint.py --deepspeed_config deepspeed/deepspeed_config_zero2.json
+              resources:
+                limits:
+                  nvidia.com/gpu: 0
+                requests:
+                  cpu: "2"
+                  memory: "4Gi"
+    Worker:
+      replicas: 2
+      template:
+        spec:
+          containers:
+            - name: deepspeed-worker
+              image: registry.example.com/aegis/deepspeed-trainer:latest
+              command:
+                - /bin/bash
+                - -lc
+                - |
+                  echo "Worker ready, sleeping until launcher starts"
+                  sleep infinity
+              resources:
+                limits:
+                  nvidia.com/gpu: 4
+                requests:
+                  cpu: "8"
+                  memory: "32Gi"
+
+# NOTE:
+# - This MPIJob requires the Kubeflow MPI Operator installed in the cluster.
+# - Adjust replica counts, GPU counts, resources and the deepspeed command for your environment.
*** End Patch
*** Begin Patch
*** Add File:deepspeed/README_MULTI_NODE.md
+DeepSpeed multi-node validation (Aegis)
+
+This scaffold provides an MPIJob to run a multi-node DeepSpeed validation using the Kubeflow MPI Operator.
+
+Requirements:
+- MPI Operator installed (https://github.com/kubeflow/mpi-operator).
+- A GPU node pool with at least N nodes matching the MPIJob worker replicas and GPUs per node configured in the job.
+- deepspeed-trainer image with deepspeed and training code pushed to registry.example.com/aegis/deepspeed-trainer:latest
+
+How to run:
+1. Edit deepspeed/mpi/deepspeed_mpi_job.yaml to adjust replicas and GPU counts.
+2. kubectl apply -f deepspeed/mpi/deepspeed_mpi_job.yaml -n aegis
+3. Monitor pods and MPIJob via kubectl or the MPI Operator UI.
+
+Acceptance checks:
+- Training step throughput logged (samples/sec), GPUs > 70% util.
+- Checkpoint write times measured and documented.
+- Job completes successfully and artifacts (metrics & checkpoints) are uploaded to MODEL_ARTIFACT_BUCKET.
+
*** End Patch
*** Begin Patch
*** Add File:triton/deployment/triton_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: triton-inference
+  namespace: aegis
+  labels:
+    app: triton
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: triton
+  template:
+    metadata:
+      labels:
+        app: triton
+    spec:
+      containers:
+        - name: triton
+          image: nvcr.io/nvidia/tritonserver:23.06-py3
+          args:
+            - tritonserver
+            - --model-repository=/models
+            - --model-control-mode=explicit
+          ports:
+            - containerPort: 8000
+            - containerPort: 8001
+            - containerPort: 8002
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "4"
+              memory: "16Gi"
+            requests:
+              nvidia.com/gpu: 1
+              cpu: "2"
+              memory: "8Gi"
+          volumeMounts:
+            - name: model-repo
+              mountPath: /models
+      volumes:
+        - name: model-repo
+          persistentVolumeClaim:
+            claimName: triton-model-repo-pvc
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: triton-svc
+  namespace: aegis
+spec:
+  selector:
+    app: triton
+  ports:
+    - name: http
+      port: 8000
+      targetPort: 8000
+    - name: metrics
+      port: 8002
+      targetPort: 8002
+
+---
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: triton-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: triton-inference
+  minReplicas: 1
+  maxReplicas: 4
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 70
+
+# Notes:
+# - Provision PVC named triton-model-repo-pvc with adequate capacity and performance (NVMe-backed if possible).
+# - Use a model preparation job (below) to place ONNX/Triton model folders into /models on the PVC.
*** End Patch
*** Begin Patch
*** Add File:triton/model_prep_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: triton-model-prep
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: prep
+          image: python:3.10-slim
+          command: [sh, -c]
+          args:
+            - |
+              set -euo pipefail
+              mkdir -p /models/my_model/1
+              # Expect model.onnx to be provided via MODEL_SOURCE (s3:// or local path)
+              if [ -n "${MODEL_SOURCE:-}" ]; then
+                if echo "${MODEL_SOURCE}" | grep -q '^s3://'; then
+                  pip install boto3 && python3 - <<PY
+import boto3,os,sys
+src=os.environ['MODEL_SOURCE']
+bucket=src.split('/')[2]
+key='/'.join(src.split('/')[3:])
+b=boto3.client('s3')
+b.download_file(bucket,key,'/tmp/model.onnx')
+PY
+                else
+                  cp "${MODEL_SOURCE}" /tmp/model.onnx || true
+                fi
+              fi
+              # move ONNX into model repo path in triton format (model name folder with version subfolder)
+              mv /tmp/model.onnx /models/my_model/1/model.onnx
+              # optionally write config.pbtxt if provided as env var or default
+              cat > /models/my_model/config.pbtxt <<EOF
+name: "my_model"
+platform: "onnxruntime_onnx"
+max_batch_size: 8
+input [
+  {
+    name: "input"
+    data_type: TYPE_FP32
+    dims: [3,224,224]
+  }
+]
+output [
+  {
+    name: "output"
+    data_type: TYPE_FP32
+    dims: [1000]
+  }
+]
+EOF
+              echo "Prepared model repo"
+          volumeMounts:
+            - name: model-repo
+              mountPath: /models
+      restartPolicy: OnFailure
+      volumes:
+        - name: model-repo
+          persistentVolumeClaim:
+            claimName: triton-model-repo-pvc
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:feast/feast_stream_processor_high_availability.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: feast-stream-processor
+  namespace: aegis
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: feast-stream-processor
+  template:
+    metadata:
+      labels:
+        app: feast-stream-processor
+    spec:
+      containers:
+        - name: processor
+          image: registry.example.com/aegis/feast-stream-processor:latest
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 8080
+            initialDelaySeconds: 10
+            periodSeconds: 10
+          livenessProbe:
+            httpGet:
+              path: /live
+              port: 8080
+            initialDelaySeconds: 20
+            periodSeconds: 20
+          env:
+            - name: KAFKA_BOOTSTRAP
+              value: "aegis-kafka-bootstrap:9092"
+            - name: REDIS_HOST
+              value: "redis-headless.aegis.svc"
+      affinity:
+        podAntiAffinity:
+          requiredDuringSchedulingIgnoredDuringExecution:
+            - labelSelector:
+                matchExpressions:
+                  - key: app
+                    operator: In
+                    values:
+                      - feast-stream-processor
+              topologyKey: "kubernetes.io/hostname"
+
*** End Patch
*** Begin Patch
*** Add File:ops/dr/restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Restore drill: download latest checkpoint from S3 (MODEL_ARTIFACT_BUCKET),
+# restore to local NVMe, measure time, and upload a verification report to EVIDENCE_BUCKET.
+#
+BUCKET=${MODEL_ARTIFACT_BUCKET:-}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-}
+PREFIX=${CHECKPOINT_PREFIX:-"world_model/checkpoints/"}
+OUT_DIR=${OUT_DIR:-"/tmp/restore_drill"}
+mkdir -p "$OUT_DIR"
+
+echo "Starting restore drill at $(date -u)" > "$OUT_DIR/restore_drill.log"
+if [ -z "$BUCKET" ]; then
+  echo "MODEL_ARTIFACT_BUCKET not set; aborting" >> "$OUT_DIR/restore_drill.log"
+  exit 2
+fi
+
+echo "Listing objects in s3://$BUCKET/$PREFIX" >> "$OUT_DIR/restore_drill.log"
+aws s3 ls "s3://$BUCKET/$PREFIX" --recursive | sort -r > "$OUT_DIR/objects.txt"
+LATEST=$(awk '{print $4; exit}' "$OUT_DIR/objects.txt" || true)
+if [ -z "$LATEST" ]; then
+  echo "No checkpoint objects found under prefix $PREFIX" >> "$OUT_DIR/restore_drill.log"
+  exit 2
+fi
+echo "Latest object: $LATEST" >> "$OUT_DIR/restore_drill.log"
+
+START=$(date +%s)
+aws s3 cp "s3://$BUCKET/$LATEST" "$OUT_DIR/$(basename $LATEST)"
+END=$(date +%s)
+DT=$((END-START))
+echo "Downloaded checkpoint in ${DT}s" >> "$OUT_DIR/restore_drill.log"
+
+# Simulate restore (extract / copy to NVMe)
+NVME_PATH=${NVME_PATH:-"/mnt/local-nvme"}
+mkdir -p "$NVME_PATH/restore"
+START_RESTORE=$(date +%s)
+tar -xzf "$OUT_DIR/$(basename $LATEST)" -C "$NVME_PATH/restore" || true
+END_RESTORE=$(date +%s)
+RT=$((END_RESTORE-START_RESTORE))
+echo "Restored checkpoint to NVMe in ${RT}s" >> "$OUT_DIR/restore_drill.log"
+
+REPORT="{\"download_seconds\": ${DT}, \"restore_seconds\": ${RT}, \"object\": \"${LATEST}\", \"time\": $(date +%s)}"
+echo "$REPORT" > "$OUT_DIR/restore_drill_report.json"
+echo "Wrote report to $OUT_DIR/restore_drill_report.json" >> "$OUT_DIR/restore_drill.log"
+
+if [ -n "$EVIDENCE_BUCKET" ]; then
+  aws s3 cp "$OUT_DIR/restore_drill_report.json" "s3://$EVIDENCE_BUCKET/restore_drills/restore_drill_$(date -u +%s).json" || true
+  echo "Uploaded report to s3://$EVIDENCE_BUCKET/restore_drills/" >> "$OUT_DIR/restore_drill.log"
+fi
+
+cat "$OUT_DIR/restore_drill.log"
+cat "$OUT_DIR/restore_drill_report.json"
+
*** End Patch
*** Begin Patch
*** Add File:argo/restore_drill_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: restore-drill-
+  namespace: aegis
+spec:
+  entrypoint: restore-drill
+  templates:
+    - name: restore-drill
+      container:
+        image: amazonlinux:2
+        command: [sh, -c]
+        args:
+          - yum install -y awscli tar gzip python3 && python3 - <<'PY'
+import os,subprocess
+os.system("bash ops/dr/restore_drill.sh")
+PY
+
*** End Patch
*** Begin Patch
*** Add File:autoscale/model_latency_hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: model-endpoint-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: model-serving
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Pods
+      pods:
+        metric:
+          name: request_latency_ms
+        target:
+          type: AverageValue
+          averageValue: "200"
+
+# Notes:
+# - This HPA assumes Prometheus Adapter exposes a custom metric named "request_latency_ms" (ms).
+# - Configure the adapter mapping to translate Prometheus histogram/summary into the custom metric.
*** End Patch
*** Begin Patch
*** Add File:rollout/model_rollout.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: model-serving-rollout
+  namespace: aegis
+spec:
+  replicas: 3
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+        - pause: {duration: 10m}
+        - setWeight: 50
+        - pause: {duration: 30m}
+        - setWeight: 100
+  selector:
+    matchLabels:
+      app: model-serving
+  template:
+    metadata:
+      labels:
+        app: model-serving
+        model.risk: "high"
+    spec:
+      containers:
+        - name: model
+          image: registry.example.com/aegis/model-serving:latest
+          ports:
+            - containerPort: 8080
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 8080
+            initialDelaySeconds: 5
+            periodSeconds: 10
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+
+---
+# Ensure manual approval requirement exists for high-risk models (Gatekeeper/Constraint earlier added)
+apiVersion: policy/v1
+kind: PodDisruptionBudget
+metadata:
+  name: model-serving-pdb
+  namespace: aegis
+spec:
+  minAvailable: 2
+  selector:
+    matchLabels:
+      app: model-serving
+
*** End Patch
*** Begin Patch
*** Add File:prometheus/adapter/README_PROMETHEUS_ADAPTER.md
+Prometheus Adapter guidance (Aegis)
+
+To enable HPA on latency you must install Prometheus Adapter (or custom metrics adapter) and map Prometheus metrics into the Custom Metrics API.
+
+Example mapping (prometheus-adapter config):
+rules:
+  seriesQuery: 'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))'
+  seriesFilters: []
+  resources:
+    overrides:
+      service:
+        resource: service
+
+Then HPA can reference the custom metric exposed by the adapter (e.g., request_latency_ms).
+
*** End Patch
*** End Patch
