*** Begin Patch
*** Add File: inference/explainer/service.py
+#!/usr/bin/env python3
+"""
+Explainer sidecar service
+- Accepts inference requests or explanation requests
+- Supports `explain=true` (on-demand) and sampled background explanations
+- Computes SHAP for tabular/ndarray inputs (KernelExplainer) and returns/stores
+  explanation artifacts linked to model_id and request_id
+- Stores explanation artifacts to MLflow or object store and emits Prometheus metrics
+
+Notes:
+- This is a lightweight, extensible example. Replace KernelExplainer with a model-specific
+  explainer (DeepExplainer, Integrated Gradients, LIME) for production performance.
+"""
+import os
+import time
+import json
+import tempfile
+from io import BytesIO
+from typing import Any, Dict
+
+from flask import Flask, request, jsonify
+import numpy as np
+import mlflow
+import prometheus_client
+from prometheus_client import Counter, Histogram, Gauge
+
+# Optional dependencies (shap may be heavy)
+try:
+    import shap
+except Exception:
+    shap = None
+
+from privacy.phi_scrubber import scrub_text
+
+app = Flask(__name__)
+
+# Prometheus metrics
+EXPLAIN_COUNTER = Counter("aegis_explain_requests_total", "Total explanation requests", ["model"])
+EXPLAIN_LATENCY = Histogram("aegis_explain_request_seconds", "Explain latency seconds", ["model"])
+EXPLAINED_RATIO = Gauge("aegis_explained_fraction", "Fraction of requests that include explanations", ["model"])
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
+mlflow.set_tracking_uri(MLFLOW_URI)
+
+
+def store_artifact(run_id: str, filename: str, payload: bytes):
+    tmp = tempfile.mkdtemp()
+    path = f"{tmp}/{filename}"
+    with open(path, "wb") as fh:
+        fh.write(payload)
+    try:
+        mlflow.log_artifact(path, artifact_path=f"explanations/{run_id}")
+    except Exception as e:
+        app.logger.warning("mlflow log artifact failed: %s", e)
+
+
+def compute_shap_kernel(model_predict_fn, background: np.ndarray, x: np.ndarray) -> Dict[str, Any]:
+    if shap is None:
+        raise RuntimeError("shap library not available")
+    explainer = shap.KernelExplainer(model_predict_fn, background)
+    shap_values = explainer.shap_values(x)
+    return {"shap_values": np.array(shap_values).tolist()}
+
+
+@app.route("/healthz")
+def health():
+    return "ok"
+
+
+@app.route("/explain", methods=["POST"])
+def explain():
+    """
+    Expected JSON:
+    {
+      "model_id": "model:1",
+      "request_id": "req-123",
+      "input": { ... },      # ndarray-like or structured
+      "explain": true|false, # optional; if false, will only optionally sample
+      "sample": true|false   # server-side sampling for background explanations
+    }
+    """
+    payload = request.get_json(force=True)
+    model_id = payload.get("model_id", "unknown")
+    request_id = payload.get("request_id", f"r-{int(time.time()*1000)}")
+    explain_flag = bool(payload.get("explain", False))
+    sample_flag = bool(payload.get("sample", False))
+
+    EXPLAIN_COUNTER.labels(model=model_id).inc()
+    start = time.time()
+    try:
+        # Basic privacy handling: scrub any free-text fields
+        if "text" in payload.get("input", {}):
+            payload["input"]["text"] = scrub_text(payload["input"]["text"])
+
+        # For demo: convert input to numpy array if provided under "tensor"
+        x = None
+        if "tensor" in payload.get("input", {}):
+            x = np.array(payload["input"]["tensor"], dtype=float)
+
+        explanation = None
+        if explain_flag or (sample_flag and np.random.rand() < 0.05):
+            # Placeholder model_predict_fn: in prod supply callable to model server/predictor
+            def model_predict_fn(data):
+                # naive stub: return zeros or call out to local predictor
+                return np.zeros((len(data), 1))
+
+            if x is None:
+                raise ValueError("no tensor provided for explanation")
+            # Background selection: small set of zeros or random; in prod use cached background
+            background = np.zeros((5, ) + x.shape[1:]) if x.ndim > 1 else np.zeros((5, x.shape[0]))
+            try:
+                explanation = compute_shap_kernel(model_predict_fn, background, x)
+            except Exception as e:
+                app.logger.exception("explainer failed: %s", e)
+                explanation = {"error": str(e)}
+
+            # Persist explanation to MLflow artifacts
+            run_id = request_id  # use request id for grouping; in prod link to model run id
+            body = json.dumps({"model_id": model_id, "request_id": request_id, "explanation": explanation}).encode("utf-8")
+            try:
+                store_artifact(run_id, f"explanation_{request_id}.json", body)
+            except Exception:
+                app.logger.exception("failed to store explanation artifact")
+
+        latency = time.time() - start
+        EXPLAIN_LATENCY.labels(model=model_id).observe(latency)
+        # For coverage gauge: naive increment (in prod track ratio over window)
+        EXPLAINED_RATIO.labels(model=model_id).set(1.0 if explanation is not None else 0.0)
+
+        return jsonify({"request_id": request_id, "model_id": model_id, "explanation": explanation}), 200
+    except Exception as e:
+        return jsonify({"error": str(e)}), 500
+
+
+if __name__ == "__main__":
+    prometheus_client.start_http_server(8000)
+    app.run(host="0.0.0.0", port=8081)
+
*** End Patch
*** Begin Patch
*** Add File: inference/kserve/multimodal_explainer_inference.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "multimodal-service-with-explainer"
+  namespace: aegis
+spec:
+  predictor:
+    custom:
+      container:
+        image: aegis/multimodal-predictor:latest
+        ports:
+          - containerPort: 8080
+  transformer:
+    custom:
+      container:
+        image: aegis/multimodal-transformer:latest
+        ports:
+          - containerPort: 8080
+  explainer:
+    custom:
+      container:
+        image: aegis/multimodal-explainer:latest
+        command: ["python", "service.py"]
+        ports:
+          - containerPort: 8081
+  autoscaler:
+    minReplicas: 1
+    maxReplicas: 3
+
*** End Patch
*** Begin Patch
*** Add File: policies/opa/model_card_required.rego
+package aegis.policies.model_card
+
+# Rego policy to assert model metadata includes a model_card artifact.
+# Intended to be used by a CI gate or admission controller that provides `input.model` JSON.
+default allow = false
+
+allow {
+  input.model != null
+  has_model_card(input.model)
+}
+
+has_model_card(model) {
+  # model.artifacts must include an entry with key "model_card" or "model_card.md"
+  some i
+  artifacts := model.artifacts
+  artifacts[i].name == "model_card.md"
+}
+
*** End Patch
*** Begin Patch
*** Add File: tools/opa_check_model_card.py
+#!/usr/bin/env python3
+"""
+CI helper: fetch model metadata (MLflow) and evaluate OPA policy to ensure model_card present.
+Usage:
+  python tools/opa_check_model_card.py --model-uri runs:/<run_id>/model
+"""
+import argparse
+import mlflow
+import json
+import subprocess
+import tempfile
+import os
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--run-id", required=True)
+    p.add_argument("--mlflow-uri", default=os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
+    return p.parse_args()
+
+
+def main():
+    args = parse_args()
+    mlflow.set_tracking_uri(args.mlflow_uri)
+    client = mlflow.tracking.MlflowClient()
+    try:
+        artifacts = client.list_artifacts(args.run_id, path=".")
+    except Exception as e:
+        print("failed to list artifacts", e)
+        raise SystemExit(2)
+    model = {"artifacts": [{"name": a.path} for a in artifacts]}
+    # Write input and policy to temp files and run opa eval (assuming opa binary installed)
+    with tempfile.NamedTemporaryFile("w", delete=False) as inf:
+        json.dump({"model": model}, inf)
+        inf.flush()
+        inp = inf.name
+    policy_path = os.path.join(os.path.dirname(__file__), "../policies/opa/model_card_required.rego")
+    try:
+        res = subprocess.run(["opa", "eval", "-d", policy_path, "-i", inp, "data.aegis.policies.model_card.allow"], capture_output=True, text=True)
+        print(res.stdout)
+        if "true" in res.stdout:
+            print("OPA policy allow == true")
+            return 0
+        else:
+            print("OPA policy denied: model_card missing")
+            return 1
+    finally:
+        os.unlink(inp)
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
+
*** End Patch
*** Begin Patch
*** Add File: pipeline/checks/fairness_check.py
+#!/usr/bin/env python3
+"""
+Simple fairness check:
+- Input: predictions CSV with columns: id, group (e.g., demographic), label, pred_prob
+- Computes group-wise AUC or average predicted probability and flags if difference > threshold
+"""
+import argparse
+import pandas as pd
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--preds-csv", required=True)
+    p.add_argument("--threshold", type=float, default=0.1, help="max allowed gap in mean pred")
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    df = pd.read_csv(args.preds_csv)
+    groups = df["group"].unique()
+    means = df.groupby("group")["pred_prob"].mean()
+    gap = means.max() - means.min()
+    print("group means:", means.to_dict(), "gap:", gap)
+    if gap > args.threshold:
+        print("FAIRNESS CHECK FAILED")
+        raise SystemExit(2)
+    print("FAIRNESS CHECK PASSED")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: pipeline/checks/robustness_check.py
+#!/usr/bin/env python3
+"""
+Robustness tests: run base predictions and perturbed predictions and compare stability.
+This script applies gaussian noise or simple image corruptions and checks metric delta.
+"""
+import argparse
+import numpy as np
+import requests
+import json
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--endpoint", required=True)
+    p.add_argument("--n", type=int, default=50)
+    p.add_argument("--max-delta", type=float, default=0.05)
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    # for demo: assume endpoint accepts JSON {"tensor": [..]}
+    base = np.random.randn(1, 224, 224, 3).tolist()
+    perturbed = (np.array(base) + 0.01 * np.random.randn(*np.array(base).shape)).tolist()
+    r1 = requests.post(args.endpoint, json={"tensor": base}, timeout=10).json()
+    r2 = requests.post(args.endpoint, json={"tensor": perturbed}, timeout=10).json()
+    # naive compare on predicted scores
+    s1 = float(r1.get("score", 0.0))
+    s2 = float(r2.get("score", 0.0))
+    delta = abs(s1 - s2)
+    print("robustness delta", delta)
+    if delta > args.max_delta:
+        print("ROBUSTNESS CHECK FAILED")
+        raise SystemExit(2)
+    print("ROBUSTNESS CHECK PASSED")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: pipeline/checks/privacy_check.py
+#!/usr/bin/env python3
+"""
+Privacy heuristics: scan model artifacts or explanations for PHI-like patterns and fail if found.
+"""
+import argparse
+import re
+from pathlib import Path
+
+SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--artifact-dir", required=True)
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    p = Path(args.artifact_dir)
+    for f in p.rglob("*.json"):
+        text = f.read_text(errors="ignore")
+        if SSN_RE.search(text):
+            print("Found SSN-like pattern in", f)
+            raise SystemExit(2)
+    print("Privacy heuristic passed")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/explainer_prom_rule.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-explainer-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: explanainer.rules
+      rules:
+        - alert: HighExplainerLatency
+          expr: aegis_explain_request_seconds_bucket{le="10"} == 0
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Explainer latency is high"
+            description: "Explainer latency > 10s"
+        - alert: LowExplanationCoverage
+          expr: aegis_explained_fraction < 0.1
+          for: 10m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Low explanation coverage for model"
+            description: "Less than 10% of requests have explanations"
+
*** End Patch
*** Begin Patch
*** Add File: governance/human_approval/README.md
+Human-in-the-loop approval pattern
+---------------------------------
+
+This folder describes a minimal pattern for gated model promotion that requires human approval:
+
+1) CI produces a model bundle and pushes artifacts to MLflow and a promotion branch (refs/promotion/<model>-<runid>)
+2) Promotion workflow creates a GitHub environment deployment requiring reviewers (set under repository settings -> Environments)
+3) When reviewers approve the environment deployment, the workflow continues and ArgoCD syncs the staging app.
+4) All approvals recorded in GitHub audit and decision_log.
+
+See .github/workflows/promote_with_approval.yml for an example workflow (requires repo admin to configure environment reviewers).
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/promote_with_approval.yml
+name: Promote model with approval
+on:
+  workflow_dispatch:
+jobs:
+  promote:
+    runs-on: ubuntu-latest
+    environment:
+      name: model-promotion
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run promotion checks
+        run: |
+          echo "Run fairness, robustness, privacy checks"
+          python pipeline/checks/fairness_check.py --preds-csv preds.csv || exit 1
+          python pipeline/checks/robustness_check.py --endpoint http://predictor:8080/predict || exit 1
+          python pipeline/checks/privacy_check.py --artifact-dir artifacts || exit 1
+      - name: Record signoff
+        run: echo "Approval granted by reviewers; promoting model"
+      - name: Trigger ArgoCD sync
+        run: |
+          # requires argocd CLI authenticated on runner or use API token
+          echo "Triggering ArgoCD sync (placeholder)"
+
*** End Patch
*** Begin Patch
*** Add File: auditor/metabase/queries/explanations_manifest.sql
+-- Metabase SQL to show explanation artifacts and links
+-- Assumes MLflow artifacts are exported to a table `mlflow_artifacts` with columns (run_id, path, url, created_at)
+SELECT
+  run_id,
+  path,
+  url,
+  created_at
+FROM mlflow_artifacts
+WHERE path LIKE 'explanations/%'
+ORDER BY created_at DESC
+LIMIT 200;
+
*** End Patch
*** Begin Patch
*** Add File: privacy/synthetic_exemplar.py
+#!/usr/bin/env python3
+"""
+Generate a synthetic exemplar from an input instance to avoid exposing raw PHI in explanations.
+This simple approach adds noise and shuffles noncritical tokens; use DP/synthetic generators for prod.
+"""
+import numpy as np
+import json
+
+def synthetic_image_exemplar(img_array, noise_scale=0.05):
+    arr = np.array(img_array).astype(float)
+    arr = arr + noise_scale * np.random.randn(*arr.shape)
+    arr = np.clip(arr, 0, 255).astype("uint8")
+    return arr.tolist()
+
+def synthetic_text_exemplar(text):
+    # naive redact names and shuffle words
+    tokens = text.split()
+    for i, t in enumerate(tokens):
+        if t.istitle() and len(t) > 2:
+            tokens[i] = "[NAME]"
+    np.random.shuffle(tokens)
+    return " ".join(tokens)
+
+def main_demo():
+    print(synthetic_text_exemplar("Patient John Doe visited on 2020-01-01 with MRN: 12345"))
+
+if __name__ == "__main__":
+    main_demo()
+
*** End Patch
*** Begin Patch
*** Add File: tests/e2e/explainer_explainability_test.py
+import requests, time, os
+from io import BytesIO
+from PIL import Image, ImageDraw
+
+ENDPOINT = os.environ.get("EXPLAINER_URL", "http://localhost:8081/explain")
+
+def make_image_bytes():
+    img = Image.new("RGB", (224,224), color=(73,109,137))
+    d = ImageDraw.Draw(img)
+    d.text((10,10), "exp", fill=(255,255,0))
+    b = BytesIO()
+    img.save(b, format="PNG")
+    b.seek(0)
+    return b.getvalue()
+
+def test_explainer_smoke():
+    payload = {
+        "model_id": "demo-model",
+        "request_id": "test-1",
+        "input": {"tensor": [ [0.0]*224*224*3 ]},
+        "explain": True
+    }
+    r = requests.post(ENDPOINT, json=payload, timeout=30)
+    assert r.status_code == 200
+    j = r.json()
+    assert "explanation" in j
+
+def test_explainer_chaos():
+    # This test ensures explainer fallback behavior if model fails (simulated by sending no tensor)
+    payload = {"model_id": "demo-model", "request_id": "test-2", "input": {}, "explain": True}
+    r = requests.post(ENDPOINT, json=payload, timeout=10)
+    # explainer should return 500 or provide structured error payload but not hang forever
+    assert r.status_code in (200, 500)
+
*** End Patch
*** Begin Patch
*** Add File: chaos/kill_explainer_pod.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS="${1:-aegis}"
+LABEL="${2:-app=multimodal-explainer}"
+POD=$(kubectl get pods -n "$NS" -l "$LABEL" -o jsonpath='{.items[0].metadata.name}')
+echo "Killing explainer pod $POD in namespace $NS"
+kubectl delete pod "$POD" -n "$NS"
+
*** End Patch
*** End Patch
