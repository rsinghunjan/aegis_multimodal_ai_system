*** Begin Patch
*** Add File: k8s/cron/minio_backup_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-minio-backup
+  namespace: aegis
+spec:
+  schedule: "30 3 * * *" # daily 03:30 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+          - name: minio-backup
+            image: quay.io/bitnami/minio-client:latest
+            env:
+              - name: MINIO_ENDPOINT
+                value: "http://minio.aegis.svc:9000"
+              - name: MINIO_ACCESS_KEY
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-minio-credentials
+                    key: accesskey
+              - name: MINIO_SECRET_KEY
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-minio-credentials
+                    key: secretkey
+              - name: BACKUP_BUCKET
+                value: "aegis-backups"
+            command:
+              - /bin/sh
+              - -c
+              - |
+                set -euo pipefail
+                TS=$(date -u +%Y%m%dT%H%M%SZ)
+                echo "Creating snapshot of MinIO bucket 'mlflow-artifacts' to ${BACKUP_BUCKET}/minio/${TS}/"
+                mc alias set local "${MINIO_ENDPOINT}" "${MINIO_ACCESS_KEY}" "${MINIO_SECRET_KEY}"
+                mc mb local/"${BACKUP_BUCKET}" || true
+                mc cp --recursive local/mlflow-artifacts local/"${BACKUP_BUCKET}"/minio/"${TS}"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: backup/scripts/vault_snapshot.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Vault snapshot script (operator host or k8s job)
+# Uploads snapshot to S3 bucket; requires VAULT_ADDR and VAULT_TOKEN env vars and AWS creds.
+TS=$(date -u +%Y%m%dT%H%M%SZ)
+OUT="/tmp/vault_snapshot_${TS}.snap"
+echo "Writing Vault snapshot to $OUT"
+vault operator raft snapshot save "$OUT"
+if [ -n "${S3_BUCKET:-}" ]; then
+  echo "Uploading snapshot to s3://${S3_BUCKET}/vault_snapshots/${TS}.snap"
+  aws s3 cp "$OUT" "s3://${S3_BUCKET}/vault_snapshots/${TS}.snap"
+else
+  echo "S3_BUCKET not set; snapshot kept locally"
+fi
+echo "Snapshot complete"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/cron/vault_snapshot_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-vault-snapshot
+  namespace: aegis
+spec:
+  schedule: "0 4 * * *" # daily 04:00 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+          - name: vault-snapshot
+            image: alpine:3.18
+            env:
+              - name: VAULT_ADDR
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-vault-credentials
+                    key: addr
+              - name: VAULT_TOKEN
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-vault-credentials
+                    key: token
+              - name: AWS_ACCESS_KEY_ID
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-s3-credentials
+                    key: access_key
+              - name: AWS_SECRET_ACCESS_KEY
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-s3-credentials
+                    key: secret_key
+              - name: S3_BUCKET
+                value: "aegis-backups"
+            volumeMounts:
+              - name: scripts
+                mountPath: /opt/backup
+            command:
+              - /bin/sh
+              - -c
+              - |
+                set -euo pipefail
+                /opt/backup/vault_snapshot.sh
+          restartPolicy: OnFailure
+          volumes:
+            - name: scripts
+              configMap:
+                name: aegis-backup-scripts
+
*** End Patch
*** Begin Patch
*** Add File: k8s/configmap/backup-scripts-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: aegis-backup-scripts
+  namespace: aegis
+data:
+  vault_snapshot.sh: |
+    #!/usr/bin/env bash
+    set -euo pipefail
+    TS=$(date -u +%Y%m%dT%H%M%SZ)
+    OUT="/tmp/vault_snapshot_${TS}.snap"
+    vault operator raft snapshot save "$OUT"
+    if [ -n "${S3_BUCKET:-}" ]; then
+      aws s3 cp "$OUT" "s3://${S3_BUCKET}/vault_snapshots/${TS}.snap"
+    fi
+
*** End Patch
*** Begin Patch
*** Add File: ansible/hsm/incident_playbook.yml
+- name: HSM Incident Response Playbook
+  hosts: hsm_admin
+  become: true
+  vars:
+    audit_upload_bucket: "{{ lookup('env','COMPLIANCE_BUCKET') | default('aegis-compliance') }}"
+  tasks:
+    - name: Stop signing services to prevent further signing
+      systemd:
+        name: cosign-signing.service
+        state: stopped
+      ignore_errors: yes
+
+    - name: Dump HSM audit logs (operator must adapt to HSM vendor)
+      shell: |
+        # vendor-specific command placeholder
+        echo "HSM audit dump placeholder" > /tmp/hsm_audit_$(date -u +%Y%m%dT%H%M%SZ).log
+      register: hsm_audit
+
+    - name: Upload HSM audit to compliance S3
+      aws_s3:
+        bucket: "{{ audit_upload_bucket }}"
+        object: "hsm_incidents/{{ inventory_hostname }}_$(date -u +%Y%m%dT%H%M%SZ).log"
+        src: "{{ hsm_audit.stdout }}"
+      ignore_errors: yes
+
+    - name: Rotate cosign key (operator must perform HSM-backed key rotation with vendor tools)
+      debug:
+        msg: "Rotate cosign key per vendor procedure (manual step). Document rotation and Rekor entries."
+
+    - name: Create incident ticket (operator should integrate with ticketing)
+      debug:
+        msg: "Create incident ticket and attach /tmp/hsm_audit_* logs for audit"
+
*** End Patch
*** Begin Patch
*** Add File: docs/hsm_runbook.md
+# HSM Operational Runbook (Aegis)
+
+Overview
+- HSM admin host is the only machine permitted to access PKCS11 signing keys.
+- All production artifact signing must be performed on this host using cosign PKCS11.
+
+Daily/Weekly
+- Verify HSM health and connection to admin host.
+- Run signing drill (monthly) and verify Rekor entries.
+
+Incident Response
+- If HSM compromise suspected:
+ 1. Stop signing services (systemctl stop cosign-signing.service).
+ 2. Dump HSM audit logs and upload to compliance bucket.
+ 3. Rotate cosign keys (vendor procedure).
+ 4. Revoke affected Rekor entries if necessary and issue new signed artifacts where required.
+
+Signing Procedure (operator)
+- Place artifacts in /opt/aegis/artifacts_to_sign
+- Run: ansible-playbook -i inventory hsm/hsm_signing_operator_playbook.yml
+- Verify Rekor entries via rekor-cli
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/model_safety_pipeline.yml
+name: Model Safety Pipeline - Shadow → Train → Canary → Auto-Rollback
+on:
+  workflow_dispatch:
+    inputs:
+      state-file:
+        description: "Path or S3 URL to broker_state snapshot for training"
+        required: true
+      canary-manifest:
+        description: "K8s manifest path for canary deployment"
+        required: true
+      prom-url:
+        description: "Prometheus URL for SLO checks"
+        required: true
+
+jobs:
+  shadow-collect:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Snapshot broker state (operator must provide)
+        run: |
+          echo "Operator must place a snapshot at provider_state.json or provide S3 URL; this step is a placeholder"
+
+  train:
+    needs: shadow-collect
+    runs-on: ubuntu-latest
+    steps:
+      - name: Setup Python
+        run: python -m pip install --upgrade pip && pip install scikit-learn numpy
+      - name: Fetch state file
+        run: |
+          cp ${{ github.event.inputs.state-file }} /tmp/provider_state.json || true
+      - name: Train model
+        run: |
+          python provider/learner/train_policy.py --state /tmp/provider_state.json --out /tmp/policy_model.pkl
+      - name: Upload model artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: policy-model
+          path: /tmp/policy_model.pkl
+
+  canary-deploy:
+    needs: train
+    runs-on: ubuntu-latest
+    steps:
+      - name: Download model artifact
+        uses: actions/download-artifact@v4
+        with:
+          name: policy-model
+      - name: Operator: deploy canary (explicit manual step)
+        run: |
+          echo "Operator must deploy model artifact to S3 and update broker to point to new model in canary mode."
+      - name: Monitor canary SLO (5 minutes)
+        env:
+          PROM_URL: ${{ github.event.inputs.prom-url }}
+          SLO_MS: 300
+        run: |
+          python - <<'PY'
+import os,requests,time,sys,subprocess
+PROM=os.environ['PROM_URL']
+SLO=float(os.environ.get('SLO_MS',300))
+query='histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="provider-broker"}[5m])) by (le))'
+for i in range(5):
+    try:
+        r=requests.get(f"{PROM}/api/v1/query", params={"query":query}, timeout=10)
+        if r.ok and r.json().get("data",{}).get("result"):
+            v=float(r.json()["data"]["result"][0]["value"][1])
+            print("p95:",v)
+            if v>SLO:
+                print("SLO breached; rolling back")
+                subprocess.run(["kubectl","rollout","undo","deployment/provider-broker-learned"], check=False)
+                sys.exit(1)
+    except Exception as e:
+        print("error",e)
+    time.sleep(60)
+print("Canary passed checks")
+PY
+
+  safety-gates:
+    needs: canary-deploy
+    runs-on: ubuntu-latest
+    steps:
+      - name: Run A/B evaluation job (operator supplies data)
+        run: |
+          echo "Operator: run provider experiments to gather A/B data and produce /tmp/ab_a.json and /tmp/ab_b.json"
+      - name: Evaluate A/B
+        run: |
+          python provider/learner/evaluate_ab_results.py --a "$(cat /tmp/ab_a.json)" --b "$(cat /tmp/ab_b.json)" --out /tmp/ab_verdict.json || (echo "A/B test failed" && exit 2)
+      - name: Upload verdict
+        uses: actions/upload-artifact@v4
+        with:
+          name: ab-verdict
+          path: /tmp/ab_verdict.json
+
*** End Patch
*** Begin Patch
*** Add File: opa/policies/model_safety.rego
+package aegis.safety
+
+# Input: {"model_manifest":{"name":"policy_model.pkl","size":12345},"safety":{"min_fidelity":0.8,"max_latency_ms":500}}
+
+deny[msg] {
+  input.safety.min_fidelity
+  input.safety.observed_fidelity
+  input.safety.observed_fidelity < input.safety.min_fidelity
+  msg = sprintf("Observed fidelity %v lower than minimum %v", [input.safety.observed_fidelity, input.safety.min_fidelity])
+}
+
+deny[msg] {
+  input.safety.max_latency_ms
+  input.safety.observed_latency_ms
+  input.safety.observed_latency_ms > input.safety.max_latency_ms
+  msg = sprintf("Observed latency %v exceeds max %v", [input.safety.observed_latency_ms, input.safety.max_latency_ms])
+}
+
+allow { not deny[_] }
+
*** End Patch
*** Begin Patch
*** Add File: edge/power_sensor_agent.py
+#!/usr/bin/env python3
+"""
+Lightweight device power telemetry agent.
+ - Prefer a real sensor interface; this prototype tries to read a local sysfs path or falls back to IPMI or random.
+ - Exposes Prometheus metrics and a small HTTP health endpoint.
+"""
+from prometheus_client import start_http_server, Gauge
+import time, os, random, argparse
+
+POWER_GAUGE = Gauge("aegis_device_power_w", "Device power in Watts", ["device"])
+
+def read_sys_power(sys_path):
+    try:
+        with open(sys_path) as fh:
+            return float(fh.read().strip())
+    except Exception:
+        return None
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--device", default=os.environ.get("DEVICE_ID", "device-1"))
+    p.add_argument("--sys-path", default="/sys/class/power_supply/BAT0/power_now")
+    p.add_argument("--port", type=int, default=9101)
+    args = p.parse_args()
+    start_http_server(args.port)
+    while True:
+        val = read_sys_power(args.sys_path)
+        if val is None:
+            # attempt ipmitool or fallback
+            try:
+                # placeholder for ipmitool reading
+                val = 30.0 + random.random()*5.0
+            except Exception:
+                val = 30.0 + random.random()*10.0
+        POWER_GAUGE.labels(device=args.device).set(val)
+        time.sleep(10)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: telemetry/attestation_integration.py
+#!/usr/bin/env python3
+"""
+Fetch attestation results from attestation server and persist to audit DB or S3.
+ - Expected to run as operator batch job after attestation_worker uploads quotes.
+"""
+import os, requests, json
+from datetime import datetime
+
+ATTESTATION_SERVER = os.environ.get("ATTESTATION_SERVER", "https://attest.example")
+AUDIT_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def fetch_reports():
+    r = requests.get(f"{ATTESTATION_SERVER}/reports", timeout=30)
+    if r.ok:
+        return r.json()
+    return []
+
+def save_local(reports):
+    out = f"/tmp/attest_reports_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
+    with open(out, "w") as fh:
+        json.dump(reports, fh, indent=2)
+    return out
+
+if __name__ == "__main__":
+    reports = fetch_reports()
+    if not reports:
+        print("No attestation reports")
+        raise SystemExit(0)
+    out = save_local(reports)
+    print("Saved attestation reports to", out)
+    if AUDIT_BUCKET:
+        print("Operator: upload", out, "to compliance bucket", AUDIT_BUCKET)
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/slo_rules.yaml
+groups:
+- name: aegis-slos
+  rules:
+  - record: job:aegis_broker_p95:raw
+    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="provider-broker"}[5m])) by (le))
+  - alert: BrokerP95SLOBreach
+    expr: job:aegis_broker_p95:raw > 0.3
+    for: 2m
+    labels:
+      severity: page
+    annotations:
+      summary: "Broker p95 SLO breach"
+      description: "Broker p95 is {{ $value }}s which exceeds target"
+
+  - alert: DevicePowerMissing
+    expr: absent(aegis_device_power_w)
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Device power telemetry missing"
+      description: "No device power metrics detected; check exporters"
+
*** End Patch
*** Begin Patch
*** Add File: metrics/grafana/observability_dashboard.json
+{
+  "dashboard": {
+    "title": "Aegis Observability & Impact",
+    "panels": [
+      { "type": "graph", "title": "Broker p95 (s)", "targets": [{ "expr": "job:aegis_broker_p95:raw", "legendFormat": "broker p95" }] },
+      { "type": "graph", "title": "Device Power (W)", "targets": [{ "expr": "avg by (device) (aegis_device_power_w)", "legendFormat": "{{device}}" }] },
+      { "type": "graph", "title": "kgCO2e per job (recent)", "targets": [{ "expr": "billing_job_kgco2e_sum", "legendFormat": "{{tenant}}" }] },
+      { "type": "stat", "title": "Recent A/B Verdict", "targets": [{ "expr": "last_over_time(aegis_broker_ab_verdict[1d])", "legendFormat": "ab_verdict" }] }
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: signoff/dr_test_runbook.yml
+---
+# DR Test runbook (Aegis) - quarterly exercise
+- name: DR Test - restore from backups
+  hosts: local
+  tasks:
+    - name: Create staging cluster (operator step)
+      debug:
+        msg: "Provision staging cluster via Terraform / operator scripts"
+
+    - name: Restore Postgres from latest backup (operator)
+      debug:
+        msg: "Download latest backup from s3://aegis-backups/postgres and run pg_restore into staging DB"
+
+    - name: Restore MinIO artifacts
+      debug:
+        msg: "Restore minio artifacts into staging minio instance"
+
+    - name: Start MLflow pointing to restored DB and MinIO
+      debug:
+        msg: "Start MLflow server and verify runs and artifacts present"
+
+    - name: Perform smoke tests
+      shell: |
+        python scripts/nightly_verification_run.py || true
+      register: smoke
+
+    - name: Collect DR evidence
+      shell: |
+        tar -czf /tmp/dr_evidence_$(date -u +%Y%m%dT%H%M%SZ).tgz /tmp/aegis_verification_report.json || true
+      register: evidence
+
+    - name: Upload DR evidence to compliance bucket (operator)
+      debug:
+        msg: "Upload /tmp/dr_evidence_*.tgz to compliance bucket for auditors"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/generate_dr_evidence.yml
+name: Generate DR Evidence (trigger post-dr-test)
+on:
+  workflow_dispatch:
+    inputs:
+      evidence-path:
+        description: "Path to DR evidence tarball (operator uploads)"
+        required: true
+
+jobs:
+  upload:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Upload DR evidence to compliance bucket
+        env:
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          S3_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          TAR="${{ github.event.inputs.evidence-path }}"
+          if [ ! -f "$TAR" ]; then
+            echo "Evidence tar not found: $TAR"
+            exit 1
+          fi
+          aws s3 cp "$TAR" "s3://${S3_BUCKET}/dr_evidence/$(basename $TAR)"
+          echo "Uploaded to s3://${S3_BUCKET}/dr_evidence/$(basename $TAR)"
+
*** End Patch
*** End Patch
