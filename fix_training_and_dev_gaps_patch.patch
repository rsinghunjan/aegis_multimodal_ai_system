*** Begin Patch
*** Add File: argo/deepspeed_zeo_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: deepspeed-zeo-train
+  namespace: staging
+spec:
+  entrypoint: train
+  templates:
+  - name: train
+    dag:
+      tasks:
+      - name: launch
+        template: deepspeed-job
+
+  - name: deepspeed-job
+    container:
+      image: ghcr.io/yourorg/deepspeed-training:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -e
+          export NCCL_DEBUG=INFO
+          export NCCL_SOCKET_IFNAME=eth0
+          export DS_VERBOSE=2
+          export MASTER_ADDR=$(hostname -i)
+          export MASTER_PORT=29500
+          deepspeed --num_nodes ${NUM_NODES:-2} --num_gpus ${NUM_GPUS_PER_NODE:-8} train.py --deepspeed_config ds_config.json --ckpt_path /data/checkpoints || exit 1
+      resources:
+        limits:
+          nvidia.com/gpu: "{{workflow.parameters.gpus}}"
+        requests:
+          memory: "256Gi"
+          cpu: "32"
+    nodeSelector:
+      aegis/worker-type: dgx
+    tolerations:
+    - key: nvidia.com/gpu
+      operator: Exists
+    volumes:
+    - name: data
+      persistentVolumeClaim:
+        claimName: deepspeed-data-pvc
+    volumeMounts:
+    - name: data
+      mountPath: /data
+
*** End Patch
*** Begin Patch
*** Add File: docker/deepspeed.Dockerfile
+FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y build-essential git wget python3 python3-pip && rm -rf /var/lib/apt/lists/*
+RUN python3 -m pip install --upgrade pip setuptools
+# Install PyTorch + DeepSpeed (pin versions as appropriate)
+RUN pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118
+RUN pip install deepspeed==2.9.1 transformers datasets accelerate
+RUN pip install mlflow psutil nvidia-ml-py3
+WORKDIR /workspace
+COPY train.py /workspace/train.py
+COPY ds_config.json /workspace/ds_config.json
+ENTRYPOINT ["bash","-lc"]
+CMD ["python train.py"]
+
*** End Patch
*** Begin Patch
*** Add File: k8s/nccl-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: nccl-tuning
+  namespace: staging
+data:
+  NCCL_SOCKET_IFNAME: "eth0"
+  NCCL_IB_GID_INDEX: "3"
+  NCCL_IB_HCA: "mlx5_0"
+  NCCL_DEBUG: "INFO"
+  NCCL_P2P_LEVEL: "NVL"
+
*** End Patch
*** Begin Patch
*** Add File: argo/dgx_distributed_deepspeed_validation.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: deepspeed-validation
+  namespace: staging
+spec:
+  entrypoint: validate
+  templates:
+  - name: validate
+    steps:
+      - - name: run-deepspeed
+          template: deepspeed-job
+
+  - name: deepspeed-job
+    container:
+      image: ghcr.io/yourorg/deepspeed-training:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -euxo pipefail
+          # start training for a short smoke run and then simulate node preemption
+          deepspeed --num_nodes 2 --num_gpus 8 train.py --epochs 1 --ckpt_path /data/checkpoints & sleep 30
+          PID=$!
+          echo "Simulating preemption: kill -9 $PID" && kill -9 $PID || true
+          # Wait then restart to validate resume
+          sleep 10
+          deepspeed --num_nodes 2 --num_gpus 8 train.py --resume_from_checkpoint /data/checkpoints || exit 1
+      resources:
+        limits:
+          nvidia.com/gpu: 8
+        requests:
+          memory: "200Gi"
+          cpu: "24"
+    nodeSelector:
+      aegis/worker-type: dgx
+    volumeMounts:
+    - name: data
+      mountPath: /data
+    volumes:
+    - name: data
+      persistentVolumeClaim:
+        claimName: deepspeed-data-pvc
+
*** End Patch
*** Begin Patch
*** Add File: argo/ray_tune_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: ray-tune-hpo
+  namespace: staging
+spec:
+  entrypoint: hpo
+  templates:
+  - name: hpo
+    container:
+      image: ghcr.io/yourorg/ray-tune:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          pip install ray[tune]==2.5.0
+          python run_ray_tune.py --experiment $EXPERIMENT_NAME --num_samples 20 --cpus 8 --gpus 1
+      resources:
+        requests:
+          cpu: "8"
+          memory: "64Gi"
+        limits:
+          nvidia.com/gpu: 1
+
*** End Patch
*** Begin Patch
*** Add File: services/hpo/run_ray_tune.py
+import argparse, os
+from ray import tune, init
+
+def train(config):
+    # placeholder training function; integrate with your train loop and MLflow
+    import time, random
+    for i in range(10):
+        time.sleep(1)
+    tune.report(mean_accuracy=random.random())
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--experiment", default="aegis-hpo")
+    parser.add_argument("--num_samples", type=int, default=10)
+    parser.add_argument("--cpus", type=int, default=1)
+    parser.add_argument("--gpus", type=int, default=0)
+    args = parser.parse_args()
+    init(address="auto")
+    tune.run(train, name=args.experiment, num_samples=args.num_samples, resources_per_trial={"cpu": args.cpus, "gpu": args.gpus})
+
*** End Patch
*** Begin Patch
*** Add File: helm/jupyterhub/values.yaml
+# JupyterHub Helm values tailored for Aegis developer workspaces
+singleuser:
+  image:
+    name: jupyter/base-notebook
+    tag: latest
+  storage:
+    capacity: 50Gi
+  extraEnv:
+    MLFLOW_TRACKING_URI: "http://mlflow.staging.svc.cluster.local:5000"
+  persistence:
+    enabled: true
+    storageClass: "fast-ssd"
+hub:
+  baseUrl: /hub
+  services:
+    - name: aegis-proxy
+      url: http://proxy:8080
+auth:
+  type: "github"
+  github:
+    clientId: "${GITHUB_CLIENT_ID}"
+    clientSecret: "${GITHUB_CLIENT_SECRET}"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/setup_jupyterhub.sh
+#!/usr/bin/env bash
+set -euo pipefail
+helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ || true
+helm repo update
+helm upgrade --install aegis-jupyterhub jupyterhub/jupyterhub -n dev --create-namespace -f helm/jupyterhub/values.yaml
+echo "JupyterHub deployed. Configure OAuth client and Vault secrets for user pods."
+
*** End Patch
*** Begin Patch
*** Add File: mlflow/mlflow_standard_config.yaml
+server:
+  backend_store_uri: postgresql://mlflow:mlflow@mlflow-postgres.staging.svc.cluster.local:5432/mlflow
+  default_artifact_root: s3://aegis-compliance/mlflow-artifacts
+  rbac: true
+  allowed_experiments_prefix: "aegis-"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/create_tfrecords.py
+#!/usr/bin/env python3
+import os, sys
+import tensorflow as tf
+
+def _bytes_feature(value):
+    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
+
+def create_tfrecords(image_paths, out_path):
+    with tf.io.TFRecordWriter(out_path) as writer:
+        for p in image_paths:
+            img = tf.io.gfile.GFile(p, 'rb').read()
+            feature = {"image_raw": _bytes_feature(img)}
+            example = tf.train.Example(features=tf.train.Features(feature=feature))
+            writer.write(example.SerializeToString())
+
+if __name__ == "__main__":
+    # usage: create_tfrecords.py /data/images.lst output.tfrecord
+    paths_file = sys.argv[1]
+    out = sys.argv[2]
+    paths = [l.strip() for l in open(paths_file)]
+    create_tfrecords(paths, out)
+    print("Wrote", out)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/webdataset_sharder.py
+#!/usr/bin/env python3
+"""
+Produce sharded WebDataset tar shards from input files for high-throughput training.
+"""
+import os, sys, tarfile
+from pathlib import Path
+
+def shard(paths, out_prefix, shard_size=1000):
+    shard_idx = 0
+    file_idx = 0
+    tf = None
+    for p in paths:
+        if file_idx % shard_size == 0:
+            if tf:
+                tf.close()
+            shard_name = f"{out_prefix}-{shard_idx:05d}.tar"
+            tf = tarfile.open(shard_name, "w")
+            shard_idx += 1
+        arcname = os.path.basename(p)
+        tf.add(p, arcname=arcname)
+        file_idx += 1
+    if tf:
+        tf.close()
+
+if __name__ == "__main__":
+    paths = [l.strip() for l in open(sys.argv[1])]
+    shard(paths, sys.argv[2], int(sys.argv[3]) if len(sys.argv) > 3 else 1000)
+    print("Sharding complete")
+
*** End Patch
*** Begin Patch
*** Add File: profiling/pytorch_profiler_runner.py
+#!/usr/bin/env python3
+"""
+Run a training step under PyTorch profiler and upload traces to compliance storage.
+"""
+import torch, os, time, subprocess
+from torch.profiler import profile, record_function, ProfilerActivity
+
+def profile_step(model, input):
+    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
+        with record_function("model_infer"):
+            model(input)
+    prof.export_chrome_trace("/tmp/trace.json")
+    return "/tmp/trace.json"
+
+if __name__ == "__main__":
+    # placeholder: load small model and run
+    import torchvision.models as models
+    m = models.resnet18().cuda()
+    inp = torch.randn(1,3,224,224).cuda()
+    trace = profile_step(m, inp)
+    # upload trace (AWS/GCP/Azure depending on env)
+    if os.environ.get("COMPLIANCE_BUCKET"):
+        if os.environ.get("CLOUD_PROVIDER") == "aws":
+            subprocess.run(["aws","s3","cp",trace, os.environ["COMPLIANCE_BUCKET"] + "/profiling/" + os.path.basename(trace)])
+        else:
+            subprocess.run(["gsutil","cp",trace, os.environ["COMPLIANCE_BUCKET"] + "/profiling/" + os.path.basename(trace)])
+    print("Profile trace uploaded:", trace)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/spot_autoscaler_helper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper hints to run spot/autoscaling training jobs:
+# - Use nodeSelector/taints to target spot node groups (e.g., taint spot=true:NoSchedule)
+# - Use pod tolerations in job spec and set priorityClass
+# - Add preStop hook to checkpoint frequently and use pod disruption budgets if appropriate
+cat <<'YAML'
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: spot-training-job
+spec:
+  template:
+    spec:
+      tolerations:
+      - key: "spot"
+        operator: "Equal"
+        value: "true"
+        effect: "NoSchedule"
+      nodeSelector:
+        node.kubernetes.io/instance-type: "spot-gpu"
+      priorityClassName: "spot-training"
+      containers:
+      - name: trainer
+        image: ghcr.io/yourorg/deepspeed-training:latest
+        lifecycle:
+          preStop:
+            exec:
+              command: ["/bin/bash","-lc","python train.py --checkpoint --fast-save"]
+      restartPolicy: Never
+YAML
+
+echo "Template printed. Replace nodeSelector with your cluster labels and configure spot node pool."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/budget_watcher.py
+#!/usr/bin/env python3
+"""
+Simple budget watcher that checks cloud billing API and posts alerts or annotates training Jobs.
+This is a placeholder example â€” integrate with your cloud billing metrics/alerts.
+"""
+import os, time, requests
+
+THRESHOLD = float(os.environ.get("BUDGET_THRESHOLD", "90.0"))
+CHECK_INTERVAL = int(os.environ.get("BUDGET_CHECK_INTERVAL", "300"))
+NOTIFY_WEBHOOK = os.environ.get("BUDGET_WEBHOOK")
+
+def check_billing():
+    # Placeholder: query cloud billing API (implement per-cloud)
+    # For demo, read from a metrics endpoint or external input
+    return {"usage_pct": float(os.environ.get("CURRENT_USAGE_PCT", "50"))}
+
+def notify(msg):
+    if NOTIFY_WEBHOOK:
+        requests.post(NOTIFY_WEBHOOK, json={"text": msg})
+    else:
+        print("ALERT:", msg)
+
+if __name__ == "__main__":
+    while True:
+        b = check_billing()
+        if b["usage_pct"] > THRESHOLD:
+            notify(f"Budget over threshold: {b['usage_pct']}%")
+        time.sleep(CHECK_INTERVAL)
+
*** End Patch
*** Begin Patch
*** Add File: argo/chaos/chaos_train_drill.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: chaos-train-drill
+  namespace: staging
+spec:
+  entrypoint: chaos
+  templates:
+  - name: chaos
+    steps:
+    - - name: start-train
+        template: run-train
+      - name: inject-failure
+        template: kill-rank
+        arguments:
+          parameters:
+          - name: target-pod
+            value: "{{steps.start-train.outputs.parameters.trainer-pod}}"
+
+  - name: run-train
+    script:
+      image: ghcr.io/yourorg/deepspeed-training:latest
+      command: [bash]
+      source: |
+        set -euxo pipefail
+        # launch background training (short)
+        python train.py --epochs 2 --ckpt_path /data/checkpoints & echo $! > /tmp/train.pid
+        sleep 10
+        # output trainer pod name so failure step can target it
+        echo "trainer-pod=$(hostname -f)" > /tmp/out
+      outputs:
+        parameters:
+        - name: trainer-pod
+          valueFrom:
+            path: /tmp/out
+
+  - name: kill-rank
+    script:
+      image: bitnami/kubectl:latest
+      command: [bash]
+      source: |
+        set -euxo pipefail
+        # This would run a kubectl delete on a pod to simulate failure (must have RBAC)
+        echo "Simulating node/pod kill; operator must implement safe kill with RBAC"
+        # example: kubectl delete pod $TARGET_POD -n staging
+        sleep 2
+        echo "Injected failure"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/chaos_kill_node.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Kills a node by cordon + drain or deletes a pod to simulate failure. Requires cluster-admin access.
+TARGET_POD=${1:-}
+if [ -z "$TARGET_POD" ]; then
+  echo "Usage: $0 <pod-name>"
+  exit 2
+fi
+kubectl delete pod "$TARGET_POD" --grace-period=0 --force || true
+echo "Deleted pod $TARGET_POD to simulate failure"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/pod_security/job_least_privilege.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: training-sa
+  namespace: staging
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: training-role
+  namespace: staging
+rules:
+- apiGroups: [""]
+  resources: ["pods","pods/log","persistentvolumeclaims"]
+  verbs: ["get","list","watch","create"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: training-rolebinding
+  namespace: staging
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: training-role
+subjects:
+- kind: ServiceAccount
+  name: training-sa
+  namespace: staging
+
*** End Patch
*** Begin Patch
*** Add File: k8s/csi_encrypted_pvc_example.yaml
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: encrypted-data-pvc
+  namespace: staging
+spec:
+  storageClassName: encrypted-sc
+  accessModes:
+    - ReadWriteOnce
+  resources:
+    requests:
+      storage: 500Gi
+
+---
+# StorageClass and CSI driver installation is environment-specific. Use cloud provider-managed encrypted volumes or configure a CSI encrypting provider.
+
*** End Patch
*** Begin Patch
*** Add File: argo/tf_multiworker_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: tf-multiworker-train
+  namespace: staging
+spec:
+  entrypoint: tf-train
+  templates:
+  - name: tf-train
+    dag:
+      tasks:
+      - name: ps
+        template: tf-worker
+        arguments:
+          parameters:
+          - name: role
+            value: ps
+      - name: worker
+        template: tf-worker
+        arguments:
+          parameters:
+          - name: role
+            value: worker
+
+  - name: tf-worker
+    inputs:
+      parameters:
+      - name: role
+    container:
+      image: ghcr.io/yourorg/tf-training:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          ROLE={{inputs.parameters.role}}
+          python run_tf_distributed.py --role $ROLE --cluster-config /workspace/cluster.json
+      resources:
+        requests:
+          cpu: "8"
+          memory: "32Gi"
+
*** End Patch
*** Begin Patch
*** Add File: docs/training_hardening_runbook.md
+# Aegis: Heavy Training & Large-Model Development Runbook
+
+Overview
+- Use the provided DeepSpeed/Argo workflows, Ray Tune templates, TF multiworker flows, and DGX tuning files to run large-scale training.
+
+Quickstart
+1. Build & push the deepspeed image: docker build -f docker/deepspeed.Dockerfile -t $REGISTRY/deepspeed:latest .
+2. Create PVC (encrypted) and bind to deepspeed-data-pvc.
+3. Apply NCCL tuning ConfigMap and node labels for DGX nodes.
+4. Submit argo/deepspeed_zeo_workflow.yaml or argo/dgx_distributed_deepspeed_validation.yaml
+
+Resilience tests
+- Use argo/chaos/chaos_train_drill.yaml and scripts/chaos_kill_node.sh to simulate failure and validate checkpoint resume.
+
+Autoscaling / Spot
+- Use spot_autoscaler_helper.sh templates and configure node pools with taints/labels for spot nodes. Use controller to detect preemption via node events and checkpoint frequently.
+
+Profiling
+- Run profiling/pytorch_profiler_runner.py during a training job and upload traces to compliance bucket for analysis.
+
+HPO
+- Use argo/ray_tune_workflow.yaml and services/hpo/run_ray_tune.py to run distributed HPO; integrate with MLflow for experiment logging.
+
+Security & data
+- Use k8s/pod_security/job_least_privilege.yaml to run jobs with restricted RBAC.
+- Mount datasets on encrypted PVCs (k8s/csi_encrypted_pvc_example.yaml). Use Vault for secrets and per-job least privilege service accounts.
+
+Observability
+- Collect metrics and traces to Prometheus and Grafana; wire job-level metrics to enable Argo Rollout analysis and auto-rollback if SLOs degrade.
+
*** End Patch
*** End Patch
