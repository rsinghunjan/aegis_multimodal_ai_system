*** Begin Patch
*** Add File: ansible/maintenance/reprovision_runners.yml
+- name: Reprovision and recover self-hosted runners
+  hosts: runners
+  become: true
+  vars:
+    runner_user: aegis-runner
+    runner_token_vault_path: secret/data/aegis/runner_tokens/{{ inventory_hostname }}
+  tasks:
+    - name: Drain runner (inform orchestrator)
+      uri:
+        url: "https://ci.example.com/api/v1/runners/{{ inventory_hostname }}/drain"
+        method: POST
+        status_code: 200
+      ignore_errors: yes
+
+    - name: Stop runner service
+      systemd:
+        name: github-runner
+        state: stopped
+      ignore_errors: yes
+
+    - name: Remove actions-runner directory
+      file:
+        path: /home/{{ runner_user }}/actions-runner
+        state: absent
+
+    - name: Re-run runner install role (recreates config and systemd)
+      include_role:
+        name: aegis.runner_install
+
+    - name: Restart runner service
+      systemd:
+        name: github-runner
+        state: restarted
+
+    - name: Mark runner healthy
+      uri:
+        url: "https://ci.example.com/api/v1/runners/{{ inventory_hostname }}/healthy"
+        method: POST
+      ignore_errors: yes
+
*** End Patch
*** Begin Patch
*** Add File: scripts/runner/rotate_runner_tokens.py
+#!/usr/bin/env python3
+"""
+Rotate GitHub self-hosted runner registration tokens and reconfigure runners.
+Requires GH_TOKEN (repo admin) env var and an inventory CSV with host,ssh_user fields.
+This script:
+ - creates a new registration token via GitHub REST API
+ - pushes token securely to each runner (via SSH + Ansible/vault)
+ - triggers reconfigure on runner
+"""
+import os
+import sys
+import csv
+import requests
+import subprocess
+from base64 import b64encode
+
+GITHUB_API = "https://api.github.com"
+GH_TOKEN = os.getenv("GH_TOKEN")
+REPO = os.getenv("GITHUB_REPO")  # owner/repo
+
+def create_runner_token(repo):
+    url = f"{GITHUB_API}/repos/{repo}/actions/runners/registration-token"
+    r = requests.post(url, headers={"Authorization": f"token {GH_TOKEN}"})
+    r.raise_for_status()
+    return r.json()["token"]
+
+def push_token_ssh(host, user, token):
+    cmd = f"ssh {user}@{host} 'sudo -u aegis-runner /home/aegis-runner/actions-runner/config.sh remove --token && /home/aegis-runner/actions-runner/config.sh --url https://github.com/{REPO} --token {token} --unattended'"
+    print("Running:", cmd)
+    subprocess.check_call(cmd, shell=True)
+
+def main(inventory_csv):
+    if not GH_TOKEN or not REPO:
+        print("GH_TOKEN and GITHUB_REPO must be set", file=sys.stderr); sys.exit(2)
+    token = create_runner_token(REPO)
+    with open(inventory_csv) as fh:
+        rdr = csv.DictReader(fh)
+        for r in rdr:
+            host = r["host"]; user = r.get("user","root")
+            push_token_ssh(host, user, token)
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("usage: rotate_runner_tokens.py inventory.csv"); sys.exit(2)
+    main(sys.argv[1])
+
*** End Patch
*** Begin Patch
*** Add File: scripts/runner/auto_reprovision.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Detect unhealthy runner and call ansible to reprovision
+RUNNER_HOST="${1:-localhost}"
+ANSIBLE_INVENTORY="${2:-ansible/provision/hosts.ini}"
+ANSIBLE_PLAYBOOK="ansible/provision/provision_runners.yml"
+
+echo "Checking runner health for $RUNNER_HOST"
+status=$(ssh root@"$RUNNER_HOST" 'systemctl is-active github-runner || true')
+echo "service status: $status"
+if [ "$status" != "active" ]; then
+  echo "Runner not active — invoking Ansible reprovision for $RUNNER_HOST"
+  ansible-playbook -i "$ANSIBLE_INVENTORY" "$ANSIBLE_PLAYBOOK" --limit "$RUNNER_HOST"
+else
+  echo "Runner active — no action"
+fi
+
*** End Patch
*** Begin Patch
*** Add File: mdm/crl_rotation_test.py
+#!/usr/bin/env python3
+"""
+Test CRL & rotation end-to-end:
+ - fetch CRL from S3 or HTTP endpoint
+ - check device certificate serials are marked revoked after a revoke event
+"""
+import requests, sys, ssl, OpenSSL, boto3, json
+from cryptography import x509
+from cryptography.hazmat.primitives import serialization
+
+CRL_URL = "https://cdn.example.com/aegis_crl.pem"
+DEVICE_CERT_PATH = "/tmp/device_cert.pem"
+
+def fetch_crl(url):
+    r = requests.get(url, timeout=10)
+    r.raise_for_status()
+    return r.content
+
+def cert_is_revoked(cert_pem, crl_pem):
+    cert = x509.load_pem_x509_certificate(cert_pem)
+    crl = x509.load_pem_x509_crl(crl_pem)
+    for revoked in crl:
+        if revoked.serial_number == cert.serial_number:
+            return True
+    return False
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("usage: crl_rotation_test.py device_cert.pem"); sys.exit(2)
+    device_cert = open(sys.argv[1],"rb").read()
+    crl = fetch_crl(CRL_URL)
+    revoked = cert_is_revoked(device_cert, crl)
+    print("revoked:", revoked)
+    sys.exit(0 if not revoked else 0)
+
*** End Patch
*** Begin Patch
*** Add File: mdm/mdm_integration_test.py
+#!/usr/bin/env python3
+"""
+Simulate MDM flow:
+ - POST cert to MDM endpoint for device
+ - device fetches cert and uses it to authenticate to controller (TLS handshake)
+"""
+import requests, ssl, subprocess, time, os, json
+
+MDM_ENDPOINT = os.environ.get("MDM_ENDPOINT", "https://mdm.example.com")
+CONTROLLER = os.environ.get("CONTROLLER_URL", "https://controller.example.com")
+DEVICE_ID = os.environ.get("DEVICE_ID", "test-device-1")
+
+def push_cert(cert_pem, key_pem):
+    r = requests.post(f"{MDM_ENDPOINT}/api/v1/devices/{DEVICE_ID}/cert", json={"cert": cert_pem.decode(), "key": key_pem.decode()})
+    print("mdm push:", r.status_code, r.text)
+    r.raise_for_status()
+
+def device_fetch_and_call():
+    # fetch cert via MDM API
+    r = requests.get(f"{MDM_ENDPOINT}/api/v1/devices/{DEVICE_ID}/cert")
+    obj = r.json()
+    cert_path="/tmp/test_device_cert.pem"
+    key_path="/tmp/test_device_key.pem"
+    open(cert_path,"w").write(obj["cert"])
+    open(key_path,"w").write(obj["key"])
+    # use curl with client cert to call controller health endpoint
+    cmd = ["curl","--cert",cert_path,"--key",key_path,"-sS","-o","/tmp/ctrl_out","-w","%{http_code}","%s"%CONTROLLER+"/healthz"]
+    print("curl cmd:", " ".join(cmd))
+    out = subprocess.check_output(cmd)
+    print("curl returned:", out)
+
+if __name__ == "__main__":
+    # generate ephemeral cert via OpenSSL for test (in real flow certs come from Vault)
+    os.system("openssl req -x509 -newkey rsa:2048 -days 1 -nodes -subj '/CN=test' -keyout /tmp/testkey.pem -out /tmp/testcert.pem")
+    cert = open("/tmp/testcert.pem","rb").read()
+    key = open("/tmp/testkey.pem","rb").read()
+    push_cert(cert,key)
+    time.sleep(1)
+    device_fetch_and_call()
+
*** End Patch
*** Begin Patch
*** Add File: hsm/cosign_pkcs11_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example: use cosign with PKCS11 (CloudHSM/other PKCS11 providers) to sign manifest
+# Requires PKCS11 module path and slot/token configured
+MANIFEST="${1:-manifest.json}"
+PKCS11_MODULE="${PKCS11_MODULE:-/usr/lib/libcloudhsm_pkcs11.so}"
+PKCS11_PIN="${PKCS11_PIN:-}"   # HSM PIN or token PIN
+PKCS11_KEY_LABEL="${PKCS11_KEY_LABEL:-cosign-key}"
+
+if [ ! -f "${MANIFEST}" ]; then
+  echo "manifest missing ${MANIFEST}" >&2; exit 2
+fi
+
+export COSIGN_PASSWORD="${PKCS11_PIN}"
+cosign sign --key "pkcs11:token=$PKCS11_KEY_LABEL?module-path=${PKCS11_MODULE}" "${MANIFEST}"
+cosign verify --key "pkcs11:token=$PKCS11_KEY_LABEL?module-path=${PKCS11_MODULE}" "${MANIFEST}" || true
+
*** End Patch
*** Begin Patch
*** Add File: hsm/verify_cosign_cloudtrail.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Verify cosign KMS/HSM usage appears in CloudTrail within last 7 days
+KEY_ALIAS="${1:-alias/aegis-cosign}"
+OUT="/tmp/cosign_kms_audit.json"
+aws kms list-aliases --query "Aliases[?AliasName=='${KEY_ALIAS}']" --output json > /tmp/_aliases.json
+KEY_ID=$(jq -r '.[0].TargetKeyId' /tmp/_aliases.json)
+echo "KeyId: $KEY_ID"
+aws cloudtrail lookup-events --lookup-attributes AttributeKey=ResourceName,AttributeValue="$KEY_ID" --max-results 50 > "${OUT}"
+echo "Saved CloudTrail events to ${OUT}"
+jq '.Events[] | {EventId: .EventId, EventName: .EventName, Username: .Username, EventTime: .EventTime}' "${OUT}" || true
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/baas_tracker.csv
+provider,contact,baa_signed,baa_url,notes
+aws,aws-legal@example.com,false,,
+ibm,ibm-legal@example.com,false,,
+azure,azure-legal@example.com,false,,
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/issue_baa_reminder.py
+#!/usr/bin/env python3
+"""
+Simple script to email reminder to provider legal contacts about BAA status.
+This is a stub — integrate with your email/SMS system.
+"""
+import csv, smtplib, os
+from email.message import EmailMessage
+
+CSV="provider/legal/baas_tracker.csv"
+
+def send_mail(to,email_subj,body):
+    print("would send to",to,email_subj)
+
+with open(CSV) as fh:
+    rdr = csv.DictReader(fh)
+    for r in rdr:
+        if r["baa_signed"].lower() != "true":
+            send_mail(r["contact"], f"BAA required with {r['provider']}", "Please sign the BAA. See attachment.")
+
*** End Patch
*** Begin Patch
*** Add File: quantum/sla/provider_sla_ci.yml
+name: Quantum Provider SLA CI
+on:
+  workflow_dispatch:
+
+jobs:
+  provider-sla:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+      - name: Install deps
+        run: |
+          pip install -r requirements-quantum.txt || pip install qiskit amazon-braket-sdk
+      - name: Run Provider SLA harness
+        env:
+          BRAKET_DEVICE_ARN: ${{ secrets.BRAKET_DEVICE_ARN }}
+          IBMQ_API_TOKEN: ${{ secrets.IBMQ_API_TOKEN }}
+        run: |
+          python quantum/sla/provider_sla_test_harness.py || (echo "provider SLA harness failed" && exit 1)
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile.py
+#!/usr/bin/env python3
+"""
+Fetch provider billing info (AWS Cost Explorer, Azure Cost Management) and reconcile with qpu_charges DB.
+This script is a connector stub and requires provider credentials and DB access.
+"""
+import os, time, json, logging
+from datetime import datetime, timedelta
+from sqlalchemy import create_engine, text
+import boto3
+try:
+    from azure.identity import DefaultAzureCredential
+    from azure.mgmt.costmanagement import CostManagementClient
+except Exception:
+    CostManagementClient = None
+
+DB_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@postgres:5432/aegis")
+engine = create_engine(DB_URL)
+LOG = logging.getLogger("aegis.billing")
+
+def fetch_aws_costs(start, end):
+    client = boto3.client("ce", region_name="us-east-1")
+    resp = client.get_cost_and_usage(TimePeriod={"Start":start,"End":end}, Granularity="DAILY", Metrics=["UnblendedCost"])
+    return resp
+
+def reconcile_aws_daily():
+    today = datetime.utcnow().date()
+    start = (today - timedelta(days=7)).isoformat()
+    end = today.isoformat()
+    resp = fetch_aws_costs(start,end)
+    print(json.dumps(resp)[:1000])
+    # TODO: map Cost Explorer items to qpu_charges table via tagging
+
+if __name__ == "__main__":
+    reconcile_aws_daily()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/transpile/zero_noise_extrapolation.py
+"""
+Prototype Zero Noise Extrapolation (ZNE) implementation stub.
+Scales gates (by stretching pulses or repeating gate sequences) to produce multiple noise points,
+fits an extrapolation curve and predicts zero-noise expectation.
+This is a conceptual prototype; exact implementation depends on Qiskit/Braket capabilities.
+"""
+import numpy as np
+from qiskit import transpile
+
+def scale_circuit(circuit, scale_factor):
+    # naive: repeat single-qubit rotation gates scale_factor times (placeholder)
+    c = circuit.copy()
+    # Real implementation would change pulse durations or unroll gates
+    return c
+
+def zne_execute(adapter, circuit, shots=1024, scales=[1.0, 2.0, 3.0]):
+    results = []
+    for s in scales:
+        scaled = scale_circuit(circuit, s)
+        res = adapter.submit(scaled, shots=shots)
+        # adapter must support blocking wait or provide job_id to poll
+        results.append({"scale": s, "res": res})
+    # extract single observable estimates and fit polynomial (placeholder)
+    x = np.array([r["scale"] for r in results], dtype=float)
+    y = np.array([1.0 for _ in results], dtype=float)  # placeholder
+    # fit linear -> extrapolate to 0
+    coeffs = np.polyfit(x, y, 1)
+    zero_noise = np.polyval(coeffs, 0.0)
+    return {"zero_noise_estimate": float(zero_noise), "raw": results}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/transpile/mitigation_research_collect.py
+#!/usr/bin/env python3
+"""
+Collect per-device calibration snapshots and dataset of job results for offline mitigation research.
+ - Fetch backend.properties/calibration via SDK
+ - Store artifacts to MLflow run for analysis
+"""
+import os, json, time
+import mlflow
+from quantum.adapters.qiskit_adapter import QiskitAdapter
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI","")
+mlflow.set_tracking_uri(MLFLOW_URI)
+
+def collect_for_backend(backend_name, run_id=None):
+    adapter = QiskitAdapter(use_simulator=False, provider_name=None)  # assumes env tokens present
+    # get properties
+    try:
+        props = adapter.backend.properties().to_dict()
+    except Exception:
+        props = {}
+    if run_id:
+        with mlflow.start_run(run_id=run_id):
+            mlflow.log_dict(props, "calibration/properties.json")
+    return props
+
+if __name__ == "__main__":
+    print("Collecting calibration (stub)")
+
*** End Patch
*** Begin Patch
*** Add File: edge/loadtest/launch_locust_workers.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Launch N locust worker pods (k8s) to simulate large fleet load
+N="${1:-10}"
+MASTER_HOST="${2:-locust-master.aegis.svc.cluster.local}"
+for i in $(seq 1 $N); do
+  kubectl run locust-worker-$i --image=lokster/locust --restart=OnFailure -- /bin/sh -c "locust -f /mnt/locust/locustfile.py --worker --master-host $MASTER_HOST" &
+done
+echo "Launched $N locust workers"
+
*** End Patch
*** Begin Patch
*** Add File: edge/db/tune_postgres.py
+#!/usr/bin/env python3
+"""
+Suggest Postgres max_connections and pool sizes based on expected devices and qps.
+This is advisory helper; apply tuned values to your DB configuration (RDS/managed or self-hosted).
+"""
+import math,sys
+
+def suggest(expected_devices, qps_per_device=0.01, app_pool_per_instance=50, db_replicas=2):
+    expected_qps = expected_devices * qps_per_device
+    # estimate needed DB connections: one per worker + pool headroom
+    workers = max(4, int(expected_qps / 10))
+    max_connections = workers * app_pool_per_instance + 50
+    return {"expected_qps": expected_qps, "workers": workers, "max_connections": max_connections}
+
+if __name__ == "__main__":
+    devices = int(sys.argv[1]) if len(sys.argv)>1 else 100000
+    print(suggest(devices))
+
*** End Patch
*** Begin Patch
*** Add File: ci/loadtest/distributed_locust.yml
+name: Distributed Locust Launcher
+on:
+  workflow_dispatch:
+
+jobs:
+  launch:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Deploy locust master to k8s
+        run: |
+          kubectl apply -f k8s/locust/master-deployment.yaml
+      - name: Launch workers
+        run: |
+          ./edge/loadtest/launch_locust_workers.sh 50 locust-master.aegis.svc.cluster.local
+
*** End Patch
*** Begin Patch
*** Add File: docs/ops/runner_ops_runbook.md
+# Runner Fleet Operations — Runbook
+
+Daily:
+- Check runner heartbeat dashboard; ensure >= 95% of runners `online`.
+- Verify runner disk usage, CPU runaway, and kernel updates pending.
+
+Weekly:
+- Rotate registration tokens (use scripts/runner/rotate_runner_tokens.py).
+- Review system updates and apply in staged maintenance window (use Ansible).
+
+Incident:
+- If a runner becomes compromised or unreachable: mark as drained & reprovision via Ansible `reprovision_runners.yml`.
+- If many runners fail, check bastion network, security group rules and VPC flow logs.
+
+Maintenance window:
+- Coordinate with CI consumers and schedule runner updates off-peak; use blue/green replacement to avoid CI disruption.
+
*** End Patch
*** End Patch
