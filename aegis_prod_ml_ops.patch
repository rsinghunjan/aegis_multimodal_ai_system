*** Begin Patch
*** Add File: monitoring/model_metrics_exporter_v2.py
+#!/usr/bin/env python3
+"""
+Prometheus-compatible model metrics exporter with push endpoint.
+Exposes:
+ - aegis_model_request_count_total (Counter, by model)
+ - aegis_model_request_latency_seconds (Histogram, by model)
+ - aegis_model_prediction_positive_rate (Gauge, by model)
+ - aegis_model_auc (Gauge, by model)
+ - aegis_model_brier_score (Gauge, by model)
+ - aegis_model_cohort_auc (Gauge, by model, cohort)
+
+Push endpoint accepts JSON payloads to set/update metrics:
+  { "model":"m1", "latency":0.12, "predictions":[...], "auc":0.92, "brier":0.02, "cohort_auc":{"age<30":0.90} }
+"""
+from prometheus_client import start_http_server, Counter, Histogram, Gauge, CollectorRegistry, generate_latest
+from flask import Flask, request, Response, jsonify
+import time, os, threading
+
+app = Flask(__name__)
+REGISTRY = CollectorRegistry()
+
+REQUEST_COUNT = Counter("aegis_model_request_count_total", "Total model requests", ["model"], registry=REGISTRY)
+REQUEST_LATENCY = Histogram("aegis_model_request_latency_seconds", "Inference latency seconds", ["model"], registry=REGISTRY)
+PRED_POS_RATE = Gauge("aegis_model_prediction_positive_rate", "Fraction positive predictions", ["model"], registry=REGISTRY)
+MODEL_AUC = Gauge("aegis_model_auc", "Model AUC", ["model"], registry=REGISTRY)
+MODEL_BRIER = Gauge("aegis_model_brier_score", "Model Brier score (calibration)", ["model"], registry=REGISTRY)
+COHORT_AUC = Gauge("aegis_model_cohort_auc", "Cohort AUC", ["model", "cohort"], registry=REGISTRY)
+
+@app.route("/metrics")
+def metrics():
+    data = generate_latest(REGISTRY)
+    return Response(data, mimetype="text/plain; version=0.0.4")
+
+@app.route("/push", methods=["POST"])
+def push():
+    payload = request.get_json() or {}
+    model = payload.get("model", "unknown")
+    # count & latency
+    if "count" in payload:
+        REQUEST_COUNT.labels(model=model).inc(payload.get("count",1))
+    if "latency" in payload:
+        REQUEST_LATENCY.labels(model=model).observe(float(payload["latency"]))
+    if "predictions" in payload:
+        preds = payload["predictions"]
+        if preds:
+            pos = sum(1 for p in preds if float(p) > 0.5)/len(preds)
+            PRED_POS_RATE.labels(model=model).set(pos)
+    if "auc" in payload:
+        MODEL_AUC.labels(model=model).set(float(payload["auc"]))
+    if "brier" in payload:
+        MODEL_BRIER.labels(model=model).set(float(payload["brier"]))
+    if "cohort_auc" in payload and isinstance(payload["cohort_auc"], dict):
+        for cohort, val in payload["cohort_auc"].items():
+            COHORT_AUC.labels(model=model, cohort=str(cohort)).set(float(val))
+    return jsonify({"ok": True})
+
+@app.route("/health")
+def health():
+    return jsonify({"ok": True})
+
+def run():
+    # Start Prometheus exposition on port 8000 and Flask on 8080 (Flask used for push)
+    start_http_server(8000, registry=REGISTRY)
+    app.run(host="0.0.0.0", port=int(os.environ.get("PUSH_PORT", "8080")))
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus_model_alerts_rules.yml
+groups:
+- name: aegis-model-alerts
+  rules:
+  - alert: AegisModelAucDrop
+    expr: (avg_over_time(aegis_model_auc[1h]) < (avg_over_time(aegis_model_auc[6h]) * 0.9))
+    for: 15m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Model AUC regression detected for {{ $labels.model }}"
+      description: "1h average AUC is >10% below 6h baseline for model {{ $labels.model }}."
+
+  - alert: AegisModelCohortAucDrop
+    expr: (avg_over_time(aegis_model_cohort_auc[1h]) < (avg_over_time(aegis_model_cohort_auc[6h]) * 0.9))
+    for: 15m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Cohort AUC drop detected (model={{ $labels.model }}, cohort={{ $labels.cohort }})"
+      description: "1h cohort AUC dropped >10% vs 6h baseline."
+
+  - alert: AegisModelCalibrationDegraded
+    expr: aegis_model_brier_score > 0.1
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Model calibration (Brier) exceeded threshold for {{ $labels.model }}"
+      description: "Model Brier score > 0.1."
+
+  - alert: AegisModelHighLatencyP95
+    expr: histogram_quantile(0.95, sum(rate(aegis_model_request_latency_seconds_bucket[5m])) by (le, model)) > 1.0
+    for: 5m
+    labels:
+      severity: page
+    annotations:
+      summary: "High Model P95 latency for {{ $labels.model }}"
+      description: "95th percentile latency exceeds 1s."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/grafana_model_dashboard_v2.json
+{
+  "title": "Aegis Model Ops (Enhanced)",
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Request Rate (per model)",
+      "targets": [
+        { "expr": "sum(rate(aegis_model_request_count_total[5m])) by (model)", "legendFormat": "{{model}}" }
+      ],
+      "id": 1
+    },
+    {
+      "type": "graph",
+      "title": "Latency P95",
+      "targets": [
+        { "expr": "histogram_quantile(0.95, sum(rate(aegis_model_request_latency_seconds_bucket[5m])) by (le, model))", "legendFormat": "{{model}}" }
+      ],
+      "id": 2
+    },
+    {
+      "type": "graph",
+      "title": "AUC (recent)",
+      "targets": [
+        { "expr": "aegis_model_auc", "legendFormat": "{{model}}" }
+      ],
+      "id": 3
+    },
+    {
+      "type": "graph",
+      "title": "Cohort AUC (last value)",
+      "targets": [
+        { "expr": "aegis_model_cohort_auc", "legendFormat": "{{model}} - {{cohort}}" }
+      ],
+      "id": 4
+    },
+    {
+      "type": "graph",
+      "title": "Brier Score (calibration)",
+      "targets": [
+        { "expr": "aegis_model_brier_score", "legendFormat": "{{model}}" }
+      ],
+      "id": 5
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/dcgm-exporter-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: dcgm-exporter
+  template:
+    metadata:
+      labels:
+        app: dcgm-exporter
+    spec:
+      containers:
+      - name: dcgm-exporter
+        image: nvidia/dcgm-exporter:2.2.9-2.5.0-ubuntu20.04
+        ports:
+        - containerPort: 9400
+        resources:
+          requests:
+            cpu: 100m
+            memory: 128Mi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  ports:
+  - port: 9400
+  selector:
+    app: dcgm-exporter
+
+# Note: Prometheus should scrape dcgm-exporter to collect GPU metrics.
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus-adapter-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: custom-metrics-config
+  namespace: monitoring
+data:
+  config.yaml: |
+    rules:
+      - seriesQuery: 'power{device="gpu"}'
+        resources:
+          overrides:
+            namespace:
+              resource: namespace
+        name:
+          matches: "^(.*)$"
+          as: "nvidia_gpu_utilization"
+        metricsQuery: 'avg(rate(power[5m])) by (namespace, pod)'
+
+# This is an example stub. In production use prometheus-adapter with correct seriesQuery and metricsQuery
+# mapped to the DCGM metrics to expose them via the Custom Metrics API for HPA/metrics-based autoscaling.
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/retrain_with_validation_v2.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-retrain-v2-
+  namespace: aegis
+spec:
+  entrypoint: retrain-validate-package
+  templates:
+  - name: retrain-validate-package
+    steps:
+      - - name: check-drift
+          template: drift-check
+      - - name: retrain
+          template: retrain
+          when: "{{steps.check-drift.outputs.parameters.drift}} == true"
+      - - name: validate
+          template: validate
+          when: "{{steps.retrain.outputs.parameters.retrained}} == true"
+      - - name: package_and_attest
+          template: package
+          when: "{{steps.validate.outputs.parameters.passed}} == true"
+      - - name: request-approval
+          template: request-approval
+          when: "{{steps.package_and_attest.outputs.parameters.attested}} == true"
+
+  - name: drift-check
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          set -e
+          aws s3 cp s3://$METRICS_BUCKET/reference.csv /tmp/reference.csv
+          aws s3 cp s3://$METRICS_BUCKET/production_recent.csv /tmp/production.csv
+          python3 scripts/drift_check.py /tmp/reference.csv /tmp/production.csv || echo "drift=false"
+          if [ $? -eq 2 ]; then echo "true" > /tmp/out; else echo "false" > /tmp/out; fi
+    outputs:
+      parameters:
+        - name: drift
+          valueFrom:
+            path: /tmp/out
+
+  - name: retrain
+    container:
+      image: ghcr.io/yourorg/aegis-train:latest
+      command: [sh, -c]
+      args:
+        - |
+          set -e
+          ./train_and_package.sh
+          echo "true" > /tmp/retrained
+    outputs:
+      parameters:
+      - name: retrained
+        valueFrom:
+          path: /tmp/retrained
+
+  - name: validate
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          set -e
+          # Run extended validation producing validation.json and pushing metrics to exporter
+          python3 scripts/validate_model_metrics_extended.py --pred predictions.csv --labels labels.csv --cohort-cols age,region --min_auc 0.85 --out validation.json
+          python3 - <<'PY'
+import json,sys
+with open('validation.json') as fh:
+  r=json.load(fh)
+print(r)
+sys.exit(0 if r.get('passed') else 1)
+PY
+    outputs:
+      parameters:
+      - name: passed
+        valueFrom:
+          path: /tmp/validation_passed
+
+  - name: package
+    container:
+      image: ghcr.io/yourorg/aegis-tools:latest
+      command: [sh, -c]
+      args:
+        - |
+          ./scripts/package_and_attest_rekor_canonical.sh model_dir ./artifact.tar.gz
+          echo "true" > /tmp/attested
+    outputs:
+      parameters:
+        - name: attested
+          valueFrom:
+            path: /tmp/attested
+
+  - name: request-approval
+    container:
+      image: curlimages/curl:7.85.0
+      command: [sh, -c]
+      args:
+        - |
+          curl -sS -X POST -H "Content-Type: application/json" -H "X-API-KEY: ${GOV_API_KEY}" -d '{"artifact_key":"model-archives/artifact.tar.gz","approver":"auto","notes":"retrained"}' ${GOV_URL}/approve || true
+
+# Notes: provide METRICS_BUCKET, GOV_API_KEY, GOV_URL as env or cluster secrets.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/validate_model_metrics_extended.py
+#!/usr/bin/env python3
+"""
+Extended validation script:
+ - computes AUC, accuracy, Brier score
+ - computes cohort AUCs for given cohort columns
+ - computes fairness deltas (max cohort AUC difference)
+ - writes validation.json and a pass/fail flag file
+ - pushes metrics to metrics exporter if METRICS_PUSH_URL is set
+"""
+import argparse, json, os
+import pandas as pd
+from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss
+import requests
+
+def cohort_aucs(df_pred, df_label, cohort_cols):
+    merged = df_pred.copy()
+    merged["label"] = df_label.iloc[:,0].values
+    cohorts = {}
+    for col in cohort_cols:
+        if col in merged.columns:
+            groups = merged.groupby(col)
+            for gname, gdf in groups:
+                if len(gdf) < 10: continue
+                try:
+                    auc = float(roc_auc_score(gdf["score"], gdf["label"]))
+                except Exception:
+                    auc = None
+                cohorts[f"{col}={gname}"] = auc
+    return cohorts
+
+def push_metrics(url, payload):
+    try:
+        r = requests.post(url, json=payload, timeout=5)
+        r.raise_for_status()
+    except Exception as e:
+        print("Failed to push metrics:", e)
+
+def main():
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--pred", required=True)
+    ap.add_argument("--labels", required=True)
+    ap.add_argument("--cohort-cols", default="", help="comma-separated cohort columns")
+    ap.add_argument("--min_auc", type=float, default=0.8)
+    ap.add_argument("--out", default="validation.json")
+    args = ap.parse_args()
+
+    pred = pd.read_csv(args.pred)
+    labels = pd.read_csv(args.labels)
+    y_score = pred["score"] if "score" in pred.columns else pred.iloc[:,0]
+    y_true = labels.iloc[:,0]
+
+    results = {}
+    try:
+        results["auc"] = float(roc_auc_score(y_true, y_score))
+    except Exception:
+        results["auc"] = None
+    pred_label = (y_score > 0.5).astype(int)
+    results["accuracy"] = float(accuracy_score(y_true, pred_label))
+    try:
+        results["brier"] = float(brier_score_loss(y_true, y_score))
+    except Exception:
+        results["brier"] = None
+
+    cohort_cols = [c for c in args.cohort_cols.split(",") if c]
+    results["cohort_auc"] = cohort_aucs(pred, labels, cohort_cols) if cohort_cols else {}
+
+    # fairness delta: max difference between cohort AUCs and global AUC
+    aucs = [v for v in results["cohort_auc"].values() if v is not None]
+    if aucs and results["auc"] is not None:
+        max_delta = max(abs(results["auc"] - v) for v in aucs)
+    else:
+        max_delta = 0.0
+    results["fairness_max_cohort_auc_delta"] = float(max_delta)
+
+    # pass criteria
+    passed = True
+    if results["auc"] is None or results["auc"] < args.min_auc:
+        passed = False
+    # calibration threshold
+    if results.get("brier") is not None and results["brier"] > 0.15:
+        passed = False
+    # cohort fairness threshold (example): if max delta > 0.1 fail
+    if results["fairness_max_cohort_auc_delta"] > 0.1:
+        passed = False
+
+    results["passed"] = passed
+    with open(args.out, "w") as fh:
+        json.dump(results, fh, indent=2)
+    # write passing flag path used by Argo template
+    with open("/tmp/validation_passed", "w") as fh:
+        fh.write("true" if passed else "false")
+
+    # push metrics to metrics exporter if configured
+    push_url = os.environ.get("METRICS_PUSH_URL")
+    if push_url:
+        payload = {
+            "model": os.environ.get("MODEL_NAME","aegis-model"),
+            "auc": results.get("auc"),
+            "brier": results.get("brier"),
+            "cohort_auc": results.get("cohort_auc")
+        }
+        push_metrics(push_url, payload)
+    print("Validation results:", results)
+    return 0 if passed else 1
+
+if __name__ == "__main__":
+    exit(main())
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/redis_statefulset.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  serviceName: "feast-redis"
+  replicas: 3
+  selector:
+    matchLabels:
+      app: feast-redis
+  template:
+    metadata:
+      labels:
+        app: feast-redis
+    spec:
+      containers:
+      - name: redis
+        image: bitnami/redis:6.2
+        ports:
+        - containerPort: 6379
+        resources:
+          requests:
+            memory: "256Mi"
+            cpu: "200m"
+        volumeMounts:
+        - name: data
+          mountPath: /data
+  volumeClaimTemplates:
+  - metadata:
+      name: data
+    spec:
+      accessModes: ["ReadWriteOnce"]
+      resources:
+        requests:
+          storage: 5Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  ports:
+  - port: 6379
+  clusterIP: None
+  selector:
+    app: feast-redis
+
+# Note: For production, use managed Redis or a clustered operator (e.g., bitnami/redis helm).
+
*** End Patch
*** Begin Patch
*** Add File: labeling/annotator_service.py
+#!/usr/bin/env python3
+"""
+Lightweight annotator service that serves unlabeled items from Redis and accepts annotations.
+API:
+ - GET /next -> returns next item JSON
+ - POST /submit -> { "id": ..., "label": ... } -> stores in S3 or Redis done list
+"""
+from flask import Flask, request, jsonify
+import os, json, redis, boto3, datetime
+
+app = Flask(__name__)
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+QUEUE = os.environ.get("LABEL_QUEUE", "aegis-label-queue")
+BUCKET = os.environ.get("LABEL_BUCKET", "")
+
+r = redis.from_url(REDIS_URL)
+s3 = boto3.client("s3") if BUCKET else None
+
+@app.route("/next", methods=["GET"])
+def next_item():
+    item = r.lpop(QUEUE)
+    if not item:
+        return jsonify({"ok": False, "msg": "no-items"}), 204
+    return jsonify({"ok": True, "item": json.loads(item)})
+
+@app.route("/submit", methods=["POST"])
+def submit():
+    payload = request.get_json() or {}
+    item_id = payload.get("id")
+    label = payload.get("label")
+    if not item_id:
+        return jsonify({"error":"id required"}), 400
+    rec = {"id": item_id, "label": label, "ts": datetime.datetime.utcnow().isoformat(), "meta": payload.get("meta",{})}
+    if s3:
+        key = f"labels/{item_id}.json"
+        s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(rec))
+        return jsonify({"ok": True, "s3_key": key})
+    else:
+        # fallback: store in Redis list "labels_done"
+        r.rpush("labels_done", json.dumps(rec))
+        return jsonify({"ok": True})
+
+@app.route("/health")
+def health():
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8070")))
+
*** End Patch
*** Begin Patch
*** Add File: explainability/shap_dashboard_enhanced.py
+#!/usr/bin/env python3
+"""
+Serve SHAP explanations and fairness summaries with lightweight endpoints and a simple UI page.
+ - GET /explanations?run_id=...
+ - GET /fairness-summary
+ - GET /ui -> simple HTML page to inspect runs
+"""
+from flask import Flask, request, jsonify, render_template_string
+import os, json
+
+app = Flask(__name__)
+DATA_DIR = os.environ.get("SHAP_DIR", "/data/shap")
+
+@app.route("/explanations")
+def explanations():
+    run_id = request.args.get("run_id")
+    if not run_id:
+        return jsonify({"error":"run_id required"}), 400
+    path = os.path.join(DATA_DIR, f"{run_id}_shap.json")
+    if not os.path.exists(path):
+        return jsonify({"error":"not found"}), 404
+    return jsonify(json.load(open(path)))
+
+@app.route("/fairness-summary")
+def fairness():
+    path = os.path.join(DATA_DIR, "fairness_summary.json")
+    if not os.path.exists(path):
+        return jsonify({"error":"no fairness data"}), 404
+    return jsonify(json.load(open(path)))
+
+@app.route("/ui")
+def ui():
+    template = """
+    <html><head><title>SHAP Dashboard</title></head><body>
+    <h1>SHAP runs</h1>
+    <div id="content">Use /explanations?run_id=... and /fairness-summary</div>
+    </body></html>
+    """
+    return render_template_string(template)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8082")))
+
*** End Patch
*** Begin Patch
*** Add File: scripts/promote_if_validated.sh
+#!/usr/bin/env bash
+#
+# Helper to call governance API to request promotion after validation and attestation.
+# Usage:
+#   ./scripts/promote_if_validated.sh s3://bucket/model-archives/artifact.tar.gz
+set -euo pipefail
+
+ARTIFACT_KEY="${1:-}"
+
+if [ -z "$ARTIFACT_KEY" ]; then
+  echo "Usage: $0 <artifact_s3_key>"
+  exit 2
+fi
+
+if [ -z "${GOV_URL:-}" ] || [ -z "${GOV_API_KEY:-}" ]; then
+  echo "GOV_URL and GOV_API_KEY must be set"
+  exit 3
+fi
+
+echo "Requesting governance approval for $ARTIFACT_KEY"
+curl -sS -X POST -H "Content-Type: application/json" -H "X-API-KEY: ${GOV_API_KEY}" \
+  -d "{\"artifact_key\":\"${ARTIFACT_KEY}\",\"approver\":\"auto\",\"notes\":\"Validated & attested\"}" \
+  "${GOV_URL}/approve" | jq .
+
*** End Patch
*** Begin Patch
*** Add File: docs/ML_PROD_MONITORING_AND_SERVING_README.md
+ML Production: Monitoring, Serving, Retrain & Explainability (summary)
+
+This patch adds production-grade scaffolding for:
+- Model monitoring: enhanced metrics exporter with AUC, calibration (Brier), cohort AUCs, Prometheus alert rules, and Grafana dashboard.
+- Serving hardening: KServe production InferenceService hints, Istio Gateway + VirtualService for low-latency routing, DCGM exporter and example prometheus-adapter config to expose GPU metrics for autoscaling.
+- Closed-loop retraining: Argo retrain workflow (retrain_with_validation_v2.yaml) that runs drift detection, retrains, validates (AUC/calibration/cohort fairness), packages & attests only on passing validation, and posts an approval request to governance.
+- Feature store & online serving: Redis StatefulSet for Feast online store (HA hint) and materialize helper (see existing feature_store/feast_materialize.sh).
+- Explainability & labeling: SHAP dashboard and annotator service to scale labeling workflows and collect labels to S3/queue.
+
+Quickstart (high-level)
+1. Deploy monitoring components:
+   - kubectl apply -f monitoring/dcgm-exporter-deployment.yaml
+   - Add monitoring/prometheus_model_alerts_rules.yml to PrometheusRule or your Prometheus config.
+   - Deploy metrics exporter as a Deployment (package monitoring/model_metrics_exporter_v2.py into a container).
+2. Deploy serving stack:
+   - kubectl apply -f k8s/serving/kserve_production.yaml (adjust image + MODEL_BUCKET)
+   - Apply Istio gateway: k8s/ingress/istio_gateway.yaml (or your ingress)
+   - Ensure prometheus-adapter is configured to map DCGM metrics to custom metrics for HPA usage.
+3. Retrain & validation:
+   - Configure METRICS_BUCKET, GOV_API_KEY, GOV_URL as cluster secrets for Argo.
+   - Submit argo/workflows/retrain_with_validation_v2.yaml or schedule it.
+4. Feature store:
+   - kubectl apply -f feature_store/redis_statefulset.yaml (namespace feast)
+   - Configure Feast repo to use this online store and run materialize when ready.
+5. Explainability & labeling:
+   - Deploy labeling/annotator_service.py as a container and point labeling UI to it.
+   - Run explainability/shap_dashboard_enhanced.py to serve SHAP outputs.
+
+Notes & next steps
+- Containerize the Python apps (metrics exporter, annotator, shap dashboard) and add Deployments/Service manifests.
+- Wire RBAC and ServiceAccounts with minimal permissions for S3/Redis access.
+- Tune KServe concurrency, batch sizes, and Istio timeout/retry settings to satisfy P95 SLOs discovered via load testing.
+- Integrate prometheus-adapter and configure HPA to autoscale on CPU/GPU metrics exposed by DCGM + custom adapter.
+
*** End Patch
*** End Patch   
