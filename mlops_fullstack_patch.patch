*** Begin Patch
*** Add File: mlops/README.md
+Aegis MLOps — Full-Stack Quickstart
+==================================
+
+What this patch provides
+- MLflow tracking & model registry manifest (k8s)
+- Argo Workflow training pipeline with dataset snapshot, train, validate, register steps
+- KServe InferenceService sample + canary promotion helper
+- GitHub Actions pipeline to build trainer image, trigger training (Argo) and handle promotion
+- DVC skeleton for dataset versioning
+- Drift detection and model validation scripts
+- Vault + OIDC usage examples (CI & runtime)
+- Operational runbooks for bootstrapping and signoff
+
+High-level flow (recommended)
+1. Deploy MLflow and configure artifact storage to your CMK-encrypted bucket.
+2. Deploy Argo Workflows and KServe to the cluster (if not present).
+3. Build trainer image via CI (workflow included) and push to your registry.
+4. Run training via Argo pipeline (CI or manually). Pipeline snapshots data (DVC/LakeFS), trains, validates, registers model in MLflow.
+5. Promote model via GitHub Action which applies a KServe InferenceService manifest with canary weights and requires environment approvals.
+
+See docs/INSTALL.md for step-by-step deployment.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/INSTALL.md
+INSTALLATION & BOOTSTRAP
+========================
+
+Prerequisites
+- Kubernetes cluster with access to:
+  - Argo Workflows (optional but recommended)
+  - KServe (for serving)
+  - Vault (or HashiCorp Vault as a service)
+  - Access to object storage (S3/GCS/Azure/OCI) with CMK configured
+  - GitHub Actions runner access and repo secrets configured
+
+Secrets to set (GitHub / k8s secrets)
+- GitHub repo secrets:
+  - VAULT_ADDR, VAULT_GITHUB_ROLE
+  - DOCKER_REGISTRY, DOCKER_USER, DOCKER_PASSWORD
+  - MLFLOW_ARTIFACT_BUCKET, MLFLOW_S3_ENDPOINT (if S3-compatible), MLFLOW_S3_REGION
+  - K8S_NAMESPACE (staging/prod)
+  - K8S_KUBECONFIG (if you plan to run kubectl from CI)
+
+- Kubernetes secrets:
+  - aegis-mlflow-db (SQLALCHEMY_DATABASE_URI)
+  - aegis-cloud-creds (optional fallback credentials)
+
+Quick bootstrap
+1. Deploy MLflow:
+   kubectl apply -f mlops/k8s/mlflow-deployment.yaml -n <namespace>
+   Create k8s secret with SQLALCHEMY_DATABASE_URI pointing to your managed DB.
+
+2. Install Argo Workflows and KServe (follow upstream docs).
+
+3. Configure Vault OIDC trust for GitHub Actions and set VAULT_GITHUB_ROLE in your repo secrets.
+
+4. Trigger .github/workflows/mlops_full_pipeline.yml via Actions UI to build trainer image.
+
+5. Submit Argo workflow:
+   kubectl apply -f mlops/argo/train_pipeline.yaml -n <namespace>
+
+6. After runs complete, use the promotion workflow (GitHub Actions) to deploy to KServe (canary).
+
+Notes
+- Replace sqlite example in MLflow manifest with a managed DB in production.
+- Use OIDC from GitHub Actions to Vault to provide short-lived credentials to the CI jobs and avoid static secrets.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/k8s/mlflow-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: mlflow
+  labels:
+    app: mlflow
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: mlflow
+  template:
+    metadata:
+      labels:
+        app: mlflow
+    spec:
+      containers:
+      - name: mlflow
+        image: python:3.10-slim
+        env:
+        - name: SQLALCHEMY_DATABASE_URI
+          valueFrom:
+            secretKeyRef:
+              name: aegis-mlflow-db
+              key: SQLALCHEMY_DATABASE_URI
+        - name: MLFLOW_S3_ENDPOINT_URL
+          valueFrom:
+            secretKeyRef:
+              name: aegis-cloud-creds
+              key: MLFLOW_S3_ENDPOINT
+        - name: AWS_REGION
+          valueFrom:
+            secretKeyRef:
+              name: aegis-cloud-creds
+              key: AWS_REGION
+        - name: MLFLOW_ARTIFACT_BUCKET
+          valueFrom:
+            secretKeyRef:
+              name: aegis-cloud-creds
+              key: MLFLOW_ARTIFACT_BUCKET
+        ports:
+        - containerPort: 5000
+        command: ["sh", "-c"]
+        args:
+          - |
+            pip install --no-cache-dir "mlflow[extras]" boto3 psycopg2-binary waitress && \
+            mlflow server \
+              --backend-store-uri ${SQLALCHEMY_DATABASE_URI} \
+              --default-artifact-root "s3://${MLFLOW_ARTIFACT_BUCKET}/mlflow-artifacts" \
+              --host 0.0.0.0 --port 5000
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: mlflow
+spec:
+  selector:
+    app: mlflow
+  ports:
+    - port: 5000
+      targetPort: 5000
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File: mlops/argo/train_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-ml-train-
+spec:
+  entrypoint: train-and-validate
+  templates:
+  - name: train-and-validate
+    steps:
+    - - name: snapshot-data
+        template: snapshot-data
+    - - name: train
+        template: train
+    - - name: validate
+        template: validate
+    - - name: register
+        template: register
+
+  - name: snapshot-data
+    container:
+      image: ghcr.io/your-org/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          # Example: use DVC or LakeFS to snapshot dataset; here we simulate with a script
+          python3 mlops/scripts/snapshot_data.py --out /tmp/dataset_snapshot || true
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: train
+    container:
+      image: ghcr.io/your-org/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          python3 scripts/train_and_register.py --mode train-only
+      env:
+      - name: MLFLOW_TRACKING_URI
+        value: "http://mlflow:5000"
+      resources:
+        limits:
+          cpu: "4"
+          memory: "8Gi"
+          nvidia.com/gpu: 1
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: validate
+    container:
+      image: ghcr.io/your-org/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          python3 mlops/scripts/validate_model.py --run-id ${RUN_ID:-} || (echo "Validation failed" && exit 1)
+      env:
+      - name: MLFLOW_TRACKING_URI
+        value: "http://mlflow:5000"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: register
+    container:
+      image: ghcr.io/your-org/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          python3 scripts/train_and_register.py --mode register --run-id ${RUN_ID:-}
+      env:
+      - name: MLFLOW_TRACKING_URI
+        value: "http://mlflow:5000"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  volumes:
+  - name: work
+    emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: mlops/kserve/inference_canary_template.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-ml-canary
+spec:
+  predictor:
+    serviceAccountName: aegis-model-sa
+    tensorflow:
+      storageUri: "s3://{{MODEL_BUCKET}}/{{MODEL_PATH}}"
+      resources:
+        limits:
+          cpu: "1"
+          memory: "2Gi"
+  explainer:
+    alibi:
+      runtimeVersion: "0.7.0"
+      resources:
+        limits:
+          cpu: "0.5"
+          memory: "1Gi"
+
+# To deploy a canary, replace {{MODEL_BUCKET}} and {{MODEL_PATH}} and apply manifest,
+# then use traffic-splitting annotations or InferenceService canary strategy (if installed).
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/train_and_register.py
+#!/usr/bin/env python3
+"""
+Minimal trainer & MLflow integration. (Improved version of earlier script)
+ - Trains a simple model
+ - Logs to MLflow (uses MLFLOW_TRACKING_URI env var)
+ - Exits with run id printed to stdout
+"""
+import os
+import mlflow
+import mlflow.sklearn
+from sklearn.datasets import load_iris
+from sklearn.model_selection import train_test_split
+from sklearn.ensemble import RandomForestClassifier
+from sklearn.metrics import accuracy_score
+import joblib
+import argparse
+
+def train_and_log():
+    mlflow.set_experiment("aegis-example")
+    X, y = load_iris(return_X_y=True)
+    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
+    with mlflow.start_run() as run:
+        clf = RandomForestClassifier(n_estimators=50, random_state=42)
+        clf.fit(X_train, y_train)
+        preds = clf.predict(X_test)
+        acc = accuracy_score(y_test, preds)
+        mlflow.log_metric("accuracy", acc)
+        mlflow.sklearn.log_model(clf, "model")
+        run_id = run.info.run_id
+        print(run_id)
+        joblib.dump(clf, "/tmp/aegis-model.joblib")
+    return run_id
+
+def register_model(run_id, name="aegis-model"):
+    client = mlflow.tracking.MlflowClient()
+    model_uri = f"runs:/{run_id}/model"
+    mv = client.create_model_version(name, model_uri, run_id)
+    print("registered:", mv.version)
+    return mv.version
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--mode", choices=["train-only", "register"], default="train-only")
+    p.add_argument("--run-id", default=None)
+    args = p.parse_args()
+    mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
+    if args.mode == "train-only":
+        run_id = train_and_log()
+        print("Train complete:", run_id)
+    else:
+        if not args.run_id:
+            raise SystemExit("Provide --run-id to register")
+        register_model(args.run_id)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/validate_model.py
+#!/usr/bin/env python3
+"""
+validate_model.py
+ - Loads a model from MLflow given a run id or the latest registered model
+ - Runs validation tests and emits non-zero exit code on failure
+"""
+import os
+import sys
+import mlflow
+import mlflow.sklearn
+from sklearn.datasets import load_iris
+from sklearn.metrics import accuracy_score
+import argparse
+
+def validate_run(run_id=None, min_accuracy=0.8):
+    client = mlflow.tracking.MlflowClient()
+    if run_id:
+        model_uri = f"runs:/{run_id}/model"
+    else:
+        # fallback: latest registered model
+        regs = client.get_latest_versions("aegis-model")
+        if not regs:
+            print("No registered model found")
+            return 2
+        model_uri = f"models:/aegis-model/{regs[-1].version}"
+
+    model = mlflow.sklearn.load_model(model_uri)
+    X, y = load_iris(return_X_y=True)
+    preds = model.predict(X)
+    acc = accuracy_score(y, preds)
+    print("Validation accuracy:", acc)
+    if acc < min_accuracy:
+        print("Validation failed: accuracy below threshold")
+        return 3
+    return 0
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--run-id", default=None)
+    p.add_argument("--min-accuracy", type=float, default=0.8)
+    args = p.parse_args()
+    rc = validate_run(args.run_id, args.min_accuracy)
+    sys.exit(rc)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/drift_detector.py
+#!/usr/bin/env python3
+"""
+Simple drift detector: compare recent inference sample distribution vs training distribution.
+This is a skeleton — integrate with your metrics pipeline in production.
+"""
+import json
+import numpy as np
+from sklearn.datasets import load_iris
+
+def baseline_stats():
+    X, y = load_iris(return_X_y=True)
+    return {"mean": list(X.mean(axis=0)), "std": list(X.std(axis=0))}
+
+def check_drift(sample_stats, baseline):
+    diffs = np.abs(np.array(sample_stats["mean"]) - np.array(baseline["mean"]))
+    return diffs.tolist()
+
+def main():
+    baseline = baseline_stats()
+    # In production, fetch sample statistics from Prometheus or aggregate logs
+    sample = {"mean": baseline["mean"], "std": baseline["std"]}
+    diffs = check_drift(sample, baseline)
+    print("Feature mean diffs:", diffs)
+    # If diffs exceed thresholds, alert to SIEM/Slack
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: mlops/dvc/snapshot_data.py
+#!/usr/bin/env python3
+"""
+Snapshot data example (placeholder). Replace with DVC/LakeFS calls in production.
+"""
+import argparse
+import os
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--out", default="/tmp/dataset_snapshot")
+    args = p.parse_args()
+    os.makedirs(args.out, exist_ok=True)
+    with open(os.path.join(args.out, "manifest.txt"), "w") as f:
+        f.write("snapshot: example\n")
+    print("Snapshot written to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/mlops_full_pipeline.yml
+name: Aegis MLOps Full Pipeline
+
+on:
+  workflow_dispatch:
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  build-trainer:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: 3.10
+      - name: Build trainer image
+        run: |
+          IMAGE=ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:${{ github.sha }}
+          docker build -t $IMAGE -f mlops/docker/Dockerfile .
+          echo $IMAGE > image.txt
+      - name: Login to GHCR
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Push image
+        run: |
+          IMAGE=$(cat image.txt)
+          docker push $IMAGE
+          echo "IMAGE=$IMAGE" >> $GITHUB_OUTPUT
+
+  run-training:
+    needs: build-trainer
+    runs-on: ubuntu-latest
+    environment: staging
+    steps:
+      - uses: actions/checkout@v4
+      - name: Authenticate to Vault (OIDC)
+        uses: hashicorp/vault-action@v2
+        with:
+          url: ${{ secrets.VAULT_ADDR }}
+          method: github_oidc
+          role: ${{ secrets.VAULT_GITHUB_ROLE }}
+      - name: Trigger Argo training workflow (kubectl)
+        env:
+          KUBECONFIG: ${{ secrets.K8S_KUBECONFIG }}
+        run: |
+          IMAGE=$(cat image.txt)
+          # Replace trainer image in Argo workflow manifest and submit
+          sed "s|ghcr.io/your-org/aegis-ml-trainer:latest|$IMAGE|g" mlops/argo/train_pipeline.yaml > /tmp/train_pipeline.yaml
+          kubectl apply -f /tmp/train_pipeline.yaml -n ${{ secrets.K8S_NAMESPACE }}
+          # Optionally watch workflow with Argo CLI if available
+
+  post-train-validate:
+    needs: run-training
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Wait & fetch Argo artifacts (manual or implement Argo wait)
+        run: |
+          echo "In CI, you may query Argo API to wait for completion and download artifacts. This workflow leaves that step for the operator."
+      - name: Notify
+        run: |
+          echo "Training submitted. Inspect Argo UI and MLflow server for run artifacts and registered models."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/docker/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY mlops/requirements.txt /app/requirements.txt
+RUN pip install --no-cache-dir -r requirements.txt
+COPY mlops /app
+ENTRYPOINT ["python3"]
+
*** End Patch
*** Begin Patch
*** Add File: mlops/requirements.txt
+mlflow==2.4.1
+scikit-learn==1.2.2
+boto3==1.26.0
+joblib==1.2.0
+requests==2.31.0
+
*** End Patch
*** Begin Patch
*** Add File: mlops/docs/runbook_mlflow_and_argo.md
+# MLflow + Argo Runbook (Aegis MLOps)
+
+1) MLflow
+- Provide a managed Postgres/Cloud SQL and create k8s secret `aegis-mlflow-db` containing SQLALCHEMY_DATABASE_URI.
+- Ensure the artifact bucket exists and is CMK-encrypted. Create k8s secret `aegis-cloud-creds` with MLFLOW_ARTIFACT_BUCKET and MLFLOW_S3_ENDPOINT.
+- Deploy mlflow: kubectl apply -f mlops/k8s/mlflow-deployment.yaml -n <ns>
+
+2) Argo
+- Install Argo Workflows and ensure the cluster has a GPU node pool if you run GPU training.
+- Apply the Argo workflow manifest: kubectl apply -f mlops/argo/train_pipeline.yaml -n <ns>
+- Replace trainer image placeholder with the built image (CI workflow does this).
+
+3) KServe
+- Install KServe to enable model serving and canary deployments.
+- Use mlops/kserve/inference_canary_template.yaml as the base for canary manifests.
+
+4) CI/Secrets
+- Configure GitHub Actions secrets (VAULT_ADDR, VAULT_GITHUB_ROLE, K8S_KUBECONFIG, K8S_NAMESPACE, DOCKER creds).
+- Use GitHub OIDC -> Vault to fetch short-lived credentials in CI.
+
+5) Validation & Signoff
+- After a successful Argo run and MLflow registration, run model validation script:
+  kubectl run --rm -i --restart=Never validate --image=ghcr.io/<org>/aegis-ml-trainer:<tag> -- python3 mlops/scripts/validate_model.py
+- Bundle artifacts: use scripts/bundle_sign_artifacts.sh (from earlier patches) to create a signed artifact tarball for SRE/Sec.
+
*** End Patch
*** Begin Patch
*** Add File: mlops/docs/mlops_next_steps.md
+# Next steps & recommended additions
+
+- Replace sqlite in dev with a production DB (managed Postgres / Cloud SQL / Azure Database).
+- Add automated Argo wait & artifact download step in CI: query Argo API to fetch run status and artifacts.
+- Implement LakeFS for dataset versioning and configure DVC pipelines to snapshot datasets into artifact storage.
+- Add Prometheus exporters for inference latency and custom metrics; wire drift_detector to alert when thresholds exceeded.
+- Add automated canary traffic shifting (KServe + Istio/Knative) and integration tests.
+- Harden images: pin base images, produce SBOMs, run Trivy in CI (we added SBOM workflow earlier).
+
*** End Patch
*** End Patch
