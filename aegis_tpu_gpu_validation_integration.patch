*** Begin Patch
*** Add File: tpu/train_on_tpu.py
+#!/usr/bin/env python3
+"""
+Minimal example training script that uses TPUStrategy when a TPU is available,
+and falls back to CPU/GPU otherwise.
+
+Usage:
+  python train_on_tpu.py --epochs 2 --model-out /output/saved_model
+
+Notes:
+ - For Cloud TPU VMs set the environment so that TPUClusterResolver can find the TPU
+   (e.g., in TPU VM, no extra args are needed). For GKE node-pools with TPU nodes,
+   ensure node has access to TPU and runtime supports TPU (cloud-specific).
+ - This script is intentionally simple to validate TPU runtime wiring.
+"""
+import argparse, os, sys
+import tensorflow as tf
+
+def get_strategy():
+    try:
+        resolver = None
+        # Try automatic detection for TPU (works on TPU VMs / Cloud TPU)
+        try:
+            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
+            print("TPU detected:", resolver.cluster_spec())
+        except Exception:
+            resolver = None
+        if resolver:
+            tf.config.experimental_connect_to_cluster(resolver)
+            tf.tpu.experimental.initialize_tpu_system(resolver)
+            return tf.distribute.TPUStrategy(resolver)
+    except Exception as e:
+        print("TPU initialization failed:", e)
+    # Fallbacks: multi-GPU MirroredStrategy or default strategy
+    if tf.config.list_physical_devices("GPU"):
+        print("Using MirroredStrategy (GPU)")
+        return tf.distribute.MirroredStrategy()
+    print("Using default strategy (CPU)")
+    return tf.distribute.get_strategy()
+
+def build_model():
+    from tensorflow import keras
+    model = keras.Sequential([
+        keras.layers.Input(shape=(28,28,1)),
+        keras.layers.Conv2D(16,3,activation='relu'),
+        keras.layers.Flatten(),
+        keras.layers.Dense(10, activation='softmax')
+    ])
+    return model
+
+def main():
+    ap = argparse.ArgumentParser()
+    ap.add_argument("--epochs", type=int, default=1)
+    ap.add_argument("--model-out", type=str, default="/output/saved_model")
+    args = ap.parse_args()
+
+    strategy = get_strategy()
+    print("Using strategy:", type(strategy).__name__)
+    with strategy.scope():
+        model = build_model()
+        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
+
+    # Tiny dataset for smoke run
+    import numpy as np
+    x = np.random.rand(256,28,28,1).astype("float32")
+    y = (np.random.rand(256) * 10).astype("int32")
+
+    model.fit(x, y, epochs=args.epochs, batch_size=32)
+    os.makedirs(args.model_out, exist_ok=True)
+    model.save(args.model_out, include_optimizer=False)
+    print("Saved model to", args.model_out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: tpu/Dockerfile.tpu_trainer
+FROM python:3.10-slim
+WORKDIR /app
+COPY tpu/train_on_tpu.py /app/train_on_tpu.py
+RUN pip install --no-cache-dir "tensorflow>=2.11" cloud-tpu-client
+# Note: adjust tensorflow version to match TPU runtime; for TPU v4 you may need a specific TF build.
+ENTRYPOINT ["python", "/app/train_on_tpu.py"]
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/tpu_training_nodepool.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-tpu-train-
+  namespace: aegis
+spec:
+  entrypoint: tpu-train
+  templates:
+  - name: tpu-train
+    container:
+      image: ghcr.io/yourorg/tpu-trainer:latest
+      command: [sh, -c]
+      args:
+        - |
+          echo "Starting TPU-aware training job"
+          python train_on_tpu.py --epochs 1 --model-out /tmp/saved_model
+          tar -czf /tmp/artifact.tar.gz -C /tmp saved_model
+          aws s3 cp /tmp/artifact.tar.gz s3://$OBJECT_STORE_BUCKET/model-archives/tpu-artifact-$(date +%s).tar.gz
+      resources:
+        limits:
+          cpu: "4"
+          memory: "16Gi"
+      volumeMounts:
+        - name: aws-creds
+          mountPath: /root/.aws
+    nodeSelector:
+      aegis.cloud/tpu: "true"         # Example node label for TPU node pool; adapt per infra
+    tolerations:
+      - key: "aegis.cloud/tpu"
+        operator: "Equal"
+        value: "true"
+        effect: "NoSchedule"
+    volumes:
+      - name: aws-creds
+        secret:
+          secretName: aws-credentials
+
+  # Notes:
+  # - This workflow assumes a GKE/cluster node pool labeled for TPU capacity (aegis.cloud/tpu=true).
+  # - Alternatively, run TPU jobs via cloud provider-specific commands (TPU VM) outside K8s.
+
*** End Patch
*** Begin Patch
*** Add File: tpu/README_TPU_INTEGRATION.md
+````markdown
+name=docs/TPU_INTEGRATION.md
+## TPU Integration Guide (Aegis)
+
+This document explains two integration patterns for using TPUs with Aegis:
+
+1) TPU node-pool in Kubernetes (nodeSelector/tolerations)
+   - Provision a node pool in your cloud/cluster that has TPU access and label it (example label: `aegis.cloud/tpu=true`).
+   - Ensure the TPU runtime and drivers required by your trainer are present on nodes (cloud-specific).
+   - Use the provided Argo template `argo/workflows/tpu_training_nodepool.yaml` that uses nodeSelector/tolerations to schedule training to TPU nodes.
+   - Build the trainer image `ghcr.io/yourorg/tpu-trainer:latest` using `tpu/Dockerfile.tpu_trainer` then push to registry.
+
+2) TPU VMs (recommended for GCP Cloud TPU)
+   - Provision TPU VMs via GCP `gcloud` (or your cloud provider). Submit training jobs to TPU VMs directly or use Argo to run a small controller that provisions a TPU VM, runs the job and tears down the TPU VM.
+   - On TPU VM, use the `tf.distribute.TPUStrategy` and Cloud TPU runtime. The trainer script `tpu/train_on_tpu.py` auto-detects TPU and will use TPUStrategy if available.
+
+Trainer image
+- `tpu/Dockerfile.tpu_trainer` builds a lightweight trainer with TF and `cloud-tpu-client`. Adjust TF version to match TPU runtime.
+
+Node pool creation example (GKE)
+```
+gcloud container node-pools create tpu-pool \
+  --cluster=my-gke-cluster \
+  --machine-type=n1-standard-16 \
+  --num-nodes=1 \
+  --node-labels=aegis.cloud/tpu=true \
+  --enable-autoscaling --min-nodes=0 --max-nodes=2
+```
+
+Security & IAM
+- Trainer needs permission to write artifacts to your object store and to call any governance APIs. Use short-lived credentials where possible.
+
+Acceptance criteria
+- Trainer runs on TPU node (or TPU VM) and completes a smoke training run.
+- Artifact is uploaded to object store and packaged/attested by existing packaging workflow.
+- TPU utilization logs/metrics (cloud-specific) show activity.
+
+Troubleshooting
+- If TPUStrategy fails to initialize, confirm TPU IP/VM access, TF runtime compatibility and that node has TPU support.
+- For GKE TPU node pools, ensure cloud provider configuration and any required device plugins are installed.
+
+````

*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus_adapter_dcgm_mapping.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: prometheus-adapter-dcgm-mapping
+  namespace: monitoring
+data:
+  config.yaml: |
+    rules:
+      # Map DCGM GPU utilization metrics to a custom metric for HPA
+      - seriesQuery: 'dcgm_*gpu*'
+        resources:
+          overrides:
+            namespace:
+              resource: namespace
+        name:
+          as: "nvidia_gpu_utilization"
+        metricsQuery: 'avg(dcgm_gpu_utilization) by (namespace, pod)'
+
+    # IMPORTANT:
+    # - The exact seriesQuery and metricsQuery need to be adapted to the DCGM exporter
+    #   metric names observed in your Prometheus instance. Use the Prometheus UI:
+    #     http://<PROM_URL>/api/v1/series?match[]=dcgm*
+    #   to list available series and update this mapping accordingly.
+
*** End Patch
*** Begin Patch
*** Add File: tools/gpu_validation/e2e_gpu_validation.sh
+#!/usr/bin/env bash
+#
+# End-to-end GPU validation script:
+#  - Verifies NVIDIA device plugin on nodes
+#  - Verifies DCGM exporter is running and Prometheus has GPU metrics
+#  - Submits a GPU stress pod/job and checks that HPA scales (via custom metric)
+#  - Produces a JSON report in /tmp/gpu_validation_report.json
+
+set -euo pipefail
+PROM_URL="${PROM_URL:-http://prometheus.monitoring.svc.cluster.local:9090}"
+NAMESPACE="${NAMESPACE:-aegis}"
+HPA_NAME="${HPA_NAME:-aegis-model-gpu-hpa}"
+REPORT="/tmp/gpu_validation_report.json"
+
+report() {
+  python3 - <<PY
+import json,sys
+r={'ok': $1, 'details': $2}
+print(json.dumps(r, indent=2))
+with open("$REPORT","w") as fh:
+    json.dump(r, fh, indent=2)
+PY
+}
+
+echo "Checking NVIDIA device plugin daemonset"
+if kubectl get daemonset nvidia-device-plugin-daemonset -n kube-system >/dev/null 2>&1; then
+  dp_ok=true
+else
+  dp_ok=false
+fi
+
+echo "Querying Prometheus for DCGM metrics"
+if curl -sS "${PROM_URL%/}/api/v1/series?match[]=dcgm_*" | jq -e '.data | length > 0' >/dev/null 2>&1; then
+  prom_metrics_ok=true
+else
+  prom_metrics_ok=false
+fi
+
+echo "Submitting GPU stress pod (short-lived)"
+kubectl -n "$NAMESPACE" run gpu-stress-validate --image=ghcr.io/yourorg/gpu-stress:latest --restart=Never --limits='nvidia.com/gpu=1' --command -- /app/gpu_stress.py &>/tmp/gpu_stress_run.log || true
+sleep 5
+
+echo "Polling HPA for scaling events (30 iterations)"
+scaled=false
+for i in $(seq 1 30); do
+  echo "Iteration $i"
+  HPA_JSON=$(kubectl get hpa "$HPA_NAME" -n "$NAMESPACE" -o json 2>/dev/null || echo "{}")
+  replicas=$(echo "$HPA_JSON" | jq -r '.status.currentReplicas // 0')
+  desired=$(echo "$HPA_JSON" | jq -r '.status.desiredReplicas // 0')
+  echo "currentReplicas=$replicas desiredReplicas=$desired"
+  if [ "$replicas" -gt "1" ] || [ "$desired" -gt "1" ]; then
+    scaled=true
+    break
+  fi
+  sleep 5
+done
+
+kubectl delete pod/gpu-stress-validate -n "$NAMESPACE" --ignore-not-found || true
+
+OK=$( [ "$dp_ok" = true ] && [ "$prom_metrics_ok" = true ] && [ "$scaled" = true ] && echo 1 || echo 0 )
+DETAILS={"dp_ok":$dp_ok,"prom_metrics_ok":$prom_metrics_ok,"scaled":$scaled}
+report "$OK" "$DETAILS"
+echo "Wrote report to $REPORT"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/gpu_validation.yml
+name: GPU E2E Validation
+
+on:
+  workflow_dispatch:
+
+jobs:
+  gpu-validate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install kubectl & jq
+        run: |
+          sudo apt-get update && sudo apt-get install -y jq
+          curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
+          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
+      - name: Run e2e GPU validation
+        env:
+          PROM_URL: ${{ secrets.PROM_URL }}
+          NAMESPACE: aegis
+        run: |
+          chmod +x tools/gpu_validation/e2e_gpu_validation.sh
+          ./tools/gpu_validation/e2e_gpu_validation.sh
+      - name: Upload report
+        uses: actions/upload-artifact@v4
+        with:
+          name: gpu-validation-report
+          path: /tmp/gpu_validation_report.json
+
*** End Patch
*** Begin Patch
*** Add File: docs/GPU_DEEP_VALIDATION.md
+````markdown
+name=docs/GPU_DEEP_VALIDATION.md
+# GPU Deep Validation (Aegis)
+
+This document describes how to run the end-to-end GPU validation included in the repo.
+
+What the validation does
+- Verifies NVIDIA device plugin daemonset present
+- Verifies DCGM exporter metrics are present in Prometheus
+- Runs a GPU stress workload in the aegis namespace (image: ghcr.io/yourorg/gpu-stress:latest)
+- Polls HPA (`aegis-model-gpu-hpa`) for scale events
+- Produces a JSON report at `/tmp/gpu_validation_report.json` and uploads via GitHub Action
+
+How to run locally
+1. Ensure kubeconfig points to your test cluster and you have `kubectl` access.
+2. Ensure Prometheus is accessible from the cluster (PROM_URL env var).
+3. Ensure gpu-stress image built and available in registry.
+4. Run:
+   PROM_URL=http://prometheus.monitoring.svc.cluster.local:9090 NAMESPACE=aegis ./tools/gpu_validation/e2e_gpu_validation.sh
+
+Interpreting results
+- `dp_ok`: whether nvidia-device-plugin daemonset was discovered
+- `prom_metrics_ok`: whether Prometheus returned DCGM-like series
+- `scaled`: whether HPA observed increased replicas
+
+Accept criteria
+- All three flags true. If not, inspect Prometheus scrape config, DCGM exporter pods and prometheus-adapter mapping.
+
+Notes
+- The GPU stress image uses `pynvml` to read device stats and print utilization; replace with a real GPU benchmark for stronger load tests.
+- Update `monitoring/prometheus_adapter_dcgm_mapping.yaml` to match the exact DCGM metric names seen in your Prometheus instance.
+````

*** End Patch
*** End Patch
