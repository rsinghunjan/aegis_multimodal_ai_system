*** Begin Patch
*** Add File:scripts/operator/bootstrap_all.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Operator "one-shot" bootstrap script to bring the core Aegis platform components up in the
+# correct order and perform basic verification. Intended to be run from a secure operator bastion
+# with kubectl, helm, terraform, aws, vault, and argo CLIs available.
+#
+# WARNING: This script touches secrets and will attempt to run initialization steps that must be
+# executed by an authorized operator. Do not run in an environment where credentials should not be present.
+#
+# Required environment variables (examples):
+#   EVIDENCE_BUCKET=my-evidence-bucket
+#   COSIGN_KMS_KEY_ARN=arn:aws:kms:us-west-2:123456789012:key/abcd-...
+#   VAULT_KMS_ARN=arn:aws:kms:us-west-2:123456789012:key/efgh-...
+#   AWS_REGION=us-west-2
+#   MONGO_URI=mongodb://user:pass@mongo.aegis.svc:27017/aegis
+#   REKOR_URL=https://rekor.example.com
+#   MODEL_ARTIFACT_BUCKET=my-model-bucket
+#   GITHUB_REPO=owner/repo
+#   ARGO_NAMESPACE=aegis
+# Optional:
+#   PROVISION_GPU={gke|eks|skip}
+
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${COSIGN_KMS_KEY_ARN:?COSIGN_KMS_KEY_ARN required}"
+: "${VAULT_KMS_ARN:?VAULT_KMS_ARN required}"
+: "${AWS_REGION:?AWS_REGION required}"
+: "${MONGO_URI:?MONGO_URI required}"
+: "${REKOR_URL:?REKOR_URL required}"
+: "${MODEL_ARTIFACT_BUCKET:?MODEL_ARTIFACT_BUCKET required}"
+
+PROVISION_GPU="${PROVISION_GPU:-skip}"
+ARGO_NAMESPACE="${ARGO_NAMESPACE:-aegis}"
+NAMESPACE_VAULT="${NAMESPACE_VAULT:-vault}"
+
+echo "Starting Aegis bootstrap. This may take many minutes. CTRL-C to abort."
+
+echo "1) Apply required k8s namespaces, RBAC and NetworkPolicy"
+kubectl apply -f ops/k8s/namespaces_and_rbac.yaml
+kubectl apply -f ops/k8s/networkpolicy-core.yaml
+
+echo "2) Apply required secrets template (fill values using sealed-secrets or sops in production)"
+# Convert template to real secret if the operator has replaced placeholders; here we warn if the file still has REPLACE
+if grep -q "REPLACE" secrets/required-secrets.yaml.template 2>/dev/null; then
+  echo "WARN: secrets/required-secrets.yaml.template contains REPLACE placeholders. Please create sealed secret or apply a filled secrets file first."
+else
+  kubectl -n aegis apply -f secrets/required-secrets.yaml.template || true
+fi
+
+if [ "${PROVISION_GPU}" != "skip" ]; then
+  echo "3) Provision GPU node pool via Terraform (${PROVISION_GPU})"
+  ./scripts/provision_gpu_nodepool.sh "${PROVISION_GPU}"
+  echo "Wait for GPU nodes to be Ready..."
+  kubectl wait --for=condition=Ready nodes --all --timeout=300s || true
+fi
+
+echo "4) Install Vault via Helm into namespace ${NAMESPACE_VAULT}"
+./deploy/vault/helm_install_and_bootstrap.sh
+
+echo "5) Run Vault bootstrap & rotation helper from secure bastion (requires AWS creds)"
+VAULT_ADDR=${VAULT_ADDR:-https://vault.${NAMESPACE_VAULT}.svc:8200}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET} KMS_ARN=${VAULT_KMS_ARN} AWS_REGION=${AWS_REGION} ./deploy/vault/auto_init_rotate_advanced.sh || true
+
+echo "6) Deploy production services (Velero, Seldon, Triton PVC, Milvus statefulset)"
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET} AWS_REGION=${AWS_REGION} ./scripts/deploy_prod_services.sh
+
+echo "7) Provision Rekor RDS (Terraform) and run failover/restore test"
+./terraform/rekor_rds/deploy.sh || true
+sleep 10
+deploy/rekor/test_failover_and_restore.sh || true
+
+echo "8) Integrate token-budget into LangGraph runtime (patch deployment)"
+./scripts/integrate_token_budget.sh || true
+
+echo "9) Run full E2E smoke (train -> convert -> deploy -> infer -> evidence -> promote)"
+if [ -f scripts/run_full_e2e.sh ]; then
+  EVIDENCE_BUCKET=${EVIDENCE_BUCKET} COSIGN_KMS_KEY_ARN=${COSIGN_KMS_KEY_ARN} MANIFEST_S3=${MANIFEST_S3:-} ./scripts/run_full_e2e.sh || true
+else
+  echo "No scripts/run_full_e2e.sh found; please run smoke test scripts/smoke/full_promotion_smoke.sh manually"
+fi
+
+echo "10) Run pilot harness for quantum/edge/hpc (optional)"
+python3 test_harness/automated_test_harness.py || true
+
+echo "Bootstrap complete. Verify each component (Vault, Rekor, Triton, Seldon, Milvus, Velero) and consult docs/OPERATOR_CHECKLIST_FOR_E2E.md for detailed checks."
+
*** End Patch
*** Begin Patch
*** Add File:triton/hardened_convert_and_load_test.py
+#!/usr/bin/env python3
+"""
+Hardened Triton conversion helper + simple HTTP load testing plan generator.
+
+What it does:
+- Converts a PyTorch / Hugging Face seq2seq or encoder-decoder model to ONNX with dynamic axes.
+- Produces a Triton model repository layout (model_name/1/model.onnx + config.pbtxt).
+- Emits a small load test script (triton/load_test_client.py) and a suggested locust/grpc plan.
+
+Usage:
+  python3 triton/hardened_convert_and_load_test.py --model-dir /path/to/hf-or-pytorch --model-name rag_reader --out-repo /tmp/triton_repo
+
+Notes:
+- This script is a best-effort converter. Complex models (large decoder-only LLMs) may need custom export logic.
+- Always validate outputs via unit tests and run load tests in staging before production.
+"""
+import argparse, os, subprocess, json, shutil
+from pathlib import Path
+
+def export_onnx(model_dir, out_onnx, sample_text="example"):
+    try:
+        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
+        import torch
+    except Exception as e:
+        raise SystemExit("Install transformers and torch in your env: pip install transformers torch")
+    tokenizer = AutoTokenizer.from_pretrained(model_dir)
+    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
+    model.eval().cpu()
+    sample = tokenizer(sample_text, return_tensors="pt")
+    input_ids = sample["input_ids"]
+    # For seq2seq, export the encode+decode may be complex; we export a simple forward that accepts input_ids
+    torch.onnx.export(
+        model,
+        (input_ids,),
+        out_onnx,
+        opset_version=13,
+        input_names=["input_ids"],
+        output_names=["logits"],
+        dynamic_axes={"input_ids": {0: "batch_size", 1: "sequence"}, "logits": {0: "batch_size", 1: "sequence"}},
+    )
+
+def write_triton_config(out_repo, model_name, max_batch=8):
+    cfg = f"""
+name: "{model_name}"
+platform: "onnxruntime_onnx"
+max_batch_size: {max_batch}
+input [
+  {{
+    name: "input_ids"
+    data_type: TYPE_INT32
+    dims: [-1]
+  }}
+]
+output [
+  {{
+    name: "logits"
+    data_type: TYPE_FP32
+    dims: [-1]
+  }}
+]
+"""
+    with open(os.path.join(out_repo, model_name, "config.pbtxt"), "w") as f:
+        f.write(cfg)
+
+def prepare_model_repo(model_dir, model_name, out_repo):
+    os.makedirs(os.path.join(out_repo, model_name, "1"), exist_ok=True)
+    onnx_path = os.path.join(out_repo, model_name, "1", "model.onnx")
+    print("Exporting ONNX to", onnx_path)
+    export_onnx(model_dir, onnx_path)
+    write_triton_config(out_repo, model_name)
+    print("Model repo prepared at", out_repo)
+
+def create_load_test_client(out_path):
+    client = """#!/usr/bin/env python3
+import requests, json, time, sys
+TRITON_URL = "http://triton.aegis.svc:8000/v2/models/{model}/infer"
+def run_once(model, batch_size=1):
+    # Build a dummy request with INT32 input set to small integers
+    import numpy as np
+    data = [list(range(8)) for _ in range(batch_size)]
+    payload = { "inputs":[ {"name":"input_ids","shape":[batch_size,8],"datatype":"INT32","data": data} ] }
+    url = TRITON_URL.format(model=model)
+    r = requests.post(url, json=payload, timeout=10)
+    print("status", r.status_code, "resp", r.text[:200])
+
+if __name__ == '__main__':
+    m = sys.argv[1]
+    run_once(m, int(sys.argv[2]) if len(sys.argv)>2 else 1)
+"""
+    with open(out_path, "w") as f:
+        f.write(client)
+    os.chmod(out_path, 0o755)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-dir", required=True)
+    p.add_argument("--model-name", required=True)
+    p.add_argument("--out-repo", required=True)
+    p.add_argument("--max-batch", type=int, default=8)
+    args = p.parse_args()
+    prepare_model_repo(args.model_dir, args.model_name, args.out_repo)
+    load_client_path = os.path.join(args.out_repo, "load_test_client.py")
+    create_load_test_client(load_client_path)
+    print("Load test client written to", load_client_path)
+    print("Suggested quick test (from a pod that can reach Triton):")
+    print(f"  python3 {load_client_path} {args.model_name} 1")
+    print("For load testing, run multiple parallel processes or use locust/hey to hit the Triton infer endpoint.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/vault_hardening_playbook.md
+# Vault Hardening Playbook (operator-ready)
+
+This playbook provides exact Helm values, recommended commands, and safe handling guidance to bootstrap and harden Vault for production use with KMS auto-unseal.
+
+Important: run these steps from a secure operator bastion with limited network access and audited CLI usage. Do NOT store root tokens in Git.
+
+1) Example Helm values (deploy/vault/values-prod-secrets.yaml.template)
+
+Fill the following placeholders before applying:
+  - REPLACE_WITH_KMS_KEY_ARN -> COSIGN_KMS_KEY_ARN for cosign? No, use VAULT_KMS_ARN specifically for Vault unseal key.
+  - REPLACE_WITH_AWS_REGION
+
+server:
+  ha:
+    enabled: true
+    replicas: 3
+  dataStorage:
+    enabled: true
+    size: 50Gi
+unsealer:
+  type: awskms
+  awsKmsKeyId: "REPLACE_WITH_VAULT_KMS_KEY_ARN"
+  awsRegion: "REPLACE_WITH_AWS_REGION"
+audit:
+  enabled: true
+  file:
+    enabled: true
+    path: /vault/logs/audit.log
+rbac:
+  create: true
+injector:
+  enabled: true
+
+2) Helm install & namespace
+  kubectl create ns vault || true
+  helm repo add hashicorp https://helm.releases.hashicorp.com || true
+  helm repo update
+  helm upgrade --install vault hashicorp/vault -n vault -f deploy/vault/values-prod-secrets.yaml.template
+
+3) Ensure PVC for audit logs exists
+  kubectl apply -f deploy/vault/rbac-provision.yaml
+
+4) Bootstrap (manual safe steps)
+  - From a secure bastion with AWS credentials run:
+    export VAULT_ADDR=https://vault.vault.svc:8200
+    export EVIDENCE_BUCKET=<your-evidence-bucket>
+    export KMS_ARN=<VAULT_KMS_KEY_ARN>
+    export AWS_REGION=<region>
+    ./deploy/vault/auto_init_rotate_advanced.sh
+
+  The script will:
+    - initialize Vault (if not initialized)
+    - upload the init bundle to S3 (object-locked bucket recommended)
+    - create an automation token (policy-based) and store it in Secrets Manager
+    - enable kubernetes auth, PKI & transit secrets engines and enable file audit device
+
+5) Secure root token lifecycle (recommended practice)
+  - After initialization, retrieve root token from Secrets Manager (offline steps):
+    aws secretsmanager get-secret-value --secret-id aegis/vault/root-token --region ${AWS_REGION}
+  - Record root token in an offline sealed safe or HSM-backed secret manager.
+  - Create a short-lived token for day-to-day operator automation and revoke root token rotation.
+  - Use Vault's built-in root token rotation recommendations and periodic re-key / re-init practices.
+
+6) Unseal & auto-unseal verification
+  - Vault should auto-unseal via KMS after pod restart. Test by restarting a pod:
+    kubectl -n vault delete pod -l app.kubernetes.io/name=vault
+    kubectl -n vault get pods -w
+  - Check health:
+    curl --insecure https://vault.vault.svc:8200/v1/sys/health
+
+7) Audit log upload verification
+  - Confirm logs are written to PVC path /vault/logs/audit.log in the pod and that a periodic uploader (or sidecar) copies to S3.
+  - Example (check S3):
+    aws s3 ls s3://${EVIDENCE_BUCKET}/vault-audit/
+
+8) Rotation & periodic drills
+  - Ensure deploy/vault/rotate_automation_cronjob.yaml is applied to rotate automation tokens weekly (operator review required).
+  - Schedule a root key rotation drill quarterly. Archive signed rotation proofs to EVIDENCE_BUCKET with cosign signatures.
+
+9) RBAC & Network
+  - Restrict access to Vault UI and API via NetworkPolicy and Kubernetes RBAC (core team only).
+  - Audit who can create policies and roles regularly.
+
+10) Emergency procedures
+  - Keep multiple copies of init bundle in separate, geographically distributed object-locked stores (one offline).
+  - Document and exercise the unseal ceremony for recovering Vault in case of total failure.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/automated_pilot_and_report.yml
+name: Automated Pilots & Reporting
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: "0 2 * * 1" # weekly run (adjust as needed)
+
+jobs:
+  run-pilots:
+    runs-on: ubuntu-latest
+    env:
+      EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
+      JIRA_URL: ${{ secrets.JIRA_URL }}
+      JIRA_USER: ${{ secrets.JIRA_USER }}
+      JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}
+      JIRA_PROJECT_KEY: ${{ secrets.JIRA_PROJECT_KEY }}
+      REKOR_URL: ${{ secrets.REKOR_URL }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+
+      - name: Install dependencies
+        run: pip install boto3 requests pymongo
+
+      - name: Run automated test harness
+        run: |
+          python3 test_harness/automated_test_harness.py
+        continue-on-error: true
+
+      - name: Collect result and post reports
+        if: ${{ always() }}
+        run: |
+          python3 .github/workflows/post_results.py
+        env:
+          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
+          JIRA_URL: ${{ secrets.JIRA_URL }}
+          JIRA_USER: ${{ secrets.JIRA_USER }}
+          JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}
+          JIRA_PROJECT_KEY: ${{ secrets.JIRA_PROJECT_KEY }}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/post_results.py
+#!/usr/bin/env python3
+"""
+Collect a quick summary after automated_test_harness.py run and post to Slack/Jira.
+This script expects the harness to have written a summary file at /tmp/aegis_pilot_summary.json (optional).
+If no summary found, it will post a generic success/failure note based on previous step status.
+"""
+import os, json, requests, glob
+
+SLACK = os.environ.get("SLACK_WEBHOOK_URL")
+JIRA_URL = os.environ.get("JIRA_URL")
+JIRA_USER = os.environ.get("JIRA_USER")
+JIRA_TOKEN = os.environ.get("JIRA_API_TOKEN")
+JIRA_PROJECT = os.environ.get("JIRA_PROJECT_KEY")
+
+def post_slack(msg):
+    if not SLACK:
+        print("No Slack webhook configured")
+        return
+    requests.post(SLACK, json={"text": msg})
+
+def create_jira(summary, description):
+    if not (JIRA_URL and JIRA_USER and JIRA_TOKEN and JIRA_PROJECT):
+        print("Jira not configured")
+        return None
+    url = f"{JIRA_URL}/rest/api/2/issue"
+    auth = (JIRA_USER, JIRA_TOKEN)
+    payload = {"fields":{"project":{"key":JIRA_PROJECT},"summary":summary,"description":description,"issuetype":{"name":"Task"}}}
+    r = requests.post(url, auth=auth, json=payload)
+    if r.status_code in (200,201):
+        return r.json().get("key")
+    else:
+        print("Jira create failed", r.status_code, r.text)
+    return None
+
+# Attempt to find harness output logs for hinting success vs failure
+matches = glob.glob("**/test_harness_output.log", recursive=True)
+summary = None
+if os.path.exists("/tmp/aegis_pilot_summary.json"):
+    summary = json.load(open("/tmp/aegis_pilot_summary.json"))
+else:
+    # Fallback: search for harness logs
+    for m in matches:
+        text = open(m).read()
+        if "FAIL" in text or "ERROR" in text:
+            summary = {"status":"failure","details": text[-2000:]}
+            break
+    if not summary:
+        summary = {"status":"success","details":"Automated pilot run completed (no summary found)."}
+
+msg = f"Aegis automated pilot result: {summary['status']}\nDetails:\n{summary.get('details','(no details)')}"
+post_slack(msg)
+if summary['status'] != "success":
+    jira_key = create_jira("Aegis automated pilot failures", summary.get("details",""))
+    print("Created Jira:", jira_key)
+else:
+    print("No Jira created (success).")
+
*** End Patch
*** End Patch
