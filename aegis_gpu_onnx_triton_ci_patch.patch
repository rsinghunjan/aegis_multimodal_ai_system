*** Begin Patch
*** Add File:scripts/provision_and_label_gpu.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Provision GPU node pool (calls existing terraform scripts) and label nodes for Argo/Triton scheduling.
+# Usage:
+#   ./scripts/provision_and_label_gpu.sh gke
+#   ./scripts/provision_and_label_gpu.sh eks
+# Or skip provisioning and just label existing GPU nodes:
+#   ./scripts/provision_and_label_gpu.sh skip
+#
+PROVIDER=${1:-skip}
+TF_EXIT=0
+
+if [ "${PROVIDER}" = "gke" ] || [ "${PROVIDER}" = "eks" ]; then
+  echo "Provisioning GPU node pool via Terraform for ${PROVIDER}..."
+  ./scripts/provision_gpu_nodepool.sh "${PROVIDER}"
+fi
+
+echo "Detecting nodes with accelerators..."
+# Heuristic: detect nodes that report 'nvidia' or 'gpu' in allocatable/resources
+nodes=$(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"|"}{.status.capacity}{"\n"}{end}' 2>/dev/null || true)
+if [ -z "${nodes}" ]; then
+  echo "No nodes found. Exiting."
+  exit 1
+fi
+
+for node in $(kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'); do
+  gpu_count=$(kubectl get node "${node}" -o json | jq -r '.status.capacity | .["nvidia.com/gpu"] // .["gpu"] // "0"')
+  if [ -n "${gpu_count}" ] && [ "${gpu_count}" != "0" ]; then
+    echo "Labeling node ${node} as workload-type=gpu"
+    kubectl label node "${node}" workload-type=gpu --overwrite || true
+    echo "Tainting node ${node} to prefer GPU workloads (NoSchedule)"
+    kubectl taint nodes "${node}" workload-type=gpu:NoSchedule --overwrite || true
+  fi
+done
+
+echo "GPU node labeling complete. Verify with: kubectl get nodes --show-labels | grep workload-type"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/upload_dataset_to_s3.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Upload a local dataset directory to S3 and print an S3 URI for Argo templates.
+# Usage: ./scripts/upload_dataset_to_s3.sh /local/path my-bucket/path/datasets/mydataset
+#
+LOCAL_DIR=${1:?local dir required}
+S3_PREFIX=${2:?s3 prefix required (bucket/path without s3://)}
+
+if [ ! -d "${LOCAL_DIR}" ]; then
+  echo "Local dir ${LOCAL_DIR} not found"
+  exit 2
+fi
+
+echo "Syncing ${LOCAL_DIR} -> s3://${S3_PREFIX}/"
+aws s3 sync "${LOCAL_DIR}" "s3://${S3_PREFIX}/" --acl bucket-owner-full-control || true
+
+echo "Dataset uploaded to s3://${S3_PREFIX}/"
+echo "Use dataset_s3=s3://${S3_PREFIX}/ in Argo training templates"
+
*** End Patch
*** Begin Patch
*** Add File:argo/training/argo_train_rag_s3.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: train-rag-s3-
+  namespace: aegis
+spec:
+  entrypoint: train-rag
+  arguments:
+    parameters:
+      - name: dataset-s3
+        value: ""
+      - name: model-output-s3
+        value: ""
+  templates:
+    - name: train-rag
+      inputs:
+        parameters:
+          - name: dataset-s3
+          - name: model-output-s3
+      dag:
+        tasks:
+          - name: fetch-dataset
+            template: aws-sync
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{inputs.parameters.dataset-s3}}"
+                - name: dst
+                  value: "/workspace/data"
+          - name: train
+            template: train
+            dependencies: [fetch-dataset]
+          - name: upload
+            template: aws-sync
+            dependencies: [train]
+            arguments:
+              parameters:
+                - name: src
+                  value: "/workspace/model"
+                - name: dst
+                  value: "{{inputs.parameters.model-output-s3}}"
+
+    - name: aws-sync
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: amazon/aws-cli:latest
+        command: [sh, -c]
+        args:
+          - aws s3 sync "{{inputs.parameters.src}}" "{{inputs.parameters.dst}}" --no-progress
+        volumeMounts:
+          - name: work
+            mountPath: /workspace
+      volumes:
+        - name: work
+          emptyDir: {}
+
+    - name: train
+      container:
+        image: aegis/training-rag:latest
+        command: [sh, -c]
+        args:
+          - set -e; python3 /opt/train/train_finetune.py --data_dir /workspace/data --output_dir /workspace/model
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: "64Gi"
+            cpu: "8"
+      volumeMounts:
+        - name: work
+          mountPath: /workspace
+      volumes:
+        - name: work
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:argo/training/argo_train_cv_s3.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: train-cv-s3-
+  namespace: aegis
+spec:
+  entrypoint: train-cv
+  arguments:
+    parameters:
+      - name: dataset-s3
+        value: ""
+      - name: model-output-s3
+        value: ""
+  templates:
+    - name: train-cv
+      inputs:
+        parameters:
+          - name: dataset-s3
+          - name: model-output-s3
+      dag:
+        tasks:
+          - name: fetch-dataset
+            template: aws-sync
+            arguments:
+              parameters:
+                - name: src
+                  value: "{{inputs.parameters.dataset-s3}}"
+                - name: dst
+                  value: "/workspace/data"
+          - name: train
+            template: train-cv
+            dependencies: [fetch-dataset]
+          - name: upload
+            template: aws-sync
+            dependencies: [train]
+            arguments:
+              parameters:
+                - name: src
+                  value: "/workspace/model"
+                - name: dst
+                  value: "{{inputs.parameters.model-output-s3}}"
+
+    - name: aws-sync
+      inputs:
+        parameters:
+          - name: src
+          - name: dst
+      container:
+        image: amazon/aws-cli:latest
+        command: [sh, -c]
+        args:
+          - aws s3 sync "{{inputs.parameters.src}}" "{{inputs.parameters.dst}}" --no-progress
+        volumeMounts:
+          - name: work
+            mountPath: /workspace
+      volumes:
+        - name: work
+          emptyDir: {}
+
+    - name: train-cv
+      container:
+        image: aegis/training-cv:latest
+        command: [sh, -c]
+        args:
+          - set -e; python3 /opt/train/production_train.py --data_dir /workspace/data --output_dir /workspace/model
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: "96Gi"
+            cpu: "8"
+      volumeMounts:
+        - name: work
+          mountPath: /workspace
+      volumes:
+        - name: work
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:triton/hardened_export_decoder.py
+#!/usr/bin/env python3
+"""
+Best-effort exporter for decoder-only (causal) LLMs to ONNX for Triton.
+Notes:
+- Causal models often use past_key_values (attention cache). Exporting cache support for Triton is non-trivial.
+- This script exports a simplified forward (no incremental cache). Use for smaller models or for batched static inference.
+"""
+import argparse, torch
+from transformers import AutoModelForCausalLM, AutoTokenizer
+import os
+
+def export_decoder(model_name_or_path, out_onnx, max_seq=128):
+    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
+    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32)
+    model.eval().cpu()
+    sample = tokenizer("Hello world", return_tensors="pt")
+    input_ids = sample["input_ids"]
+    # Provide a dynamic batch and sequence axis
+    torch.onnx.export(
+        model,
+        (input_ids,),
+        out_onnx,
+        opset_version=13,
+        input_names=["input_ids"],
+        output_names=["logits"],
+        dynamic_axes={"input_ids": {0: "batch_size", 1: "sequence"}, "logits": {0: "batch_size", 1: "sequence"}},
+        do_constant_folding=True,
+        verbose=False,
+    )
+    print("Exported ONNX to", out_onnx)
+    print("WARNING: This export does not include incremental cache (past_key_values). For low-latency incremental decoding, implement cache-aware export or use a runtime that supports HF models natively.")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--model", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    export_decoder(args.model, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:triton/hardened_convert_detectron2.py
+#!/usr/bin/env python3
+"""
+Export Detectron2 model to ONNX for Triton.
+Requires detectron2 installed in the environment used for export.
+"""
+import argparse, torch
+from detectron2.engine import DefaultPredictor
+from detectron2.config import get_cfg
+from detectron2.modeling import build_model
+from detectron2.checkpoint import DetectionCheckpointer
+import os
+
+def export_detectron(cfg_path, weights_path, out_onnx, input_shape=(1,3,800,800)):
+    cfg = get_cfg()
+    cfg.merge_from_file(cfg_path)
+    cfg.MODEL.WEIGHTS = weights_path
+    model = build_model(cfg)
+    checkpointer = DetectionCheckpointer(model)
+    checkpointer.load(weights_path)
+    model.eval().cpu()
+    dummy = torch.randn(*input_shape)
+    torch.onnx.export(
+        model,
+        (dummy,),
+        out_onnx,
+        opset_version=11,
+        input_names=["input_image"],
+        output_names=["output"],
+        dynamic_axes={"input_image": {0: "batch_size", 2: "height", 3: "width"}},
+    )
+    print("Exported Detectron2 ONNX to", out_onnx)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--cfg", required=True)
+    p.add_argument("--weights", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    export_detectron(args.cfg, args.weights, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:triton/write_triton_config_tuned.py
+#!/usr/bin/env python3
+"""
+Write a tuned Triton config.pbtxt for an ONNX or TensorRT model.
+Usage:
+  python3 triton/write_triton_config_tuned.py --model-name mymodel --out /tmp/mymodel --platform onnxruntime_onnx --gpu true --max-batch 8 --memory-mb 8192
+"""
+import argparse, os
+
+TEMPLATE = """
+name: "{model_name}"
+platform: "{platform}"
+max_batch_size: {max_batch}
+instance_group [
+  {{
+    count: {gpu_count}
+    kind: KIND_{instance_kind}
+  }}
+]
+input [
+  {{
+    name: "input_ids"
+    data_type: TYPE_INT32
+    dims: [ -1 ]
+  }}
+]
+output [
+  {{
+    name: "logits"
+    data_type: TYPE_FP32
+    dims: [ -1 ]
+  }}
+]
+dynamic_batching {{
+  preferred_batch_size: [ 4, 8 ]
+  max_queue_delay_microseconds: 1000
+}}
+instance_group_memory_mb: {memory_mb}
+"
+"""
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model-name", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--platform", default="onnxruntime_onnx")
+    p.add_argument("--gpu", action="store_true")
+    p.add_argument("--max-batch", type=int, default=8)
+    p.add_argument("--memory-mb", type=int, default=8192)
+    args = p.parse_args()
+    instance_kind = "GPU" if args.gpu else "CPU"
+    gpu_count = 1 if args.gpu else 0
+    cfg = TEMPLATE.format(
+        model_name=args.model_name,
+        platform=args.platform,
+        max_batch=args.max_batch,
+        gpu_count=gpu_count,
+        instance_kind=instance_kind,
+        memory_mb=args.memory_mb,
+    )
+    os.makedirs(args.out, exist_ok=True)
+    with open(os.path.join(args.out, "config.pbtxt"), "w") as f:
+        f.write(cfg)
+    print("Wrote Triton config to", os.path.join(args.out, "config.pbtxt"))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:servables/seldon/preprocessor.py
+#!/usr/bin/env python3
+"""
+Simple Seldon preprocessor wrapper that tokenizes input text using HF tokenizer.
+This container is intended to be a light-weight preprocessing step in a Seldon graph.
+"""
+from flask import Flask, request, jsonify
+from transformers import AutoTokenizer
+import os
+
+app = Flask("seldon-preprocessor")
+TOKENIZER = AutoTokenizer.from_pretrained(os.environ.get("TOKENIZER", "facebook/bart-base"))
+
+@app.route("/predict", methods=["POST"])
+def predict():
+    j = request.get_json(force=True)
+    # Expect payload: {"data": {"ndarray": ["text"]}}
+    texts = j.get("data", {}).get("ndarray", [])
+    tokens = [TOKENIZER(t, return_tensors="np")["input_ids"].tolist() for t in texts]
+    return jsonify({"data": {"ndarray": tokens}})
+
+@app.route("/health")
+def health():
+    return jsonify({"status":"ok"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT",8080)))
+
*** End Patch
*** Begin Patch
*** Add File:servables/seldon/postprocessor.py
+#!/usr/bin/env python3
+"""
+Simple Seldon postprocessor wrapper that decodes logits back to text using HF tokenizer.
+"""
+from flask import Flask, request, jsonify
+from transformers import AutoTokenizer
+import numpy as np
+import os
+
+app = Flask("seldon-postprocessor")
+TOKENIZER = AutoTokenizer.from_pretrained(os.environ.get("TOKENIZER", "facebook/bart-base"))
+
+@app.route("/predict", methods=["POST"])
+def predict():
+    j = request.get_json(force=True)
+    # Expect logits in j["data"]["ndarray"]
+    logits = np.array(j.get("data", {}).get("ndarray", []))
+    # For demo, pick argmax and decode
+    if logits.size == 0:
+        return jsonify({"data":{"ndarray":[]}})
+    tokens = logits.argmax(axis=-1).tolist()
+    texts = [TOKENIZER.decode(t, skip_special_tokens=True) if isinstance(t, list) else "" for t in tokens]
+    return jsonify({"data": {"ndarray": texts}})
+
+@app.route("/health")
+def health():
+    return jsonify({"status":"ok"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT",8080)))
+
*** End Patch
*** Begin Patch
*** Add File:servables/seldon/rag_seldon_graph.yaml
+apiVersion: machinelearning.seldon.io/v1
+kind: SeldonDeployment
+metadata:
+  name: rag-seldon
+  namespace: aegis
+spec:
+  predictors:
+  - name: default
+    graph:
+      name: preprocessor
+      children:
+        - name: rag-model
+          children:
+            - name: postprocessor
+      implementation: MLFLOW_SERVER
+    componentSpecs:
+      - spec:
+          containers:
+            - name: preprocessor
+              image: registry.example.com/aegis/preprocessor:latest
+              env:
+                - name: TOKENIZER
+                  value: "facebook/bart-base"
+            - name: rag-model
+              image: registry.example.com/aegis/triton-adapter:latest
+              # This container is responsible for forwarding to Triton Model Server
+            - name: postprocessor
+              image: registry.example.com/aegis/postprocessor:latest
+
*** End Patch
*** Begin Patch
*** Add File:ci/scripts/ci_convert_and_validate.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# CI helper: convert model to ONNX, run a deterministic small inference, produce evidence bundle, upload manifest
+#
+: "${MODEL_DIR:?MODEL_DIR required (local workspace)}"
+: "${EVIDENCE_BUCKET:?EVIDENCE_BUCKET required}"
+: "${COSIGN_KMS_KEY_ARN:?COSIGN_KMS_KEY_ARN required}"
+
+OUT_REPO=${OUT_REPO:-/tmp/triton_repo}
+MODEL_NAME=${MODEL_NAME:-ci_model}
+TEST_INPUT=${TEST_INPUT:-"Hello Aegis"}
+
+echo "Converting model to ONNX (decoder export if causal)"
+python3 triton/hardened_export_decoder.py --model "${MODEL_DIR}" --out "${OUT_REPO}/${MODEL_NAME}/1/model.onnx" || \
+  python3 triton/hardened_convert_detectron2.py --cfg "${MODEL_DIR}/config.yaml" --weights "${MODEL_DIR}/model.pth" --out "${OUT_REPO}/${MODEL_NAME}/1/model.onnx" || true
+
+echo "Writing tuned Triton config"
+python3 triton/write_triton_config_tuned.py --model-name "${MODEL_NAME}" --out "${OUT_REPO}/${MODEL_NAME}" --gpu true --max-batch 8 --memory-mb 12288
+
+echo "Running deterministic inference test (single request)"
+python3 triton/hardened_convert_and_load_test.py --model-dir "${MODEL_DIR}" --model-name "${MODEL_NAME}" --out-repo "${OUT_REPO}" || true
+python3 "${OUT_REPO}/load_test_client.py" "${MODEL_NAME}" 1 > /tmp/ci_infer_out.json || true
+
+echo "Bundle evidence (manifest + inference output)"
+MANIFEST=/tmp/ci_manifest.json
+cat > ${MANIFEST} <<JSON
+{
+  "artifact_id": "${MODEL_NAME}-ci",
+  "model_name": "${MODEL_NAME}",
+  "ci_test_input": "${TEST_INPUT}",
+  "ci_test_ts": $(date +%s)
+}
+JSON
+
+EVIDENCE_TAR=/tmp/${MODEL_NAME}_evidence_$(date +%s).tgz
+tar czf ${EVIDENCE_TAR} -C "${OUT_REPO}" . -C /tmp ci_infer_out.json ${MANIFEST} || true
+
+echo "Sign and upload evidence"
+cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" ${EVIDENCE_TAR} || true
+aws s3 cp ${EVIDENCE_TAR} "s3://${EVIDENCE_BUCKET}/ci-evidence/$(basename ${EVIDENCE_TAR})" --sse aws:kms --sse-kms-key-id "${COSIGN_KMS_KEY_ARN}" || true
+
+echo "Upload manifest for promotion (artifact manifest path)"
+aws s3 cp ${MANIFEST} "s3://${MODEL_ARTIFACT_BUCKET}/artifacts/$(basename ${MANIFEST})" --sse aws:kms --sse-kms-key-id "${COSIGN_KMS_KEY_ARN}" || true
+echo "CI conversion & validation complete. Evidence S3: s3://${EVIDENCE_BUCKET}/ci-evidence/$(basename ${EVIDENCE_TAR})"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/convert_validate.yml
+name: Convert & Validate Model (CI)
+on:
+  push:
+    paths:
+      - "ml/**"
+      - "triton/**"
+      - "scripts/**"
+  workflow_dispatch:
+
+jobs:
+  convert-validate:
+    runs-on: ubuntu-latest
+    env:
+      EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+      COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+      MODEL_ARTIFACT_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install dependencies
+        run: |
+          python -m pip install --upgrade pip
+          pip install torch transformers onnx onnxruntime
+      - name: Run conversion & validation
+        env:
+          MODEL_DIR: ${{ github.workspace }}/ml/example_model
+          OUT_REPO: /tmp/triton_repo
+          MODEL_NAME: ci_model
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+          MODEL_ARTIFACT_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+        run: |
+          mkdir -p /tmp/triton_repo/ci_model/1
+          bash ci/scripts/ci_convert_and_validate.sh
+      - name: Upload CI evidence to artifact (comment on PR)
+        if: ${{ github.event_name == 'pull_request' }}
+        run: |
+          echo "CI conversion completed. Evidence and manifest uploaded to S3."
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/load_test_and_slo_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run a simple load test (parallel requests) against Triton and then call SLO monitor.
+# Usage: monitoring/load_test_and_slo_runner.sh <model> <concurrency> <requests>
+
+MODEL=${1:?model required}
+CONC=${2:-4}
+REQS=${3:-50}
+TRITON_HOST=${TRITON_HOST:-http://triton.aegis.svc:8000}
+
+echo "Running simple load test: model=${MODEL} concurrency=${CONC} requests=${REQS}"
+python3 - <<PY
+import requests, time, threading
+TRITON = "${TRITON_HOST}/v2/models/${MODEL}/infer"
+def worker(n):
+    for i in range(${REQS}//${CONC}):
+        payload = {"inputs":[{"name":"input_ids","shape":[1,8],"datatype":"INT32","data":[[1,2,3,4,5,6,7,8]]}]}
+        t0=time.time()
+        r=requests.post(TRITON, json=payload, timeout=10)
+        dt=time.time()-t0
+        print("status", r.status_code, "latency", dt)
+
+threads=[]
+for i in range(${CONC}):
+    t=threading.Thread(target=worker, args=(i,))
+    threads.append(t); t.start()
+for t in threads: t.join()
+print("Load test completed")
+PY
+
+echo "Running SLO evaluation"
+python3 monitoring/enforce_slos.py || true
+
*** End Patch
*** End Patch
