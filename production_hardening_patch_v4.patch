*** Begin Patch
*** Add File: provider/sla_manager.py
+#!/usr/bin/env python3
+"""
+SLA Manager: track provider contractual SLAs and verify attestation freshness/availability.
+ - periodic check: ensure attestations arrive within SLA window
+ - produce SLA compliance report and optionally notify an operator webhook
+"""
+import os, json, boto3, time
+from datetime import datetime, timezone, timedelta
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+ATTEST_S3_PREFIX = os.environ.get("ATTEST_S3_PREFIX", "provider-attestations/")
+TRUSTED_REGISTRY = os.environ.get("TRUSTED_REGISTRY_URL", "http://trusted-source-registry.aegis.svc:8200/list")
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+CHECK_WINDOW_MIN = int(os.environ.get("SLA_CHECK_WINDOW_MIN", "30"))
+
+s3 = boto3.client("s3")
+
+def list_providers():
+    # Query trusted registry for registered providers
+    try:
+        import requests
+        r = requests.get(TRUSTED_REGISTRY, timeout=5)
+        if r.ok:
+            return r.json()
+    except Exception:
+        pass
+    return []
+
+def check_provider(provider):
+    # Look for latest object under provider prefix
+    bucket = COMPLIANCE_BUCKET or os.environ.get("ATTEST_BUCKET")
+    prefix = f"{ATTEST_S3_PREFIX}{provider}/"
+    try:
+        res = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=10)
+        latest = None
+        for o in res.get("Contents", []):
+            ts = o["LastModified"]
+            if not latest or ts > latest:
+                latest = ts
+        if not latest:
+            return {"provider": provider, "status": "missing"}
+        age = datetime.now(timezone.utc) - latest.replace(tzinfo=timezone.utc)
+        return {"provider": provider, "status": "ok" if age < timedelta(minutes=CHECK_WINDOW_MIN) else "stale", "age_seconds": age.total_seconds()}
+    except Exception as e:
+        return {"provider": provider, "status": "error", "error": str(e)}
+
+def run():
+    providers = list_providers()
+    report = {"ts": datetime.now(timezone.utc).isoformat(), "results": []}
+    for p in providers:
+        name = p.get("name") if isinstance(p, dict) else p
+        r = check_provider(name)
+        report["results"].append(r)
+        if r.get("status") != "ok" and OPERATOR_WEBHOOK:
+            try:
+                import requests
+                requests.post(OPERATOR_WEBHOOK, json={"alert":"provider_sla_breach","provider":name,"detail":r}, timeout=5)
+            except Exception:
+                pass
+    out = f"/tmp/sla_check_{int(time.time())}.json"
+    open(out,"w").write(json.dumps(report, indent=2))
+    if COMPLIANCE_BUCKET:
+        try:
+            key = f"sla_reports/{os.path.basename(out)}"
+            s3.upload_file(out, COMPLIANCE_BUCKET, key)
+        except Exception:
+            pass
+    print("Wrote SLA report", out)
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: provider/contract_api.py
+#!/usr/bin/env python3
+"""
+Simple contract & SLA management API for operators to record provider contract terms.
+ - POST /contracts to register contract metadata (provider, sla, contact, signed_contract_s3)
+ - GET  /contracts/<provider> to fetch contract
+"""
+from flask import Flask, request, jsonify
+import os, json
+DB = os.environ.get("PROVIDER_CONTRACT_DB", "provider/contracts.json")
+
+app = Flask("provider-contract-api")
+
+def ensure_db():
+    if not os.path.exists(os.path.dirname(DB)):
+        os.makedirs(os.path.dirname(DB), exist_ok=True)
+    if not os.path.exists(DB):
+        open(DB,"w").write(json.dumps({"contracts": []}, indent=2))
+
+@app.route("/contracts", methods=["POST"])
+def add_contract():
+    ensure_db()
+    j = request.json or {}
+    if not j.get("provider"):
+        return jsonify({"error":"provider required"}), 400
+    db = json.load(open(DB))
+    db["contracts"].append(j)
+    open(DB,"w").write(json.dumps(db, indent=2))
+    return jsonify({"ok": True})
+
+@app.route("/contracts/<provider>", methods=["GET"])
+def get_contract(provider):
+    ensure_db()
+    db = json.load(open(DB))
+    for c in db["contracts"]:
+        if c.get("provider") == provider:
+            return jsonify(c)
+    return jsonify({}), 404
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8205")))
+
*** End Patch
*** Begin Patch
*** Add File: measurement/coverage_enforcer.py
+#!/usr/bin/env python3
+"""
+Enforcer to ensure measured coverage meets target for tenants.
+ - Computes coverage over recent window and alerts if below threshold
+ - Can tag tenant as 'coverage-risk' in tenant_budgets table (simple example)
+"""
+import os, json
+from sqlalchemy import create_engine, text
+from datetime import datetime, timedelta
+
+DB_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@localhost:5432/aegis")
+engine = create_engine(DB_URL)
+THRESHOLD = float(os.environ.get("COVERAGE_THRESHOLD_PCT", "95.0"))
+WINDOW_DAYS = int(os.environ.get("COVERAGE_WINDOW_DAYS", "7"))
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+def compute(window_days=WINDOW_DAYS):
+    since = datetime.utcnow() - timedelta(days=window_days)
+    with engine.connect() as conn:
+        total = conn.execute(text("SELECT count(*) FROM job_ledger WHERE created_at >= :since"), {"since": since}).scalar() or 0
+        measured = conn.execute(text("SELECT count(distinct job_id) FROM job_events WHERE event='measured' AND ts >= :since"), {"since": since}).scalar() or 0
+    pct = (measured/total*100.0) if total else 0.0
+    return {"total": total, "measured": measured, "pct": pct}
+
+def enforce():
+    r = compute()
+    if r["pct"] < THRESHOLD:
+        payload = {"alert": "coverage_below_threshold", "detail": r}
+        if OPERATOR_WEBHOOK:
+            try:
+                import requests
+                requests.post(OPERATOR_WEBHOOK, json=payload, timeout=5)
+            except Exception:
+                pass
+        # simple action: write DB tag (example)
+        with engine.begin() as conn:
+            conn.execute(text("UPDATE tenant_budgets SET coverage_flag = true"))  # illustrative only
+    out = f"/tmp/coverage_enforce_{int(datetime.utcnow().timestamp())}.json"
+    open(out,"w").write(json.dumps(r, indent=2))
+    print("Wrote coverage enforce report", out)
+
+if __name__=="__main__":
+    enforce()
+
*** End Patch
*** Begin Patch
*** Add File: forecast/ci_gate.py
+#!/usr/bin/env python3
+"""
+CI gate for model promotion:
+ - Validate backtest MAE and PI empirical coverage produced by backtest_calibrate.py
+ - Fail promotion if MAE > threshold or PI coverage < target
+"""
+import os, json, sys
+REPORT_DIR = os.environ.get("REPORT_DIR", "/tmp")
+MAE_THRESHOLD = float(os.environ.get("CI_MAE_THRESHOLD", "10.0"))
+PI_TARGET = float(os.environ.get("CI_PI_TARGET", "0.8"))
+
+def run(region):
+    # look for latest backtest report in REPORT_DIR
+    import glob
+    files = sorted(glob.glob(os.path.join(REPORT_DIR, "forecast_backtest_*")), reverse=True)
+    if not files:
+        print("no backtest report found"); return False
+    j = json.load(open(files[0]))
+    mae = j.get("avg_mae")
+    cov = j.get("avg_pi_coverage")
+    print("backtest mae", mae, "pi_cov", cov)
+    if mae is None or cov is None:
+        return False
+    if mae > MAE_THRESHOLD:
+        print("MAE too high"); return False
+    if cov < PI_TARGET:
+        print("PI coverage too low"); return False
+    return True
+
+if __name__=="__main__":
+    import argparse
+    p=argparse.ArgumentParser()
+    p.add_argument("--region", default="US")
+    args=p.parse_args()
+    ok = run(args.region)
+    if not ok:
+        print("CI gate failed"); sys.exit(2)
+    print("CI gate passed")
+
*** End Patch
*** Begin Patch
*** Add File: forecast/post_promotion_monitor.py
+#!/usr/bin/env python3
+"""
+Post-promotion monitor: sample live MAE and compare vs baseline; emit alert and optionally trigger rollback.
+This is intended to be run frequently for a window after model promotion.
+"""
+import os, json, boto3, requests, time
+from datetime import datetime
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+MODEL_REGISTRY_API = os.environ.get("MODEL_REGISTRY_API")
+MAE_MULTIPLIER = float(os.environ.get("MAE_MULTIPLIER", "1.2"))
+ALERT_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+REGION = os.environ.get("REGION", "US")
+
+s3 = boto3.client("s3")
+
+def fetch_live_mae(region):
+    key = f"forecast_monitor/prophet_monitor_{region}.json"
+    tmp = f"/tmp/prophet_monitor_{region}.json"
+    try:
+        s3.download_file(COMPLIANCE_BUCKET, key, tmp)
+        return json.load(open(tmp)).get("mae")
+    except Exception:
+        return None
+
+def get_prev_model_mae(region):
+    r = requests.get(f"{MODEL_REGISTRY_API}/models/list/{region}", timeout=5)
+    if not r.ok:
+        return None
+    models = r.json()
+    if len(models) < 2:
+        return None
+    return float(models[1].get("mae") or 0.0), models[1]
+
+def trigger_rollback(model_id):
+    try:
+        requests.post(f"{MODEL_REGISTRY_API}/models/rollback/{model_id}", timeout=10)
+    except Exception:
+        pass
+
+def run_window(minutes=30):
+    end = time.time() + minutes * 60
+    latest = None
+    while time.time() < end:
+        mae = fetch_live_mae(REGION)
+        if mae:
+            latest = mae; break
+        time.sleep(15)
+    if latest is None:
+        return
+    prev = get_prev_model_mae(REGION)
+    if not prev:
+        return
+    prev_mae, prev_model = prev
+    if latest > prev_mae * MAE_MULTIPLIER:
+        if ALERT_WEBHOOK:
+            requests.post(ALERT_WEBHOOK, json={"alert":"post_promotion_mae_increase","region":REGION,"live_mae": latest, "prev_mae": prev_mae}, timeout=5)
+        trigger_rollback(prev_model.get("id"))
+        print("Rollback triggered")
+    else:
+        print("No rollback needed")
+
+if __name__=="__main__":
+    run_window(int(os.environ.get("MONITOR_MINUTES","30")))
+
*** End Patch
*** Begin Patch
*** Add File: hsm/key_rotation_tracker.py
+#!/usr/bin/env python3
+"""
+Track HSM key rotations and ensure rotation metadata is recorded separately from CI.
+ - record rotation events to compliance bucket
+ - validate that rotated public keys are published to trusted registry
+"""
+import os, json, boto3
+from datetime import datetime
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+TRUSTED_REGISTRY = os.environ.get("TRUSTED_REGISTRY_URL")
+
+s3 = boto3.client("s3")
+
+def record_rotation(hsm_host, key_id, operator):
+    rec = {"hsm": hsm_host, "key_id": key_id, "operator": operator, "ts": datetime.utcnow().isoformat()}
+    tmp = f"/tmp/hsm_rotation_{key_id}.json"
+    open(tmp,"w").write(json.dumps(rec, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3.upload_file(tmp, COMPLIANCE_BUCKET, f"hsm_rotations/{os.path.basename(tmp)}")
+    print("Recorded rotation", key_id)
+
+if __name__=="__main__":
+    import argparse
+    p=argparse.ArgumentParser()
+    p.add_argument("--hsm", required=True)
+    p.add_argument("--key", required=True)
+    p.add_argument("--operator", required=True)
+    args=p.parse_args()
+    record_rotation(args.hsm, args.key, args.operator)
+
*** End Patch
*** Begin Patch
*** Add File: hsm/multi_hsm_recovery_validator.py
+#!/usr/bin/env python3
+"""
+Validate multi-HSM signing resilience by simulating single-host outages and ensuring remaining HSMs can sign.
+Uploads a consolidated report to the compliance bucket.
+"""
+import os, subprocess, json, boto3
+from datetime import datetime
+
+HOSTS = [h for h in os.environ.get("HSM_ADMIN_HOSTS","").split(",") if h]
+SAMPLE = os.environ.get("HSM_SAMPLE_ART","/opt/aegis/sample_snapshot.json")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+s3 = boto3.client("s3")
+
+def test(active_hosts):
+    results = []
+    for h in active_hosts:
+        try:
+            cmd = ["ssh", h, "bash -lc", f"'/opt/aegis/hsm_sign_snapshot_remote.sh \"{SAMPLE}\" >/tmp/hsm_sign.log 2>&1; tail -n 200 /tmp/hsm_sign.log'"]
+            out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, timeout=120).decode()
+            results.append({"host": h, "ok": True, "log": out[:4000]})
+        except Exception as e:
+            results.append({"host": h, "ok": False, "error": str(e)})
+    return results
+
+def main():
+    report = {"ts": datetime.utcnow().isoformat(), "scenarios": []}
+    report["scenarios"].append({"name": "all_hosts", "results": test(HOSTS)})
+    for h in HOSTS:
+        active = [x for x in HOSTS if x != h]
+        report["scenarios"].append({"name": f"without_{h}", "results": test(active)})
+    out = f"/tmp/hsm_recovery_{int(datetime.utcnow().timestamp())}.json"
+    open(out,"w").write(json.dumps(report, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3.upload_file(out, COMPLIANCE_BUCKET, f"hsm_recovery/{os.path.basename(out)}")
+    print("Wrote recovery report", out)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: admission/throttle_refill_manager.py
+#!/usr/bin/env python3
+"""
+Automated throttle refill manager which can be scheduled as CronJob.
+ - Refill tokens for all tenants according to policy and historical usage.
+ - Write refill audit record to compliance bucket.
+"""
+import os, json, boto3
+from admission.throttle_db import refill_all
+from datetime import datetime
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def run():
+    refill_all()
+    out = f"/tmp/throttle_refill_{int(datetime.utcnow().timestamp())}.json"
+    open(out,"w").write(json.dumps({"ts": datetime.utcnow().isoformat(), "action":"refill_all"}, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3 = boto3.client("s3"); s3.upload_file(out, COMPLIANCE_BUCKET, f"throttle_refills/{os.path.basename(out)}")
+    print("Refill complete")
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/backpressure_monitor.py
+#!/usr/bin/env python3
+"""
+Monitor Redis queue length and emit Prometheus metrics + alert via webhook when queue exceeds threshold.
+"""
+import os, time, redis
+from prometheus_client import start_http_server, Gauge
+from datetime import datetime
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+ZSET = "aegis:queue:zset"
+THRESHOLD = int(os.environ.get("QUEUE_ALERT_THRESHOLD", "1000"))
+WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+PORT = int(os.environ.get("METRICS_PORT", "9110"))
+
+r = redis.from_url(REDIS_URL)
+QUEUE_LEN = Gauge("aegis_queue_length", "Current queue length (monitor)")
+
+def run():
+    start_http_server(PORT)
+    while True:
+        try:
+            ln = r.zcard(ZSET)
+            QUEUE_LEN.set(ln)
+            if ln > THRESHOLD and WEBHOOK:
+                import requests
+                requests.post(WEBHOOK, json={"alert":"queue_backpressure","len": ln, "ts": datetime.utcnow().isoformat()}, timeout=5)
+        except Exception as e:
+            print("monitor error", e)
+        time.sleep(10)
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: experiments/randomization_audit_hook.py
+#!/usr/bin/env python3
+"""
+Audit hook for A/B experiments: validate randomization assignments and archive logs to compliance bucket.
+Input: experiments/ab_inputs/assignments.json
+"""
+import os, json, statistics, boto3
+from datetime import datetime
+
+ASSIGN_PATH = "experiments/ab_inputs/assignments.json"
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def run():
+    if not os.path.exists(ASSIGN_PATH):
+        print("no assignments file"); return
+    a = json.load(open(ASSIGN_PATH))
+    counts = {}
+    for k,v in a.items():
+        counts[v] = counts.get(v, 0) + 1
+    vals = list(counts.values())
+    mean = statistics.mean(vals) if vals else 0
+    stdev = statistics.pstdev(vals) if len(vals) > 1 else 0
+    report = {"ts": datetime.utcnow().isoformat(), "counts": counts, "mean": mean, "stdev": stdev}
+    out = f"/tmp/ab_randomization_{int(datetime.utcnow().timestamp())}.json"
+    open(out,"w").write(json.dumps(report, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3 = boto3.client("s3"); s3.upload_file(out, COMPLIANCE_BUCKET, f"ab_audit/{os.path.basename(out)}")
+    print("Wrote randomization audit", out)
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: compliance/pen_test_workflow.py
+#!/usr/bin/env python3
+"""
+Simple pen-test remediation tracker with evidence uploads.
+ - Create finding, link evidence file (S3 path), mark remediation status.
+"""
+import os, json, boto3
+from datetime import datetime
+
+DB = "compliance/pen_findings.json"
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def ensure_db():
+    if not os.path.exists("compliance"):
+        os.makedirs("compliance")
+    if not os.path.exists(DB):
+        open(DB,"w").write(json.dumps({"findings": []}, indent=2))
+
+def add_finding(title, severity, desc, evidence_s3=None):
+    ensure_db()
+    d = json.load(open(DB))
+    fid = f"f-{int(datetime.utcnow().timestamp())}"
+    rec = {"id": fid, "title": title, "severity": severity, "description": desc, "evidence": evidence_s3, "status": "open", "created_at": datetime.utcnow().isoformat()}
+    d["findings"].append(rec)
+    open(DB,"w").write(json.dumps(d, indent=2))
+    if COMPLIANCE_BUCKET and evidence_s3:
+        # copy evidence entry into compliance prefix for audit
+        s3 = boto3.client("s3")
+        # evidence_s3 expected s3://bucket/key
+        try:
+            # noop placeholder for demo
+            pass
+        except Exception:
+            pass
+    print("Added finding", fid)
+
+def close_finding(fid, note=""):
+    ensure_db()
+    d = json.load(open(DB))
+    for f in d["findings"]:
+        if f["id"] == fid:
+            f["status"] = "closed"
+            f.setdefault("notes", []).append({"ts": datetime.utcnow().isoformat(), "note": note})
+    open(DB,"w").write(json.dumps(d, indent=2))
+    print("Closed", fid)
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("cmd", choices=["add","close"])
+    p.add_argument("--title")
+    p.add_argument("--severity", default="medium")
+    p.add_argument("--desc", default="")
+    p.add_argument("--evidence")
+    p.add_argument("--id")
+    args = p.parse_args()
+    if args.cmd == "add":
+        add_finding(args.title, args.severity, args.desc, args.evidence)
+    else:
+        close_finding(args.id, args.desc)
+
*** End Patch
*** Begin Patch
*** Add File: security/oidc_auth_stub.py
+#!/usr/bin/env python3
+"""
+Minimal OIDC login stub to illustrate production auth integration.
+ - Redirects to OIDC provider for login (demo)
+ - Exchanges code for token (not fully implemented)
+This is a scaffold; operator must wire to real OIDC (Keycloak/Auth0).
+"""
+from flask import Flask, redirect, request, jsonify
+import os
+
+OIDC_AUTH_URL = os.environ.get("OIDC_AUTH_URL", "https://oidc.example/authorize")
+CLIENT_ID = os.environ.get("OIDC_CLIENT_ID", "client")
+REDIRECT_URI = os.environ.get("OIDC_REDIRECT", "https://ui.example/callback")
+
+app = Flask("oidc-stub")
+
+@app.route("/login")
+def login():
+    # redirect to provider (demo)
+    return redirect(f"{OIDC_AUTH_URL}?client_id={CLIENT_ID}&redirect_uri={REDIRECT_URI}&response_type=code")
+
+@app.route("/callback")
+def callback():
+    # operator: implement token exchange here
+    code = request.args.get("code")
+    return jsonify({"code": code, "note": "exchange token at provider - this is a stub"})
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8170")))
+
*** End Patch
*** Begin Patch
*** Add File: ha/automated_restore_and_rto.py
+#!/usr/bin/env python3
+"""
+Automated restore drill that restores Postgres into a staging DB, runs smoke queries, measures RTO and uploads report.
+Operator must supply PG_RESTORE_CMD env var to perform actual restore.
+"""
+import os, subprocess, time, json
+from datetime import datetime
+import boto3
+
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+PG_S3 = os.environ.get("PG_S3_PATH", "s3://aegis-backups/postgres/latest.dump")
+TMP_DUMP = "/tmp/restore.dump"
+RESTORE_CMD = os.environ.get("PG_RESTORE_CMD")  # e.g. "pg_restore -d staging_db /tmp/restore.dump"
+
+s3 = boto3.client("s3")
+
+def download():
+    s3.download_file(PG_S3.split("/",3)[2], "/".join(PG_S3.split("/",4)[3:]), TMP_DUMP) if False else None
+    # fallback: use aws cli if boto3 parsing not straightforward
+    try:
+        subprocess.check_call(["aws","s3","cp", PG_S3, TMP_DUMP])
+    except Exception as e:
+        print("download failed", e); raise
+
+def restore_and_test():
+    start = time.time()
+    if not RESTORE_CMD:
+        raise RuntimeError("PG_RESTORE_CMD not configured")
+    subprocess.check_call(RESTORE_CMD, shell=True)
+    # run a smoke query (operator must adapt)
+    time.sleep(2)
+    rto = time.time() - start
+    report = {"ts": datetime.utcnow().isoformat(), "rto_seconds": rto}
+    out = f"/tmp/restore_drill_{int(time.time())}.json"
+    open(out,"w").write(json.dumps(report, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3 = boto3.client("s3"); s3.upload_file(out, COMPLIANCE_BUCKET, f"drills/{os.path.basename(out)}")
+    print("Wrote restore report", out)
+
+if __name__=="__main__":
+    download()
+    restore_and_test()
+
*** End Patch
*** End Patch
