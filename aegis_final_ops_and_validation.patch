*** Begin Patch
*** Add File:quantum/hsm/vendor_rotation_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Test HSM key rotation end-to-end (operator-driven)
+# Steps:
+# 1. Use vendor CLI to create/generate a new key labeled NEW_LABEL (operator must run vendor-specific command).
+# 2. Export the new public key to /tmp/new_pub.pem (operator step).
+# 3. Run this script to publish the new public key to Vault, update services, and verify signing with new key.
+#
+# Usage:
+#   ./vendor_rotation_test.sh --vault-path secret/data/hsm/config --new-label pqkey-v2 --pubkey /tmp/pqkey-v2.pub --pkcs11-lib /opt/vendor/lib/pkcs11.so --slot 0 --pin 1234 --artifact /tmp/test.bin --s3-bucket my-hsm-audit-bucket
+
+VAULT_PATH=""
+NEW_LABEL=""
+PUBKEY_FILE=""
+PKCS11_LIB=""
+SLOT=""
+PIN=""
+ARTIFACT=""
+S3_BUCKET=""
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --vault-path) VAULT_PATH="$2"; shift 2;;
+    --new-label) NEW_LABEL="$2"; shift 2;;
+    --pubkey) PUBKEY_FILE="$2"; shift 2;;
+    --pkcs11-lib) PKCS11_LIB="$2"; shift 2;;
+    --slot) SLOT="$2"; shift 2;;
+    --pin) PIN="$2"; shift 2;;
+    --artifact) ARTIFACT="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${VAULT_PATH:?--vault-path required}"
+: "${NEW_LABEL:?--new-label required}"
+: "${PUBKEY_FILE:?--pubkey required}"
+: "${PKCS11_LIB:?--pkcs11-lib required}"
+: "${SLOT:?--slot required}"
+: "${PIN:?--pin required}"
+: "${ARTIFACT:?--artifact required}"
+
+echo "1) Publish new public key to Vault"
+vault kv put "${VAULT_PATH}" public_key=@${PUBKEY_FILE} key_label="${NEW_LABEL}"
+
+echo "2) Run a test sign with new key label via hybrid_signer_hsm"
+python3 quantum/crypto/hybrid_signer_hsm.py sign --artifact "${ARTIFACT}" --outdir /tmp/aegis_hsm_rotation_test --use-pkcs11 --pkcs11-lib "${PKCS11_LIB}" --pkcs11-slot "${SLOT}" --pkcs11-pin "${PIN}" --pkcs11-keylabel "${NEW_LABEL}" || { echo "Signing failed with new key label"; exit 2;}
+
+echo "3) Inspect hybrid metadata"
+jq . /tmp/aegis_hsm_rotation_test/hybrid-signature.json || cat /tmp/aegis_hsm_rotation_test/hybrid-signature.json
+
+if [ -n "${S3_BUCKET:-}" ]; then
+  echo "4) Verify HSM audit objects uploaded to s3://${S3_BUCKET}/hsm-audit/"
+  python3 quantum/hsm/vendor_validation/verify_hsm_audit.py --s3-bucket "${S3_BUCKET}" --prefix hsm-audit/ --timeout 300 || echo "No audit logs found within timeout"
+fi
+
+echo "Rotation test completed. Collect /tmp/aegis_hsm_rotation_test and include audit/S3 evidence for compliance."
+
*** End Patch
*** Begin Patch
*** Add File:quantum/hsm/verify_audit_to_siem.py
+#!/usr/bin/env python3
+"""
+Poll S3 for HSM audit objects and optionally forward a sample to a SIEM HTTP endpoint for verification.
+Usage:
+  python3 verify_audit_to_siem.py --s3-bucket my-bucket --prefix hsm-audit/ --siem-endpoint https://siem.example.com/ingest --timeout 300
+"""
+import argparse, boto3, time, requests, sys, tempfile, os
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--prefix", default="hsm-audit/")
+    p.add_argument("--timeout", type=int, default=300)
+    p.add_argument("--siem-endpoint", default=None)
+    args = p.parse_args()
+    s3 = boto3.client("s3")
+    deadline = time.time() + args.timeout
+    while time.time() < deadline:
+        resp = s3.list_objects_v2(Bucket=args.s3_bucket, Prefix=args.prefix, MaxKeys=10)
+        if resp.get("KeyCount",0) > 0:
+            key = resp["Contents"][0]["Key"]
+            print("Found audit object:", key)
+            tmp = tempfile.mktemp(suffix=os.path.basename(key))
+            s3.download_file(args.s3_bucket, key, tmp)
+            print("Downloaded to", tmp)
+            if args.siem_endpoint:
+                with open(tmp, "rb") as f:
+                    r = requests.post(args.siem_endpoint, data=f, timeout=30)
+                    print("SIEM forward status:", r.status_code, r.text)
+            sys.exit(0)
+        time.sleep(5)
+    print("Timeout waiting for audit objects")
+    sys.exit(2)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/pilot/store_creds_and_run.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Store provider credentials into Vault and run pilots via orchestrator
+# Usage:
+#  ./store_creds_and_run.sh --vault-path secret/data/quantum/providers --provider braket --key braket_device --value arn:aws:braket:... --program demo.qasm --s3-bucket my-bucket
+
+VAULT_PATH=""
+PROVIDER=""
+KEY=""
+VALUE=""
+PROGRAM=""
+S3_BUCKET=""
+IBM_BACKEND=""
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --vault-path) VAULT_PATH="$2"; shift 2;;
+    --provider) PROVIDER="$2"; shift 2;;
+    --key) KEY="$2"; shift 2;;
+    --value) VALUE="$2"; shift 2;;
+    --program) PROGRAM="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --ibm-backend) IBM_BACKEND="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${VAULT_PATH:?--vault-path required}"
+: "${PROVIDER:?--provider required}"
+: "${KEY:?--key required}"
+: "${VALUE:?--value required}"
+: "${PROGRAM:?--program required}"
+: "${S3_BUCKET:?--s3-bucket required}"
+
+echo "Writing credential to Vault..."
+vault kv put "${VAULT_PATH}" "${KEY}=${VALUE}"
+
+echo "Running pilot orchestrator for provider ${PROVIDER}..."
+if [ "${PROVIDER}" = "braket" ]; then
+  python3 providers/pilot/orchestrator.py --provider braket --vault-path "${VAULT_PATH}" --program "${PROGRAM}" --s3-bucket "${S3_BUCKET}"
+elif [ "${PROVIDER}" = "ibm" ]; then
+  if [ -z "${IBM_BACKEND}" ]; then
+    echo "For IBM supply --ibm-backend <backend-name>"; exit 2
+  fi
+  python3 providers/pilot/orchestrator.py --provider ibm --vault-path "${VAULT_PATH}" --program "${PROGRAM}" --s3-bucket "${S3_BUCKET}" --backend "${IBM_BACKEND}"
+else
+  echo "Unknown provider"; exit 1
+fi
+
+echo "Pilot triggered. Check MLflow experiment 'quantum-pilots' and S3 for artifacts."
+
*** End Patch
*** Begin Patch
*** Add File:quantum/pilot/verify_mlflow_and_playback.py
+#!/usr/bin/env python3
+"""
+Verify MLflow runs have Rekor tags and attempt playback for runs that include noise snapshots.
+Usage:
+  python3 verify_mlflow_and_playback.py --mlflow-url http://mlflow:5000 --experiment quantum-pilots --s3-bucket my-bucket --qasm-file demo.qasm --max-runs 10
+"""
+import argparse, mlflow, json, os, tempfile, boto3, subprocess, sys
+
+def download_artifacts_from_mlflow(run, outdir):
+    client = mlflow.tracking.MlflowClient()
+    artifacts = []
+    for root, _, files in os.walk("/tmp"):
+        pass
+    # Use mlflow client to download artifacts (requires mlflow server accessible)
+    client.download_artifacts(run.info.run_id, "meta", outdir)
+    return outdir
+
+def find_noise_file(artifact_dir):
+    for root, _, files in os.walk(artifact_dir):
+        for f in files:
+            if "noise" in f or "meta" in f:
+                return os.path.join(root, f)
+    return None
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--mlflow-url", required=True)
+    p.add_argument("--experiment", default="quantum-pilots")
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--qasm-file", required=True)
+    p.add_argument("--max-runs", type=int, default=10)
+    args = p.parse_args()
+    mlflow.set_tracking_uri(args.mlflow_url)
+    client = mlflow.tracking.MlflowClient()
+    exp = client.get_experiment_by_name(args.experiment)
+    if not exp:
+        print("Experiment not found:", args.experiment); sys.exit(1)
+    runs = client.search_runs([exp.experiment_id], order_by=["attributes.start_time DESC"], max_results=args.max_runs)
+    s3 = boto3.client("s3")
+    for r in runs:
+        tags = r.data.tags
+        run_id = r.info.run_id
+        print("Run:", run_id, "tags:", tags.keys())
+        if "rekor.entry" not in tags:
+            print(" - Missing rekor.entry tag for run", run_id)
+        else:
+            print(" - Rekor entry present")
+        # download artifacts to temp dir
+        outdir = tempfile.mkdtemp(prefix="mlflow_art_")
+        try:
+            client.download_artifacts(run_id, "meta", outdir)
+        except Exception as e:
+            print("  no meta artifacts or download failed:", e)
+            continue
+        noise = find_noise_file(outdir)
+        if noise:
+            print(" - Found noise file:", noise, "running playback")
+            try:
+                subprocess.check_call(["python3","repro/simulator_playback.py","--qasm", args.qasm_file,"--noise", noise])
+                print(" - Playback succeeded for run", run_id)
+            except subprocess.CalledProcessError as e:
+                print(" - Playback failed:", e)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:broker/scripts/jwt_rotate_and_verify.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS=${1:-aegis}
+SECRET_NAME=${2:-aegis-broker-secret}
+
+echo "Rotating JWT secret in Kubernetes secret ${SECRET_NAME} in namespace ${NS}"
+./quantum/job_broker/jwt_rotate.sh "${NS}" "${SECRET_NAME}"
+
+echo "Waiting for broker pods to restart/load new secret..."
+sleep 10
+kubectl rollout status deployment/aegis-quantum-broker -n "${NS}" --timeout=120s || true
+
+echo "Checking broker health"
+kubectl exec -n "${NS}" $(kubectl get pods -n "${NS}" -l app=aegis-quantum-broker -o jsonpath='{.items[0].metadata.name}') -- curl -sS http://localhost:8080/health || echo "health check failed"
+
+echo "JWT rotation completed and broker health checked."
+
*** End Patch
*** Begin Patch
*** Add File:broker/scripts/test_hpa_autoscale_and_restore.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS=${1:-aegis}
+REPLICAS_BEFORE=$(kubectl get deploy aegis-quantum-worker -n "${NS}" -o jsonpath='{.spec.replicas}' || echo "1")
+echo "Replicas before test: ${REPLICAS_BEFORE}"
+echo "Triggering load (requires a valid JWT token in env BROKER_JWT)"
+./broker/runbooks/load_test_hpa.sh "http://aegis-quantum-broker.aegis.svc.cluster.local/submit" "${BROKER_JWT:-}" 100
+echo "Waiting 2m for HPA to respond..."
+sleep 120
+kubectl get hpa -n "${NS}"
+echo "Restore: scale down workers to baseline"
+kubectl scale deployment aegis-quantum-worker -n "${NS}" --replicas=${REPLICAS_BEFORE}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/enforce_rekor_on_main.yml
+name: Enforce Rekor presence on main
+on:
+  push:
+    branches: [ "main", "master" ]
+jobs:
+  enforce:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install mlflow
+        run: pip install mlflow
+      - name: Check MLflow for Rekor tags
+        env:
+          MLFLOW_URL: ${{ secrets.MLFLOW_URL }}
+        run: |
+          python3 quantum/rekor/check_mlflow_rekor.py --mlflow-url "${MLFLOW_URL}" --experiment quantum-pilots --threshold 10
+
*** End Patch
*** Begin Patch
*** Add File:observability/cur/lambda_cur_processor.py
+"""
+AWS Lambda skeleton to process AWS CUR files (Parquet/CSV) into simplified job-cost CSV
+Operator must adapt mapping from CUR line items to Aegis job tags (e.g., tag 'aegis:job-id' or resource tags).
+Deploy as Lambda with appropriate IAM to read CUR S3 bucket and write simplified CSV to ingestion S3 path.
+"""
+import json, boto3, os, tempfile
+import pandas as pd
+
+s3 = boto3.client("s3")
+
+def handler(event, context):
+    # event contains S3 object created records from CUR delivery
+    for rec in event.get("Records", []):
+        bucket = rec["s3"]["bucket"]["name"]
+        key = rec["s3"]["object"]["key"]
+        tmpdir = tempfile.mkdtemp()
+        local_path = os.path.join(tmpdir, os.path.basename(key))
+        s3.download_file(bucket, key, local_path)
+        # For brevity assume CSV; convert to DataFrame and map to (job,cost)
+        df = pd.read_csv(local_path) if local_path.endswith(".csv") else pd.read_parquet(local_path)
+        # Placeholder: operator must adapt these columns to find job-id or tag
+        if "resourceTags" in df.columns:
+            # parse tags column to extract aegis:job-id (pseudo)
+            df["job"] = df["resourceTags"].apply(lambda x: parse_job_from_tags(x))
+        else:
+            df["job"] = "unknown"
+        df_summary = df.groupby("job")["cost"].sum().reset_index()
+        out_path = os.path.join(tmpdir, "simple_billing.csv")
+        df_summary.to_csv(out_path, index=False)
+        dest_bucket = os.environ.get("BILLING_INGEST_BUCKET")
+        dest_key = f"cur-ingest/{os.path.basename(key)}.simple.csv"
+        s3.upload_file(out_path, dest_bucket, dest_key)
+    return {"status": "ok"}
+
+def parse_job_from_tags(tag_field):
+    # operator-defined parsing
+    return "unknown"
+
*** End Patch
*** Begin Patch
*** Add File:observability/cur/README_CUR_SETUP.md
+CUR Integration quick guide
+---------------------------
+1. In AWS Billing console create a Cost & Usage Report (CUR) and deliver to S3.
+2. Create an S3 event notification to invoke the Lambda (observability/cur/lambda_cur_processor.py) on new CUR files.
+3. Lambda writes simplified CSV to S3 path configured in BILLING_INGEST_BUCKET; schedule a job to call broker/quotas/chargeback_connector.py against the simplified CSV.
+4. The Lambda has a placeholder parse_job_from_tags() — adapt it to extract Aegis job ids from resource tags or mapping table.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/auditor/evidence_manifest_template.yaml
+evidence:
+  hsm:
+    audit_logs_s3_prefix: "s3://<bucket>/hsm-audit/"
+    hybrid_metadata_paths:
+      - "/tmp/aegis_hsm_rotation_test/hybrid-signature.json"
+  pilots:
+    mlflow_experiment: "quantum-pilots"
+    artifacts:
+      - "s3://<bucket>/braket/results/<job_id>/meta.json"
+  rekor:
+    entries:
+      - artifact_hash: "<sha256>"
+        rekor_entry: "<rekor-json>"
+  broker:
+    helm_values: "broker/helm/values-production.yaml"
+    rds_output: "<terraform output job_database_url>"
+  compliance:
+    export_control_review: "<path/to/review.pdf>"
+    vendor_contracts: "<path/to/contracts.zip>"
+
*** End Patch
*** Begin Patch
*** Add File:compliance/legal/contract_signoff_template.md
+Contract Signoff Template (operator -> legal)
+--------------------------------------------
+To: Legal
+Subject: Request to finalize vendor contract for HSM / QPU provider: <vendor-name>
+
+Please include the following minimum clauses:
+- PKCS#11 support (library/module distribution & compatibility)
+- Audit log export: vendor must provide structured audit logs and ability to forward to S3/SIEM
+- Key lifecycle: generation, rotation, backup/escrow policies
+- Export-control and data residency compliance clauses
+- SLAs and support escalation paths
+- Right to audit & evidence provisioning for SOC2
+
+Attach vendor provided documents and ask vendor to countersign the Aegis Counter-sign Template located at compliance/vendor_contracts/COUNTERSIGN_TEMPLATE.md
+
*** End Patch
*** Begin Patch
*** Add File:docs/operator_runbook_remaining.md
+# Operator runbook — remaining tasks to reach 100% readiness
+
+Overview
+- This runbook sequences the remaining operational tasks (HSM provisioning, pilots, broker cutover, CI/auditor checks, billing integration, legal signoffs).
+
+Steps
+1) Vendor HSM provisioning & validation
+   - Provision HSM using cloud/hsm terraform or vendor console.
+   - Run Ansible playbook: ansible-playbook -i inventory quantum/hsm/ansible/install_pkcs11.yml --extra-vars "use_softhsm=false pkcs11_module=/opt/vendor/lib/vendor_pkcs11.so"
+   - Publish HSM config and public key to Vault:
+       ./quantum/vault/write_hsm_config_and_pubkey.sh --vault-path secret/data/hsm/config --pkcs11-lib /opt/vendor/lib/vendor_pkcs11.so --slot 0 --pubkey /tmp/vendor_pub.pem
+   - Run end-to-end sign & audit validation:
+       ./quantum/hsm/validate_hsm_end_to_end.sh --artifact /tmp/test.bin --pkcs11-lib /opt/vendor/lib/vendor_pkcs11.so --pkcs11-slot 0 --pkcs11-pin 1234 --pkcs11-keylabel pqkey --s3-bucket my-hsm-audit-bucket --rekor
+   - Run rotation test:
+       ./quantum/hsm/vendor_rotation_test.sh --vault-path secret/data/hsm/config --new-label pqkey-v2 --pubkey /tmp/pqkey-v2.pub --pkcs11-lib /opt/vendor/lib/vendor_pkcs11.so --slot 0 --pin 1234 --artifact /tmp/test.bin --s3-bucket my-hsm-audit-bucket
+
+2) Credentialed QPU pilots
+   - Store credentials in Vault via:
+       ./quantum/pilot/store_creds_and_run.sh --vault-path secret/data/quantum/providers --provider braket --key braket_device --value <device_arn> --program demo.qasm --s3-bucket <bucket>
+   - For IBM:
+       ./quantum/pilot/store_creds_and_run.sh --vault-path secret/data/quantum/providers --provider ibm --key ibm_token --value "<QISKIT_IBM_TOKEN>" --program demo.qasm --s3-bucket <bucket> --ibm-backend <backend>
+   - Verify MLflow Rekor tags and run playback:
+       python3 quantum/pilot/verify_mlflow_and_playback.py --mlflow-url http://mlflow:5000 --experiment quantum-pilots --s3-bucket <bucket> --qasm-file demo.qasm --max-runs 10
+
+3) Broker production cutover
+   - Provision RDS: push terraform in broker/terraform and capture job_database_url
+   - Create k8s secrets (prefer external-secrets)
+   - Install cert-manager and apply broker/k8s/cert-manager-issuer.yaml
+   - Deploy Helm chart: helm upgrade --install aegis-quantum-broker broker/helm -n aegis --values broker/helm/values-production.yaml
+   - Deploy worker and HPA: kubectl apply -f broker/k8s/worker-deployment-autoscale.yaml
+   - Rotate JWT and verify: ./broker/scripts/jwt_rotate_and_verify.sh aegis aegis-broker-secret
+   - Run load test to exercise autoscaling: ./broker/runbooks/load_test_hpa.sh <broker-url> <JWT> 100
+
+4) CI & auditor certification
+   - Ensure .github/workflows/enforce_rekor_on_main.yml is active and secrets.MLFLOW_URL is configured.
+   - Run .github/workflows/rekor_auditor_report.yml to collect Rekor outputs for artifacts.
+   - Produce evidence bundle: python3 compliance/packager.py (or ./compliance/audit_packager_enhanced.sh)
+   - Provide deterministic_signing_README.md to the auditor and the evidence bundle.
+
+5) Observability & billing
+   - Setup AWS CUR to deliver to S3; configure Lambda using observability/cur/lambda_cur_processor.py
+   - Configure BILLING_INGEST_BUCKET env and schedule ingestion job that runs observability/cur/ingest_cur.sh
+   - Tune QUOTA_THRESHOLD and test fallback via observability/test_cost_spike.sh
+
+6) Legal & compliance
+   - Send contract_signoff_template.md to legal and vendor, get contract countersigned
+   - Run export control review and store results in compliance/evidence
+   - Schedule SOC2 auditor handoff and provide evidence bundle
+
+Contact points
+- Ops: @infra-team
+- Security/Compliance: @secops
+- Legal: @legal
+
*** End Patch
*** End Patch
