*** Begin Patch
*** Add File:deploy/rekor/postgres-statefulset.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: rekor-postgres-secret
+  namespace: aegis
+type: Opaque
+stringData:
+  postgres-user: rekor
+  postgres-password: "REPLACE_WITH_STRONG_PASSWORD"
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  serviceName: rekor-postgres
+  replicas: 1
+  selector:
+    matchLabels:
+      app: rekor-postgres
+  template:
+    metadata:
+      labels:
+        app: rekor-postgres
+    spec:
+      containers:
+        - name: postgres
+          image: postgres:15
+          env:
+            - name: POSTGRES_USER
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-user
+            - name: POSTGRES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-password
+          ports:
+            - containerPort: 5432
+          volumeMounts:
+            - name: pgdata
+              mountPath: /var/lib/postgresql/data
+  volumeClaimTemplates:
+    - metadata:
+        name: pgdata
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 10Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  ports:
+    - port: 5432
+      targetPort: 5432
+  selector:
+    app: rekor-postgres
+
+#
+# Notes:
+# - Replace the postgres-password secret value before applying.
+# - Consider running Postgres in HA (Patroni) for production; this is a minimal durable deployment using PVC.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/rekor-deployment-postgres.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          env:
+            - name: REKOR_STORAGE_TYPE
+              value: "postgres"
+            - name: REKOR_DATABASE_URL
+              value: "postgres://$(REKOR_USER):$(REKOR_PASSWORD)@rekor-postgres.aegis.svc:5432/rekordb"
+            - name: REKOR_API_PORT
+              value: "3000"
+          ports:
+            - containerPort: 3000
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  selector:
+    app: rekor
+  ports:
+    - name: http
+      port: 3000
+      targetPort: 3000
+
+#
+# Notes:
+# - Ensure a 'rekordb' database exists; provide REKOR_USER / REKOR_PASSWORD via Kubernetes Secret or envFrom.
+# - Add readiness/liveness probes and TLS termination via an ingress or LB in front of this service.
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: signing-proxy-config
+  namespace: aegis
+data:
+  proxy.conf: |
+    # Signing-proxy configuration (scaffold)
+    # - require_client_cert: whether mTLS client cert required
+    require_client_cert: "true"
+    # - vault_addr: endpoint to call to request ephemeral signing tokens
+    vault_addr: "https://vault.aegis.svc:8200"
+    # - rekor_url: where to push transparency entries after signing
+    rekor_url: "https://rekor.aegis.svc:3000"
+    # - audit_bucket: evidence bucket to upload audit trails
+    audit_bucket: "${EVIDENCE_BUCKET}"
+    # - allowed_client_cn_prefix: only client cert CNs with this prefix are permitted (tune for your org)
+    allowed_client_cn_prefix: "cn=ci-runner-"
+    # - signing_command: placeholder indicating signing backend (hsm|cosign-kms)
+    signing_backend: "cosign-kms"
+
+#
+# Implementation: the signing-proxy container should validate client cert CN, map to Vault role
+# and request an ephemeral auth/signing operation. All requests and responses must be logged and
+# uploaded to audit_bucket and mirrored to Rekor.
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/service-and-networkpolicy.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  selector:
+    app: signing-proxy
+  ports:
+    - name: https
+      port: 8443
+      targetPort: 8443
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: signing-proxy-restrict
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: signing-proxy
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: cicd
+        - podSelector:
+            matchLabels:
+              app: gitlab-runner
+      ports:
+        - protocol: TCP
+          port: 8443
+  policyTypes:
+    - Ingress
+
+#
+# Notes: restrict ingress to CI/CD namespaces and known runners; update selectors to match your environment.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ci/signing_proxy_client.py
+#!/usr/bin/env python3
+"""
+Small cosign-signing proxy client using mTLS to request a signature from signing-proxy.
+This client uses a local client cert and key (provided via CI secrets) to authenticate.
+The signing-proxy is expected to return a cosign-compatible signature payload which the client
+will then (optionally) push to Rekor or attach to the artifact.
+"""
+import argparse, requests, json, sys
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--proxy", required=True, help="https://signing-proxy:8443")
+    p.add_argument("--artifact", required=True, help="image:tag or path to manifest")
+    p.add_argument("--client-cert", default="/tmp/client.crt")
+    p.add_argument("--client-key", default="/tmp/client.key")
+    p.add_argument("--ca-cert", default="/tmp/ca.crt")
+    args = p.parse_args()
+    url = args.proxy.rstrip("/") + "/sign"
+    payload = {"artifact": args.artifact}
+    resp = requests.post(url, json=payload, cert=(args.client_cert, args.client_key), verify=args.ca_cert, timeout=60)
+    if resp.status_code != 200:
+        print("Error from signing-proxy:", resp.status_code, resp.text)
+        sys.exit(2)
+    print("Signing response:", resp.text)
+    # Write response to stdout / file for CI to attach
+    open("/tmp/sign_res.json","w").write(resp.text)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/enable-audit-to-s3-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: vault-audit-upload
+  namespace: aegis
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: vault
+          containers:
+            - name: upload
+              image: amazon/aws-cli:latest
+              env:
+                - name: EVIDENCE_BUCKET
+                  value: "${EVIDENCE_BUCKET}"
+              command:
+                - sh
+                - -c
+                - |
+                  set -e
+                  # This assumes Vault is configured to write audit logs to /vault/logs/ ; operator must mount that path
+                  if [ -d /vault/logs ]; then
+                    aws s3 cp --recursive /vault/logs s3://${EVIDENCE_BUCKET}/vault-audit/ || true
+                    find /vault/logs -type f -mtime +7 -delete || true
+                  else
+                    echo "No audit logs found"
+                  fi
+              volumeMounts:
+                - name: vault-audit
+                  mountPath: /vault/logs
+          restartPolicy: OnFailure
+          volumes:
+            - name: vault-audit
+              emptyDir: {}
+
+#
+# Notes: Prefer configuring Vault audit device (s3, socket) natively; this CronJob is a fallback uploader for audit files.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/vault-autounseal-helper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper to patch Helm values with KMS ARN for auto-unseal and install Vault via Helm in HA mode (raft)
+KMS_ARN=${1:-}
+if [ -z "$KMS_ARN" ]; then
+  echo "Usage: $0 <kms-key-arn>"
+  exit 2
+fi
+cat > /tmp/vault_values_override.yaml <<EOF
+unsealer:
+  aws:
+    enabled: true
+    region: us-west-2
+    kmsKeyId: "${KMS_ARN}"
+s3:
+  enabled: false
+server:
+  raft:
+    enabled: true
+tls:
+  enabled: true
+EOF
+
+echo "Installing/upgrading Vault Helm with KMS auto-unseal (override file: /tmp/vault_values_override.yaml)"
+helm upgrade --install vault hashicorp/vault -n aegis -f deploy/vault/ha-values-raft.yaml -f /tmp/vault_values_override.yaml
+echo "Vault helm applied; run deploy/vault/init-vault-job.yaml to initialize if needed."
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/automation/collect_false_positives.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS=${1:-aegis-staging}
+OUT=/tmp/gatekeeper_dryrun_violations_$(date +%s).json
+echo "Collecting Gatekeeper audit reports for namespace ${NS}"
+kubectl get audit -n gatekeeper-system -o json > /tmp/gatekeeper_audit_all.json || true
+kubectl get constraint -A -o json > /tmp/gatekeeper_constraints.json || true
+# This is a simple collector; in production query Gatekeeper Audit API / OPA reporting endpoints
+echo "Saved raw Gatekeeper audit and constraint outputs to /tmp"
+echo "Tip: analyze /tmp/gatekeeper_audit_all.json to detect expected false positives and tune rules"
+echo "Output: ${OUT}"
+
*** End Patch
*** Begin Patch
*** Add File:falco/sidekick/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: falco-sidekick
+  namespace: kube-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: falco-sidekick
+  template:
+    metadata:
+      labels:
+        app: falco-sidekick
+    spec:
+      containers:
+        - name: sidekick
+          image: falcosecurity/falco-sidekick:latest
+          env:
+            - name: DRIVER
+              value: "elasticsearch"
+            - name: ELASTICSEARCH_URL
+              value: "https://elasticsearch.logging.svc:9200" # replace with your ES endpoint
+            - name: ELASTICSEARCH_USER
+              valueFrom:
+                secretKeyRef:
+                  name: elastic-credentials
+                  key: user
+            - name: ELASTICSEARCH_PASS
+              valueFrom:
+                secretKeyRef:
+                  name: elastic-credentials
+                  key: password
+          ports:
+            - containerPort: 8080
+
+# Notes:
+# - falco-sidekick forwards alerts from Falco to Elastic / Slack / PagerDuty etc.
+# - Configure Falco to send alerts to sidekick's webhook.
+
*** End Patch
*** Begin Patch
*** Add File:logging/fluentbit/output-elastic-values.yaml
+service:
+  type: ClusterIP
+  port: 9200
+output:
+  elastic:
+    host: "elasticsearch.logging.svc"
+    port: 9200
+    tls: true
+    user_secret_name: "elastic-credentials"
+    user_secret_key: "password"
+notes: |
+  - Ensure Elastic credentials are stored in the 'elastic-credentials' secret in the logging namespace.
+  - TLS should be verified in production; set tls.verify true and mount CA certs.
+
*** End Patch
*** Begin Patch
*** Add File:backups/velero/iam-policy-snapshots.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Action": [
+        "ec2:CreateSnapshot",
+        "ec2:DeleteSnapshot",
+        "ec2:DescribeSnapshots",
+        "ec2:CreateVolume",
+        "ec2:AttachVolume",
+        "ec2:DescribeVolumes"
+      ],
+      "Resource": "*"
+    },
+    {
+      "Effect": "Allow",
+      "Action": [
+        "s3:ListBucket",
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject"
+      ],
+      "Resource": [
+        "arn:aws:s3:::aegis-velero-backups",
+        "arn:aws:s3:::aegis-velero-backups/*"
+      ]
+    }
+  ]
+}
+
+#
+# Notes: attach this policy to the Velero IAM role used by the cluster to allow snapshot operations
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ops/rekor_backup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUT=/tmp/rekor_backup_$(date +%Y%m%d%H%M%S).sql
+PG_SERVICE_HOST=${PG_SERVICE_HOST:-rekor-postgres.aegis.svc}
+PG_USER=${PG_USER:-rekor}
+PG_PASS=${PG_PASS:-REPLACE_WITH_SECRET}
+PG_DB=${PG_DB:-rekordb}
+export PGPASSWORD=$PG_PASS
+echo "Dumping Rekor Postgres DB to ${OUT}"
+pg_dump -h "${PG_SERVICE_HOST}" -U "${PG_USER}" -d "${PG_DB}" -F c -f "${OUT}"
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  aws s3 cp "${OUT}" "s3://${EVIDENCE_BUCKET}/rekor-backups/${OUT##*/}"
+  echo "Uploaded to s3://${EVIDENCE_BUCKET}/rekor-backups/"
+else
+  echo "EVIDENCE_BUCKET not set; local backup at ${OUT}"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ops/rotate_signing_keys.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Scaffold: rotate signing keys by requesting new key material from HSM or KMS and update Rekor & cosign policies.
+# Operator must implement HSM vendor-specific key rotation steps. This script demonstrates high-level flow.
+
+echo "Starting signing keys rotation scaffold"
+echo "1) Request new key/certificate issuance from Vault PKI or HSM (operator to implement)"
+echo "2) Update cosign usage to point to new KMS key or new HSM key identifier"
+echo "3) Re-sign critical artifacts or create attestations noting rotation event"
+echo "4) Archive previous key identifiers into EVIDENCE_BUCKET for audit"
+
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  echo "{\"rotation_time\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"note\":\"rotate keys - operator implement steps\"}" > /tmp/signing_rotation.json
+  aws s3 cp /tmp/signing_rotation.json "s3://${EVIDENCE_BUCKET}/signing-rotations/$(date +%s).json" || true
+  echo "Recorded rotation placeholder to evidence bucket"
+fi
+
+echo "Rotation scaffold complete. Implement vendor HSM steps and replace placeholder logic."
+
*** End Patch
*** Begin Patch
*** Add File:docs/HARDENING_SUMMARY.md
+# Aegis Hardening Patch â€” Summary
+
+What this patch delivers (high level)
+- Rekor: Postgres-backed deployment (statefulset + service) for transparency log HA and durability.
+- Signing-proxy: configuration & network policy examples to require mTLS clients and forward attestations to Rekor and audit store.
+- Signing-proxy client: CI-friendly mTLS client script (signing_proxy_client.py) and GitHub Action template integration (earlier patch).
+- Vault: audit uploader CronJob, helper to install Vault with KMS auto-unseal, cert-manager CSR/certificate guidance.
+- Gatekeeper: automation helpers to collect dry-run violations and promote constraints to enforcement after tuning.
+- Falco: sidekick deployment to reliably forward alerts to Elastic (or other SIEM) and suppression examples to reduce false positives.
+- Logging: Fluent Bit elastic forwarding configuration values example.
+- Velero: IAM policy for PV snapshot support and helper scripts to create credentials and run restore drills.
+- Rekor & Vault backups: scripts to backup Rekor DB to evidence bucket and rotate signing keys (scaffold).
+
+Operator action required (short list)
+- Fill secrets (DB password, Vault root storage), provide KMS ARN, provision TLS certs or enable cert-manager, and provide Elastic/Velero credentials.
+- Implement HSM vendor-specific integration inside signing-proxy or use cosign + KMS with proper IAM.
+- Tune Gatekeeper constraints in staging using the provided dry-run helpers before flipping to enforce.
+- Tune Falco suppressions to your workload to reduce noise.
+- Run the restore drills and validate evidence preserved in EVIDENCE_BUCKET (object-lock recommended).
+
+Next steps I can automate for you
+- Automate Rekor Postgres initialization and backup/restore via Argo workflow.
+- Implement signing-proxy HSM adapter for a specific HSM vendor (provide vendor SDK).
+- Create Terraform modules that also produce Helm releases for Vault/Cert-Manager/Signing-Proxy and wire outputs into GitHub Secrets.
+
*** End Patch
*** End Patch
