*** Begin Patch
*** Add File:infra/milvus/values-production.yaml
+# Production values for Milvus Helm chart (example)
+# Replace placeholders and tune for your environment (storage class, resource sizes, replicas).
+cluster:
+  enabled: true
+  mode: cluster
+
+meta:
+  pod:
+    resources:
+      limits:
+        cpu: "4"
+        memory: "8Gi"
+      requests:
+        cpu: "1"
+        memory: "2Gi"
+
+proxy:
+  replicaCount: 2
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "2"
+      memory: "4Gi"
+
+data:
+  wal:
+    enabled: true
+  storage:
+    type: persistentVolumeClaim
+    persistentVolumeClaim:
+      accessModes: [ "ReadWriteOnce" ]
+      resources:
+        requests:
+          storage: 1000Gi
+      storageClassName: "REPLACE_WITH_STORAGECLASS"
+
+indexer:
+  resources:
+    requests:
+      cpu: "2"
+      memory: "16Gi"
+    limits:
+      cpu: "8"
+      memory: "64Gi"
+
+persistence:
+  enabled: true
+
+service:
+  type: ClusterIP
+
+securityContext:
+  runAsUser: 1000
+  runAsGroup: 1000
+
+metrics:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File:infra/milvus/pvc-production-example.yaml
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: milvus-data-pvc
+  namespace: milvus
+spec:
+  accessModes:
+    - ReadWriteOnce
+  resources:
+    requests:
+      storage: 1000Gi
+  storageClassName: "REPLACE_WITH_STORAGECLASS"
+
*** End Patch
*** Begin Patch
*** Add File:infra/redis/values-production.yaml
+# Production values for Redis (Helm) - use provider chart (bitnami/redis or your managed Redis)
+replica:
+  replicaCount: 2
+metrics:
+  enabled: true
+resources:
+  limits:
+    cpu: "2"
+    memory: "8Gi"
+  requests:
+    cpu: "500m"
+    memory: "1Gi"
+
*** End Patch
*** Begin Patch
*** Add File:infra/mlflow/values-production.yaml
+# Example production values for MLflow (chart or simple Deployment values)
+server:
+  image:
+    repository: ghcr.io/yourorg/mlflow
+    tag: latest
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "2"
+      memory: "4Gi"
+ui:
+  enabled: true
+backend:
+  store:
+    type: s3
+    s3_bucket: "REPLACE_WITH_MLFLOW_BUCKET"
+    s3_region: "REPLACE_WITH_AWS_REGION"
+
*** End Patch
*** Begin Patch
*** Add File:infra/gpu/nodegroup_eks.tf
+/*
+ Example Terraform snippet (AWS) for EKS GPU node group with taints and labels.
+ Replace placeholders with your org values.
+*/
+resource "aws_eks_node_group" "gpu_nodes" {
+  cluster_name    = var.cluster_name
+  node_group_name = "aegis-gpu-nodes"
+  node_role_arn   = var.node_role_arn
+  subnet_ids      = var.private_subnets
+
+  scaling_config {
+    desired_size = 2
+    max_size     = 10
+    min_size     = 1
+  }
+
+  instance_types = ["p4d.24xlarge"] # choose appropriate instance types
+
+  labels = {
+    "aegis.workload" = "gpu"
+    "purpose" = "llm"
+  }
+
+  taints = [{
+    key    = "aegis/gpu"
+    value  = "true"
+    effect = "NO_SCHEDULE"
+  }]
+
+  disk_size = 200
+  remote_access {
+    ec2_ssh_key = var.ssh_key_name
+  }
+
+  lifecycle {
+    create_before_destroy = true
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:infra/autoscaler/cluster-autoscaler-gpu.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: cluster-autoscaler
+  namespace: kube-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: cluster-autoscaler
+  template:
+    metadata:
+      labels:
+        app: cluster-autoscaler
+    spec:
+      serviceAccountName: cluster-autoscaler
+      containers:
+        - name: cluster-autoscaler
+          image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.3
+          command:
+            - ./cluster-autoscaler
+            - --cloud-provider=aws
+            - --nodes=0:20:aegis-gpu-autoscaling-group
+            - --skip-nodes-with-local-storage=false
+            - --scale-down-enabled=true
+            - --balance-similar-node-groups=true
+          resources:
+            requests:
+              cpu: 100m
+              memory: 300Mi
+
+---
+# Note: replace aegis-gpu-autoscaling-group with your ASG name created by TF
+
*** End Patch
*** Begin Patch
*** Add File:security/external-secrets/production-external-secrets.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: aegis-production-secrets
+  namespace: aegis
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: aws-secrets-manager
+    kind: SecretStore
+  target:
+    name: aegis-prod-secrets
+    creationPolicy: Owner
+  data:
+    - secretKey: OPENAI_API_KEY
+      remoteRef:
+        key: "aegis/prod/openai_key"
+    - secretKey: COSIGN_KMS_KEY_ARN
+      remoteRef:
+        key: "aegis/prod/cosign_kms_arn"
+    - secretKey: MLFLOW_TOKEN
+      remoteRef:
+        key: "aegis/prod/mlflow_token"
+
*** End Patch
*** Begin Patch
*** Add File:security/kms/cosign_kms.tf
+/*
+ Terraform snippet to create a KMS key for cosign and attach a restrictive policy allowing only specific principals.
+ Replace placeholders (account, principals).
+*/
+resource "aws_kms_key" "cosign" {
+  description             = "Cosign key for Aegis artifact signing"
+  deletion_window_in_days = 30
+  policy = jsonencode({
+    "Version": "2012-10-17",
+    "Id": "key-default-1",
+    "Statement": [
+      {
+        "Sid": "Allow use of the key",
+        "Effect": "Allow",
+        "Principal": {
+          "AWS": [
+            "arn:aws:iam::REPLACE_ACCOUNT:role/aegis-ci",
+            "arn:aws:iam::REPLACE_ACCOUNT:role/aegis-security"
+          ]
+        },
+        "Action": [
+          "kms:Encrypt",
+          "kms:Decrypt",
+          "kms:Sign",
+          "kms:Verify",
+          "kms:GenerateDataKey*",
+          "kms:DescribeKey"
+        ],
+        "Resource": "*"
+      }
+    ]
+  })
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign/rotate_cosign_key.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate cosign key in KMS and write a Rekor note (best-effort).
+#
+NEW_ALIAS=${1:-"alias/aegis-cosign-$(date -u +%Y%m%d%H%M%S)"}
+REKOR_URL=${REKOR_URL:-""}
+
+echo "Creating new KMS key alias: $NEW_ALIAS"
+# This script expects AWS CLI configured for operator
+KEY_ID=$(aws kms create-key --description "Rotated cosign key for aegis" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name "$NEW_ALIAS" --target-key-id "$KEY_ID"
+echo "New key created: $KEY_ID"
+
+# Update cosign config / secrets (operator step) - placeholder
+echo "Please update COSIGN_KMS_KEY_ARN in ExternalSecrets to awskms://$KEY_ID"
+
+# Optional Rekor entry (best-effort)
+if [ -n "$REKOR_URL" ]; then
+  echo "Posting Rekor note (best-effort)"
+  curl -sS -X POST -H "Content-Type: application/json" --data-binary "{\"note\":\"cosign rotated\",\"key_id\":\"$KEY_ID\",\"ts\":\"$(date -u +%s)\"}" "$REKOR_URL/api/v1/log/entries" || true
+fi
+
+echo "Rotation complete. Record old key rotation/retire policies manually."
+
*** End Patch
*** Begin Patch
*** Add File:security/certmanager/clusterissuer-corporate-ca.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: corporate-ca-issuer
+spec:
+  ca:
+    secretName: corporate-ca-secret
+
+# Note: Operators must create secret corporate-ca-secret in cert-manager namespace containing 'ca.crt' and 'ca.key'.
+
*** End Patch
*** Begin Patch
*** Add File:security/policy/pod-security-network-policy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: aegis-tool-runner-network-policy
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: tool-runner
+  policyTypes:
+    - Egress
+    - Ingress
+  egress:
+    - to:
+        - podSelector:
+            matchLabels:
+              app: milvus
+      ports:
+        - protocol: TCP
+          port: 19530
+    - to:
+        - ipBlock:
+            cidr: 10.0.0.0/8
+      ports:
+        - protocol: TCP
+          port: 443
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: aegis
+
+---
+# PodSecurity admission manifest (Kubernetes v1.25+ uses PodSecurity admission)
+apiVersion: policy/v1
+kind: PodSecurityPolicy
+metadata:
+  name: deprecated-psp-tool-runner
+  annotations:
+    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'runtime/default'
+spec:
+  privileged: false
+  allowPrivilegeEscalation: false
+  requiredDropCapabilities:
+    - ALL
+  volumes:
+    - 'configMap'
+    - 'secret'
+    - 'emptyDir'
+  runAsUser:
+    rule: 'MustRunAsNonRoot'
+
*** End Patch
*** Begin Patch
*** Add File:security/gatekeeper/constraint-seccomp.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8sseccomppolicy
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8SSeccompPolicy
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8sseccomppolicy
+        violation[{"msg": msg}] {
+          container := input.review.object.spec.template.spec.containers[_]
+          not container.securityContext
+          msg := sprintf("container %v must set securityContext", [container.name])
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8SSeccompPolicy
+metadata:
+  name: require-seccomp-tool-runner
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["aegis"]
+
*** End Patch
*** Begin Patch
*** Add File:security/iam/irsa-tool-runner.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: agent-tool-runner-sa
+  namespace: aegis
+  annotations:
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-tool-runner-role
+
+# Note: Create the IAM role with trust relationship for the EKS OIDC provider and give it minimal permissions (S3 put/get, STS assume for ephemeral creds).
+
*** End Patch
*** Begin Patch
*** Add File:safety/privacy/retention_and_hashing.py
+#!/usr/bin/env python3
+"""
+Prompt & image provenance helpers:
+ - compute SHA256 for prompts/images
+ - write provenance manifest to S3
+ - example TTL retention enforcement is deferred to S3 lifecycle policies
+"""
+import hashlib, json, time, os
+import boto3
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def hash_content_bytes(b):
+    return hashlib.sha256(b).hexdigest()
+
+def store_provenance(kind, identifier, content_bytes, meta):
+    h = hash_content_bytes(content_bytes)
+    key = f"provenance/{kind}/{identifier}_{h}.json"
+    payload = {"id": identifier, "hash": h, "meta": meta, "ts": int(time.time())}
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(payload).encode())
+    return key
+
*** End Patch
*** Begin Patch
*** Add File:ops/scale_tests/rag_vegeta_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Simple vegeta load test for RAG endpoint
+ENDPOINT=${1:-"http://rag-flask.aegis.svc.cluster.local/query"}
+DURATION=${2:-60s}
+RATE=${3:-50} # requests per second
+
+cat > targets.txt <<EOF
+POST $ENDPOINT
+Content-Type: application/json
+
+{"q":"what is the process for X?","top_k_text":3}
+EOF
+
+echo "Running vegeta attack to $ENDPOINT at $RATE rps for $DURATION"
+vegeta attack -targets=targets.txt -rate=$RATE -duration=$DURATION | vegeta report
+
*** End Patch
*** Begin Patch
*** Add File:ops/scale_tests/milvus_bulk_index.py
+#!/usr/bin/env python3
+"""
+Milvus bulk indexing performance test: batch insert synthetic vectors and measure throughput.
+"""
+import os, time, numpy as np
+from pymilvus import connections, Collection
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "milvus.milvus.svc.cluster.local")
+MILVUS_PORT = os.environ.get("MILVUS_PORT", "19530")
+COLLECTION = os.environ.get("MULTI_COLLECTION", "aegis_multimodal")
+
+connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+coll = Collection(COLLECTION)
+
+def batch_insert(n=10000, dim=512, batch=1000):
+    import uuid
+    start = time.time()
+    for i in range(0, n, batch):
+        count = min(batch, n - i)
+        ids = [str(uuid.uuid4()) for _ in range(count)]
+        vecs = np.random.random((count, dim)).astype("float32").tolist()
+        metas = ['{}' for _ in range(count)]
+        coll.insert([ids, ["image"]*count, vecs, ["image/jpeg"]*count, [int(time.time())]*count, ids, metas])
+    coll.flush()
+    elapsed = time.time() - start
+    print(f"Inserted {n} vectors in {elapsed:.2f}s ({n/elapsed:.2f} vecs/s)")
+
+if __name__ == "__main__":
+    batch_insert(n=int(os.environ.get("MLV_INSERT", "10000")), dim=int(os.environ.get("MLV_DIM","512")), batch=int(os.environ.get("MLV_BATCH","1000")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/scale_tests/deepspeed_multinode_launcher.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example launcher for distributed deepspeed training across k8s/Slurm/HPC.
+# This is a placeholder: in production you will use k8s operator (kubeflow/tf-operator) or Slurm adapter.
+#
+NUM_NODES=${1:-2}
+GPUS_PER_NODE=${2:-8}
+TRAIN_SCRIPT=${3:-"deepspeed_accelerate_finetune.py"}
+
+echo "Launching deepspeed on ${NUM_NODES} nodes x ${GPUS_PER_NODE} gpus"
+# Example with torchrun (when nodes reachable via network)
+WORLD_SIZE=$((NUM_NODES * GPUS_PER_NODE))
+torchrun --nproc_per_node=${GPUS_PER_NODE} --nnodes=${NUM_NODES} --rdzv_backend=c10d --rdzv_endpoint=${RDZV_HOST:-localhost}:29500 ${TRAIN_SCRIPT}
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/alertmanager/alertmanager-config.yaml
+global:
+  resolve_timeout: 5m
+route:
+  receiver: 'slack-or-pagerduty'
+  group_wait: 30s
+  group_interval: 5m
+  repeat_interval: 1h
+receivers:
+  - name: 'slack-or-pagerduty'
+    slack_configs:
+      - api_url: 'https://hooks.slack.com/services/REPLACE/SLACK/WEBHOOK'
+        channel: '#aegis-alerts'
+    pagerduty_configs:
+      - service_key: 'REPLACE_PAGERDUTY_SERVICE_KEY'
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana/dashboards/agent_operations.json
+{
+  "dashboard": {
+    "title": "Aegis Agent Operations (production)",
+    "panels": [
+      {"title":"Agent Runs (1m)","type":"graph","targets":[{"expr":"rate(aegis_agent_runs_total[1m])"}]},
+      {"title":"Agent Actions by Tool","type":"table","targets":[{"expr":"sum(rate(aegis_agent_actions_total[1m])) by (tool)"}]},
+      {"title":"Action Latency p95 (s)","type":"graph","targets":[{"expr":"histogram_quantile(0.95, sum(rate(aegis_agent_action_latency_seconds_bucket[5m])) by (le))"}]}
+    ],
+    "schemaVersion": 16
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/anomaly/agent_behavior_detector.py
+#!/usr/bin/env python3
+"""
+Simple anomaly detector for agent behavior:
+ - Pulls recent action counts from Prometheus and raises an alert if distribution deviates from baseline.
+ - Placeholder: in production use ML-based detection or existing monitoring tools (Prometheus rules/ML).
+"""
+import os, time, requests, statistics
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.monitoring.svc.cluster.local:9090")
+THRESHOLD_MULTIPLIER = float(os.environ.get("ANOMALY_MULT", "3.0"))
+
+def query_prom(query):
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": query}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def run_check():
+    q = 'sum(rate(aegis_agent_actions_total[5m]))'
+    res = query_prom(q)
+    value = float(res["data"]["result"][0]["value"][1]) if res["data"]["result"] else 0.0
+    # naive baseline (load baseline from file or metric)
+    baseline = float(os.environ.get("AGENT_BASELINE", "1.0"))
+    if value > baseline * THRESHOLD_MULTIPLIER:
+        print("ANOMALY detected: value", value, "baseline", baseline)
+        # In production, send to Alertmanager or PagerDuty
+    else:
+        print("OK:", value)
+
+if __name__ == "__main__":
+    run_check()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/telemetry/telemetry_collector_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-telemetry-collector
+  namespace: monitoring
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: telemetry-collector-sa
+          containers:
+            - name: collector
+              image: ghcr.io/yourorg/aegis-telemetry-collector:latest
+              env:
+                - name: EVIDENCE_BUCKET
+                  value: "REPLACE_WITH_EVIDENCE_BUCKET"
+                - name: MLFLOW_TRACKING_URI
+                  value: "http://mlflow.aegis.svc.cluster.local"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:compliance/calibration/calibration_runner_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-calibration-runner
+  namespace: aegis
+spec:
+  schedule: "0 2 * * *" # nightly calibration run
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-calibration-sa
+          containers:
+            - name: calibrator
+              image: ghcr.io/yourorg/aegis-calibrator:latest
+              env:
+                - name: EVIDENCE_BUCKET
+                  value: "REPLACE_WITH_EVIDENCE_BUCKET"
+                - name: AWS_REGION
+                  value: "REPLACE_WITH_AWS_REGION"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:compliance/audit/audit_package_collector.py
+#!/usr/bin/env python3
+"""
+Collect audit evidence for a model/agent run into a single package, sign it with cosign and optionally post to Rekor.
+ - Gathers artifacts from S3 prefixes
+ - Builds a tarball with metadata and uploads to evidence bucket under audit-packages/
+ - Signs tarball using cosign (awskms) and posts small manifest to Rekor (best-effort)
+"""
+import os, tempfile, tarfile, json, time, subprocess, boto3
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN", "")
+REKOR_URL = os.environ.get("REKOR_URL", "")
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def collect_prefix_to_tar(s3_prefix, out_tar):
+    # List objects under prefix and download to temp dir
+    tmpdir = tempfile.mkdtemp()
+    paginator = s3.get_paginator("list_objects_v2")
+    files = []
+    for page in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=s3_prefix):
+        for o in page.get("Contents", []):
+            key = o["Key"]
+            local = os.path.join(tmpdir, os.path.basename(key))
+            s3.download_file(EVIDENCE_BUCKET, key, local)
+            files.append(local)
+    # Create tar
+    with tarfile.open(out_tar, "w:gz") as tar:
+        for f in files:
+            tar.add(f, arcname=os.path.basename(f))
+    return out_tar
+
+def create_audit_package(prefixes, run_id):
+    manifest = {"run_id": run_id, "prefixes": prefixes, "ts": int(time.time())}
+    tmp = tempfile.mktemp(suffix=".tar.gz")
+    # collect each prefix into the tar
+    # For simplicity, gather into a folder and tar it
+    tarmain = tempfile.mktemp(suffix=".tar.gz")
+    collect_prefix_to_tar("", tarmain)  # no-op placeholder; operators should implement
+    # Upload tar to S3
+    key = f"audit-packages/{run_id}_package_{int(time.time())}.tar.gz"
+    s3.upload_file(tarmain, EVIDENCE_BUCKET, key)
+    print("Uploaded audit package to s3://%s/%s" % (EVIDENCE_BUCKET, key))
+    # Sign with cosign if available
+    if COSIGN_KMS:
+        try:
+            subprocess.run(["cosign","sign","--key",f"awskms://{COSIGN_KMS}", tarmain], check=True)
+            s3.upload_file(tarmain + ".sig", EVIDENCE_BUCKET, key + ".sig")
+        except Exception as e:
+            print("cosign sign failed:", e)
+    # Rekor (best-effort)
+    if REKOR_URL:
+        try:
+            requests.post(f"{REKOR_URL}/api/v1/log/entries", json={"run_id": run_id, "s3": f"s3://{EVIDENCE_BUCKET}/{key}"}, timeout=5)
+        except Exception as e:
+            print("rekor post failed:", e)
+    return key
+
+if __name__ == "__main__":
+    import sys
+    run_id = sys.argv[1] if len(sys.argv)>1 else "manual"
+    create_audit_package(["agents/"+run_id,"llm/checkpoints/"+run_id], run_id)
+
*** End Patch
*** End Patch
