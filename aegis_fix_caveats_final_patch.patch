*** Begin Patch
*** Add File:k8s/feature_server/feature-server-deployment.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: feature-server-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: feature-server-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: feature-server-rolebinding
+  namespace: aegis
+subjects:
+  - kind: ServiceAccount
+    name: feature-server-sa
+    namespace: aegis
+roleRef:
+  kind: Role
+  name: feature-server-role
+  apiGroup: rbac.authorization.k8s.io
+
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: feature-server
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: feature-server
+  template:
+    metadata:
+      labels:
+        app: feature-server
+    spec:
+      serviceAccountName: feature-server-sa
+      containers:
+        - name: feature-server
+          image: ghcr.io/yourorg/aegis-feature-server:latest
+          env:
+            - name: FEAST_REDIS_HOST
+              value: "feast-redis.aegis.svc.cluster.local"
+            - name: FEAST_REDIS_PORT
+              value: "6379"
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: cosign-kms
+                  key: EVIDENCE_BUCKET
+          ports:
+            - containerPort: 8080
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+            limits:
+              cpu: "1"
+              memory: "1Gi"
+
+---
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: feature-server-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: feature-server
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: feature-server-allow-from-internal
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: feature-server
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              role: internal
+        - podSelector:
+            matchLabels:
+              app: model-runtime
+      ports:
+        - protocol: TCP
+          port: 8080
+
+---
+# Note: Protect this Service with proper RBAC and network rules. The ExternalSecrets controller should populate `cosign-kms` secret key(s).
+
*** End Patch
*** Begin Patch
*** Add File:k8s/serving/model-runtime-hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: model-runtime-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: model-runtime
+  minReplicas: 2
+  maxReplicas: 20
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 70
+    - type: Resource
+      resource:
+        name: memory
+        target:
+          type: Utilization
+          averageUtilization: 75
+
*** End Patch
*** Begin Patch
*** Add File:tools/promote_to_prod.py
+#!/usr/bin/env python3
+"""
+Promotion helper: validate signed calibration report exists, verify signature with cosign, then apply production ConfigMap.
+Usage:
+  export KUBECONFIG_PROD=/path/to/prod/kubeconfig
+  python3 tools/promote_to_prod.py --s3-report s3://bucket/calibration/approved/validation_report.json --configmap-name aegis-power-profiles --config-file ./power_profiles.yaml
+"""
+import argparse, subprocess, tempfile, os, sys
+import boto3
+
+def download_s3(uri):
+    s3 = boto3.client("s3")
+    parts = uri[len("s3://"):].split("/",1)
+    bucket, key = parts[0], parts[1]
+    tmp = tempfile.mktemp()
+    s3.download_file(bucket, key, tmp)
+    return tmp
+
+def verify_cosign(local_path, kms_arn):
+    try:
+        subprocess.run(["cosign","verify","--key",f"awskms://{kms_arn}", local_path], check=True)
+        return True
+    except subprocess.CalledProcessError:
+        return False
+
+def apply_configmap(kubeconfig, name, file_path, namespace="aegis"):
+    cmd = ["kubectl","--kubeconfig",kubeconfig,"-n",namespace,"create","configmap",name,"--from-file="+file_path,"--dry-run=client","-o","yaml"]
+    out = subprocess.check_output(cmd)
+    apply = subprocess.Popen(["kubectl","--kubeconfig",kubeconfig,"apply","-f","-"], stdin=subprocess.PIPE)
+    apply.communicate(input=out)
+    return apply.returncode == 0
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--s3-report", required=True)
+    p.add_argument("--kms-arn", required=True)
+    p.add_argument("--configmap-name", required=True)
+    p.add_argument("--config-file", required=True)
+    p.add_argument("--kubeconfig", default=os.environ.get("KUBECONFIG_PROD"))
+    args = p.parse_args()
+    if not args.kubeconfig:
+        print("KUBECONFIG not provided", file=sys.stderr); sys.exit(2)
+    # download signed report
+    local = download_s3(args.s3_report)
+    # expect cosign signature file at same path + .sig
+    sig_local = local + ".sig"
+    if not os.path.exists(sig_local):
+        # try download .sig from s3
+        s3 = boto3.client("s3")
+        parts = args.s3_report[len("s3://"):].split("/",1)
+        bucket, key = parts[0], parts[1] + ".sig"
+        try:
+            s3.download_file(bucket, key, sig_local)
+        except Exception as e:
+            print("Signature not found in S3", e); sys.exit(3)
+    # verify
+    if not verify_cosign(local, args.kms_arn):
+        print("cosign verification failed"); sys.exit(4)
+    print("Signature verified - applying ConfigMap to production")
+    ok = apply_configmap(args.kubeconfig, args.configmap_name, args.config_file)
+    if not ok:
+        print("Apply failed"); sys.exit(5)
+    print("Promotion complete. Record promotion artifact in S3 manually for audit.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/apply_parquet_tuning.py
+#!/usr/bin/env python3
+"""
+Apply parquet tuning recommendation by creating/updating a Kubernetes ConfigMap used by ETL jobs.
+Reads a tuning JSON from s3://<bucket>/parquet/tuning/recommendation_*.json and applies as ConfigMap `aegis-parquet-config` in namespace `aegis`.
+"""
+import boto3, json, glob, tempfile, subprocess, os
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION","us-west-2")
+NAMESPACE = os.environ.get("NAMESPACE","aegis")
+
+def latest_tuning(s3):
+    prefix = "parquet/tuning/"
+    objs = s3.list_objects_v2(Bucket=EVIDENCE_BUCKET, Prefix=prefix)
+    if "Contents" not in objs:
+        return None
+    latest = max(objs["Contents"], key=lambda x: x["LastModified"])
+    key = latest["Key"]
+    obj = s3.get_object(Bucket=EVIDENCE_BUCKET, Key=key)
+    return json.loads(obj["Body"].read().decode()), key
+
+def apply_configmap(cm_name, data):
+    tmp = tempfile.mktemp(suffix=".json")
+    with open(tmp,"w") as f:
+        json.dump(data, f)
+    # use kubectl to create/update configmap from file
+    cmd = ["kubectl","-n",NAMESPACE,"create","configmap",cm_name,"--from-file=parquet_tuning.json="+tmp,"--dry-run=client","-o","yaml"]
+    out = subprocess.check_output(cmd)
+    apply = subprocess.Popen(["kubectl","-n",NAMESPACE,"apply","-f","-"], stdin=subprocess.PIPE)
+    apply.communicate(input=out)
+    return apply.returncode == 0
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET required")
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    data, key = latest_tuning(s3)
+    if not data:
+        print("No tuning recommendations found"); return
+    print("Applying tuning from", key)
+    ok = apply_configmap("aegis-parquet-config", data)
+    if ok:
+        print("Applied parquet tuning ConfigMap")
+    else:
+        print("Failed to apply ConfigMap")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:infra/cluster-autoscaler/cluster-autoscaler-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: cluster-autoscaler
+  namespace: kube-system
+  labels:
+    app: cluster-autoscaler
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: cluster-autoscaler
+  template:
+    metadata:
+      labels:
+        app: cluster-autoscaler
+    spec:
+      serviceAccountName: cluster-autoscaler
+      containers:
+        - name: cluster-autoscaler
+          image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.23.0
+          command:
+            - ./cluster-autoscaler
+            - --cloud-provider=aws
+            - --skip-nodes-with-local-storage=false
+            - --nodes=1:10:YOUR_ASG_NAME_HERE
+          resources:
+            requests:
+              cpu: 100m
+              memory: 300Mi
+
+---
+# NOTE: The deployment above is a template. Replace YOUR_ASG_NAME_HERE with the actual AutoScalingGroup name or use IAM roles + cloud provider configuration per your infra.
+
*** End Patch
*** Begin Patch
*** Add File:ci/scale_test_pipeline.yml
+name: Scale & Resilience Test Pipeline
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 1 * * 6' # weekly Saturday 01:00
+
+jobs:
+  load-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install deps
+        run: pip install boto3 requests redis
+      - name: Run Redis online store load test
+        env:
+          FEAST_REDIS_HOST: feast-redis.aegis.svc.cluster.local
+        run: |
+          python3 feature_store/redis_load_test.py --keys 50000 --batch 1000 || true
+      - name: Run Seldon load test (best-effort)
+        run: |
+          python3 scale_tests/seldon_load_test.py --endpoint "http://model-runtime.aegis.svc.cluster.local/predict" --concurrency 5 --reqs-per-worker 200 || true
+      - name: Run model endpoint vegeta (requires vegeta installed)
+        run: |
+          if command -v vegeta >/dev/null 2>&1; then
+            ./serving/load_test_vegeta.sh "http://model-runtime.aegis.svc.cluster.local/predict" 30s 100 || true
+          else
+            echo "vegeta not installed; skipping HTTP load test"
+          fi
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: scale-test-artifacts
+          path: |
+            . || true
+
*** End Patch
*** Begin Patch
*** Add File:ops/auto_run_datahub_and_superset_ownership.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Run DataHub owner mapper and Superset dataset ownership automation after dbt runs
+DBT_MANIFEST_PATH=${1:-dbt/target/manifest.json}
+export DBT_MANIFEST_PATH
+python3 datahub/datahub_owner_mapper.py || true
+python3 superset/superset_dataset_ownership.py || true
+echo "Owner mapping attempts complete (inspect logs)."
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/hpc_iot_prod_hardening_quickstart.md
+# Quickstart hardening checklist for HPC & IoT connectors (operator)
+
+HPC (Slurm)
+- Ensure Slurm head node is reachable from CI runner/jump host with least-privilege SSH keys.
+- Verify Slurm adapter uploads to S3 using instance role or STS token.
+- Add Slurm job conformance test to CI that runs connectors/slurm_adapter.py against a test partition.
+
+IoT (MQTT)
+- Place MQTT bridge behind ingress with TLS termination and mTLS for device authentication.
+- Configure RBAC so the mqtt-ingest service account only has S3 PutObject permissions for the iot/ prefix.
+- Run a device simulator to send 10k messages and validate S3 partitioning.
+
*** End Patch
*** End Patch
