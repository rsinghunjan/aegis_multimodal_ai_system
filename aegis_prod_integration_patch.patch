*** Begin Patch
*** Add File: k8s/serving/kserve_prod_full.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-model-prod
+  namespace: aegis
+  annotations:
+    # Knative/KServe tuning
+    autoscaling.knative.dev/target: "100"
+    serving.kserve.io/enable-batcher: "true"
+    serving.kserve.io/request-batch-size: "16"
+    serving.kserve.io/request-batch-max-delay: "50ms"
+    serving.kserve.io/request-batch-timeout: "50ms"
+spec:
+  predictor:
+    serviceAccountName: aegis-model-sa
+    timeout: 60s
+    container:
+      image: ghcr.io/yourorg/aegis-model-server:latest
+      name: model-server
+      args:
+        - "--model-store"
+        - "s3://MODEL_BUCKET/model-archives/aegis-model/"
+        - "--max-concurrency=16"
+        - "--enable-batching=true"
+        - "--batch-size=16"
+        - "--batch-timeout-ms=50"
+      env:
+        - name: S3_ENDPOINT
+          value: "https://s3.example"
+        - name: S3_BUCKET
+          value: "MODEL_BUCKET"
+      resources:
+        requests:
+          cpu: "1"
+          memory: "2Gi"
+        limits:
+          cpu: "4"
+          memory: "16Gi"
+          nvidia.com/gpu: "1"
+---
+# Minimal ServiceAccount + Role for model server to read secrets (S3 creds)
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-model-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: aegis-model-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: aegis-model-rolebinding
+  namespace: aegis
+subjects:
+  - kind: ServiceAccount
+    name: aegis-model-sa
+    namespace: aegis
+roleRef:
+  kind: Role
+  name: aegis-model-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ingress/istio_model_ingress.yaml
+apiVersion: networking.istio.io/v1beta1
+kind: Gateway
+metadata:
+  name: aegis-model-gateway
+  namespace: aegis
+spec:
+  selector:
+    istio: ingressgateway
+  servers:
+    - port:
+        number: 80
+        name: http
+        protocol: HTTP
+      hosts:
+        - "models.aegis.example"
+
+---
+apiVersion: networking.istio.io/v1beta1
+kind: VirtualService
+metadata:
+  name: aegis-model-vs
+  namespace: aegis
+spec:
+  hosts:
+    - "models.aegis.example"
+  gateways:
+    - aegis-model-gateway
+  http:
+    - name: "model-route"
+      match:
+        - uri:
+            prefix: "/v1/models"
+      route:
+        - destination:
+            host: aegis-model-predictor-default.aegis.svc.cluster.local
+            port:
+              number: 8080
+      timeout: 10s
+      retries:
+        attempts: 2
+        perTryTimeout: 2s
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/dcgm-exporter-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: dcgm-exporter
+  template:
+    metadata:
+      labels:
+        app: dcgm-exporter
+    spec:
+      hostPID: true
+      containers:
+      - name: dcgm-exporter
+        image: nvidia/dcgm-exporter:2.2.9-2.5.0-ubuntu20.04
+        ports:
+        - containerPort: 9400
+        resources:
+          requests:
+            cpu: 100m
+            memory: 128Mi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  ports:
+  - port: 9400
+  selector:
+    app: dcgm-exporter
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus-adapter-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: prometheus-adapter-config
+  namespace: monitoring
+data:
+  config.yaml: |
+    rules:
+      - seriesQuery: 'dcgm_gpu_metrics_utilization{job="dcgm-exporter"}'
+        resources:
+          overrides:
+            namespace:
+              resource: namespace
+        name:
+          as: "nvidia_gpu_utilization"
+        metricsQuery: 'avg(rate(dcgm_gpu_metrics_utilization[5m])) by (namespace, pod)'
+
+# NOTE:
+# - Adjust seriesQuery/metricsQuery to the exact metric names exported by your DCGM exporter.
+# - Deploy prometheus-adapter with this config to expose custom metrics for HPA.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/autoscale/gpu_hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-model-gpu-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-model-server
+  minReplicas: 1
+  maxReplicas: 20
+  metrics:
+  - type: Resource
+    resource:
+      name: cpu
+      target:
+        type: Utilization
+        averageUtilization: 70
+  - type: Pods
+    pods:
+      metric:
+        name: nvidia_gpu_utilization
+      target:
+        type: AverageValue
+        averageValue: "75"
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/metrics_exporter/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY monitoring/model_metrics_exporter_v2.py /app/model_metrics_exporter_v2.py
+RUN pip install --no-cache-dir prometheus_client flask requests
+EXPOSE 8000 8080
+CMD ["python", "model_metrics_exporter_v2.py"]
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/metrics_exporter/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-metrics-exporter
+  namespace: monitoring
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-metrics-exporter
+  template:
+    metadata:
+      labels:
+        app: aegis-metrics-exporter
+    spec:
+      containers:
+        - name: exporter
+          image: ghcr.io/yourorg/aegis-metrics-exporter:latest
+          ports:
+            - containerPort: 8000
+            - containerPort: 8080
+          env:
+            - name: PUSH_PORT
+              value: "8080"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-metrics-exporter
+  namespace: monitoring
+spec:
+  selector:
+    app: aegis-metrics-exporter
+  ports:
+    - port: 8000
+      name: prometheus
+      targetPort: 8000
+    - port: 8080
+      name: push
+      targetPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/servicemonitor_metrics_exporter.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: aegis-metrics-exporter-sm
+  namespace: monitoring
+spec:
+  selector:
+    matchLabels:
+      app: aegis-metrics-exporter
+  endpoints:
+    - port: prometheus
+      path: /metrics
+      interval: 15s
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/build_and_push_images.yml
+name: Build & push production images
+
+on:
+  workflow_dispatch:
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to GitHub Container Registry
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build & push metrics exporter
+        run: |
+          docker build -t ghcr.io/${{ github.repository_owner }}/aegis-metrics-exporter:latest -f monitoring/metrics_exporter/Dockerfile .
+          docker push ghcr.io/${{ github.repository_owner }}/aegis-metrics-exporter:latest
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/redis_statefulset.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  serviceName: "feast-redis"
+  replicas: 3
+  selector:
+    matchLabels:
+      app: feast-redis
+  template:
+    metadata:
+      labels:
+        app: feast-redis
+    spec:
+      containers:
+      - name: redis
+        image: bitnami/redis:6.2
+        ports:
+        - containerPort: 6379
+        resources:
+          requests:
+            memory: "256Mi"
+            cpu: "200m"
+        volumeMounts:
+        - name: data
+          mountPath: /data
+  volumeClaimTemplates:
+  - metadata:
+      name: data
+    spec:
+      accessModes: ["ReadWriteOnce"]
+      resources:
+        requests:
+          storage: 10Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  ports:
+  - port: 6379
+  clusterIP: None
+  selector:
+    app: feast-redis
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/redis_backup_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: redis-backup
+  namespace: feast
+spec:
+  schedule: "0 2 * * *" # daily at 02:00
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: backup
+              image: ghcr.io/yourorg/redis-backup:latest
+              env:
+                - name: REDIS_HOST
+                  value: "feast-redis.feast.svc.cluster.local"
+                - name: S3_BUCKET
+                  value: "feast-backups"
+              command: ["/bin/sh","-c"]
+              args:
+                - |
+                  redis-cli -h $REDIS_HOST --rdb /tmp/dump.rdb
+                  aws s3 cp /tmp/dump.rdb s3://$S3_BUCKET/redis/dump-$(date +%F).rdb
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/feast_secrets_and_sa.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: feast-s3-credentials
+  namespace: feast
+type: Opaque
+stringData:
+  AWS_ACCESS_KEY_ID: "<REPLACE>"
+  AWS_SECRET_ACCESS_KEY: "<REPLACE>"
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: feast-job-sa
+  namespace: feast
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: feast-job-role
+  namespace: feast
+rules:
+  - apiGroups: [""]
+    resources: ["pods","pods/exec","secrets","configmaps"]
+    verbs: ["get","list","create","update"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: feast-job-rolebinding
+  namespace: feast
+subjects:
+  - kind: ServiceAccount
+    name: feast-job-sa
+    namespace: feast
+roleRef:
+  kind: Role
+  name: feast-job-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File: scripts/get_validation_inputs.sh
+#!/usr/bin/env bash
+#
+# Fetch predictions and labels needed for validation into the working dir.
+# Usage: METRICS_BUCKET=... ./scripts/get_validation_inputs.sh
+set -euo pipefail
+
+METRICS_BUCKET="${METRICS_BUCKET:-}"
+OUT_DIR="${1:-/tmp/validation_inputs}"
+mkdir -p "$OUT_DIR"
+
+if [ -z "$METRICS_BUCKET" ]; then
+  echo "METRICS_BUCKET must be set" >&2
+  exit 2
+fi
+
+echo "Downloading production_recent.csv and labels.csv from s3://$METRICS_BUCKET/"
+aws s3 cp "s3://$METRICS_BUCKET/production_recent.csv" "$OUT_DIR/production_recent.csv"
+aws s3 cp "s3://$METRICS_BUCKET/validation_labels.csv" "$OUT_DIR/validation_labels.csv"
+echo "Wrote to $OUT_DIR"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/rotate_argo_governance_secrets.sh
+#!/usr/bin/env bash
+#
+# Rotate secrets used by Argo/Governance flows: pull from Vault and patch k8s secrets; minimal safe helper.
+#
+set -euo pipefail
+
+VAULT_PATH="${1:-secret/data/aegis/ci}"
+K8S_SECRET="${2:-aegis-ci-token}"
+NAMESPACE="${3:-aegis}"
+
+if ! command -v vault >/dev/null 2>&1; then
+  echo "vault CLI required" >&2
+  exit 2
+fi
+
+echo "Reading secrets from Vault path: $VAULT_PATH"
+TOKEN=$(vault kv get -field=token "$VAULT_PATH" || true)
+if [ -z "$TOKEN" ]; then
+  echo "No token found at $VAULT_PATH" >&2
+  exit 3
+fi
+
+kubectl -n "$NAMESPACE" create secret generic "$K8S_SECRET" --from-literal=token="$TOKEN" --dry-run=client -o yaml | kubectl apply -f -
+echo "Patched k8s secret $K8S_SECRET in namespace $NAMESPACE"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/katib_trial_output_standardize.py
+#!/usr/bin/env python3
+"""
+Standardize Katib trial outputs by writing a small JSON annotation with artifact S3 path.
+This helper runs as part of trial completion and writes a Kubernetes Annotation on the Trial object:
+  aegis.ai/artifact_s3: s3://bucket/path/to/artifact.tar.gz
+"""
+import os, sys, json, subprocess
+
+TRIAL_NAME = os.environ.get("TRIAL_NAME")
+ARTIFACT_S3 = os.environ.get("ARTIFACT_S3")
+
+if not TRIAL_NAME or not ARTIFACT_S3:
+    print("TRIAL_NAME and ARTIFACT_S3 env vars required")
+    sys.exit(2)
+
+cmd = ["kubectl","annotate","trial",TRIAL_NAME,"-n","aegis",f"aegis.ai/artifact_s3={ARTIFACT_S3}","--overwrite"]
+print("Running:", " ".join(cmd))
+subprocess.check_call(cmd)
+print("Annotated trial", TRIAL_NAME, "with", ARTIFACT_S3)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/dlq_worker.py
+#!/usr/bin/env python3
+"""
+Simple DLQ/QA worker for labeling pipeline.
+Consumes from Redis 'labels_failed' list and writes items to S3 for manual QA.
+"""
+import os, json, time
+import redis, boto3
+
+REDIS_URL = os.environ.get("REDIS_URL","redis://feast-redis:6379/0")
+BUCKET = os.environ.get("LABEL_QA_BUCKET","label-qa")
+
+r = redis.from_url(REDIS_URL)
+s3 = boto3.client("s3") if BUCKET else None
+
+def process(item):
+    data = json.loads(item)
+    key = f"qa/{data.get('id')}-{int(time.time())}.json"
+    if s3:
+        s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(data))
+        print("Wrote to s3://",BUCKET,key)
+    else:
+        with open("/tmp/"+key.replace("/","_"),"w") as fh:
+            json.dump(data, fh)
+        print("Wrote local /tmp/",key)
+
+def run():
+    while True:
+        item = r.rpop("labels_failed")
+        if not item:
+            time.sleep(5)
+            continue
+        try:
+            process(item)
+        except Exception as e:
+            print("Failed processing DLQ item:", e)
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: labeling/annotator_hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: annotator-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-annotator
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File: docs/PROD_SERVING_MONITORING_FEAST_README.md
+Production integration: serving, GPU autoscale, monitoring, Feast, retrain gating, explainability & labeling
+
+Summary
+- Deploy the KServe InferenceService (k8s/serving/kserve_prod_full.yaml) that enables batching & concurrency tuning.
+- Expose via Istio ingress (k8s/ingress/istio_model_ingress.yaml) for low-latency routing.
+- Collect GPU metrics using DCGM exporter (monitoring/dcgm-exporter-deployment.yaml), expose via prometheus-adapter (monitoring/prometheus-adapter-configmap.yaml), and autoscale using HPA (k8s/autoscale/gpu_hpa.yaml).
+- Package and deploy the metrics exporter (monitoring/metrics_exporter/Dockerfile + deployment + ServiceMonitor) and wire Prometheus/Alertmanager with PrometheusRule (monitoring/prometheus_rules_model_quality.yaml) and Grafana dashboard (monitoring/grafana_model_dashboard_enhanced.json).
+- Feast online store HA via feature_store/redis_statefulset.yaml and backups via feature_store/redis_backup_cronjob.yaml. Use feature_store/feast_secrets_and_sa.yaml for S3 auth and job SA.
+- Retrain gating: scripts/get_validation_inputs.sh fetches predictions/labels, validate_model_metrics_extended.py (existing) runs validation; Argo workflow (retrain_validation_explain.yaml previously added) runs drift → retrain → validate → explainability → package → attest → request approval. Use scripts/rotate_argo_governance_secrets.sh to rotate secrets pulled from Vault.
+- HPO: ensure Katib trials annotate artifact S3 path using scripts/katib_trial_output_standardize.py; katib handler can read annotation to promote best trial artifact.
+- Explainability & fairness: scripts/compute_explainability_and_fairness.py produces SHAP + fairness JSON; validation step gates on fairness delta <= 0.1. Use ci/fairness_promotion_gate.py (existing) in CI promotion pipeline.
+- Labeling: annotator deployment + HPA autoscale; DLQ & QA flow via scripts/dlq_worker.py; scale annotator and workers as needed; store labeled data in S3 for materialization.
+
+Next steps to run in a staging cluster
+1. Build and push images using .github/workflows/build_and_push_images.yml (or run locally).
+2. Apply manifests in order: namespaces (aegis, monitoring, feast), secrets (feature_store/feast_secrets_and_sa.yaml), Redis statefulset, DCGM exporter, prometheus-adapter, metrics exporter, ServiceMonitor, Prometheus rules, KServe InferenceService, Istio gateway, HPA.
+3. Configure Vault secrets and use scripts/rotate_argo_governance_secrets.sh to sync to K8s.
+4. Run a load test against the KServe endpoint, observe P95 latency in Grafana, tune batch size & concurrency.
+5. Run Argo retrain workflow in staging with realistic prediction & label datasets.
+
*** End Patch
*** End Patch
