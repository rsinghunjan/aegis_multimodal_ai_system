*** Begin Patch
*** Add File:security/falco/falco-rules.yaml
+## Falco rules tuned for Aegis agent/tool-runner behavior
+##
+## Note: deploy Falco as a DaemonSet with these rules loaded (see falco/deployment.yaml)
+##
+rules:
+  - rule: Tool Runner HostFiles Access
+    desc: Detect attempts by tool-runner pods to access host filesystem paths
+    condition: container and proc.name in (touch, sh, bash, python, curl, wget) and fd.name startswith /host
+    output: "Tool-runner attempting host filesystem access (user=%user.name container=%container.name pid=%proc.pid cmdline=%proc.cmdline fd=%fd.name)"
+    priority: WARNING
+    tags: [tool-runner,hostfile,exfiltration]
+
+  - rule: Tool Runner Unexpected Outbound
+    desc: Tool-runner making outbound connection to internet destinations not on allowlist
+    condition: container and outbound and (fd.sip != 10.0.0.0/8 and fd.sip != 172.16.0.0/12 and fd.sip != 192.168.0.0/16) and container.label.app in (tool-runner)
+    output: "Tool-runner outbound connection to external IP (container=%container.name user=%user.name ip=%fd.sip port=%fd.sport)"
+    priority: CRITICAL
+    tags: [egress,exfiltration]
+
+  - rule: Container Privilege Escalation
+    desc: Detect allowPrivilegeEscalation or privileged container behaviors
+    condition: container and open_write and proc.name in (su, sudo, setpriv) and container.privileged=true
+    output: "Privileged container performing suspicious write (container=%container.name pid=%proc.pid cmdline=%proc.cmdline)"
+    priority: CRITICAL
+    tags: [privilege,escape]
+
+  - rule: Suspicious Exec in Container
+    desc: Catch unexpected binary execs in tool-runner pods
+    condition: container and proc.name in (sh, bash, python, perl, ruby) and container.label.app in (tool-runner) and proc.cmdline contains "nc -l" or proc.cmdline contains "ssh"
+    output: "Suspicious command executed in tool-runner (container=%container.name cmd=%proc.cmdline user=%user.name)"
+    priority: CRITICAL
+    tags: [exec,tool-runner]
+
*** End Patch
*** Begin Patch
*** Add File:security/falco/deployment.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: falco
+  namespace: kube-system
+spec:
+  selector:
+    matchLabels:
+      app: falco
+  template:
+    metadata:
+      labels:
+        app: falco
+    spec:
+      serviceAccountName: falco
+      containers:
+        - name: falco
+          image: falcosecurity/falco:latest
+          securityContext:
+            privileged: true
+          volumeMounts:
+            - name: dev
+              mountPath: /host/dev
+              readOnly: true
+            - name: proc
+              mountPath: /host/proc
+              readOnly: true
+            - name: boot
+              mountPath: /host/boot
+              readOnly: true
+            - name: etc
+              mountPath: /host/boot
+              readOnly: true
+            - name: falco-rules
+              mountPath: /etc/falco/custom_rules.yaml
+              subPath: falco-rules.yaml
+      volumes:
+        - name: dev
+          hostPath:
+            path: /dev
+        - name: proc
+          hostPath:
+            path: /proc
+        - name: boot
+          hostPath:
+            path: /boot
+        - name: falco-rules
+          configMap:
+            name: falco-rules-cm
+
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: falco-rules-cm
+  namespace: kube-system
+data:
+  falco-rules.yaml: |
+    # include the rules shipped separately; you can mount the provided file here
+    # (the file is expected to be replaced with security/falco/falco-rules.yaml content)
+    # For convenience, operators may paste the rules directly.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/trivy_scan.yml
+name: Container image vulnerability scan (Trivy)
+
+on:
+  push:
+    paths:
+      - 'slm/**'
+      - 'serving/**'
+      - 'tool-runner/**'
+  pull_request:
+
+jobs:
+  trivy-scan:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run Trivy (scan Dockerfile images referenced in repo)
+        uses: aquasecurity/trivy-action@master
+        with:
+          scan-type: 'image'
+          image-ref: 'ghcr.io/yourorg/aegis-slm:latest'
+          format: 'table'
+          exit-code: '1'
+          severity: 'CRITICAL,HIGH'
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/gatekeeper/constraint_sigstore_template.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8scosignpolicy
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sCosignPolicy
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8scosignpolicy
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Deployment"
+          not input.review.object.metadata.annotations["aegis.cosign.signed"]
+          msg := sprintf("Deployment %v in namespace %v is missing cosign signature annotation 'aegis.cosign.signed'", [input.review.object.metadata.name, input.review.object.metadata.namespace])
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sCosignPolicy
+metadata:
+  name: require-cosign-deployments
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Deployment"]
+    namespaces: ["aegis"]
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/gatekeeper/constraint_model_signed.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sCosignPolicy
+metadata:
+  name: require-signed-models
+spec:
+  match:
+    kinds:
+      - apiGroups: ["serving.kserve.io"]
+        kinds: ["InferenceService"]
+    namespaces: ["aegis"]
+
*** End Patch
*** Begin Patch
*** Add File:security/network/namespace-default-deny.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: default-deny-all
+  namespace: aegis
+spec:
+  podSelector: {}
+  policyTypes:
+    - Ingress
+    - Egress
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: tool-runner-allow-internal
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: tool-runner
+  policyTypes:
+    - Egress
+  egress:
+    - to:
+        - podSelector:
+            matchLabels:
+              app: milvus
+      ports:
+        - protocol: TCP
+          port: 19530
+    - to:
+        - namespaceSelector:
+            matchLabels:
+              name: monitoring
+      ports:
+        - protocol: TCP
+          port: 9090
+    - to:
+        - ipBlock:
+            cidr: 10.0.0.0/8
+      ports:
+        - protocol: TCP
+          port: 443
+
*** End Patch
*** Begin Patch
*** Add File:security/irsa/tool-runner-iam.tf
+/*
+IRSA IAM role for tool-runner ServiceAccount (example AWS)
+Operators must adapt and attach a policy limiting actions to specific S3 prefixes and STS assume roles.
+*/
+resource "aws_iam_role" "aegis_tool_runner" {
+  name = "aegis-tool-runner-role"
+  assume_role_policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [{
+      Effect = "Allow"
+      Principal = {
+        Federated = "arn:aws:iam::REPLACE_ACCOUNT:oidc-provider/REPLACE_OIDC_PROVIDER"
+      }
+      Action = "sts:AssumeRoleWithWebIdentity"
+      Condition = {
+        StringEquals = {
+          "REPLACE_OIDC_PROVIDER:sub" = "system:serviceaccount:aegis:agent-tool-runner-sa"
+        }
+      }
+    }]
+  })
+}
+
+resource "aws_iam_policy" "aegis_tool_runner_policy" {
+  name = "aegis-tool-runner-policy"
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Effect = "Allow"
+        Action = [
+          "s3:PutObject",
+          "s3:GetObject",
+          "s3:ListBucket"
+        ]
+        Resource = [
+          "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET",
+          "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"
+        ]
+      },
+      {
+        Effect = "Allow"
+        Action = ["sts:AssumeRole"],
+        Resource = ["arn:aws:iam::REPLACE_ACCOUNT:role/aegis-tool-ephemeral-*"]
+      }
+    ]
+  })
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/ephemeral/assume_role_runner.py
+#!/usr/bin/env python3
+"""
+Tool-runner ephemeral credential flow (example)
+ - Uses STS AssumeRole to obtain short-lived credentials scoped to the job
+ - Uploads result artifact to evidence S3 prefix
+
+Environment:
+ - ROLE_ARN: role to assume for tool-runner job (provided via IRSA / env)
+ - EVIDENCE_BUCKET
+"""
+import os
+import boto3
+import tempfile
+
+ROLE_ARN = os.environ.get("ROLE_ARN", "")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+
+def assume_role_and_upload(result_bytes: bytes, key: str):
+    sts = boto3.client("sts")
+    resp = sts.assume_role(RoleArn=ROLE_ARN, RoleSessionName="tool-runner-job", DurationSeconds=900)
+    creds = resp["Credentials"]
+    s3 = boto3.client("s3",
+                      aws_access_key_id=creds["AccessKeyId"],
+                      aws_secret_access_key=creds["SecretAccessKey"],
+                      aws_session_token=creds["SessionToken"])
+    tmp = tempfile.mktemp()
+    with open(tmp, "wb") as f:
+        f.write(result_bytes)
+    s3.upload_file(tmp, EVIDENCE_BUCKET, key)
+    print("Uploaded via ephemeral creds to s3://%s/%s" % (EVIDENCE_BUCKET, key))
+
+if __name__ == "__main__":
+    assume_role_and_upload(b"test", "agents/ephemeral/test.txt")
+
*** End Patch
*** Begin Patch
*** Add File:security/confinement/confinement_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NAMESPACE=${1:-aegis}
+POD_NAME="confinement-test-$(date +%s)"
+
+cat > pod.yaml <<EOF
+apiVersion: v1
+kind: Pod
+metadata:
+  name: ${POD_NAME}
+  namespace: ${NAMESPACE}
+  labels:
+    app: tool-runner
+spec:
+  containers:
+  - name: tester
+    image: curlimages/curl:8.2.1
+    command: ["sleep","3600"]
+    securityContext:
+      runAsNonRoot: true
+      allowPrivilegeEscalation: false
+  restartPolicy: Never
+EOF
+
+kubectl apply -f pod.yaml
+echo "Waiting for pod..."
+kubectl wait --for=condition=Ready pod/${POD_NAME} -n ${NAMESPACE} --timeout=60s || true
+
+echo "Testing egress to external example.com (should be blocked or routed via VPC endpoints)"
+kubectl -n ${NAMESPACE} exec ${POD_NAME} -- curl -sS --max-time 5 https://example.com || echo "Egress blocked or timed out (expected)"
+
+echo "Testing hostPath write attempt (should fail if hostPath not allowed)"
+kubectl -n ${NAMESPACE} exec ${POD_NAME} -- sh -c "echo hi >/hostpath_test || echo 'hostPath write blocked (expected)'"
+
+kubectl -n ${NAMESPACE} delete pod/${POD_NAME} || true
+rm pod.yaml
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/confinement_ci.yml
+name: Confinement tests CI
+
+on:
+  workflow_dispatch:
+
+jobs:
+  confinement:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.27.3'
+      - name: Run confinement script (requires kubeconfig in secrets, run on staging cluster)
+        env:
+          KUBECONFIG: ${{ secrets.STAGING_KUBECONFIG }}
+        run: |
+          chmod +x security/confinement/confinement_test.sh
+          ./security/confinement/confinement_test.sh aegis
+
*** End Patch
*** Begin Patch
*** Add File:security/model_security/membership_inference.py
+#!/usr/bin/env python3
+"""
+Simple membership inference test harness (placeholder)
+ - Given a model endpoint and a holdout dataset, measure a proxy leakage score
+ - This is a simple baseline and should be replaced with a robust MI testing library
+"""
+import requests
+import json
+
+MODEL_ENDPOINT = "http://model-serving.aegis.svc.cluster.local/predict"
+
+def score_leakage(samples):
+    # naive: send samples and measure confidence; higher confidence may indicate membership
+    scores = []
+    for s in samples:
+        r = requests.post(MODEL_ENDPOINT, json={"prompt": s})
+        if r.status_code == 200:
+            jd = r.json()
+            scores.append(jd.get("confidence", 0.0))
+    # return mean confidence as proxy (lower is better)
+    return sum(scores)/len(scores) if scores else 0.0
+
+if __name__ == "__main__":
+    # placeholder samples
+    samples = ["private record example 1", "private record example 2"]
+    print("Leakage proxy score:", score_leakage(samples))
+
*** End Patch
*** Begin Patch
*** Add File:security/model_security/watermark_verifier.py
+#!/usr/bin/env python3
+"""
+Watermark verifier stub:
+ - Given a model artifact or model endpoint output, verify expected watermark signature/pattern.
+ - Placeholder implementation: operators must integrate concrete watermarking library.
+"""
+def verify_text_watermark(text):
+    # placeholder: check for magic token or hash embedded in outputs
+    if "WM_TOKEN:" in text:
+        return True, text.split("WM_TOKEN:")[-1].split()[0]
+    return False, None
+
+if __name__ == "__main__":
+    example = "This is text ... WM_TOKEN:abc123 more"
+    ok, tok = verify_text_watermark(example)
+    print("Watermark present:", ok, "token:", tok)
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/fluentbit/fluentbit-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluentbit-config
+  namespace: logging
+data:
+  fluent-bit.conf: |
+    [SERVICE]
+        Flush        1
+        Log_Level    info
+        Parsers_File parsers.conf
+
+    [INPUT]
+        Name              tail
+        Path              /var/log/containers/*.log
+        Parser            docker
+        Tag               kube.*
+
+    [OUTPUT]
+        Name  es
+        Match *
+        Host  REPLACE_ES_HOST
+        Port  9200
+        HTTP_User  REPLACE_USER
+        HTTP_Passwd REPLACE_PASS
+        Index  aegis-logs
+
+  parsers.conf: |
+    [PARSER]
+        Name        docker
+        Format      json
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/incident_response_runbook.md
+# Incident Response Runbook — Aegis Platform
+
+Purpose: Provide SOC/operators with step-by-step actions to contain, investigate and remediate a suspected compromise involving agent/tool-runner or model artifacts.
+
+1) Initial Triage
+- Confirm alert source (Falco/Prometheus/Fake-SOC).
+- Capture timeline and affected resources (pod names, run_id, model ids).
+- Create an incident ticket and assign an incident owner.
+
+2) Immediate Containment (do quickly)
+- Scale down or pause Agent Manager:
+  kubectl -n aegis scale deployment agent-manager --replicas=0
+- Block tool-runner service account via RBAC:
+  kubectl -n aegis delete clusterrolebinding aegis-tool-runner-binding
+- Revoke or disable cosign KMS key policy (rotate or disable key in KMS):
+  aws kms disable-key --key-id <COSIGN_KEY_ID>
+- Add a NetworkPolicy blocking egress from aegis namespace (apply security/network/namespace-default-deny.yaml)
+
+3) Preserve Evidence
+- Run audit package collector for affected run_id:
+  python3 compliance/audit/audit_package_collector.py <run_id>
+- Snapshot relevant logs (kubectl logs, kube-audit) and upload to evidence S3 under incidents/<incident_id>/
+
+4) Investigation
+- Inspect Falco events and pod exec history.
+- Check container image signatures and MLflow model signatures (cosign verify).
+- Check S3 access logs and CloudTrail for suspicious operations.
+
+5) Remediation
+- Recreate tool-runner ServiceAccount with minimal policies and IRSA updated (tool-runner-iam.tf).
+- Rebuild and re-sign all model artifacts and container images (use CI to enforce).
+- Rotate KMS keys and update ExternalSecrets with new COSIGN_KMS_ARN.
+
+6) Post‑Incident
+- Run a full security post-mortem, update runbooks, and run red-team harness.
+- Report to compliance/audit as required; attach audit package and scope of the incident.
+
*** End Patch
*** Begin Patch
*** Add File:devsecops/policy_playbooks/README.md
+# DevSecOps Policy Playbooks
+
+This directory contains example policies and playbooks:
+- Gatekeeper constraints (require cosign signatures, seccomp)
+- Falco rules and deployment manifests
+- Trivy CI integration to block vulnerable images
+- NetworkPolicy templates and IRSA examples
+
+Operators: review and adapt to your cloud provider and compliance model before applying to production clusters.
+
*** End Patch
*** End Patch
