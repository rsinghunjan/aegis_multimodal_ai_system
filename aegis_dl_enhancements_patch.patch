*** Begin Patch
*** Add File:infra/terraform/gpu_nodepool.tf
+/*
+Terraform snippet (AWS EKS Managed Node Group) to create a GPU node pool.
+Replace placeholders (cluster_name, subnet_ids, key_name, ami_id) with your values.
+*/
+resource "aws_eks_node_group" "gpu_nodes" {
+  cluster_name    = var.cluster_name
+  node_group_name = "aegis-gpu-nodepool"
+  node_role_arn   = aws_iam_role.eks_node_role.arn
+  subnet_ids      = var.subnet_ids
+
+  scaling_config {
+    desired_size = 2
+    max_size     = 10
+    min_size     = 0
+  }
+
+  instance_types = ["p4d.24xlarge"] # replace with preferred instance type (A100/P4/T4)
+
+  remote_access {
+    ec2_ssh_key = var.key_name
+  }
+
+  labels = {
+    "node.kubernetes.io/role" = "gpu"
+    "aegis/workload" = "gpu-training"
+  }
+
+  taints = [
+    "nvidia.com/gpu=true:NoSchedule"
+  ]
+
+  launch_template {
+    id      = aws_launch_template.gpu_lt.id
+    version = "$$Latest"
+  }
+
+  tags = {
+    "Name" = "aegis-gpu-nodepool"
+  }
+}
+
+resource "aws_launch_template" "gpu_lt" {
+  name_prefix   = "aegis-gpu-"
+  image_id      = var.ami_id
+  instance_type = "p4d.24xlarge"
+  block_device_mappings {
+    device_name = "/dev/xvda"
+    ebs {
+      volume_size = 200
+      volume_type = "gp3"
+    }
+  }
+  lifecycle {
+    create_before_destroy = true
+  }
+}
+
+variable "cluster_name" {}
+variable "subnet_ids" { type = list(string) }
+variable "key_name" {}
+variable "ami_id" {}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/nvidia/k8s-device-plugin.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: nvidia-device-plugin
+---
+# NVIDIA device plugin DaemonSet (official)
+# Installs the device plugin on all GPU nodes; use upstream tag appropriate for your cluster.
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: nvidia-device-plugin-daemonset
+  namespace: nvidia-device-plugin
+  labels:
+    app: nvidia-device-plugin
+spec:
+  selector:
+    matchLabels:
+      app: nvidia-device-plugin
+  template:
+    metadata:
+      labels:
+        app: nvidia-device-plugin
+    spec:
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      containers:
+        - name: nvidia-device-plugin-ctr
+          image: "nvcr.io/nvidia/k8s-device-plugin:v0.12.0"
+          securityContext:
+            privileged: true
+          env:
+            - name: FAIL_ON_INIT_ERROR
+              value: "false"
+          volumeMounts:
+            - mountPath: /var/lib/kubelet/device-plugins
+              name: device-plugin
+      volumes:
+        - name: device-plugin
+          hostPath:
+            path: /var/lib/kubelet/device-plugins
+            type: DirectoryOrCreate
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/dcgm-exporter-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: dcgm-exporter
+  template:
+    metadata:
+      labels:
+        app: dcgm-exporter
+    spec:
+      hostNetwork: true
+      tolerations:
+        - key: nvidia.com/gpu
+          operator: Exists
+          effect: NoSchedule
+      containers:
+        - name: dcgm-exporter
+          image: nvcr.io/nvidia/k8s/dcgm-exporter:2.3.5-2.6.6-ubuntu20.04
+          ports:
+            - containerPort: 9400
+              name: metrics
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+            limits:
+              cpu: "200m"
+              memory: "256Mi"
+
*** End Patch
*** Begin Patch
*** Add File:argo/deepspeed_training_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: deepspeed-train-
+  namespace: aegis
+spec:
+  entrypoint: main
+  templates:
+  - name: main
+    steps:
+    - - name: run-deepspeed
+        template: deepspeed-job
+
+  - name: deepspeed-job
+    container:
+      image: "{{IMAGE_REGISTRY}}/aegis-train:latest"
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -e
+          # Example: run torchrun for DeepSpeed (operator configure MASTER_ADDR via env if multi-node)
+          export MODEL_DIR=/workspace/checkpoints/$(date +%s)
+          mkdir -p $MODEL_DIR
+          deepspeed --num_gpus_per_node $DS_GPUS --num_nodes $DS_NODES train.py --deepspeed --output_dir $MODEL_DIR
+          # Upload checkpoint to S3 and sign
+          aws s3 cp --recursive $MODEL_DIR s3://{{EVIDENCE_BUCKET}}/checkpoints/$(date +%s)/
+          CHECKPOINT_S3="s3://{{EVIDENCE_BUCKET}}/checkpoints/$(date +%s)/"
+          # sign the checkpoint tar if present
+          tar -czf /tmp/checkpoint.tgz -C $MODEL_DIR .
+          cosign sign --key ${COSIGN_KMS_ARN} /tmp/checkpoint.tgz
+          # log run id and checkpoint ref to MLflow (assumes mlflow env configured)
+          mlflow run --no-conda .
+      env:
+        - name: DS_GPUS
+          value: "{{DS_GPUS}}"
+        - name: DS_NODES
+          value: "{{DS_NODES}}"
+        - name: AWS_REGION
+          value: "{{AWS_REGION}}"
+        - name: EVIDENCE_BUCKET
+          value: "{{EVIDENCE_BUCKET}}"
+        - name: COSIGN_KMS_ARN
+          value: "{{COSIGN_KMS_ARN}}"
+        - name: MLFLOW_TRACKING_URI
+          value: "{{MLFLOW_TRACKING_URI}}"
+      resources:
+        limits:
+          nvidia.com/gpu: "4"
+          cpu: "8000m"
+          memory: "32Gi"
+        requests:
+          nvidia.com/gpu: "4"
+          cpu: "2000m"
+          memory: "16Gi"
+
+# Notes:
+# - Replace template placeholders via Argo workflow submit parameters or CI templating.
+# - train.py should use DeepSpeed config and write checkpoints to MODEL_DIR.
+*** End Patch
*** Begin Patch
*** Add File:train/deepspeed_wrapper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# DeepSpeed wrapper to run training, checkpoint to S3, cosign-sign checkpoints, and log to MLflow.
+#
+MODEL_DIR=${MODEL_DIR:-/workspace/checkpoints}
+S3_BUCKET=${EVIDENCE_BUCKET:-aegis-evidence-12345}
+TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
+OUT_S3="s3://${S3_BUCKET}/checkpoints/${TIMESTAMP}/"
+
+echo "Running DeepSpeed with args: $*"
+deepspeed "$@"
+
+echo "Archiving checkpoint to /tmp/checkpoint_${TIMESTAMP}.tgz"
+tar -czf /tmp/checkpoint_${TIMESTAMP}.tgz -C "$MODEL_DIR" .
+
+echo "Uploading to S3: $OUT_S3"
+aws s3 cp /tmp/checkpoint_${TIMESTAMP}.tgz "${OUT_S3}checkpoint.tgz"
+
+if [ -n "${COSIGN_KMS_ARN:-}" ]; then
+  echo "Signing checkpoint with cosign KMS: $COSIGN_KMS_ARN"
+  cosign sign --key "${COSIGN_KMS_ARN}" "${OUT_S3}checkpoint.tgz" || true
+else
+  echo "COSIGN_KMS_ARN not set; skipping sign"
+fi
+
+echo "Logging checkpoint to MLflow if available"
+if [ -n "${MLFLOW_TRACKING_URI:-}" ]; then
+  python3 - <<PY
+import mlflow, os
+mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI"))
+with mlflow.start_run() as r:
+    mlflow.log_param("checkpoint_s3", "${OUT_S3}checkpoint.tgz")
+    print("Logged checkpoint to MLflow run", r.info.run_id)
+PY
+fi
+
+echo "DeepSpeed wrapper done."
+
*** End Patch
*** Begin Patch
*** Add File:kserve/triton_inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-triton"
+  namespace: "aegis"
+spec:
+  predictor:
+    minReplicas: 1
+    maxReplicas: 4
+    triton:
+      storageUri: "s3://REPLACE_EVIDENCE_BUCKET/triton_models/{{MODEL_NAME}}"
+      readinessProbe:
+        httpGet:
+          path: "/v2/health/ready"
+          port: 8000
+      resources:
+        limits:
+          nvidia.com/gpu: "1"
+          cpu: "2000m"
+          memory: "8Gi"
+        requests:
+          nvidia.com/gpu: "1"
+          cpu: "1000m"
+          memory: "4Gi"
+    logger:
+      mode: "all"
+      url: "http://mcpx-logger.aegis.svc.cluster.local:8080/log"
+  transformer:
+    containers:
+      - name: transformer
+        image: "{{IMAGE_REGISTRY}}/aegis-transformer:latest"
+        env:
+          - name: MCPX_AUTH_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: mcpx-auth
+                key: token
+
+annotations:
+  serving.kserve.io/ingress-class: "istio"
+
+# Adaptive batching and Triton config usually set in model repo config; use Triton model config for batching.
+*** End Patch
*** Begin Patch
*** Add File:ray/cluster.yaml
+apiVersion: "ray.io/v1alpha1"
+kind: "RayCluster"
+metadata:
+  name: "aegis-ray-cluster"
+  namespace: "aegis"
+spec:
+  headGroupSpec:
+    serviceType: ClusterIP
+    template:
+      spec:
+        containers:
+          - name: ray-head
+            image: rayproject/ray:2.2.0
+            resources:
+              requests:
+                cpu: "2000m"
+                memory: "8Gi"
+  workerGroupSpecs:
+    - groupName: worker-gpu
+      replicas: 2
+      template:
+        spec:
+          tolerations:
+            - key: "nvidia.com/gpu"
+              operator: "Exists"
+          containers:
+            - name: ray-worker
+              image: rayproject/ray:2.2.0
+              resources:
+                limits:
+                  nvidia.com/gpu: "1"
+                  cpu: "4000m"
+                  memory: "16Gi"
+
+# Use the official Ray Operator (install separately via helm) and adapt images/tags per environment.
+*** End Patch
*** Begin Patch
*** Add File:ray/tune_example.py
+"""
+Ray Tune example for hyperparameter search and RL experiments.
+Run: python3 ray/tune_example.py
+"""
+import ray
+from ray import tune
+
+def train_fn(config):
+    import time, random
+    # placeholder training loop; replace with real training
+    for iter in range(10):
+        acc = 0.5 + (random.random() * 0.5) * config["lr"]
+        tune.report(mean_accuracy=acc)
+        time.sleep(1)
+
+if __name__ == "__main__":
+    ray.init(address="auto")  # when running in-cluster, Ray head address will be available
+    analysis = tune.run(
+        train_fn,
+        resources_per_trial={"cpu": 2, "gpu": 0.5},
+        config={
+            "lr": tune.grid_search([0.01, 0.001, 0.0001]),
+        },
+        num_samples=1,
+    )
+    print("Best config: ", analysis.get_best_config(metric="mean_accuracy"))
+
*** End Patch
*** Begin Patch
*** Add File:data/webdataset_sharder.py
+#!/usr/bin/env python3
+"""
+Create WebDataset shards (.tar) from a directory of files (images + metadata).
+This produces N shards that can be uploaded to S3/lakeFS and consumed by WebDataset.
+"""
+import os, tarfile, argparse, glob, math
+
+def shard_files(src_dir, out_dir, shards=100):
+    os.makedirs(out_dir, exist_ok=True)
+    files = glob.glob(os.path.join(src_dir, "*"))
+    if not files:
+        print("No files found in", src_dir); return
+    per_shard = max(1, math.ceil(len(files)/shards))
+    for i in range(shards):
+        start = i*per_shard
+        end = start+per_shard
+        shard_files = files[start:end]
+        if not shard_files: break
+        out_path = os.path.join(out_dir, f"shard-{i:05d}.tar")
+        with tarfile.open(out_path, "w") as tar:
+            for f in shard_files:
+                tar.add(f, arcname=os.path.basename(f))
+        print("Wrote", out_path)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--src", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--shards", type=int, default=100)
+    args = p.parse_args()
+    shard_files(args.src, args.out, args.shards)
+
*** End Patch
*** Begin Patch
*** Add File:train/webdataset_train_template.py
+#!/usr/bin/env python3
+"""
+Training template using WebDataset + PyTorch DDP.
+Replace datasets_s3_prefix with your S3/lakeFS shards.
+"""
+import os, torch
+from torch.utils.data import DataLoader
+import webdataset as wds
+import argparse
+
+def create_dataloader(shards_prefix, batch_size=32, num_workers=4):
+    # shards_prefix example: "s3://bucket/path/shard-{00000..00099}.tar"
+    dataset = (
+        wds.WebDataset(shards_prefix, handler=wds.warn_and_continue)
+        .shuffle(1000)
+        .decode("pil")
+        .to_tuple("jpg", "json")
+        .map_tuple(lambda img: preprocess_img(img), lambda j: j["label"])
+    )
+    loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)
+    return loader
+
+def preprocess_img(img):
+    import torchvision.transforms as T
+    tf = T.Compose([T.Resize((224,224)), T.ToTensor()])
+    return tf(img)
+
+def train_loop(args):
+    loader = create_dataloader(args.shards, args.batch_size, args.num_workers)
+    model = torch.nn.Linear(224*224*3, 10)  # placeholder; replace with real model
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    model.to(device)
+    optim = torch.optim.Adam(model.parameters(), lr=1e-4)
+    for epoch in range(args.epochs):
+        for batch in loader:
+            imgs, labels = batch
+            imgs = imgs.to(device)
+            labels = labels.to(device)
+            # forward/backward
+            preds = model(imgs.view(imgs.size(0), -1))
+            loss = torch.nn.functional.cross_entropy(preds, labels)
+            optim.zero_grad()
+            loss.backward()
+            optim.step()
+        print(f"Epoch {epoch} done")
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--shards", required=True)
+    parser.add_argument("--batch-size", type=int, default=32)
+    parser.add_argument("--num-workers", type=int, default=4)
+    parser.add_argument("--epochs", type=int, default=3)
+    args = parser.parse_args()
+    train_loop(args)
+
*** End Patch
*** Begin Patch
*** Add File:README_DL_ENHANCEMENTS.md
+# Aegis Deep Learning Enhancements
+
+What this patch adds
+- GPU nodepool Terraform snippet (infra/terraform/gpu_nodepool.tf) with taints/labels for dedicated GPU scheduling.
+- NVIDIA device plugin DaemonSet manifest to enable GPU scheduling on nodes (k8s/nvidia/k8s-device-plugin.yaml).
+- DCGM exporter deployment for GPU metrics (monitoring/dcgm-exporter-deployment.yaml).
+- Argo Workflow template to run DeepSpeed distributed training and a DeepSpeed wrapper script that uploads/checkpoints/signs models (argo/deepspeed_training_workflow.yaml, train/deepspeed_wrapper.sh).
+- KServe Triton InferenceService template for GPU serving with logger wiring (kserve/triton_inferenceservice.yaml).
+- RayCluster manifest and Ray Tune example for distributed HP tuning and RL (ray/cluster.yaml, ray/tune_example.py).
+- WebDataset sharder and training template for fast, sharded I/O with NVMe caching (data/webdataset_sharder.py, train/webdataset_train_template.py).
+
+How to use (quickstart)
+1. Provision GPU nodepool via Terraform (edit variables) and ensure nodes are labeled/tainted:
+   terraform apply -var "cluster_name=..." -var 'subnet_ids=["..."]' -var "ami_id=..." ...
+2. Apply device plugin and DCGM exporter:
+   kubectl apply -f k8s/nvidia/k8s-device-plugin.yaml
+   kubectl apply -f monitoring/dcgm-exporter-deployment.yaml
+3. Build and push images (use existing build workflows) and then submit Argo workflow:
+   argo submit -n aegis argo/deepspeed_training_workflow.yaml -p IMAGE_REGISTRY=ghcr.io/yourorg -p EVIDENCE_BUCKET=aegis-evidence-12345 -p DS_GPUS=4 -p DS_NODES=1
+4. Use data/webdataset_sharder.py to create shards, upload to S3 and run train/webdataset_train_template.py pointing to the shard prefix.
+5. Deploy Ray operator and apply ray/cluster.yaml; run ray/tune_example.py using Ray cluster address.
+6. Deploy Triton model repository to S3 and apply kserve/triton_inferenceservice.yaml (replace model names and URIs).
+
+Notes & next steps
+- Replace all REPLACE_* placeholders (S3 buckets, image registries, ARNs) prior to use.
+- Validate IAM roles & IRSA for pods needing S3/KMS access.
+- Use spot/preemptible GPUs for cost savings and robust checkpointing/resume logic.
+- Tune resource requests/limits, PVC sizes and cluster autoscaling to match workload.
+- For very large models, integrate DeepSpeed+Megatron and consider FSDP; orchestration with Ray or native DeepSpeed operator recommended.
+
*** End Patch
*** End Patch
