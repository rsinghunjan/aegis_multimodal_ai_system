diff --git a/k8s/manifests/staging-deploy.sh b/k8s/manifests/staging-deploy.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/k8s/manifests/staging-deploy.sh
@@ -0,0 +1,84 @@
+#!/usr/bin/env bash
+#
+# Apply Aegis staging manifests and create required Kubernetes secrets from files or Vault.
+# IMPORTANT: Do NOT store plaintext secrets in git. This script expects secrets to be provided
+# as files (mounted by CI/CD), or uses Vault (see vault/README_VAULT_INTEGRATION.md).
+#
+# Usage:
+#   ./k8s/manifests/staging-deploy.sh --namespace aegis-ml --secrets-dir ./secrets
+#
+set -euo pipefail
+
+NAMESPACE="aegis-ml"
+SECRETS_DIR="${PWD}/secrets"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --secrets-dir) SECRETS_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 1;;
+  esac
+done
+
+echo "Using namespace: $NAMESPACE"
+kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
+
+echo "Applying core manifests..."
+kubectl apply -n "$NAMESPACE" -f k8s/postgres/postgres-deployment.yaml
+kubectl apply -n "$NAMESPACE" -f k8s/redis/redis-deployment.yaml || echo "Redis optional; continuing..."
+kubectl apply -n "$NAMESPACE" -f k8s/manifests/agent-serviceaccount.yaml
+kubectl apply -n "$NAMESPACE" -f k8s/manifests/kill-switch-configmap.yaml
+kubectl apply -n "$NAMESPACE" -f k8s/manifests/orchestrator-deployment.yaml
+kubectl apply -n "$NAMESPACE" -f argo-events/eventsource-webhook.yaml
+kubectl apply -n "$NAMESPACE" -f argo-events/sensor-retrain.yaml
+
+echo "Applying Gatekeeper constraint template (requires Gatekeeper installed)"
+kubectl apply -n "$NAMESPACE" -f gatekeeper/constraint_auto_promote.yaml || echo "Gatekeeper not installed - skip."
+
+echo "Applying OPA policy (if OPA server installed, use your OPA deployment flow)"
+kubectl apply -f policy/opa/agent_policies_extended.rego --dry-run=client || true
+echo "NOTE: Load Rego into OPA as per your OPA deployment (e.g., using /v1/policies or configmap)."
+
+# Create K8s secrets from files if provided. Files should not be checked into git.
+if [ -d "$SECRETS_DIR" ]; then
+  echo "Creating Kubernetes secrets from $SECRETS_DIR"
+  # GitHub App key (pem), DB creds, Vault token (optional)
+  if [ -f "${SECRETS_DIR}/github_app.pem" ]; then
+    kubectl -n "$NAMESPACE" create secret generic aegis-github-app-key --from-file=github_app.pem="${SECRETS_DIR}/github_app.pem" --dry-run=client -o yaml | kubectl apply -f -
+    echo "Created secret aegis-github-app-key"
+  fi
+  if [ -f "${SECRETS_DIR}/db_password" ]; then
+    kubectl -n "$NAMESPACE" create secret generic aegis-postgres-secret --from-file=password="${SECRETS_DIR}/db_password" --dry-run=client -o yaml | kubectl apply -f -
+    echo "Created secret aegis-postgres-secret (password)"
+  fi
+  if [ -f "${SECRETS_DIR}/vault_token" ]; then
+    kubectl -n "$NAMESPACE" create secret generic aegis-vault-token --from-file=token="${SECRETS_DIR}/vault_token" --dry-run=client -o yaml | kubectl apply -f -
+    echo "Created secret aegis-vault-token"
+  fi
+else
+  echo "Secrets dir $SECRETS_DIR not found. Ensure you provision secrets via Vault, SealedSecrets or another secret manager."
+fi
+
+echo "All manifests applied. Please follow docs/staging_runbook.md to continue verification."
diff --git a/vault/README_VAULT_INTEGRATION.md b/vault/README_VAULT_INTEGRATION.md
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/vault/README_VAULT_INTEGRATION.md
@@ -0,0 +1,79 @@
+# Vault integration for Aegis agents (staging)
+
+This document provides example steps and policies to store GitHub App private key and DB credentials in Vault,
+and to allow K8s workloads (agents) to fetch short-lived credentials.
+
+WARNING: Do not store secrets in git. Use Vault or SealedSecrets for secrets in Kubernetes.
+
+1) Vault high-level steps (operator)
+- Install HashiCorp Vault (Helm or operator) in a secure namespace.
+- Enable Kubernetes auth:
+  vault auth enable kubernetes
+- Configure the Kubernetes auth role binding:
+  vault write auth/kubernetes/config \
+    token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
+    kubernetes_host="https://$KUBERNETES_PORT_443_TCP_ADDR:443" \
+    kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
+
+- Create a Vault policy for agents (agents-policy.hcl), allow read access to specific paths.
+
+2) Example Vault policy (agents-policy.hcl)
+```
+path "secret/data/aegis/github_app" {
+  capabilities = ["read"]
+}
+path "secret/data/aegis/postgres/*" {
+  capabilities = ["read","list"]
+}
+path "secret/data/aegis/rotation/*" {
+  capabilities = ["read","update"]
+}
+```
+
+3) Create a role that maps a Kubernetes service account to the Vault policy:
+```
+vault write auth/kubernetes/role/aegis-agent \
+  bound_service_account_names=aegis-agent-sa \
+  bound_service_account_namespaces=aegis-ml \
+  policies=agents-policy \
+  ttl=1h
+```
+
+4) Store GitHub App key & DB credentials in Vault (kv v2):
+```
+vault kv put secret/aegis/github_app private_key=@/tmp/github_app.pem app_id=123456 installation_id=98765
+vault kv put secret/aegis/postgres creds='{"username":"aegis","password":"<strongpass>","dbname":"aegis","host":"aegis-postgres.aegis-ml.svc.cluster.local"}'
+```
+
+5) Use Vault Agent Injector / CSI to mount secrets into pods (recommended):
+- Configure an annotation on the orchestrator deployment to request secrets from Vault.
+- See example manifest snippet in this repo (k8s/manifests/orchestrator-deployment.yaml) - adapt to use Vault Injector.
+
+6) Rotation and short-lived tokens:
+- For DB credentials, consider using dynamic secrets (Database secrets engine) to issue short-lived DB users per-agent.
+- For GitHub App private key, store PEM and use the app id/installation id to generate installation tokens at runtime using agents/common/github_app.py
+
+7) CI/CD:
+- In your CI pipeline, fetch vault secrets and pass them to kubectl as files or apply them via sealed-secrets after encrypting.
+
+References:
+- https://www.vaultproject.io/docs/auth/kubernetes
+- https://www.vaultproject.io/docs/secrets
+- Vault Agent Injector for Kubernetes
diff --git a/policy/opa/data/models.json b/policy/opa/data/models.json
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/policy/opa/data/models.json
@@ -0,0 +1,22 @@
+{
+  "models": {
+    "low-demo-model": {
+      "risk": "low",
+      "team": "ads",
+      "budget_usd_per_month": 200.0,
+      "owners": ["alice","bob"]
+    },
+    "fraud-detector-v1": {
+      "risk": "high",
+      "team": "fraud",
+      "budget_usd_per_month": 1000.0,
+      "owners": ["sre","security"]
+    },
+    "default": {
+      "risk": "medium",
+      "team": "ml",
+      "budget_usd_per_month": 500.0,
+      "owners": ["ml-team"]
+    }
+  }
+}
diff --git a/policy/opa/agent_policies_extended.rego b/policy/opa/agent_policies_extended.rego
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/policy/opa/agent_policies_extended.rego
@@ -0,0 +1,155 @@
+package aegis.policies
+
+# Extended policies for Aegis Orchestrator
+# Input:
+# {
+#  "action": "...",
+#  "model": "...",
+#  "params": {...},
+#  "env": "staging"|"production",
+#  "timestamp": "..."
+# }
+
+import future.keywords.in  # allow 'in' in older OPA versions
+
+default allow = false
+
+# load model metadata
+models := data.models
+
+model_meta(m) = meta {
+  some key
+  meta = models[key]
+  key == m
+} else = models["default"]
+
+# helper: model risk
+is_low_risk {
+  model_meta(input.model).risk == "low"
+}
+
+is_medium_risk {
+  model_meta(input.model).risk == "medium"
+}
+
+is_high_risk {
+  model_meta(input.model).risk == "high"
+}
+
+# Provenance required for production actions that change infra/traffic
+provenance_ok {
+  input.params.run_id
+  input.params.lakefs_commit
+  input.params.image_digest
+}
+
+# Escalation flow: For medium-risk promotions require an approval flag in params.approved_by or
+# external human review; high-risk always requires manual approval / will return allow=false
+escalation_required {
+  is_medium_risk
+  not input.params.approved_by
+}
+
+# Action categories requiring different checks
+action_is_retrain { input.action == "retrain" }
+action_is_promote { input.action == "promote" }
+action_is_remediate { input.action == "remediate" }
+action_is_create_pr { input.action == "create_pr" }
+
+# Allow rules:
+# - Staging: allow most actions
+allow {
+  input.env == "staging"
+}
+
+# - Production: allow only if low risk and provenance present for direct auto execution
+allow {
+  input.env == "production"
+  is_low_risk
+  provenance_ok
+}
+
+# - For promotions in production: medium-risk allowed only if approved_by present
+allow {
+  input.env == "production"
+  action_is_promote
+  is_medium_risk
+  input.params.approved_by
+  provenance_ok
+}
+
+# Deny rules for high-risk models for auto actions
+deny_high_risk_auto {
+  is_high_risk
+  input.env == "production"
+  input.action != "create_pr"
+}
+
+# Compose reason
+default reason = "policy: not allowed"
+reason = r {
+  deny_high_risk_auto
+  r = "denied: high-risk model auto-actions are prohibited"
+}
+reason = r {
+  not deny_high_risk_auto
+  not allow
+  r = "denied: policy default"
+}
+reason = "allowed by policy" {
+  allow
+}
+
+# Provide allow_auto_execute flag for orchestrator to decide whether to execute immediately
+allow_auto_execute = true {
+  allow
+  input.env == "staging"
+}
+allow_auto_execute = true {
+  allow
+  input.env == "production"
+  is_low_risk
+  provenance_ok
+}
+allow_auto_execute = false {
+  not allow_auto_execute
+}
+
+# Expose a result object the orchestrator expects
+result = {"allow": allow, "reason": reason, "allow_auto_execute": allow_auto_execute}
diff --git a/k8s/manifests/gatekeeper-installation-notes.md b/k8s/manifests/gatekeeper-installation-notes.md
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/k8s/manifests/gatekeeper-installation-notes.md
@@ -0,0 +1,26 @@
+# Gatekeeper installation & constraint application (notes)
+
+1. Install Gatekeeper (official Helm chart):
+   helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
+   helm repo update
+   helm install gatekeeper/gatekeeper --name-template gatekeeper --namespace gatekeeper-system --create-namespace
+
+2. Verify pods ready:
+   kubectl get pods -n gatekeeper-system
+
+3. Apply the ConstraintTemplate and Constraint in this repo:
+   kubectl apply -f gatekeeper/constraint_auto_promote.yaml
+
+4. Test:
+   - Try to create a Deployment with label `aegis.auto=true` and without `aegis.run_id` label; Gatekeeper should deny admission.
+
+5. Tune templates/regos according to your cluster conventions.
diff --git a/.github/CODEOWNERS b/.github/CODEOWNERS
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/.github/CODEOWNERS
@@ -0,0 +1,9 @@
+# CODEOWNERS file - list owners for model promotion PRs
+# Any PR touching promoted models should request these codeowners
+# Format: file pattern  @github-user-or-team
+
+# Default: ML team owns model promotion PRs
+/docs/* @ml-team
+mlops/* @ml-team
+models/* @ml-team
+
+# Security and SRE must review promotion PRs
+PROMOTIONS/* @sre @security
diff --git a/.github/workflows/ci-agent-tests.yml b/.github/workflows/ci-agent-tests.yml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/.github/workflows/ci-agent-tests.yml
@@ -0,0 +1,62 @@
+name: CI - Agent Tests
+
+on:
+  push:
+    paths:
+      - "agents/**"
+      - "tests/**"
+      - "policy/**"
+  pull_request:
+
+jobs:
+  unit-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install test deps
+        run: |
+          pip install -r agents/orchestrator/requirements.txt
+          pip install -r tests/requirements-tests.txt || true
+      - name: Run pytest
+        run: pytest -q
+
+  e2e-synthetic:
+    runs-on: ubuntu-latest
+    needs: unit-tests
+    if: github.event_name == 'push' && success() # only on successful pushes, adjust as needed
+    env:
+      KUBECONFIG: ${{ secrets.KUBECONFIG }} # Optional: provide kubeconfig in secrets for staging cluster
+    steps:
+      - uses: actions/checkout@v4
+      - name: Wait for cluster & deploy staging manifests (if KUBECONFIG provided)
+        if: env.KUBECONFIG != ''
+        run: |
+          echo "$KUBECONFIG" > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+          ./k8s/manifests/staging-deploy.sh --namespace aegis-ml --secrets-dir ./secrets || true
+      - name: Run synthetic drift test
+        if: env.KUBECONFIG != ''
+        run: |
+          # Port-forward eventsource and run synthetic script
+          kubectl -n aegis-ml port-forward svc/aegis-webhook-es 12000:12000 & pid=$!
+          sleep 2
+          python tests/send_synthetic_drift.py --model low-demo-model
+          kill $pid || true
+      - name: Collect artifacts
+        if: env.KUBECONFIG != ''
+        run: |
+          kubectl -n aegis-ml get pods -o wide
diff --git a/grafana/dashboards/aegis_orchestrator_dashboard.json b/grafana/dashboards/aegis_orchestrator_dashboard.json
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/grafana/dashboards/aegis_orchestrator_dashboard.json
@@ -0,0 +1,73 @@
+{
+  "dashboard": {
+    "id": null,
+    "uid": "aegis-orch",
+    "title": "Aegis Orchestrator - Agent Metrics",
+    "tags": ["aegis","orchestrator"],
+    "panels": [
+      {
+        "type": "graph",
+        "title": "Decisions (allow/deny)",
+        "targets": [
+          {
+            "expr": "sum by (result) (aegis_decisions_total)",
+            "legendFormat": "{{result}}"
+          }
+        ],
+        "gridPos": {"x":0,"y":0,"w":12,"h":6}
+      },
+      {
+        "type": "graph",
+        "title": "Rate limit rejects",
+        "targets": [
+          {
+            "expr": "aegis_rate_limit_rejects_total",
+            "legendFormat": "rate_limit_rejects"
+          }
+        ],
+        "gridPos": {"x":12,"y":0,"w":12,"h":4}
+      },
+      {
+        "type": "graph",
+        "title": "Budget rejects",
+        "targets": [
+          {
+            "expr": "aegis_budget_rejects_total",
+            "legendFormat": "budget_rejects"
+          }
+        ],
+        "gridPos": {"x":12,"y":4,"w":12,"h":4}
+      },
+      {
+        "type": "graph",
+        "title": "Actions executed",
+        "targets": [
+          {
+            "expr": "sum by (action) (aegis_actions_executed_total)",
+            "legendFormat": "{{action}}"
+          }
+        ],
+        "gridPos": {"x":0,"y":6,"w":24,"h":6}
+      }
+    ],
+    "schemaVersion": 27,
+    "version": 1
+  }
+}
diff --git a/k8s/chaos/worker_preempt.sh b/k8s/chaos/worker_preempt.sh
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/k8s/chaos/worker_preempt.sh
@@ -0,0 +1,36 @@
+#!/usr/bin/env bash
+# Simple chaos script for staging: evict or delete a GPU worker pod to simulate preemption.
+# WARNING: Run only in staging namespace.
+#
+# Usage: ./k8s/chaos/worker_preempt.sh aegis-ml "app=gpu-worker"
+set -euo pipefail
+
+NAMESPACE="${1:-aegis-ml}"
+LABEL="${2:-app=gpu-worker}"
+
+echo "Listing candidate worker pods with label $LABEL in namespace $NAMESPACE"
+kubectl -n "$NAMESPACE" get pods -l "$LABEL" -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'
+
+POD=$(kubectl -n "$NAMESPACE" get pods -l "$LABEL" -o jsonpath='{.items[0].metadata.name}')
+if [ -z "$POD" ]; then
+  echo "No pod found"
+  exit 1
+fi
+
+echo "Deleting pod $POD to simulate preemption"
+kubectl -n "$NAMESPACE" delete pod "$POD" --grace-period=0 --force
+echo "Deleted pod $POD"
+echo "Observe orchestrator behavior and trainer checkpoint/resume logic"
diff --git a/billing/budget_guard_adapter.py b/billing/budget_guard_adapter.py
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/billing/budget_guard_adapter.py
@@ -0,0 +1,64 @@
+#!/usr/bin/env python3
+"""
+Simple adapter example to integrate cloud billing/chargeback data into the orchestrator's budget guard.
+
+- In production, this should query your cloud billing export (BigQuery / Athena / Cost API) or internal chargeback DB.
+- This adapter exposes a small HTTP endpoint that returns current usage for a model or team.
+
+Usage (example):
+  python billing/budget_guard_adapter.py --port 8085
+"""
+import argparse
+import json
+from flask import Flask, request, jsonify
+
+app = Flask(__name__)
+
+# Example in-memory sample usage data; replace with real billing lookups
+USAGE = {
+  "low-demo-model": 25.0,
+  "fraud-detector-v1": 900.0
+}
+
+@app.route("/usage", methods=["GET"])
+def get_usage():
+  model = request.args.get("model")
+  if not model:
+    return jsonify({"error":"missing model param"}), 400
+  usage = USAGE.get(model, 0.0)
+  return jsonify({"model": model, "usage_usd": usage})
+
+if __name__ == "__main__":
+  parser = argparse.ArgumentParser()
+  parser.add_argument("--port", type=int, default=8085)
+  args = parser.parse_args()
+  app.run(host="0.0.0.0", port=args.port)
diff --git a/docs/staging_runbook.md b/docs/staging_runbook.md
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/docs/staging_runbook.md
@@ -0,0 +1,64 @@
+# Aegis Staging Runbook (quick steps)
+
+Purpose: deploy orchestrator & supporting services into staging and run a synthetic drift â†’ retrain demonstration.
+
+Prereqs:
+- kubectl configured to target staging cluster (do not run in production).
+- Vault available or secrets files prepared in `./secrets` (see vault/README_VAULT_INTEGRATION.md).
+
+Steps:
+1. Deploy manifests and create secrets:
+   ./k8s/manifests/staging-deploy.sh --namespace aegis-ml --secrets-dir ./secrets
+
+2. Ensure OPA is running and load Rego:
+   # If OPA is a deployment in cluster, load policy via HTTP or ConfigMap
+   kubectl apply -f policy/opa/agent_policies_extended.rego  # adjust per your OPA deployment method
+
+3. Initialize Postgres decision_log schema:
+   kubectl exec -n aegis-ml deploy/aegis-postgres -- psql -U aegis -d aegis -f /sql/001_create_decision_log.sql
+   (or run psql from a pod with network access to the DB)
+
+4. Deploy Gatekeeper and apply constraint template (if Gatekeeper installed):
+   kubectl apply -f gatekeeper/constraint_auto_promote.yaml
+
+5. Start Grafana & import dashboard:
+   - Import grafana/dashboards/aegis_orchestrator_dashboard.json into staging Grafana.
+
+6. Run unit tests locally:
+   pip install -r agents/orchestrator/requirements.txt
+   pip install -r tests/requirements-tests.txt
+   pytest -q
+
+7. Port-forward EventSource and run synthetic drift:
+   kubectl -n aegis-ml port-forward svc/aegis-webhook-es 12000:12000 &
+   python tests/send_synthetic_drift.py --model low-demo-model
+
+8. Verify:
+   - Argo UI: retrain workflow submitted
+   - Postgres: entries in decision_log
+   - GitHub: promotion PR created in staging repo (if DRY_RUN=false and GitHub App configured)
+   - Grafana: metrics updated
+
+9. Toggle DRY_RUN=false (only after sufficient testing) by updating orchestrator Deployment env and reloading configuration.
+
+10. Run chaos test (simulate preemption):
+   ./k8s/chaos/worker_preempt.sh aegis-ml "app=gpu-worker"
+
+Notes:
+- For production promotion, ensure CODEOWNERS, branch protection, SBOM checks and required SRE/Security approvals are in place.
+
