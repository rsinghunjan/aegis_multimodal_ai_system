*** Begin Patch
*** Add File: .github/workflows/ci_security_pipeline.yml
+name: CI Security Pipeline — SBOM, SCA, Sign & Rekor
+on:
+  push:
+    branches:
+      - main
+  pull_request:
+    branches:
+      - main
+
+jobs:
+  build-and-scan:
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+      packages: write
+      id-token: write
+    env:
+      REGISTRY_HOST: ${{ secrets.REGISTRY_HOST }}
+      IMAGE_NAME: ${{ secrets.REGISTRY_HOST }}/aegis-cloud-tpu:${{ github.sha }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.REGISTRY_HOST }}
+          username: ${{ secrets.REGISTRY_USER }}
+          password: ${{ secrets.REGISTRY_TOKEN }}
+
+      - name: Build image
+        run: |
+          docker build -t "$IMAGE_NAME" -f cloud_tpu/Dockerfile.tpu .
+
+      - name: Generate SBOM (syft)
+        uses: anchore/syft-action@v1
+        with:
+          image: ${{ env.IMAGE_NAME }}
+          output: sbom.json
+
+      - name: Scan image with Trivy
+        uses: aquasecurity/trivy-action@v1
+        with:
+          image-ref: ${{ env.IMAGE_NAME }}
+          format: json
+          output: trivy.json
+
+      - name: Fail on critical CVEs
+        run: |
+          if jq -e '.[] | select(.Vulnerabilities != null) | .Vulnerabilities[] | select(.Severity=="CRITICAL")' trivy.json >/dev/null 2>&1; then
+            echo "Critical vulnerability found"; jq . trivy.json; exit 1
+          else
+            echo "No critical vulnerabilities"
+          fi
+
+      - name: Push image
+        run: docker push "$IMAGE_NAME"
+
+      - name: Get image digest
+        id: digest
+        run: |
+          DIGEST=$(docker image inspect --format='{{index .RepoDigests 0}}' "$IMAGE_NAME" | awk -F@ '{print $2}')
+          echo "digest=$DIGEST" >> $GITHUB_OUTPUT
+
+      - name: Sign image with cosign (KMS/HSM recommended)
+        env:
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }} # use KMS reference or base64 key as appropriate
+        run: |
+          # example: cosign sign --key $COSIGN_KEY $IMAGE_NAME@${{ steps.digest.outputs.digest }}
+          echo "Signing step: configure cosign to use your key; this is a placeholder"
+
+      - name: Record digest and SBOM artifact
+        run: |
+          mkdir -p artifacts
+          echo "${{ steps.digest.outputs.digest }}" > artifacts/image.digest
+          mv sbom.json artifacts/sbom.json || true
+          mv trivy.json artifacts/trivy.json || true
+          tar -czf artifacts.tar.gz artifacts || true
+        # Optionally upload to artifact storage or COMPLIANCE_BUCKET in a later step
+
+      - name: Update manifests/image-digests.json (attempt)
+        env:
+          GIT_EMAIL: "aegis-bot@example.com"
+          GIT_NAME: "aegis-bot"
+        run: |
+          git config user.email "$GIT_EMAIL"
+          git config user.name "$GIT_NAME"
+          IMAGE="${{ env.IMAGE_NAME }}"
+          DIG="${{ steps.digest.outputs.digest }}"
+          mkdir -p manifests
+          jq -n --arg image "$IMAGE" --arg digest "$DIG" '{($image): $digest}' > /tmp/new.json
+          if [ -f manifests/image-digests.json ]; then
+            jq -s '.[0] * .[1]' manifests/image-digests.json /tmp/new.json > /tmp/merged.json && mv /tmp/merged.json manifests/image-digests.json
+          else
+            mv /tmp/new.json manifests/image-digests.json
+          fi
+          git add manifests/image-digests.json || true
+          git commit -m "ci: update image digest for $IMAGE" || true
+          git push origin HEAD || echo "push failed; ensure repo write access"
+
+  rekor-record:
+    needs: build-and-scan
+    runs-on: ubuntu-latest
+    if: success()
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Upload signature to Rekor (placeholder)
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          echo "Configure cosign to upload to Rekor during signing step in your environment. This step is a placeholder to indicate where Rekor upload/check would occur."
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constrainttemplate_image_attestation.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8simageattestation
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sImageAttestation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8simageattestation
+
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "AdmissionReview"
+          some container
+          containers := input.review.object.spec.template.spec.containers
+          container := containers[_]
+          image := container.image
+          not image_signed(image)
+          msg := sprintf("Image %v is not signed or not present in manifests/image-digests.json", [image])
+        }
+
+        # Lookup in a ConfigMap or external source could be implemented; for preview we disallow images without @sha
+        image_signed(image) {
+          # quick policy: require pinned digest (image@sha)
+          contains(image, "@")
+        }
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_image_attestation.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sImageAttestation
+metadata:
+  name: require-image-attestation
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["staging", "ops"]  # apply to target namespaces
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File: governance/opa/policies/require_signed_and_rekor.rego
+package aegis.policies
+
+default allow = false
+
+# Allow promotion only if artifact is signed and has a rekor entry and no critical CVEs flagged
+allow {
+  input.action == "promote_model"
+  artifact := input.artifact
+  artifact.cosign == true
+  artifact.rekor != ""
+  not has_critical_cve(artifact.sbom)
+}
+
+has_critical_cve(sbom) {
+  some i
+  sbom.Results[i].Vulnerabilities[_].Severity == "CRITICAL"
+}
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/k8s_client.py
+"""
+Utility using the Kubernetes Python client to create a Job with a projected token (TokenRequest)
+and wait for completion. This provides an example of TokenRequest usage (requires cluster >=1.20).
+"""
+import os
+import json
+import time
+from kubernetes import client, config
+
+def load_kubeconfig(kubeconfig_path=None):
+    if kubeconfig_path:
+        config.load_kube_config(config_file=kubeconfig_path)
+    else:
+        # in-cluster fallback
+        config.load_incluster_config()
+
+def create_projected_token_secret(sa_name, namespace="agent-exec", expiration_seconds=600):
+    """
+    Create a TokenRequest for a ServiceAccount and return the token string.
+    """
+    v1 = client.CoreV1Api()
+    tr = client.V1TokenRequest(spec=client.V1TokenRequestSpec(expiration_seconds=expiration_seconds, audiences=["api"]))
+    token_req = v1.create_namespaced_service_account_token(sa_name, namespace, tr)
+    return token_req.status.token
+
+def create_job_from_template(job_name, namespace, image, tool_name, args_json, sa_name):
+    batch = client.BatchV1Api()
+    pod_spec = client.V1PodSpec(
+        service_account_name=sa_name,
+        containers=[
+            client.V1Container(
+                name="tool-runner",
+                image=image,
+                env=[
+                    client.V1EnvVar(name="TOOL_NAME", value=tool_name),
+                    client.V1EnvVar(name="ARGS_JSON", value=json.dumps(args_json)),
+                ],
+                resources=client.V1ResourceRequirements(limits={"cpu":"500m","memory":"256Mi"})
+            )
+        ],
+        restart_policy="Never"
+    )
+    template = client.V1PodTemplateSpec(spec=pod_spec)
+    job_spec = client.V1JobSpec(template=template, backoff_limit=0, active_deadline_seconds=300)
+    job = client.V1Job(metadata=client.V1ObjectMeta(name=job_name, namespace=namespace), spec=job_spec)
+    batch.create_namespaced_job(namespace=namespace, body=job)
+    return job_name
+
+def wait_for_job_completion(job_name, namespace="agent-exec", timeout_seconds=240):
+    batch = client.BatchV1Api()
+    start = time.time()
+    while time.time() - start < timeout_seconds:
+        j = batch.read_namespaced_job(job_name, namespace)
+        if j.status.succeeded and j.status.succeeded > 0:
+            return True
+        if j.status.failed and j.status.failed > 0:
+            return False
+        time.sleep(2)
+    return False
+
+def get_pod_logs_for_job(job_name, namespace="agent-exec"):
+    v1 = client.CoreV1Api()
+    pods = v1.list_namespaced_pod(namespace, label_selector=f"job-name={job_name}")
+    if pods.items:
+        name = pods.items[0].metadata.name
+        return v1.read_namespaced_pod_log(name, namespace)
+    return ""
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/app_k8s_patch.py
+"""
+Small patch illustrating how to call k8s_client utilities from the controller
+Replace kubectl subprocess-based functions with calls to the Kubernetes client.
+This file is an adjunct and must be integrated into services/agent_controller/app.py where LOCAL_EXEC is false.
+"""
+from services.agent_controller import k8s_client
+
+def execute_tool_k8s(action, agent_id):
+    tool_name = action.get("tool")
+    args = action.get("args", {})
+    sa_name = f"tool-sa"  # in production, create ephemeral SA or reuse a minimal SA and use TokenRequest
+    job_name = f"tool-exec-{agent_id}-{int(time.time())}"
+    k8s_client.load_kubeconfig(os.environ.get("KUBECONFIG"))
+    k8s_client.create_job_from_template(job_name, namespace="agent-exec", image=os.environ.get("TOOL_JOB_IMAGE"), tool_name=tool_name, args_json=args, sa_name=sa_name)
+    ok = k8s_client.wait_for_job_completion(job_name, namespace="agent-exec", timeout_seconds=300)
+    logs = k8s_client.get_pod_logs_for_job(job_name, namespace="agent-exec")
+    # cleanup can be done via Kubernetes TTL controller or delete job here
+    return {"ok": ok, "logs": logs}
+
*** End Patch
*** Begin Patch
*** Add File: ansible/edge_tpu/verify_ota.yml
+---
- name: Verify OTA signed artifact on edge device
+  hosts: edge
+  become: yes
+  vars:
+    ota_artifact_url: "{{ ota_artifact_url | default('https://example.com/artifacts/firmware.bin') }}"
+    ota_signature_url: "{{ ota_signature_url | default('https://example.com/artifacts/firmware.bin.sig') }}"
+    cosign_pub_key_path: /etc/aegis/cosign.pub
+
+  tasks:
+    - name: Ensure cosign is installed
+      ansible.builtin.command: which cosign
+      register: cosign_check
+      failed_when: cosign_check.rc != 0
+
+    - name: Download artifact
+      ansible.builtin.get_url:
+        url: "{{ ota_artifact_url }}"
+        dest: /tmp/firmware.bin
+
+    - name: Download signature
+      ansible.builtin.get_url:
+        url: "{{ ota_signature_url }}"
+        dest: /tmp/firmware.bin.sig
+
+    - name: Verify signature with cosign
+      ansible.builtin.command: >
+        cosign verify-blob --key {{ cosign_pub_key_path }} /tmp/firmware.bin
+      register: cosign_verify
+      failed_when: cosign_verify.rc != 0
+
+    - name: Apply OTA (demo - replace with safe apply)
+      ansible.builtin.shell: |
+        echo "Applying OTA (demo)"; ls -l /tmp/firmware.bin
+      when: cosign_verify.rc == 0
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/ota_runbook.md
+# OTA Signed Update Verification Runbook (Edge)
+
+Overview
+- This runbook verifies signed OTA artifacts on edge devices using cosign verification via Ansible.
+
+Prerequisites
+- Upload cosign public key to devices: /etc/aegis/cosign.pub
+- Ensure cosign binary present on devices
+- Ensure Ansible inventory contains reachable devices
+
+Run verification
+- Example:
+  ansible-playbook -i ansible/inventory/edge_devices.ini ansible/edge_tpu/verify_ota.yml -e "ota_artifact_url=https://bucket/firmware.bin ota_signature_url=https://bucket/firmware.bin.sig"
+
+Acceptance criteria
+- cosign verify-blob returns exit 0 and task completes.
+- Artifact is not applied unless signature verification passes.
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/agent_metrics_rules.yaml
+groups:
+- name: aegis.agent.metrics
+  rules:
+  - alert: AgentHighToolInvocationRate
+    expr: increase(aegis_agent_tool_invocations_total[5m]) > 50
+    for: 5m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Agent tool invocation rate high"
+      description: "More than 50 tool invocations in 5m across agents."
+
+  - alert: AgentOpaDenialSpike
+    expr: increase(aegis_agent_opa_denials_total[10m]) > 5
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "OPA denials spike"
+      description: "More than 5 OPA denials in 10m — investigate policy or attacker attempts."
+
+  - alert: AgentRunawayTokens
+    expr: increase(aegis_agent_tokens_consumed_total[1h]) > 100000
+    for: 10m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Agent token usage high"
+      description: "Agent(s) consumed a high number of tokens in the last hour; consider pausing agent(s)."
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/grafana/agent_dashboard.json
+{
+  "dashboard": {
+    "title": "Aegis Agents",
+    "uid": "aegis-agents",
+    "panels": [
+      {
+        "title": "Running Agents",
+        "type": "stat",
+        "targets": [{"expr": "aegis_agent_running_gauge"}],
+        "id": 1
+      },
+      {
+        "title": "Tool Invocations (rate 5m)",
+        "type": "graph",
+        "targets": [{"expr": "increase(aegis_agent_tool_invocations_total[5m])", "legendFormat": "calls"}],
+        "id": 2
+      },
+      {
+        "title": "OPA Denials (rate 10m)",
+        "type": "graph",
+        "targets": [{"expr": "increase(aegis_agent_opa_denials_total[10m])", "legendFormat": "denials"}],
+        "id": 3
+      }
+    ],
+    "schemaVersion": 16
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: tests/e2e_checklist.md
+# End-to-End Acceptance Checklist
+
+TPU Cloud
+- [ ] Secrets added (GCP_SA_KEY, GCP_PROJECT, COMPLIANCE_BUCKET, AWS creds)
+- [ ] Build & sign image; manifests/image-digests.json updated
+- [ ] Argo workflow run completes; checkpoint uploaded to COMPLIANCE_BUCKET
+- [ ] checkpoint_restore_job.yml validates restore
+- [ ] No TPUJobStuck alerts triggered for the test
+
+Edge
+- [ ] At least one edge device in inventory
+- [ ] Ansible role completes on device
+- [ ] In-cluster hostPath smoke job completes on labeled node
+- [ ] OTA artifact signed and verified by cosign on device
+
+Agentic
+- [ ] Agent controller deployed in staging (k8s mode)
+- [ ] Agent runs multi-step flow that invokes at least two tools
+- [ ] Tool invocations executed as sandboxed Jobs using TokenRequest/projected tokens
+- [ ] OPA denies a malicious tool attempt and logs a denial
+- [ ] Audit events present in audit_collector with tool outputs stored in COMPLIANCE_BUCKET
+
+DevSecOps
+- [ ] CI SBOM and Trivy run for image builds
+- [ ] Image signed with cosign and Rekor entry recorded
+- [ ] Gatekeeper rejects unsigned images (try applying unsigned Pod)
+- [ ] IaC scanning (Checkov/tfsec) runs on PRs
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/e2e_acceptance.yml
+name: E2E Acceptance Orchestrator (manual run)
+on:
+  workflow_dispatch:
+    inputs:
+      run_tpu:
+        description: "Run TPU cloud tests"
+        required: false
+        default: "true"
+      run_edge:
+        description: "Run Edge tests (requires inventory)"
+        required: false
+        default: "false"
+
+jobs:
+  checklist:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Print checklist
+        run: |
+          echo "Run manual E2E checklist from tests/e2e_checklist.md"
+          sed -n '1,200p' tests/e2e_checklist.md
+      - name: Trigger TPU promotion gate (optional)
+        if: ${{ github.event.inputs.run_tpu == 'true' }}
+        run: |
+          echo "This workflow is an orchestrator; it prints checklist and may trigger workflows (manual actions required for secrets and infra)."
+
*** End Patch
*** End Patch
