{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import os\
import logging\
import torch\
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\
from PIL import Image\
import requests\
\
# Set up logging\
logging.basicConfig(level=logging.INFO)\
logger = logging.getLogger(__name__)\
\
class MultimodalAISystem:\
    # Configuration constants\
    MODEL_IDS = \{\
        "core_llm": "deepseek-ai/deepseek-llm-67b",\
        "vision_model": "llava-hf/llava-1.5-7b-hf",\
        "image_gen": "stabilityai/stable-diffusion-xl-base-1.0"\
    \}\
    WEATHER_API_KEY = os.environ.get("WEATHER_API_KEY")  # Set as env var\
\
    def __init__(self):\
        self.device = "cuda" if torch.cuda.is_available() else "cpu"\
        self.models = \{\}\
        self.tokenizer = None\
\
    def load_core_llm(self):\
        try:\
            if "core_llm" not in self.models:\
                logger.info("Loading core LLM model...")\
                self.models["core_llm"] = AutoModelForCausalLM.from_pretrained(\
                    self.MODEL_IDS["core_llm"], torch_dtype=torch.float16 if self.device == "cuda" else torch.float32\
                ).to(self.device)\
                self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_IDS["core_llm"])\
        except Exception as e:\
            logger.error(f"Failed to load core LLM: \{e\}")\
            raise\
\
    def load_vision_model(self):\
        try:\
            if "vision_model" not in self.models:\
                logger.info("Loading vision model...")\
                self.models["vision_model"] = pipeline(\
                    "image-to-text", model=self.MODEL_IDS["vision_model"], device=0 if self.device == "cuda" else -1\
                )\
        except Exception as e:\
            logger.error(f"Failed to load vision model: \{e\}")\
            raise\
\
    def load_image_generator(self):\
        try:\
            if "image_gen" not in self.models:\
                logger.info("Loading image generator...")\
                self.models["image_gen"] = pipeline(\
                    "text-to-image", model=self.MODEL_IDS["image_gen"], device=0 if self.device == "cuda" else -1\
                )\
        except Exception as e:\
            logger.error(f"Failed to load image generator: \{e\}")\
            raise\
\
    def handle_text_query(self, prompt):\
        self.load_core_llm()\
        try:\
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)\
            with torch.no_grad():\
                outputs = self.models["core_llm"].generate(\
                    **inputs,\
                    max_new_tokens=1024,\
                    temperature=0.7,\
                    do_sample=True,\
                    pad_token_id=self.tokenizer.eos_token_id\
                )\
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\
            # Return only the generated part after the prompt\
            answer = response[len(prompt):].strip()\
            return answer\
        except Exception as e:\
            logger.error(f"Text query failed: \{e\}")\
            return "Sorry, I could not process your request."\
\
    def handle_image_query(self, image_path):\
        self.load_vision_model()\
        try:\
            image = Image.open(image_path)\
            result = self.models["vision_model"](image)\
            return result[0]['generated_text'] if result else "No description generated."\
        except Exception as e:\
            logger.error(f"Image query failed: \{e\}")\
            return "Sorry, I could not process your image."\
\
    def generate_image(self, prompt):\
        self.load_image_generator()\
        try:\
            result = self.models["image_gen"](prompt)\
            if isinstance(result, list) and result:\
                image = result[0]\
                output_path = "generated_image.png"\
                image.save(output_path)\
                return f"Image generated and saved to \{output_path\}."\
            return "Image generation failed."\
        except Exception as e:\
            logger.error(f"Image generation failed: \{e\}")\
            return "Sorry, I could not generate the image."\
\
    def access_real_time_data(self, query):\
        # Example for weather data\
        city = self.extract_city_from_query(query)\
        if not city:\
            return "Please specify a city for the weather information."\
        if not self.WEATHER_API_KEY:\
            return "Weather API key is not set."\
        try:\
            response = requests.get(\
                f"https://api.openweathermap.org/data/2.5/weather?q=\{city\}&appid=\{self.WEATHER_API_KEY\}&units=metric"\
            )\
            data = response.json()\
            if response.status_code != 200 or 'main' not in data:\
                return "Failed to retrieve weather data."\
            return f"Current temperature in \{city\} is \{data['main']['temp']\}\'b0C."\
        except Exception as e:\
            logger.error(f"Weather API failed: \{e\}")\
            return "Failed to access real-time data."\
\
    @staticmethod\
    def extract_city_from_query(query):\
        # Very naive city extraction, for real use integrate with NER or regex for cities\
        words = query.split()\
        for i, word in enumerate(words):\
            if word.lower() == "in" and i + 1 < len(words):\
                return words[i+1]\
        return None\
\
    def generate_safe_response(self, query):\
        # Example keyword-based routing; for production use intent classification\
        q_lower = query.lower()\
        if "generate an image" in q_lower:\
            prompt = query.replace("generate an image", "").strip()\
            return self.generate_image(prompt)\
        elif "describe image" in q_lower:\
            # Example assumes user specifies path after the phrase\
            image_path = query.replace("describe image", "").strip()\
            return self.handle_image_query(image_path)\
        elif "weather" in q_lower:\
            return self.access_real_time_data(query)\
        else:\
            return self.handle_text_query(query)\
\
if __name__ == "__main__":\
    ai_system = MultimodalAISystem()\
    print("Multimodal AI System Initialized. Type 'quit' to exit.")\
    try:\
        while True:\
            user_query = input("\\nUser: ")\
            if user_query.lower() == 'quit':\
                print("Goodbye!")\
                break\
            response = ai_system.generate_safe_response(user_query)\
            print(f"Assistant: \{response\}")\
    except KeyboardInterrupt:\
        print("\\nExiting gracefully.")}