  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 url=https://github.com/rsinghunjan/aegis_multimodal_ai_system/blob/main/kserve/triton-inferenceservice.yaml
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "demo-triton-model"
  namespace: "aegis"
spec:
  predictor:
    custom:
      container:
        # This container runs Triton (or an ONNX/Triton-compatible serving image).
        image: "nvcr.io/nvidia/tritonserver:23.08-py3" # adjust to the version you want
        args:
          - tritonserver
          - --model-repository=/models
        # Mountpoint where initContainer will place verified model files
        volumeMounts:
          - name: model-volume
            mountPath: /models
        resources:
          limits:
            nvidia.com/gpu: 1
  # We deploy an initContainer to verify and fetch the signed artifact before Triton starts.
  # This keeps verification on CPU and fail-closes the pod if verification fails.
  # KServe supports init containers via the podSpecOverride; this is an example approach.
  podSpecOverride:
    initContainers:
      - name: verify-and-fetch
        image: "ghcr.io/rsinghunjan/aegis-tools:latest" # image that contains aws, cosign and the helper script
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -euo pipefail
            # Example usage:
            # /opt/aegis/verify_and_fetch.sh s3://$BUCKET/model-archives/demo/$RUN/model.tar.gz /models/model.tar.gz
            /opt/aegis/verify_and_fetch.sh "${ARTIFACT_S3_URI}" /models/model.tar.gz
        env:
          - name: ARTIFACT_S3_URI
            value: "s3://REPLACE_WITH_BUCKET/model-archives/demo/{{workflow.name}}/cifar-demo-0.1.tar.gz"
          - name: ARTIFACT_SIG_S3_URI
            value: "s3://REPLACE_WITH_BUCKET/model-archives/demo/{{workflow.name}}/cifar-demo-0.1.tar.gz.sig"
        volumeMounts:
          - name: model-volume
            mountPath: /models
          - name: cosign-pub
            mountPath: /etc/cosign
            readOnly: true
    containers:
      - name: dummy # required placeholder; KServe will replace with actual server container spec
    volumes:
      - name: model-volume
        emptyDir: {}
      - name: cosign-pub
        secret:
          secretName: cosign-public-key
kserve/triton-inferenceservice.yaml
