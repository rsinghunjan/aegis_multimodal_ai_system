*** Begin Patch
*** Add File:ops/llm/inference_adapter_quant.py
+#!/usr/bin/env python3
+"""
+Inference adapter extension with quantized model routing.
+
+- Detects model metadata (model registry manifest) and loads appropriate runtime:
+  - Triton/FP16 backend (via HTTP) for fp16/bf16 models
+  - Transformers + bitsandbytes for int8/4-bit (bnb/GPTQ/AWQ) on GPU
+  - ggml/llama.cpp runtime for CPU quantized models (edge)
+  - Falls back to existing inference-adapter proxy if model not available locally
+
+Notes:
+ - This module provides helper functions and a small Flask app demonstrating usage.
+ - Production deployments should integrate this logic into the main inference-adapter service
+   and use proper model caching, concurrency control and GPU middleware (CUDA/torch).
+"""
+import os
+import json
+import time
+import logging
+from flask import Flask, request, jsonify
+
+LOG = logging.getLogger("inference-adapter-quant")
+LOG.setLevel(os.environ.get("LOG_LEVEL", "INFO"))
+
+MODEL_REGISTRY_DIR = os.environ.get("MODEL_REGISTRY_DIR", "/models/registry")
+TRITON_ENDPOINT = os.environ.get("TRITON_ENDPOINT", "http://triton-server.aegis.svc.cluster.local:8000")
+
+app = Flask(__name__)
+
+def load_model_metadata(model_name: str):
+    """
+    Look for a model metadata JSON in MODEL_REGISTRY_DIR/<model_name>/metadata.json.
+    Expected fields:
+      - name
+      - version
+      - precision: "fp16" | "bf16" | "int8" | "4bit" | "ggml"
+      - format: "transformers" | "onnx" | "triton" | "ggml"
+      - runtime: recommended runtime string
+    """
+    path = os.path.join(MODEL_REGISTRY_DIR, model_name, "metadata.json")
+    if not os.path.exists(path):
+        return None
+    with open(path, "r") as fh:
+        return json.load(fh)
+
+def call_triton(model_name: str, prompt: str):
+    # Simple Triton HTTP call (placeholder)
+    import requests
+    payload = {"model": model_name, "input": prompt}
+    resp = requests.post(f"{TRITON_ENDPOINT}/v1/models/{model_name}/infer", json=payload, timeout=30)
+    resp.raise_for_status()
+    return resp.json()
+
+# Lazy model cache for bnb/transformers models (simple)
+_bnb_models = {}
+
+def load_bnb_model(model_dir: str, model_name: str):
+    """
+    Load a transformers model with bitsandbytes quantization support.
+    Expects model_dir to contain a transformers-compatible model or repo id.
+    """
+    if model_name in _bnb_models:
+        return _bnb_models[model_name]
+    try:
+        # Import here to keep optional deps optional
+        from transformers import AutoModelForCausalLM, AutoTokenizer
+        import torch
+        import bitsandbytes as bnb
+        LOG.info("Loading bnb model %s from %s", model_name, model_dir)
+        tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
+        model = AutoModelForCausalLM.from_pretrained(
+            model_dir,
+            load_in_8bit=True,
+            device_map="auto",
+        )
+        _bnb_models[model_name] = (model, tokenizer)
+        return model, tokenizer
+    except Exception as e:
+        LOG.exception("Failed to load bnb model: %s", e)
+        return None
+
+def generate_with_bnb(model_name: str, model_dir: str, prompt: str, max_tokens: int = 128):
+    pair = load_bnb_model(model_dir, model_name)
+    if not pair:
+        raise RuntimeError("bnb model not available")
+    model, tokenizer = pair
+    import torch
+    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
+    with torch.no_grad():
+        outputs = model.generate(**inputs, max_new_tokens=max_tokens)
+    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
+    return {"text": text}
+
+def call_ggml_runtime(model_path: str, prompt: str):
+    """
+    Placeholder: call a local ggml-based binary or service (llama.cpp style)
+    For demo purposes we return a stub response.
+    """
+    return {"text": f"[ggml inference stub for {os.path.basename(model_path)}] Echo: {prompt}"}
+
+@app.route("/v1/complete", methods=["POST"])
+def complete():
+    body = request.json or {}
+    model_name = body.get("model", "default")
+    prompt = body.get("prompt", "")
+    meta = load_model_metadata(model_name)
+    if not meta:
+        # fallback to existing adapter (proxy) - return a helpful message for now
+        return jsonify({"error": f"model metadata for {model_name} not found"}), 404
+    precision = meta.get("precision", "fp16")
+    runtime = meta.get("runtime", "")
+    try:
+        if runtime == "triton" or precision in ("fp16", "bf16"):
+            res = call_triton(model_name, prompt)
+            return jsonify(res)
+        elif precision in ("int8", "4bit") and meta.get("format") in ("transformers", "bnb"):
+            model_dir = os.path.join(MODEL_REGISTRY_DIR, model_name)
+            out = generate_with_bnb(model_name, model_dir, prompt, max_tokens=body.get("max_tokens", 128))
+            return jsonify({"id": "bnb-local", "choices": [{"message": {"content": out["text"]}}]})
+        elif precision == "ggml":
+            model_path = os.path.join(MODEL_REGISTRY_DIR, model_name, "model.ggml")
+            out = call_ggml_runtime(model_path, prompt)
+            return jsonify({"id": "ggml-local", "choices": [{"message": {"content": out["text"]}}]})
+        else:
+            return jsonify({"error": "unsupported precision/runtime"}), 400
+    except Exception as e:
+        LOG.exception("Inference failed: %s", e)
+        return jsonify({"error": "internal inference error", "detail": str(e)}), 500
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8081)))
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantization/quantize_bnb.py
+#!/usr/bin/env python3
+"""
+Create an int8 quantized model artifact using bitsandbytes (bnb) / transformers.
+
+This script is a convenience; in many production cases you'll run quantization
+with accelerate or dedicated tools. This script demonstrates the flow and
+produces a model directory suitable for the inference_adapter_quant loader.
+
+Requirements:
+  pip install transformers accelerate bitsandbytes
+
+Usage:
+  python ops/quantization/quantize_bnb.py --source-model facebook/opt-6.7b --out-dir /tmp/quant_model
+"""
+import argparse
+import os
+import shutil
+import logging
+
+logging.basicConfig(level=logging.INFO)
+log = logging.getLogger("quantize-bnb")
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--source-model", required=True, help="transformers model id or local path")
+    p.add_argument("--out-dir", required=True)
+    args = p.parse_args()
+
+    # For simplicity, we will use from_pretrained and save_pretrained with device_map="auto" and load_in_8bit
+    # This produces files that the inference loader can use. In a production flow, run a distributed
+    # conversion and validate outputs.
+    try:
+        from transformers import AutoModelForCausalLM, AutoTokenizer
+        import torch
+        log.info("Loading model %s", args.source_model)
+        tokenizer = AutoTokenizer.from_pretrained(args.source_model, use_fast=True)
+        model = AutoModelForCausalLM.from_pretrained(args.source_model, device_map="auto", load_in_8bit=True)
+        os.makedirs(args.out_dir, exist_ok=True)
+        log.info("Saving tokenizer and model to %s", args.out_dir)
+        tokenizer.save_pretrained(args.out_dir)
+        # Save model in a format that transformers can reload (this may be adapter state)
+        model.save_pretrained(args.out_dir)
+        # Write a basic metadata.json
+        meta = {
+            "name": os.path.basename(args.source_model.replace("/", "-")),
+            "version": "bnb-int8-1",
+            "precision": "int8",
+            "format": "transformers",
+            "runtime": "bnb"
+        }
+        with open(os.path.join(args.out_dir, "metadata.json"), "w") as fh:
+            import json
+            json.dump(meta, fh)
+        log.info("Quantized model artifact created at %s", args.out_dir)
+    except Exception as e:
+        log.exception("Failed to quantize (placeholder): %s", e)
+        raise
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantization/quantize_gptq.py
+#!/usr/bin/env python3
+"""
+GPTQ / AWQ 4-bit quantization helper (placeholder).
+
+This script shows how you would orchestrate an offline GPTQ quantization job.
+It expects the environment to have a GPTQ implementation available (e.g., ngrok/GPTQ repo),
+and it writes metadata.json for the model registry.
+
+In production, prefer an orchestrated job (Argo Workflow) that runs on GPU nodes.
+"""
+import argparse
+import os
+import json
+import logging
+
+logging.basicConfig(level=logging.INFO)
+log = logging.getLogger("quantize-gptq")
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--source-model", required=True)
+    p.add_argument("--out-dir", required=True)
+    p.add_argument("--method", choices=["gptq","awq"], default="gptq")
+    args = p.parse_args()
+
+    os.makedirs(args.out_dir, exist_ok=True)
+    # Placeholder: Operator should run real GPTQ tool here.
+    # For now create a metadata.json to mark this artifact as 4-bit GPTQ.
+    meta = {
+        "name": os.path.basename(args.source_model.replace("/", "-")),
+        "version": f"{args.method}-4bit-1",
+        "precision": "4bit",
+        "format": "transformers",
+        "quant_method": args.method,
+        "runtime": "bnb"
+    }
+    with open(os.path.join(args.out_dir, "metadata.json"), "w") as fh:
+        json.dump(meta, fh)
+    log.info("Wrote placeholder metadata for GPTQ artifact at %s", args.out_dir)
+    log.info("To perform real GPTQ quantization, invoke your GPTQ toolchain on a GPU node and populate out-dir with model artifacts.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantization/validate_quant.py
+#!/usr/bin/env python3
+"""
+A/B testing harness: run prompts against a baseline model and a quantized variant,
+compute simple similarity scores (SequenceMatcher) and report deltas.
+
+This is intentionally lightweight and avoids heavy NLP dependencies.
+Use this as a quick gate in CI to detect catastrophic quality regressions.
+"""
+import argparse
+import json
+import os
+from difflib import SequenceMatcher
+
+def similarity(a: str, b: str) -> float:
+    return SequenceMatcher(None, a, b).ratio()
+
+def call_local_adapter(model: str, prompt: str, url: str):
+    import requests
+    payload = {"model": model, "prompt": prompt, "max_tokens": 64}
+    r = requests.post(f"{url}/v1/complete", json=payload, timeout=30)
+    r.raise_for_status()
+    data = r.json()
+    # Support different output shapes
+    if isinstance(data, dict):
+        # try to extract text
+        try:
+            return data.get("choices", [{}])[0].get("message", {}).get("content", "") or data.get("text", "")
+        except Exception:
+            return json.dumps(data)
+    return str(data)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--baseline-model", required=True)
+    p.add_argument("--quant-model", required=True)
+    p.add_argument("--prompts-file", required=True)
+    p.add_argument("--adapter-url", default="http://localhost:8081")
+    p.add_argument("--report", default="quant_report.json")
+    args = p.parse_args()
+
+    prompts = [l.strip() for l in open(args.prompts_file) if l.strip()]
+    results = []
+    for prompt in prompts:
+        base = call_local_adapter(args.baseline_model, prompt, args.adapter_url)
+        quant = call_local_adapter(args.quant_model, prompt, args.adapter_url)
+        sim = similarity(base, quant)
+        results.append({"prompt": prompt, "baseline": base, "quant": quant, "similarity": sim})
+        print(f"PROMPT: {prompt[:60]}... SIM={sim:.3f}")
+    summary = {
+        "baseline": args.baseline_model,
+        "quant": args.quant_model,
+        "avg_similarity": sum(r["similarity"] for r in results) / len(results) if results else 0.0,
+        "results": results
+    }
+    with open(args.report, "w") as fh:
+        json.dump(summary, fh, indent=2)
+    print(f"Wrote report to {args.report}")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/scheduler/model_placement.py
+#!/usr/bin/env python3
+"""
+Simple placement helper that selects a node pool label for a model artifact based on metadata.
+This can be used by a job admission controller or by a scheduler wrapper to pick the right node selector.
+
+Example mapping:
+ - fp16/bf16 -> gpu.fp16 nodes (large GPU, tensor cores)
+ - int8/4bit -> gpu.small nodes (cheaper GPUs)
+ - ggml -> edge nodes
+"""
+import os
+import json
+
+POOL_MAPPING = {
+    "fp16": {"nodeSelector": {"node-role.kubernetes.io/gpu": "true"}, "tolerations": []},
+    "bf16": {"nodeSelector": {"node-role.kubernetes.io/gpu": "true"}, "tolerations": []},
+    "int8": {"nodeSelector": {"node-role.kubernetes.io/gpu-small": "true"}, "tolerations": []},
+    "4bit": {"nodeSelector": {"node-role.kubernetes.io/gpu-small": "true"}, "tolerations": []},
+    "ggml": {"nodeSelector": {"node-role.kubernetes.io/edge": "true"}, "tolerations": []},
+}
+
+def choose_placement(metadata: dict):
+    precision = metadata.get("precision", "fp16")
+    return POOL_MAPPING.get(precision, POOL_MAPPING["fp16"])
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--metadata", required=True)
+    args = p.parse_args()
+    meta = json.load(open(args.metadata))
+    print(json.dumps(choose_placement(meta), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/registry/model_metadata_schema.json
+{
+  "$schema": "http://json-schema.org/draft-07/schema#",
+  "title": "Aegis Model Metadata",
+  "type": "object",
+  "properties": {
+    "name": { "type": "string" },
+    "version": { "type": "string" },
+    "precision": { "type": "string", "enum": ["fp32", "fp16", "bf16", "int8", "4bit", "ggml"] },
+    "format": { "type": "string", "enum": ["transformers", "onnx", "triton", "ggml"] },
+    "runtime": { "type": "string" },
+    "quant_method": { "type": ["string", "null"] },
+    "requirements": { "type": ["object", "null"] }
+  },
+  "required": ["name", "version", "precision", "format"]
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/inference/model_cache_manager.py
+#!/usr/bin/env python3
+"""
+Model cache manager for multiple precision variants.
+ - Keeps local cache of multiple artifacts under /models/registry/<model>/variants/<variant>
+ - Evicts least recently used variants when disk limit reached.
+"""
+import os
+import time
+import shutil
+from pathlib import Path
+
+CACHE_ROOT = Path(os.environ.get("MODEL_REGISTRY_DIR", "/models/registry"))
+MAX_CACHE_BYTES = int(os.environ.get("MODEL_CACHE_MAX_BYTES", str(50 * 1024 * 1024 * 1024)))  # 50 GiB
+
+def get_cache_size():
+    total = 0
+    for p in CACHE_ROOT.rglob('*'):
+        if p.is_file():
+            total += p.stat().st_size
+    return total
+
+def evict_if_needed():
+    size = get_cache_size()
+    if size <= MAX_CACHE_BYTES:
+        return
+    # Simple eviction: remove oldest variant directories by mtime
+    variants = []
+    for d in CACHE_ROOT.rglob('variants/*'):
+        if d.is_dir():
+            variants.append((d.stat().st_mtime, d))
+    variants.sort()
+    for mtime, d in variants:
+        if get_cache_size() <= MAX_CACHE_BYTES * 0.8:
+            break
+        print(f"Evicting {d}")
+        shutil.rmtree(d, ignore_errors=True)
+
+def ensure_variant_present(model_name: str, variant: str):
+    path = CACHE_ROOT / model_name / "variants" / variant
+    if path.exists():
+        # touch mtime
+        os.utime(path, None)
+        return True
+    return False
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--action", choices=["size","evict","check"], default="size")
+    p.add_argument("--model")
+    p.add_argument("--variant")
+    args = p.parse_args()
+    if args.action == "size":
+        print(get_cache_size())
+    elif args.action == "evict":
+        evict_if_needed()
+    elif args.action == "check":
+        print(ensure_variant_present(args.model, args.variant))
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/pack_ggml.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Convert a model checkpoint to ggml format for llama.cpp/ggml runtimes.
+# This script is a placeholder that outlines the steps. For real conversion,
+# build and run the llama.cpp conversion tool on a machine with sufficient RAM.
+#
+SRC=${1:-}
+OUT=${2:-./artifacts/ggml}
+if [ -z "${SRC}" ]; then
+  echo "Usage: $0 /path/to/checkpoint [out_dir]"
+  exit 2
+fi
+mkdir -p "${OUT}"
+echo "[ggml] Placeholder conversion: copy files and write metadata"
+cp -r "${SRC}"/* "${OUT}/" || true
+cat > "${OUT}/metadata.json" <<EOF
+{
+  "name": "$(basename ${SRC})",
+  "version": "ggml-1",
+  "precision": "ggml",
+  "format": "ggml",
+  "runtime": "ggml"
+}
+EOF
+echo "[ggml] Place converted ggml binary/model under ${OUT}/model.ggml (operator must run actual converter)"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/quantization_pipeline.yml
+name: Quantization Pipeline & A/B Validation
+
+on:
+  workflow_dispatch:
+  push:
+    paths:
+      - 'ops/quantization/**'
+      - 'ops/llm/**'
+
+jobs:
+  quantize-and-validate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.11"
+      - name: Install deps (fast)
+        run: |
+          python -m pip install --upgrade pip
+          pip install transformers bitsandbytes accelerate requests
+      - name: Run bitsandbytes quantization (example)
+        env:
+          SOURCE_MODEL: "facebook/opt-1.3b"
+        run: |
+          python ops/quantization/quantize_bnb.py --source-model "${SOURCE_MODEL}" --out-dir ./artifacts/opt-1.3b-bnb
+      - name: Register metadata (simulate model registry)
+        run: |
+          mkdir -p /tmp/model-registry/opt-1.3b-bnb
+          cp -r artifacts/opt-1.3b-bnb/* /tmp/model-registry/opt-1.3b-bnb/ || true
+          cat /tmp/model-registry/opt-1.3b-bnb/metadata.json || true
+      - name: Start local inference adapter (quant-enabled) in background
+        run: |
+          python -m pip install flask
+          # run the quant adapter on port 8081
+          nohup python ops/llm/inference_adapter_quant.py > /tmp/adapter.log 2>&1 &
+          sleep 3
+      - name: Run A/B validation
+        run: |
+          echo "hello world" > /tmp/prompts.txt
+          python ops/quantization/validate_quant.py --baseline-model "opt-1.3b" --quant-model "opt-1.3b-bnb" --prompts-file /tmp/prompts.txt --adapter-url http://localhost:8081 --report /tmp/quant_report.json
+          cat /tmp/quant_report.json
+      - name: Upload report artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: quant_report
+          path: /tmp/quant_report.json
+
*** End Patch
*** Begin Patch
*** Add File:tests/quant_ab_test.py
+import os
+import json
+from ops.quantization.validate_quant import similarity
+
+def test_sequence_similarity():
+    a = "The quick brown fox"
+    b = "The quick brown fox jumped"
+    s = similarity(a, b)
+    assert 0.6 < s <= 1.0
+
+if __name__ == "__main__":
+    test_sequence_similarity()
+    print("Local quant similarity test passed")
+
*** End Patch
*** Begin Patch
*** Add File:docs/quantization_and_deployment.md
+# Quantization & Deployment Guide (Aegis)
+
+This document describes added artifacts in the quantization patch and how to use them to
+produce quantized model artifacts, validate them (A/B), and deploy via the quant-aware inference adapter.
+
+Files added
+- ops/llm/inference_adapter_quant.py: Example adapter that routes by model metadata and supports bnb/ggml/triton runtimes.
+- ops/quantization/quantize_bnb.py: Produce an int8/bitsandbytes artifact (simple workflow).
+- ops/quantization/quantize_gptq.py: Placeholder for GPTQ/AWQ 4-bit quantization orchestration.
+- ops/quantization/validate_quant.py: Lightweight A/B harness using SequenceMatcher to gate quality regressions.
+- ops/scheduler/model_placement.py: Helper to choose node selectors based on model precision.
+- ops/registry/model_metadata_schema.json: Schema for model metadata stored in the model registry.
+- ops/inference/model_cache_manager.py: LRU-style model variant cache manager.
+- ops/edge/pack_ggml.sh: Placeholder to outline ggml conversion for edge devices.
+- .github/workflows/quantization_pipeline.yml: CI workflow to run quantization and A/B testing.
+
+Quickstart
+1. Convert a model to bitsandbytes int8 (locally or in a GPU staging job):
+   python ops/quantization/quantize_bnb.py --source-model facebook/opt-1.3b --out-dir /tmp/opt-1.3b-bnb
+
+2. Place artifacts in your model registry:
+   mkdir -p /models/registry/opt-1.3b-bnb
+   cp -r /tmp/opt-1.3b-bnb/* /models/registry/opt-1.3b-bnb/
+
+3. Start the quant-aware adapter (for testing):
+   MODEL_REGISTRY_DIR=/models/registry python ops/llm/inference_adapter_quant.py
+
+4. Run the validation harness:
+   python ops/quantization/validate_quant.py --baseline-model opt-1.3b --quant-model opt-1.3b-bnb --prompts-file tests/prompts.txt --adapter-url http://localhost:8081
+
+Notes & Production guidance
+- The quantize_bnb and quantize_gptq scripts are orchestration helpers; run real GPU quantization workflows on GPU nodes via Argo for scale and reliability.
+- Validate quantized artifacts with a robust A/B testing framework for each task/benchmark (not just simple similarity).
+- Integrate placement logic into admission or scheduler to ensure quantized models land on appropriate node pools.
+- Add monitoring: measure latency, token costs, and quality drift for quantized variants to allow automated rollback.
+- For edge CPU deployments, convert artifacts to ggml/llama.cpp formats and distribute via OTA pipeline.
+
*** End Patch
*** End Patch
