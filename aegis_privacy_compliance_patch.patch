*** Begin Patch
*** Add File:compliance/README_PRIVACY.md
+Privacy & Compliance Quick Guide
+
+Purpose:
+- Provide actionable controls and scripts to move Aegis toward compliance with GDPR, HIPAA, CCPA, PCI-DSS.
+
+What’s included:
+- Data Subject Request API (DSR): ui/backend/dsr_api.py
+- Data erasure worker and exporter: compliance/dsr/delete_user_data.py
+- Consent store schema: compliance/sql/001_create_consent_table.sql
+- Audit logging middleware: ui/backend/audit_middleware.py
+- PII/PHI/PCI scanner: compliance/pii_scanner.py
+- Retention CronJob for telemetry/data: compliance/k8s/data_retention_cronjob.yaml
+- S3/KMS policy example: security/s3_kms_policy.json
+- Secret-scan CI (gitleaks) workflow: .github/workflows/secret_scan.yml
+- PCI tokenization proxy example: payments/tokenize_proxy.py
+- Legal templates: legal/data_processing_agreement.md, legal/dpia_checklist.md, legal/breach_notification_template.md
+
+Important next steps (operational):
+1. Legal: sign DPAs / BAA with third parties; perform DPIA for personal data flows.
+2. Security: enable KMS for S3 buckets, configure Key rotation, validate Vault policies, enforce mTLS.
+3. Ops: schedule a staging run of DSR erasure & SAR export; validate backups are deleted per retention policy.
+4. Audit: run PII scanner against all buckets and fix findings.
+
+Notes:
+- These scripts are starting points. Adapt DB queries, bucket names, and access controls to your environment.
+- For PCI: never store PAN; use tokenization providers and ensure your platform never processes raw card numbers.
+
*** End Patch
*** Begin Patch
*** Add File:ui/backend/dsr_api.py
+from flask import Blueprint, request, jsonify
+import os, json, subprocess, uuid, time
+
+bp = Blueprint("dsr", __name__, url_prefix="/api/dsr")
+DSR_STORE = os.environ.get("DSR_STORE", "/var/aegis/dsr")
+os.makedirs(DSR_STORE, exist_ok=True)
+
+def require_auth():
+    # placeholder: integrate with Keycloak / OIDC
+    token = request.headers.get("Authorization")
+    if not token:
+        return False
+    return True
+
+@bp.route("/request", methods=["POST"])
+def create_dsr():
+    if not require_auth():
+        return jsonify({"error":"unauthenticated"}), 401
+    payload = request.json or {}
+    subject = payload.get("subject_id")
+    if not subject:
+        return jsonify({"error":"subject_id required"}), 400
+    req_id = str(uuid.uuid4())
+    rec = {"id": req_id, "subject_id": subject, "type": payload.get("type","access"), "status": "created", "created_at": time.time(), "notes": payload.get("notes","")}
+    with open(os.path.join(DSR_STORE, req_id + ".json"), "w") as f:
+        json.dump(rec, f)
+    # enqueue worker: for demo, call the export tool in background
+    subprocess.Popen(["python3","compliance/dsr/delete_user_data.py","--action","export","--subject",subject,"--out","/tmp/dsr_"+req_id+".tar.gz"])
+    return jsonify({"request_id": req_id, "status":"created"})
+
+@bp.route("/status/<req_id>", methods=["GET"])
+def status(req_id):
+    p = os.path.join(DSR_STORE, req_id + ".json")
+    if not os.path.exists(p):
+        return jsonify({"error":"not found"}), 404
+    return jsonify(json.load(open(p)))
+
+@bp.route("/erase", methods=["POST"])
+def erase():
+    if not require_auth():
+        return jsonify({"error":"unauthenticated"}), 401
+    payload = request.json or {}
+    subject = payload.get("subject_id")
+    if not subject:
+        return jsonify({"error":"subject_id required"}), 400
+    # enqueue erase
+    req_id = str(uuid.uuid4())
+    subprocess.Popen(["python3","compliance/dsr/delete_user_data.py","--action","erase","--subject",subject])
+    return jsonify({"request_id": req_id, "status":"enqueued"})
+
*** End Patch
*** Begin Patch
*** Add File:compliance/dsr/delete_user_data.py
+#!/usr/bin/env python3
+"""
+Delete or export all user-related data for a subject_id.
+This script must be adapted to your storage locations and DB schema.
+Usage:
+  python3 delete_user_data.py --action export --subject alice --out /tmp/alice_bundle.tar.gz
+  python3 delete_user_data.py --action erase --subject alice
+"""
+import argparse, os, tarfile, subprocess, tempfile, shutil, boto3
+
+def export_data(subject, out):
+    tmp = tempfile.mkdtemp(prefix="dsr_")
+    # example: export rows from Postgres (replace with your queries)
+    try:
+        # placeholder: dump DB rows related to subject
+        with open(os.path.join(tmp, "db_export.json"), "w") as f:
+            f.write(f'{{"subject":"{subject}","rows":[]}}')
+        # example: collect S3 objects with subject in key prefix (adapt)
+        s3 = boto3.client("s3")
+        bucket = os.environ.get("MODEL_ARTIFACT_BUCKET")
+        if bucket:
+            # naive prefix search (customize)
+            prefix = f"users/{subject}/"
+            resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
+            for o in resp.get("Contents", []):
+                key = o["Key"]
+                local = os.path.join(tmp, os.path.basename(key))
+                s3.download_file(bucket, key, local)
+        # package tar.gz
+        with tarfile.open(out, "w:gz") as tar:
+            tar.add(tmp, arcname=os.path.basename(tmp))
+    finally:
+        shutil.rmtree(tmp, ignore_errors=True)
+    print("Exported data to", out)
+
+def erase_data(subject):
+    # implement deletion in DB and object storage; must include backups / replicas
+    # WARNING: be careful; this operation is destructive.
+    # Example: delete rows from DB and S3 keys under users/<subject> prefix
+    s3 = boto3.client("s3")
+    bucket = os.environ.get("MODEL_ARTIFACT_BUCKET")
+    if bucket:
+        prefix = f"users/{subject}/"
+        resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
+        for o in resp.get("Contents", []):
+            s3.delete_object(Bucket=bucket, Key=o["Key"])
+    # DB deletion placeholder (implement with real DB connection)
+    print("Erased data for", subject)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--action", choices=["export","erase"], required=True)
+    p.add_argument("--subject", required=True)
+    p.add_argument("--out", default="/tmp/dsr_out.tar.gz")
+    args = p.parse_args()
+    if args.action == "export":
+        export_data(args.subject, args.out)
+    else:
+        erase_data(args.subject)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/sql/001_create_consent_table.sql
+-- Consent table for tracking user consents (simple baseline)
+CREATE TABLE IF NOT EXISTS user_consent (
+  id SERIAL PRIMARY KEY,
+  subject_id TEXT NOT NULL,
+  consent_scope TEXT NOT NULL,
+  granted BOOLEAN NOT NULL DEFAULT FALSE,
+  granted_at TIMESTAMPTZ NULL,
+  revoked_at TIMESTAMPTZ NULL,
+  metadata JSONB NULL
+);
+
*** End Patch
*** Begin Patch
*** Add File:ui/backend/audit_middleware.py
+import time, json, os
+from flask import request
+AUDIT_LOG = os.environ.get("AUDIT_LOG", "/var/log/aegis/audit.log")
+SIEM_WEBHOOK = os.environ.get("SIEM_WEBHOOK")
+
+def audit_record(app):
+    @app.before_request
+    def _audit_before():
+        # minimal audit event
+        ev = {
+            "ts": time.time(),
+            "method": request.method,
+            "path": request.path,
+            "remote_addr": request.remote_addr,
+            "user": request.headers.get("X-User","unknown"),
+            "auth": "bearer" in (request.headers.get("Authorization") or "").lower()
+        }
+        try:
+            os.makedirs(os.path.dirname(AUDIT_LOG), exist_ok=True)
+            with open(AUDIT_LOG, "a") as f:
+                f.write(json.dumps(ev) + "\n")
+        except Exception:
+            pass
+        # optional SIEM forward
+        if SIEM_WEBHOOK:
+            try:
+                import requests
+                requests.post(SIEM_WEBHOOK, json=ev, timeout=2)
+            except Exception:
+                pass
+
*** End Patch
*** Begin Patch
*** Add File:compliance/pii_scanner.py
+#!/usr/bin/env python3
+"""
+Simple PII scanner for local directories or S3 bucket prefixes.
+Uses regex to find potential emails, SSNs, credit card numbers.
+Not exhaustive; use as baseline and extend with specialized DLP for production.
+"""
+import re, os, argparse, boto3, tempfile
+
+PII_PATTERNS = {
+    "email": re.compile(rb"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+"),
+    "ssn": re.compile(rb"\b\d{3}-\d{2}-\d{4}\b"),
+    "credit_card": re.compile(rb"\b(?:\d[ -]*?){13,16}\b")
+}
+
+def scan_file(path):
+    findings = []
+    with open(path,"rb") as f:
+        data = f.read()
+    for k,p in PII_PATTERNS.items():
+        if p.search(data):
+            findings.append(k)
+    return findings
+
+def scan_local(dirpath):
+    out = {}
+    for root,_,files in os.walk(dirpath):
+        for fn in files:
+            path = os.path.join(root,fn)
+            try:
+                f = scan_file(path)
+                if f:
+                    out[path] = f
+            except Exception:
+                pass
+    return out
+
+def scan_s3(bucket, prefix="", max_objects=1000):
+    s3 = boto3.client("s3")
+    out = {}
+    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=max_objects)
+    for o in resp.get("Contents", []):
+        key = o["Key"]
+        local = tempfile.NamedTemporaryFile(delete=False)
+        s3.download_file(bucket, key, local.name)
+        try:
+            f = scan_file(local.name)
+            if f:
+                out[key] = f
+        except Exception:
+            pass
+    return out
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--local", help="local dir to scan")
+    p.add_argument("--s3-bucket", help="s3 bucket to scan")
+    p.add_argument("--s3-prefix", default="")
+    args = p.parse_args()
+    if args.local:
+        print(scan_local(args.local))
+    if args.s3_bucket:
+        print(scan_s3(args.s3_bucket, args.s3_prefix))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/k8s/data_retention_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-data-retention
+  namespace: aegis
+spec:
+  schedule: "0 3 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-agent
+          containers:
+          - name: retention
+            image: python:3.10-slim
+            command: ["bash","-lc"]
+            args:
+              - pip install boto3 && python3 compliance/dsr/delete_user_data.py --action erase --subject retention_dummy || true
+            env:
+              - name: RETENTION_DAYS
+                value: "365"
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:security/s3_kms_policy.json
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {
+      "Sid":"EnforceEncryptionInTransitAndAtRest",
+      "Effect":"Allow",
+      "Principal": {"AWS":"*"},
+      "Action":["s3:PutObject"],
+      "Resource":["arn:aws:s3:::your-bucket-name/*"],
+      "Condition":{
+        "StringEquals":{"s3:x-amz-server-side-encryption":"aws:kms"}
+      }
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/secret_scan.yml
+name: Secret scan
+on:
+  push:
+    branches: ["main"]
+  pull_request:
+    branches: ["main"]
+
+jobs:
+  gitleaks-scan:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run gitleaks
+        uses: zricethezav/gitleaks-action@v2
+        with:
+          args: detect --source . --report-path gitleaks_report.json
+      - name: Upload report
+        if: failure()
+        uses: actions/upload-artifact@v4
+        with:
+          name: gitleaks-report
+          path: gitleaks_report.json
+
*** End Patch
*** Begin Patch
*** Add File:payments/tokenize_proxy.py
+#!/usr/bin/env python3
+"""
+Payment tokenization proxy: accept payment data (from UI), forward to tokenization provider and return token.
+DO NOT log or store PAN. This proxy should run in an isolated, minimal-scope environment and only handle transient in-memory requests.
+"""
+from flask import Flask, request, jsonify
+import os, requests
+
+app = Flask(__name__)
+TOKENIZER_URL = os.environ.get("TOKENIZER_URL","https://tokenizer.example/v1/tokenize")
+API_KEY = os.environ.get("TOKENIZER_API_KEY","")
+
+@app.route("/tokenize", methods=["POST"])
+def tokenize():
+    payload = request.json or {}
+    card = payload.get("card")
+    if not card:
+        return jsonify({"error":"card required"}), 400
+    # Forward to tokenization provider; do not log card details
+    resp = requests.post(TOKENIZER_URL, json={"card": card}, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=10)
+    resp.raise_for_status()
+    token = resp.json().get("token")
+    return jsonify({"token": token})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8089")))
+
*** End Patch
*** Begin Patch
*** Add File:legal/data_processing_agreement.md
+# Data Processing Agreement (DPA) — Template
+
+Parties:
+- Data Controller: [CONTROLLER NAME]
+- Data Processor: Aegis [YOUR ORG]
+
+Purpose:
+- This DPA governs processing of personal data by the Data Processor on behalf of the Controller.
+
+Key Clauses (fill in):
+- Subject matter and duration
+- Nature and purpose of processing
+- Types of personal data and categories of data subjects
+- Obligations of the Processor: confidentiality, technical & organizational measures (list KMS, Vault, HSM, encryption, access logging), subprocessor management.
+- Assistance with Data Subject Requests (DSR)
+- Security incident / breach notification timelines (e.g., 72 hours)
+- Data deletion & return on termination
+- Audit rights & certifications (SOC2, ISO27001)
+
+This is a template. Legal must finalize and sign with customers & vendors.
+
*** End Patch
*** Begin Patch
*** Add File:legal/dpia_checklist.md
+# DPIA (Data Protection Impact Assessment) Checklist
+
+1. Describe processing: purpose, categories of data subjects, types of personal data.
+2. Necessity & proportionality: why processing is necessary and minimal.
+3. Risks to rights & freedoms: identify significant risks.
+4. Mitigations: encryption, anonymization, access controls, retention periods.
+5. Data transfers: cross-border transfers and safeguards (SCCs).
+6. Consultation: involve Data Protection Officer and legal.
+7. Documentation & record: store DPIA results and review schedule.
+
*** End Patch
*** Begin Patch
*** Add File:legal/breach_notification_template.md
+Subject: Security Incident Notification — [Short description]
+
+Body:
+Date/time detected: [UTC timestamp]
+Description: [summary of incident]
+Data affected: [categories of personal data]
+Likely impact: [assessment]
+Mitigation steps taken: [steps]
+Contact: [security@org.example]
+
+Follow-up: detailed report within [X days], remediation timeline, regulatory notifications as required.
+
*** End Patch
*** End Patch
