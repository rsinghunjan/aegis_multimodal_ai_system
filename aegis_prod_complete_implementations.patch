*** Begin Patch
*** Add File:scripts/staging_installer.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Staging installer for Aegis full E2E demo and productionized Feast + Redis + telemetry + HSM validation + federation test
+# Usage:
+#   export KUBECONFIG=...
+#   export MODEL_ARTIFACT_BUCKET=...
+#   export EVIDENCE_BUCKET=...
+#   export VAULT_ADDR=... VAULT_TOKEN=...
+#   ./scripts/staging_installer.sh
+
+NAMESPACE="${NAMESPACE:-aegis}"
+EVIDENCE_BUCKET="${EVIDENCE_BUCKET:-}"
+MODEL_ARTIFACT_BUCKET="${MODEL_ARTIFACT_BUCKET:-}"
+
+echo "Starting Aegis staging installer..."
+
+function check_tool() {
+  if ! command -v "$1" >/dev/null 2>&1; then
+    echo "Missing required tool: $1" >&2
+    exit 2
+  fi
+}
+
+for t in kubectl helm argo aws; do
+  check_tool $t || true
+done
+
+echo "Create namespace ${NAMESPACE}"
+kubectl create ns "${NAMESPACE}" || true
+
+echo "1) Install Redis Cluster (bitnami) via helm"
+helm repo add bitnami https://charts.bitnami.com/bitnami || true
+helm repo update || true
+kubectl -n "${NAMESPACE}" create secret generic redis-creds --from-literal=password="$(openssl rand -hex 16)" || true
+helm upgrade --install redis-cluster bitnami/redis-cluster -n "${NAMESPACE}" -f redis/redis-cluster-helm-values-production.yaml
+
+echo "2) Install Feast (helm chart provided)"
+helm upgrade --install aegis-feast helm/feast -n "${NAMESPACE}" -f helm/feast/production-values.yaml --wait
+
+echo "3) Install Kafka (Strimzi) for streaming"
+kubectl apply -f kafka/strimzi/kafka-cluster.yaml || true
+
+echo "4) Install Prometheus & Grafana (assumes kube-prometheus-stack available)"
+helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
+helm repo update || true
+helm upgrade --install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace --wait
+
+echo "Apply relabeling config and recording rules"
+kubectl apply -f observability/prometheus/relabelling_enforce_configmap.yaml || true
+kubectl apply -f observability/prometheus/recording_rules_strict.yaml || true
+
+echo "5) Deploy Thanos receive (optional stub) - operator recommended for prod"
+# For staging we won't install a full Thanos stack; expect an existing thanos-receive endpoint when present
+echo "If you need Thanos, deploy your provider or use observability/prometheus/remote_write_retention_example.yaml"
+
+echo "6) Deploy HSM helper cronjob and healthcheck (if HSM available)"
+kubectl apply -f ops/hsm/hsm-latency-cronjob.yaml -n "${NAMESPACE}" || true
+
+echo "7) Deploy Quantum operator (simulator-first) if desired"
+kubectl apply -f k8s/crd/quantumjob_crd.yaml || true
+# The operator image must be built & pushed; operator can be run locally too
+
+echo "8) Deploy federation server (for DP tests)"
+kubectl apply -f federation/k8s/federation-deploy.yaml || true
+
+echo "9) Deploy Marquez connector & Amundsen skeleton"
+kubectl apply -f marquez/deployment/marquez-deploy.yaml -n "${NAMESPACE}" || true
+kubectl apply -f catalog/marquez-connector-cronjob.yaml -n "${NAMESPACE}" || true
+
+echo "10) Run staging E2E demo (Argo) and collect evidence"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/world_model_pipeline.yaml -n "${NAMESPACE}" --watch || true
+else
+  echo "argo CLI not found; please run argo submit argo/world_model_pipeline.yaml -n ${NAMESPACE} manually"
+fi
+
+echo "11) Run Bench & acceptance tests"
+bash scripts/run_acceptance_tests.sh || true
+
+echo "Staging installer completed. Check Argo workflows and logs. Upload any evidence bundles to ${EVIDENCE_BUCKET} (if configured)."
+
*** End Patch
*** Begin Patch
*** Add File:scripts/run_acceptance_tests.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NAMESPACE="${NAMESPACE:-aegis}"
+EVIDENCE_BUCKET="${EVIDENCE_BUCKET:-}"
+echo "Running acceptance tests: Redis / Feast bench, HSM validation, federation secure-agg, telemetry audit"
+
+echo "A) Run Feast/Redis benchmark via Argo"
+kubectl -n "${NAMESPACE}" apply -f argo/benchmarks/feast_redis_benchmark.yaml || true
+echo "Launched bench workflow; check Argo UI for completion"
+
+echo "B) Run HSM validation workflow"
+kubectl -n "${NAMESPACE}" apply -f ops/hsm/hsm_validation_workflow.yaml || true
+
+echo "C) Run federation secure-aggregation demo locally (requires python deps)"
+python3 -c "from federation import client_sdk; cid=client_sdk.register({'name':'ci'}); client_sdk.submit(cid,[0.1,0.2,0.3],n=10); print('submitted and aggregate', client_sdk.aggregate())"
+
+echo "D) Run metric cardinality audit immediately (job scheduled hourly as cronjob)"
+kubectl -n monitoring apply -f observability/metric_cardinality_cronjob.yaml || true
+
+echo "E) Collect artifacts & assemble audit bundle"
+python3 compliance/assemble_audit_bundle.py || true
+if [ -n \"$EVIDENCE_BUCKET\" ]; then
+  echo \"Evidence bundle uploaded to ${EVIDENCE_BUCKET} (if configured by bundle script)\"
+fi
+
+echo "Acceptance tests triggered. Please inspect Argo workflows, HSM reports, and audit bundles for pass/fail."
+
*** End Patch
*** Begin Patch
*** Add File:helm/feast/values-production-tuned.yaml
+# Tuned Feast values for production-like staging (resources tuned for P95 SLOs)
+image:
+  repository: registry.example.com/aegis/feast
+  tag: stable
+offlineStore:
+  s3:
+    bucket: ${MODEL_ARTIFACT_BUCKET:-"your-bucket"}
+    prefix: feast/offline
+onlineStore:
+  type: redis
+  redis:
+    host: redis-headless.aegis.svc
+    port: 6379
+job:
+  resources:
+    requests:
+      cpu: "1"
+      memory: "2Gi"
+    limits:
+      cpu: "4"
+      memory: "8Gi"
+service:
+  type: ClusterIP
+prometheus:
+  enabled: true
+  serviceMonitor:
+    enabled: true
+    interval: "15s"
+hpa:
+  enabled: true
+  minReplicas: 1
+  maxReplicas: 5
+
*** End Patch
*** Begin Patch
*** Add File:scripts/feast_tune_playbook.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Run the Redis/Feast bench and provide simple scaling guidance (non-automatic)
+NAMESPACE="${NAMESPACE:-aegis}"
+THRESHOLD_P95_MS="${THRESHOLD_P95_MS:-50}"
+echo "Running Redis latency bench (short sample) to measure current P95"
+python3 benchmarks/redis_latency_bench.py --host redis-headless.aegis.svc --port 6379 --duration 20 --qps 200 > /tmp/redis_bench.json || true
+cat /tmp/redis_bench.json
+P95=$(python3 -c "import sys,json; j=json.load(open('/tmp/redis_bench.json')); print(j.get('p95_ms',0))")
+echo "Observed P95: ${P95} ms"
+if [ -z "${P95}" ] || [ "${P95}" == "None" ] ; then
+  echo "No P95 result; check bench job logs"
+  exit 1
+fi
+if (( $(echo "$P95 > $THRESHOLD_P95_MS" | bc -l) )); then
+  echo "P95 above threshold ${THRESHOLD_P95_MS}ms. Suggested actions:"
+  echo "- Increase Redis CPU/memory and number of replicas"
+  echo "- Move Redis pods to faster nodes or add PodAffinity to NVMe nodes"
+  echo "- Ensure Feast stream processor has resources and multiple replicas"
+  echo "- Consider using managed Redis (ElastiCache) if cluster performance limited"
+else
+  echo "P95 within threshold"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/k8s/hsm-rotation-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: hsm-rotation-run
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: aegis-agent
+      containers:
+        - name: hsm-rotation
+          image: python:3.10-slim
+          command: ["bash","-lc"]
+          args:
+            - pip install boto3 && bash ops/hsm/hsm_rotation_and_validation.sh
+      restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/vendor_rotation_integration.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper to call vendor-specific HSM rotation CLI if available.
+# This script will:
+#  - detect vendor-specific CLI (example placeholders: yubihsmctl, utimaco-cli)
+#  - run rotation command (placeholder); operators must edit to match vendor CLI arguments.
+LOG_DIR=${LOG_DIR:-/tmp/hsm_rotation}
+mkdir -p "$LOG_DIR"
+TIMESTAMP=$(date -u +%Y%m%dT%H%M%SZ)
+LOG="$LOG_DIR/rotation_${TIMESTAMP}.log"
+
+echo "Starting vendor rotation at $(date -u)" | tee -a "$LOG"
+
+if command -v yubihsmctl >/dev/null 2>&1; then
+  echo "Detected yubihsmctl - running example rotate command (DRY-RUN)..." | tee -a "$LOG"
+  echo "yubihsmctl --rotate-key --keyid 1 --label aegis-signing-key" | tee -a "$LOG"
+  # Actual command commented out - operator must enable after review:
+  # yubihsmctl --rotate-key --keyid 1 --label aegis-signing-key >> "$LOG" 2>&1
+elif command -v utimaco-cli >/dev/null 2>&1; then
+  echo "Detected utimaco-cli - run your rotation command here (example placeholder)" | tee -a "$LOG"
+  # Example:
+  # utimaco-cli rotate-key --label aegis-signing-key >> "$LOG" 2>&1
+else
+  echo "No known vendor CLI found. Rotation must be performed using vendor instructions. See ops/hsm/README_VENDOR.md" | tee -a "$LOG"
+fi
+
+echo "Completed rotation wrapper at $(date -u)" | tee -a "$LOG"
+
*** End Patch
*** Begin Patch
*** Add File:federation/crypten_orchestrator.py
+#!/usr/bin/env python3
+"""
+Simple demo orchestrator to run local multi-party CrypTen secure aggregation worker processes.
+This launches N worker processes on the same host for demonstration/testing (not production).
+Each worker will run federation/crypten_worker.py
+"""
+import subprocess, sys, os, time
+
+NUM_WORKERS = int(os.environ.get("CRYPTEN_WORKERS","3"))
+PYTHON = sys.executable
+workers=[]
+for i in range(NUM_WORKERS):
+    env = os.environ.copy()
+    env["CRYPTEN_RANK"] = str(i)
+    env["CRYPTEN_WORLD_SIZE"] = str(NUM_WORKERS)
+    p = subprocess.Popen([PYTHON, "federation/crypten_worker.py"], env=env)
+    workers.append(p)
+    time.sleep(0.2)
+
+print(f"Launched {NUM_WORKERS} crypten workers (PIDs {[p.pid for p in workers]})")
+for p in workers:
+    p.wait()
+    print("Worker exit code:", p.returncode)
+
*** End Patch
*** Begin Patch
*** Add File:federation/crypten_worker.py
+#!/usr/bin/env python3
+"""
+CrypTen worker stub for secure aggregation demo.
+This script assumes crypten and torch are installed and will run a simple secure sum
+within a single-machine multi-process environment.
+"""
+import os, time, json
+try:
+    import torch
+    import crypten
+    import crypten.communicator as comm
+except Exception as e:
+    print("Crypten not installed or not configured:", e)
+    raise
+
+def main():
+    rank = int(os.environ.get("CRYPTEN_RANK","0"))
+    world_size = int(os.environ.get("CRYPTEN_WORLD_SIZE","1"))
+    crypten.init()
+    # Use file store for single-machine simulation if available
+    x = torch.tensor([float(rank+1), 0.5], dtype=torch.float32)
+    enc = crypten.cryptensor(x)
+    s = enc
+    for i in range(1, world_size):
+        if i == rank:
+            continue
+    # In single-process demo, sum by using all-reduce via crypten
+    sum_enc = crypten.communicator.get().allreduce(enc)
+    res = sum_enc.get_plain_text()
+    print(f"Rank {rank} sum result:", res.tolist())
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:observability/thanos/README_THANOS.md
+# Thanos / Long-term metrics guidance for Aegis
+
+This folder contains guidance & example remote_write configuration provided earlier.
+
+Key steps:
+1. Deploy Thanos Receive/Store/Query cluster (managed or self-hosted). For production, use HA and proper object storage backend.
+2. Configure Prometheus remote_write using observability/prometheus/remote_write_retention_example.yaml (ensure secret prometheus-remote-write-secret exists).
+3. Use write_relabel_configs to drop or aggregate high-cardinality labels before sending to Thanos (see example).
+4. Set retention and compaction settings on Thanos side to control cost.
+
+Note: Installing a full Thanos stack is non-trivial; use a managed solution if possible.
+
*** End Patch
*** Begin Patch
*** Add File:observability/prometheus/thanos_remote_write_secret.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: prometheus-remote-write-secret
+  namespace: monitoring
+type: Opaque
+stringData:
+  token: "__REPLACE_WITH_TOKEN__"
+
*** End Patch
*** Begin Patch
*** Add File:compliance/README_DPA_PROCESS.md
+# DPA / BAA Process and checklist
+
+This document lists the steps to engage legal and operations to complete DPA/BAA and external audit readiness:
+
+1. Inventory all subprocessors and vendors (HSM vendor, cloud storage, IBMQ/Braket if used).
+2. Populate compliance/DPA_Documentation_TEMPLATE.md and attach signed agreements in /var/aegis/legal.
+3. Run compliance/assemble_audit_bundle.py weekly and deliver to legal for review.
+4. Coordinate with external auditor (SOC2/HIPAA) to schedule readiness assessment.
+
*** End Patch
*** Begin Patch
*** Add File:docs/DEPLOY_HSM_VENDOR_INSTRUCTIONS.md
+# HSM vendor rotation & validation instructions (operator guidance)
+
+1. Get vendor CLI & credentials (e.g., yubihsmctl, utimaco-cli, nshield CLI) and install on a bastion/or operator container.
+2. Review ops/hsm/vendor_rotation_integration.sh and replace placeholders with vendor-specific rotation command(s).
+3. Test rotation in staging:
+   - Run ops/hsm/k8s/hsm-rotation-job.yaml job (kubectl -n aegis apply -f ops/hsm/k8s/hsm-rotation-job.yaml)
+   - Verify logs at /tmp/hsm_rotation/* and upload to evidence bucket.
+4. Run ops/hsm/vendor_validation_suite.py to capture latency & success-rate reports; include in evidence bundle.
+
*** End Patch
*** Begin Patch
*** Add File:training/world_model/README_RUNBOOK_TRAINING.md
+# Training runbook for world models (ops)
+
+1. Provision GPU nodes per docs/WORLD_MODEL_COMPUTE_SIZING.md
+2. Build trainer image: docker build -t registry.example.com/aegis/world-model-trainer:latest -f Dockerfile .
+3. Push image to registry and update training/world_model/job.yaml with image
+4. Start Argo pipeline: argo submit argo/world_model_pipeline.yaml -n aegis --watch
+5. Monitor MLflow, GPU utilization and NVMe usage. Collect checkpoints, sign and upload via MCP process.
+
*** End Patch
*** End Patch
