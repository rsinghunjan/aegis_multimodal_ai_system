*** Begin Patch
*** Add File:issues/FINAL_ISSUES_FOR_IMPORT.md
+# Final prioritized issues (ready to import)
+
+Use this file to create GitHub Issues. Each entry contains title, description, acceptance criteria, estimate (days), priority and suggested owner.
+
+- Title: Confirm telemetry collection & baseline health
+  Description: |
+    Deploy node-power-exporter DaemonSet to representative node types. Ensure telemetry is collected for at least 7–14 days for representative node types and workload classes.
+    Run telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 14 and upload reports to S3.
+  Acceptance criteria:
+    - S3 reports exist under s3://<EVIDENCE_BUCKET>/telemetry/health/
+    - Prometheus shows node-power-exporter targets healthy and metrics flowing for representative nodes.
+  Estimate: 2–5
+  Priority: high
+  Owner: @platform
+
+- Title: Iterative calibration & holdout validation (staging)
+  Description: |
+    Run calibration convergence workflow (ci/calibration_converge.yml or calibration/iterative_convergence_runner.py).
+    If median_relative_error > 0.10, iterate calibration & retraining until threshold reached for representative workloads.
+    Upload validation reports and draft profiles to S3 under calibration/.
+  Acceptance criteria:
+    - validation report median_relative_error ≤ 0.10 for representative workloads over a 14-day window.
+    - Draft profiles present under s3://<EVIDENCE_BUCKET>/calibration/drafts/
+  Estimate: 3–14 (iterative)
+  Priority: high
+  Owner: @ml-platform
+
+- Title: Enforcement staged rollout & coverage validation
+  Description: |
+    Deploy scheduler extender and Argo admission enforcer to staging. Run enforcement/test_harness/enforcement_coverage_harness.py and iterate policy rules until enforcement_coverage ≥ 0.95.
+    Keep logs and coverage reports in S3 for audit.
+  Acceptance criteria:
+    - enforcement_coverage ≥ 0.95 for harness runs and representative traffic.
+    - Coverage reports present in s3://<EVIDENCE_BUCKET>/enforcement/coverage_reports/
+  Estimate: 7–14
+  Priority: high
+  Owner: @k8s-sre
+
+- Title: DW provisioning, bulk load & MERGE validation (dev)
+  Description: |
+    Provision a dev warehouse (Redshift/BigQuery/Snowflake) or use an existing dev cluster.
+    Run parquet pipeline + manifest + bulk-load into dev and validate MERGE/upsert idempotency and query performance.
+    Tune parquet batch size and partitioning based on data sample.
+  Acceptance criteria:
+    - Idempotent loads (re-running pipeline does not duplicate data).
+    - Manifest and loader logs present in S3 and DW.
+    - Parquet tuning recommendations applied and validated.
+  Estimate: 5–10
+  Priority: high
+  Owner: @data-eng
+
+- Title: Security finalization (PKI, KMS, cosign, RBAC)
+  Description: |
+    Provision corporate CA secret for cert-manager, create KMS key(s) for cosign with restrictive policies, rotate cosign key in staging and record Rekor/log entry, run RBAC audit and remediate broad privileges.
+  Acceptance criteria:
+    - cert-manager corporate ClusterIssuer active and webhooks have valid CA bundle.
+    - COSIGN_KMS_KEY_ARN secret updated and rotation recorded.
+    - RBAC audit report in s3://<EVIDENCE_BUCKET>/security/
+  Estimate: 7–14
+  Priority: high
+  Owner: @security
+
+- Title: Operationalization: alerts, DR drills & runbook exercises
+  Description: |
+    Wire Alertmanager to Slack and PagerDuty, schedule monthly DR/resume drills, execute runbook exercises and collect drill artifacts to evidence S3.
+  Acceptance criteria:
+    - Alertmanager configured with Slack/PagerDuty receivers.
+    - Monthly DR DAG executed and drill artifacts archived.
+    - Runbook exercise findings documented and remediations tracked.
+  Estimate: 3–7
+  Priority: medium
+  Owner: @sre
+
+---
+Notes:
+- Replace <EVIDENCE_BUCKET> and other placeholders before creating issues.
+- Link to relevant files in each issue (scripts, manifests, runbooks).
+
*** End Patch
*** Begin Patch
*** Add File:prs/open_pr_and_push.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper to create a branch, push, and open a PR using the GitHub CLI `gh`.
+#
+# Usage:
+#   ./open_pr_and_push.sh <owner/repo> <base_branch> "<pr_title>" "<pr_body>"
+#
+REPO=$1
+BASE=${2:-main}
+TITLE=${3:-"Aegis: finalize production readiness"}
+BODY=${4:-"This PR adds production readiness workflows, enforcement harness, DW loader job, prioritized backlog, operator handoff, and audit collection tooling. Deploy to staging and validate before promoting to prod."}
+
+BRANCH="aegis/prod-finalize-$(date -u +%Y%m%dT%H%M%SZ)"
+
+git checkout -b "$BRANCH"
+git add .
+git commit -m "$TITLE"
+git push -u origin "$BRANCH"
+
+if command -v gh >/dev/null 2>&1; then
+  echo "Opening PR via gh..."
+  gh pr create --repo "$REPO" --base "$BASE" --head "$BRANCH" --title "$TITLE" --body "$BODY"
+else
+  echo "gh CLI not found. Branch pushed to origin/$BRANCH. Create PR at: https://github.com/$REPO/compare/$BRANCH?expand=1"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:operator/populate_handoff.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Populate the operator handoff template with your environment values.
+#
+# Usage:
+#   ./populate_handoff.sh \
+#     --evidence-bucket my-evidence-bucket \
+#     --aws-region us-west-2 \
+#     --cosign-kms arn:aws:kms:... \
+#     --redshift-dsn "postgresql://user:pass@host:5439/db" \
+#     --redshift-role-arn arn:aws:iam::123456789012:role/aegis-redshift \
+#     --kubeconfig-staging /path/to/staging-kubeconfig \
+#     --kubeconfig-prod /path/to/prod-kubeconfig \
+#     --pushgateway pushgateway.monitoring.svc:9091 \
+#     --slack-webhook https://hooks.slack.com/services/...
+#
+ARGS=$(getopt -o '' -l evidence-bucket:,aws-region:,cosign-kms:,redshift-dsn:,redshift-role-arn:,kubeconfig-staging:,kubeconfig-prod:,pushgateway:,slack-webhook:,contact-email: -- "$@")
+eval set -- "$ARGS"
+
+declare -A V
+while true; do
+  case "$1" in
+    --evidence-bucket) V[EVIDENCE_BUCKET]=$2; shift 2;;
+    --aws-region) V[AWS_REGION]=$2; shift 2;;
+    --cosign-kms) V[COSIGN_KMS_KEY_ARN]=$2; shift 2;;
+    --redshift-dsn) V[REDSHIFT_DSN]=$2; shift 2;;
+    --redshift-role-arn) V[REDSHIFT_IAM_ROLE_ARN]=$2; shift 2;;
+    --kubeconfig-staging) V[KUBECONFIG_STAGING]=$2; shift 2;;
+    --kubeconfig-prod) V[KUBECONFIG_PROD]=$2; shift 2;;
+    --pushgateway) V[PUSHGATEWAY]=$2; shift 2;;
+    --slack-webhook) V[SLACK_WEBHOOK]=$2; shift 2;;
+    --contact-email) V[CONTACT_EMAIL]=$2; shift 2;;
+    --) shift; break;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+OUT="/tmp/OPERATOR_HANDOFF_POPULATED.md"
+TEMPLATE="operator/OPERATOR_HANDOFF_FINAL.md.template"
+if [ ! -f "$TEMPLATE" ]; then
+  echo "Template missing: $TEMPLATE"
+  exit 2
+fi
+
+cp "$TEMPLATE" "$OUT"
+for k in "${!V[@]}"; do
+  sed -i "s|{{${k}}}|${V[$k]}|g" "$OUT"
+done
+
+echo "Populated handoff written to $OUT"
+echo "Review and commit it into repo or distribute to operators."
+
*** End Patch
*** Begin Patch
*** Add File:operator/OPERATOR_HANDOFF_FINAL.md.template
+# Operator Handoff — Final (populated by populate_handoff.sh)
+
+Replace placeholders with actual values or use populate_handoff.sh to produce the final file.
+
+EVIDENCE_BUCKET: {{EVIDENCE_BUCKET}}
+AWS_REGION: {{AWS_REGION}}
+COSIGN_KMS_KEY_ARN: {{COSIGN_KMS_KEY_ARN}}
+REDSHIFT_DSN: {{REDSHIFT_DSN}}
+REDSHIFT_IAM_ROLE_ARN: {{REDSHIFT_IAM_ROLE_ARN}}
+KUBECONFIG_STAGING: {{KUBECONFIG_STAGING}}
+KUBECONFIG_PROD: {{KUBECONFIG_PROD}}
+PUSHGATEWAY: {{PUSHGATEWAY}}
+SLACK_WEBHOOK: {{SLACK_WEBHOOK}}
+CONTACT_EMAIL: {{CONTACT_EMAIL}}
+
+Steps (exact commands)
+
+1) Telemetry healthcheck (staging)
+ - export EVIDENCE_BUCKET={{EVIDENCE_BUCKET}} AWS_REGION={{AWS_REGION}}
+ - python3 telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 14
+ - aws s3 ls s3://{{EVIDENCE_BUCKET}}/telemetry/health/
+
+2) Enforcement harness (staging)
+ - Ensure extender & enforcer deployed in staging and MutatingWebhookConfiguration CA is valid.
+ - python3 enforcement/test_harness/enforcement_coverage_harness.py
+ - aws s3 ls s3://{{EVIDENCE_BUCKET}}/enforcement/coverage_reports/
+ - If coverage < 0.95:
+   kubectl -n aegis logs -l app=aegis-argo-enforcer
+   kubectl -n kube-system logs -l app=aegis-scheduler-extender
+
+3) Parquet pipeline & manifest
+ - python3 etl/s3_parquet_pipeline_v3.py
+ - python3 etl/copy_manifest_generator.py
+
+4) Dev DW loader (in-cluster)
+ - Ensure etl/redshift_copy_job.yaml has REDSHIFT_IAM_ROLE set to {{REDSHIFT_IAM_ROLE_ARN}} and EVIDENCE_BUCKET set to {{EVIDENCE_BUCKET}}.
+ - kubectl --kubeconfig={{KUBECONFIG_STAGING}} -n aegis apply -f etl/redshift_copy_job.yaml
+ - kubectl --kubeconfig={{KUBECONFIG_STAGING}} -n aegis wait --for=condition=complete job/redshift-copy-loader --timeout=1800s
+
+5) Run dbt (dev)
+ - cd dbt
+ - dbt deps && dbt run --profiles-dir ./profiles && dbt test --profiles-dir ./profiles
+
+6) GE validations
+ - python3 data_quality/run_expectations.py
+ - aws s3 ls s3://{{EVIDENCE_BUCKET}}/data-quality/validations/
+
+7) Iterative calibration & convergence
+ - python3 calibration/iterative_convergence_runner.py
+ - Inspect validation reports: aws s3 ls s3://{{EVIDENCE_BUCKET}}/calibration/validation_reports/
+ - Repeat until median_relative_error ≤ 0.10
+
+8) Promotion (manual)
+ - aws s3 cp s3://{{EVIDENCE_BUCKET}}/calibration/approved/power_profiles.yaml ./power_profiles.yaml
+ - cosign sign --key "awskms://{{COSIGN_KMS_KEY_ARN}}" ./power_profiles.yaml
+ - kubectl --kubeconfig={{KUBECONFIG_PROD}} -n aegis create configmap aegis-power-profiles --from-file=power_profiles.yaml=./power_profiles.yaml --dry-run=client -o yaml | kubectl apply -f -
+ - Upload promotion note to s3://{{EVIDENCE_BUCKET}}/audit/<release-id>/
+
+Contact: {{CONTACT_EMAIL}}
+
*** End Patch
*** Begin Patch
*** Add File:audit/audit_package_collector.py
+#!/usr/bin/env python3
+"""
+Collect audit artifacts from S3 (and optionally local paths) and produce a zip
+package that can be submitted to auditors or stored in secure archival S3 location.
+
+Usage:
+  export EVIDENCE_BUCKET=my-evidence-bucket AWS_REGION=us-west-2
+  python3 audit/audit_package_collector.py --release-id release-2025-01-01 --s3-prefix audit_inputs.json --output /tmp/audit_release.zip
+
+The script can read a small JSON file (either local or in S3) listing artifact keys to include,
+or it will collect default locations automatically.
+"""
+import os, json, argparse, tempfile, zipfile, boto3, time
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+def s3_client():
+    return boto3.client("s3", region_name=AWS_REGION)
+
+def download_s3_keys(s3, bucket, keys, destdir):
+    for k in keys:
+        path = os.path.join(destdir, os.path.basename(k))
+        try:
+            s3.download_file(bucket, k, path)
+        except Exception:
+            # attempt to skip if large or not found
+            with open(path + ".missing", "w") as f:
+                f.write("missing")
+
+def list_default_artifacts():
+    # default artifact prefixes - adjust as needed
+    return [
+        "calibration/validation_reports/",
+        "calibration/drafts/",
+        "enforcement/coverage_reports/",
+        "telemetry/health/",
+        "data-quality/validations/",
+        "audit/"
+    ]
+
+def collect_and_zip(bucket, release_id, keys_or_prefixes, output_path):
+    s3 = s3_client()
+    tmpd = tempfile.mkdtemp()
+    collected = []
+    for p in keys_or_prefixes:
+        if p.endswith("/"):
+            # treat as prefix
+            paginator = s3.get_paginator("list_objects_v2")
+            for page in paginator.paginate(Bucket=bucket, Prefix=p):
+                for obj in page.get("Contents", []):
+                    k = obj["Key"]
+                    try:
+                        s3.download_file(bucket, k, os.path.join(tmpd, os.path.basename(k)))
+                        collected.append(k)
+                    except Exception:
+                        # write a placeholder note
+                        with open(os.path.join(tmpd, os.path.basename(k) + ".download_failed"), "w") as f:
+                            f.write("download failed")
+        else:
+            # single key
+            try:
+                s3.download_file(bucket, p, os.path.join(tmpd, os.path.basename(p)))
+                collected.append(p)
+            except Exception:
+                with open(os.path.join(tmpd, os.path.basename(p) + ".download_failed"), "w") as f:
+                    f.write("download failed")
+    # create metadata
+    meta = {"release_id": release_id, "collected_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()), "keys": collected}
+    with open(os.path.join(tmpd, "audit_metadata.json"), "w") as f:
+        json.dump(meta, f, indent=2)
+    # zip
+    with zipfile.ZipFile(output_path, "w", zipfile.ZIP_DEFLATED) as z:
+        for fname in os.listdir(tmpd):
+            z.write(os.path.join(tmpd, fname), arcname=fname)
+    print("Wrote audit package:", output_path)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--release-id", required=True)
+    p.add_argument("--inputs", help="JSON file with list of keys/prefixes (local or s3://bucket/key)")
+    p.add_argument("--output", default="/tmp/audit_package.zip")
+    args = p.parse_args()
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET env required")
+    keys = []
+    if args.inputs:
+        if args.inputs.startswith("s3://"):
+            # download and parse
+            s3 = s3_client()
+            parts = args.inputs[len("s3://"):].split("/",1)
+            b = parts[0]
+            k = parts[1]
+            obj = s3.get_object(Bucket=b, Key=k)
+            keys = json.loads(obj["Body"].read().decode())
+        else:
+            with open(args.inputs) as f:
+                keys = json.load(f)
+    else:
+        keys = list_default_artifacts()
+    collect_and_zip(EVIDENCE_BUCKET, args.release_id, keys, args.output)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:audit/README_AUDIT_PACKAGE.md
+# Audit package collector
+
+This folder provides a script to assemble an audit package for a promotion/release event.
+
+How it works
+- The collector reads a list of S3 keys or prefixes (or uses sensible defaults) and downloads artifacts to a temporary directory.
+- It produces a zip file containing artifacts and a metadata JSON with the release_id and collected keys.
+- Upload the zip to a secure audit S3 prefix (e.g., s3://<EVIDENCE_BUCKET>/audit/<release-id>/).
+
+Example
+- export EVIDENCE_BUCKET=my-evidence-bucket AWS_REGION=us-west-2
+- python3 audit/audit_package_collector.py --release-id release-2025-01-01 --output /tmp/audit_release.zip
+- aws s3 cp /tmp/audit_release.zip s3://$EVIDENCE_BUCKET/audit/release-2025-01-01/audit_release.zip
+
+Notes
+- Ensure the runner has AWS credentials with read access to the evidence bucket.
+- For compliance, store the resulting zip in a location with object lock/versioning enabled and restrict access.
+
*** End Patch
*** Begin Patch
*** Add File:ci/upload_audit_package.yml
+name: Upload Audit Package
+
+on:
+  workflow_dispatch:
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+
+jobs:
+  package:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install boto3
+      - name: Run audit package collector
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          python3 audit/audit_package_collector.py --release-id "manual-$(date -u +%Y%m%dT%H%M%SZ)" --output /tmp/audit_package.zip
+          aws s3 cp /tmp/audit_package.zip s3://${EVIDENCE_BUCKET}/audit/manual-$(date -u +%Y%m%dT%H%M%SZ)/audit_package.zip
+
*** End Patch
*** End Patch
