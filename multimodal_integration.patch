*** Begin Patch
*** Add File: k8s/multimodal/train_mlflow/train.py
+#!/usr/bin/env python3
+"""
+Minimal multimodal "training" example that:
+- Reads image+caption pairs from a local CSV
+- Computes CLIP embeddings for images and text
+- Logs the embeddings and a small metadata file to MLflow
+
+This is meant as a lightweight example to show MLflow integration in a multimodal pipeline.
+"""
+import os
+import sys
+import csv
+import argparse
+from pathlib import Path
+
+import mlflow
+import numpy as np
+from PIL import Image
+from transformers import CLIPProcessor, CLIPModel
+
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--data-csv", default="/data/dataset.csv", help="CSV with columns: image_path,caption")
+    p.add_argument("--artifact-dir", default="/mlflow/artifacts", help="where to store model/artifacts")
+    p.add_argument("--mlflow-tracking-uri", default=os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
+    return p.parse_args()
+
+
+def load_dataset(csv_path):
+    items = []
+    with open(csv_path, newline="") as fh:
+        rdr = csv.DictReader(fh)
+        for r in rdr:
+            items.append((r["image_path"], r["caption"]))
+    return items
+
+
+def main():
+    args = parse_args()
+    mlflow.set_tracking_uri(args.mlflow_tracking_uri)
+    mlflow.set_experiment("aegis-multimodal-demo")
+
+    print("Loading CLIP model...")
+    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
+    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
+
+    items = load_dataset(args.data_csv)
+    image_embs = []
+    text_embs = []
+    meta = []
+
+    for image_path, caption in items:
+        img = Image.open(image_path).convert("RGB")
+        inputs = processor(text=[caption], images=img, return_tensors="pt", truncation=True)
+        with torch.no_grad():
+            outputs = model(**inputs)
+        # CLIP returns image and text embeddings
+        img_emb = outputs.image_embeds[0].cpu().numpy()
+        txt_emb = outputs.text_embeds[0].cpu().numpy()
+        image_embs.append(img_emb)
+        text_embs.append(txt_emb)
+        meta.append({"image": Path(image_path).name, "caption": caption})
+
+    image_embs = np.vstack(image_embs)
+    text_embs = np.vstack(text_embs)
+
+    with mlflow.start_run() as run:
+        run_id = run.info.run_id
+        print(f"Logging embeddings to MLflow run {run_id}")
+        os.makedirs(args.artifact_dir, exist_ok=True)
+        np.save(os.path.join(args.artifact_dir, "image_embs.npy"), image_embs)
+        np.save(os.path.join(args.artifact_dir, "text_embs.npy"), text_embs)
+        mlflow.log_artifact(os.path.join(args.artifact_dir, "image_embs.npy"))
+        mlflow.log_artifact(os.path.join(args.artifact_dir, "text_embs.npy"))
+        # minimal metadata
+        meta_path = os.path.join(args.artifact_dir, "meta.csv")
+        with open(meta_path, "w", newline="") as fh:
+            w = csv.DictWriter(fh, fieldnames=["image", "caption"])
+            w.writeheader()
+            for m in meta:
+                w.writerow(m)
+        mlflow.log_artifact(meta_path)
+        mlflow.log_param("n_examples", len(items))
+
+    print("Done.")
+
+
+if __name__ == "__main__":
+    # keep imports local to avoid failure on systems without torch if not running
+    import torch
+
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/train_mlflow/job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: multimodal-train
+  namespace: aegis
+spec:
+  template:
+    spec:
+      restartPolicy: Never
+      containers:
+        - name: trainer
+          image: python:3.9-slim
+          command: ["/bin/sh", "-c"]
+          args:
+            - pip install --no-cache-dir torch transformers pillow mlflow numpy && \
+              mkdir -p /data && \
+              # user should mount a ConfigMap or PVC with dataset.csv and images under /data
+              python /workspace/train.py --data-csv /data/dataset.csv --artifact-dir /mlflow/artifacts
+          volumeMounts:
+            - name: workspace
+              mountPath: /workspace
+            - name: dataset
+              mountPath: /data
+          env:
+            - name: MLFLOW_TRACKING_URI
+              value: "http://mlflow:5000"
+      volumes:
+        - name: workspace
+          configMap:
+            name: multimodal-train-scripts
+        - name: dataset
+          persistentVolumeClaim:
+            claimName: multimodal-dataset-pvc
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/train_mlflow/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: multimodal-train-scripts
+  namespace: aegis
+data:
+  train.py: |
+    # copied entrypoint; keep file name consistent with job args
+    from train import main
+    if __name__ == "__main__":
+      main()
+  train.py.orig: |
+    # placeholder to indicate train.py is expected; actual code lives in /workspace/train.py
+  train.py: |
+    (paste the train.py script into this file when applying in your pipeline)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/inference/predictor/app.py
+#!/usr/bin/env python3
+"""
+Simple prediction server that accepts an image file and a text string and returns:
+- CLIP similarity score
+- image/text embeddings (optionally)
+
+Exposes:
+ - POST /predict  (multipart/form-data: "image" file, form field "text")
+"""
+from io import BytesIO
+import os
+from PIL import Image
+from flask import Flask, request, jsonify
+from transformers import CLIPProcessor, CLIPModel
+import torch
+
+app = Flask(__name__)
+
+print("Loading CLIP model...")
+model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
+processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
+device = "cuda" if torch.cuda.is_available() else "cpu"
+model.to(device)
+
+
+@app.route("/healthz", methods=["GET"])
+def health():
+    return "ok"
+
+
+@app.route("/predict", methods=["POST"])
+def predict():
+    if "image" not in request.files or "text" not in request.form:
+        return jsonify({"error": "image file and text field required"}), 400
+    imgfile = request.files["image"]
+    text = request.form["text"]
+    img = Image.open(BytesIO(imgfile.read())).convert("RGB")
+    inputs = processor(text=[text], images=img, return_tensors="pt", truncation=True)
+    for k, v in inputs.items():
+        inputs[k] = v.to(device)
+    with torch.no_grad():
+        outputs = model(**inputs)
+    img_emb = outputs.image_embeds[0].cpu().tolist()
+    txt_emb = outputs.text_embeds[0].cpu().tolist()
+    # compute cosine similarity
+    img_vec = torch.tensor(img_emb)
+    txt_vec = torch.tensor(txt_emb)
+    sim = torch.nn.functional.cosine_similarity(img_vec, txt_vec, dim=0).item()
+    return jsonify({"similarity": float(sim), "image_emb_len": len(img_emb), "text_emb_len": len(txt_emb)})
+
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8080)))
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/inference/predictor/requirements.txt
+flask
+torch
+transformers
+Pillow
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/inference/predictor/Dockerfile
+FROM python:3.9-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN pip install --no-cache-dir -r requirements.txt
+COPY app.py .
+EXPOSE 8080
+CMD ["python", "app.py"]
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/inference/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: multimodal-predictor
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: multimodal-predictor
+  template:
+    metadata:
+      labels:
+        app: multimodal-predictor
+    spec:
+      containers:
+        - name: predictor
+          image: aegis/multimodal-predictor:latest
+          ports:
+            - containerPort: 8080
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "512Mi"
+            limits:
+              cpu: "1"
+              memory: "2Gi"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/multimodal/inference/service.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: multimodal-predictor
+  namespace: aegis
+spec:
+  selector:
+    app: multimodal-predictor
+  ports:
+    - port: 8080
+      targetPort: 8080
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/labelstudio/labelstudio-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: labelstudio
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: labelstudio
+  template:
+    metadata:
+      labels:
+        app: labelstudio
+    spec:
+      containers:
+        - name: labelstudio
+          image: heartexlabs/label-studio:1.6.1
+          env:
+            - name: LABEL_STUDIO_HOST
+              value: "0.0.0.0"
+          ports:
+            - containerPort: 8080
+          volumeMounts:
+            - name: labelstudio-data
+              mountPath: /label-studio/data
+      volumes:
+        - name: labelstudio-data
+          persistentVolumeClaim:
+            claimName: labelstudio-pvc
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/labelstudio/service.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: labelstudio
+  namespace: aegis
+spec:
+  selector:
+    app: labelstudio
+  ports:
+    - port: 8080
+      targetPort: 8080
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/labelstudio/argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-labelstudio
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://github.com/yourorg/yourrepo
+    path: preprocessing/labelstudio
+    targetRevision: HEAD
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/vectorize/vectorize.py
+#!/usr/bin/env python3
+"""
+Vectorize labeled artifacts (images + captions) with CLIP and push to Milvus.
+Expect a CSV with columns: image_path, caption, id
+
+Writes vectors to Milvus collection "aegis_multimodal" with fields:
+ - id (int64)
+ - image_emb (FLOAT_VECTOR)
+ - text_emb (FLOAT_VECTOR)
+ - meta (string JSON)
+"""
+import os
+import csv
+import json
+import argparse
+from pathlib import Path
+from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
+import numpy as np
+from transformers import CLIPProcessor, CLIPModel
+from PIL import Image
+import torch
+
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--csv", required=True)
+    p.add_argument("--images-root", default=".")
+    p.add_argument("--milvus-host", default=os.environ.get("MILVUS_HOST", "milvus"))
+    p.add_argument("--milvus-port", default=os.environ.get("MILVUS_PORT", "19530"))
+    return p.parse_args()
+
+
+def ensure_collection(conn, dim=512, collection_name="aegis_multimodal"):
+    if utility.has_collection(collection_name):
+        return Collection(collection_name)
+    fields = [
+        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
+        FieldSchema(name="image_emb", dtype=DataType.FLOAT_VECTOR, dim=dim),
+        FieldSchema(name="text_emb", dtype=DataType.FLOAT_VECTOR, dim=dim),
+        FieldSchema(name="meta", dtype=DataType.VARCHAR, max_length=4096),
+    ]
+    schema = CollectionSchema(fields, description="Multimodal embeddings")
+    coll = Collection(name=collection_name, schema=schema)
+    coll.create_index(field_name="image_emb", index_params={"index_type":"IVF_FLAT","metric_type":"L2","params":{"nlist":128}})
+    coll.create_index(field_name="text_emb", index_params={"index_type":"IVF_FLAT","metric_type":"L2","params":{"nlist":128}})
+    coll.load()
+    return coll
+
+
+def main():
+    args = parse_args()
+    print("Connecting to Milvus", args.milvus_host, args.milvus_port)
+    connections.connect(host=args.milvus_host, port=args.milvus_port)
+    # load CLIP
+    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
+    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model.to(device)
+
+    coll = ensure_collection(connections, dim=512)
+
+    ids = []
+    imgs = []
+    texts = []
+    metas = []
+
+    with open(args.csv, newline="") as fh:
+        rdr = csv.DictReader(fh)
+        for r in rdr:
+            idx = int(r["id"])
+            imgp = Path(args.images_root) / r["image_path"]
+            caption = r["caption"]
+            img = Image.open(imgp).convert("RGB")
+            inputs = processor(text=[caption], images=img, return_tensors="pt", truncation=True)
+            for k, v in inputs.items():
+                inputs[k] = v.to(device)
+            with torch.no_grad():
+                out = model(**inputs)
+            img_emb = out.image_embeds[0].cpu().numpy().astype("float32")
+            txt_emb = out.text_embeds[0].cpu().numpy().astype("float32")
+            ids.append(idx)
+            imgs.append(img_emb.tolist())
+            texts.append(txt_emb.tolist())
+            metas.append(json.dumps({"image": imgp.name, "caption": caption}))
+
+    print("Inserting to Milvus collection")
+    coll.insert([ids, imgs, texts, metas])
+    coll.flush()
+    print("Done.")
+
+
+if __name__ == "__main__":
+    import torch
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: milvus/argo-app.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Application
+metadata:
+  name: aegis-milvus
+  namespace: argocd
+spec:
+  project: default
+  destination:
+    server: https://kubernetes.default.svc
+    namespace: aegis
+  source:
+    repoURL: https://milvus-io.github.io/milvus-helm/
+    chart: milvus
+    targetRevision: latest
+    helm:
+      values: |
+        persistence:
+          persistentVolumeClaim:
+            create: true
+            size: 10Gi
+        image:
+          tag: "2.2.11"
+  syncPolicy:
+    automated:
+      prune: true
+      selfHeal: true
+
*** End Patch
*** Begin Patch
*** Add File: rag/examples/query_rag.py
+#!/usr/bin/env python3
+"""
+Example RAG query that:
+- computes an embedding for a supplied image and/or text
+- queries Milvus for top-k image/text neighbors
+- returns a combined context object suitable for passing into an LLM or downstream pipeline
+"""
+import argparse
+import json
+from pathlib import Path
+from pymilvus import connections, Collection
+from transformers import CLIPProcessor, CLIPModel
+from PIL import Image
+import torch
+
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--image", help="path to query image", required=False)
+    p.add_argument("--text", help="query text", required=False)
+    p.add_argument("--milvus-host", default="milvus")
+    p.add_argument("--milvus-port", default="19530")
+    p.add_argument("--topk", type=int, default=5)
+    return p.parse_args()
+
+
+def main():
+    args = parse_args()
+    connections.connect(host=args.milvus_host, port=args.milvus_port)
+    coll = Collection("aegis_multimodal")
+    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
+    proc = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model.to(device)
+
+    query_emb = None
+    if args.image:
+        img = Image.open(args.image).convert("RGB")
+        inputs = proc(images=img, return_tensors="pt")
+        for k, v in inputs.items():
+            inputs[k] = v.to(device)
+        with torch.no_grad():
+            out = model(**inputs)
+        query_emb = out.image_embeds[0].cpu().numpy().astype("float32")
+        field = "image_emb"
+    elif args.text:
+        inputs = proc(text=[args.text], return_tensors="pt")
+        for k, v in inputs.items():
+            inputs[k] = v.to(device)
+        with torch.no_grad():
+            out = model(**inputs)
+        query_emb = out.text_embeds[0].cpu().numpy().astype("float32")
+        field = "text_emb"
+    else:
+        raise SystemExit("provide --image or --text")
+
+    results = coll.search([query_emb.tolist()], anns_field=field, limit=args.topk, output_fields=["meta"])
+    hits = results[0]
+    context = []
+    for h in hits:
+        meta = h.entity.get("meta")
+        score = float(h.distance)
+        context.append({"meta": meta, "score": score})
+    print(json.dumps({"query_field": field, "results": context}, indent=2))
+
+
+if __name__ == "__main__":
+    import torch
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: inference/ensemble/triton_example/README.md
+Example Triton ensemble (conceptual)
+-----------------------------------
+
+This folder contains a minimal example config for a Triton ensemble model that combines
+image_encoder and text_encoder models and then performs a small post-processing step.
+
+Note: providing a working Triton ensemble requires a models repository layout and the actual
+models exported in a Triton-compatible format (ONNX/plan/etc). This example shows the config
+pattern; adapt it to your environment.
+
+Files:
+- config.pbtxt : ensemble model config
+- image_encoder/  : placeholder for image encoder model
+- text_encoder/   : placeholder for text encoder model
+
*** End Patch
*** Begin Patch
*** Add File: inference/ensemble/triton_example/config.pbtxt
+name: "multimodal_ensemble"
+platform: "ensemble"
+input [
+  {
+    name: "IMAGE_INPUT"
+    data_type: TYPE_FP32
+    dims: [3, 224, 224]
+  },
+  {
+    name: "TEXT_INPUT"
+    data_type: TYPE_STRING
+    dims: [ -1 ]
+  }
+]
+output [
+  {
+    name: "SIMILARITY"
+    data_type: TYPE_FP32
+    dims: [1]
+  }
+]
+ensemble_scheduling {
+  step [
+    {
+      model_name: "image_encoder"
+      model_version: -1
+      input_map {
+        key: "IMAGE_INPUT"
+        value: "IMAGE_INPUT"
+      }
+      output_map {
+        key: "IMAGE_EMB"
+        value: "IMAGE_EMB"
+      }
+    },
+    {
+      model_name: "text_encoder"
+      model_version: -1
+      input_map {
+        key: "TEXT_INPUT"
+        value: "TEXT_INPUT"
+      }
+      output_map {
+        key: "TEXT_EMB"
+        value: "TEXT_EMB"
+      }
+    },
+    {
+      model_name: "postprocessor"
+      model_version: -1
+      input_map {
+        key: "IMAGE_EMB"
+        value: "IMAGE_EMB"
+      }
+      input_map {
+        key: "TEXT_EMB"
+        value: "TEXT_EMB"
+      }
+      output_map {
+        key: "SIMILARITY"
+        value: "SIMILARITY"
+      }
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: ci/workflows/multimodal-integration.yml
+name: Multimodal integration test
+on:
+  workflow_dispatch:
+
+jobs:
+  build-and-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+
+      - name: Install test deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests Pillow
+
+      - name: Smoke test predictor (requires K8s cluster & kubeconfig in secrets)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          # Deploy predictor to cluster (assumes manifest present)
+          kubectl apply -f k8s/multimodal/inference/deployment.yaml
+          kubectl apply -f k8s/multimodal/inference/service.yaml
+          # wait for pod
+          kubectl wait --for=condition=available --timeout=120s deployment/multimodal-predictor -n aegis || true
+          # port-forward in background
+          kubectl port-forward svc/multimodal-predictor 8080:8080 -n aegis >/tmp/forward.log 2>&1 &
+          sleep 5
+          python ci/run_smoke_test.py
+
*** End Patch
*** Begin Patch
*** Add File: ci/run_smoke_test.py
+import requests, sys, time
+from io import BytesIO
+from PIL import Image, ImageDraw
+
+def make_test_image():
+    img = Image.new("RGB", (224,224), color=(73,109,137))
+    d = ImageDraw.Draw(img)
+    d.text((10,10), "test", fill=(255,255,0))
+    b = BytesIO()
+    img.save(b, format="PNG")
+    b.seek(0)
+    return b
+
+def main():
+    url = "http://localhost:8080/predict"
+    files = {"image": ("test.png", make_test_image(), "image/png")}
+    data = {"text": "a test image"}
+    for i in range(10):
+        try:
+            r = requests.post(url, files=files, data=data, timeout=10)
+            print("status", r.status_code, r.text)
+            if r.status_code == 200:
+                print("smoke test passed")
+                return
+        except Exception as e:
+            print("request failed", e)
+        time.sleep(2)
+    raise SystemExit("smoke test failed")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: docs/multimodal_README.md
+Multimodal examples and integration (overview)
+--------------------------------------------
+
+This set of examples demonstrates:
+- Multimodal "training": compute and log image+text embeddings to MLflow (k8s/multimodal/train_mlflow/*)
+- A simple multimodal predictor service (Flask + CLIP) that accepts an image + text and returns a similarity score (k8s/multimodal/inference/*)
+- A LabelStudio deployment and Argo app to host annotation projects (preprocessing/labelstudio/* + preprocessing/labelstudio/argo-app.yaml)
+- A vectorization pipeline that computes CLIP embeddings and writes them to Milvus (preprocessing/vectorize/vectorize.py)
+- A RAG example that queries Milvus for image/text neighbors and builds context (rag/examples/query_rag.py)
+- A conceptual Triton ensemble config to show how an image encoder + text encoder + postprocessor could be wired in a Triton model repository (inference/ensemble/triton_example/)
+- CI smoke test that deploys the predictor and runs a simple multimodal request (ci/*)
+
+Notes and next steps
+- Replace placeholder images, dataset PVCs and container image references with your production images and artifact locations.
+- The predictor image (aegis/multimodal-predictor:latest) should be built from k8s/multimodal/inference/predictor/Dockerfile and pushed to your registry accessible by the cluster.
+- Milvus: we provide an Argo app to deploy Milvus via helm; adjust resource/values for production.
+- Triton ensemble: the example is conceptual. To run a Triton ensemble, export your encoder models in Triton-compatible formats and place them in the model repository layout described by Triton.
+
*** End Patch
*** End Patch
