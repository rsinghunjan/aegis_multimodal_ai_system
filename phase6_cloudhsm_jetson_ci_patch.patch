*** Begin Patch
*** Add File: scripts/cloudhsm/operator_checklist.sh
+#!/usr/bin/env bash
+#
+# Operator Checklist & Runnable Script for CloudHSM initialization, PKCS11 key creation,
+# cosign PKCS11 validation and rotation drill.
+#
+# This script prints the exact commands to run (with placeholders) and can optionally
+# execute them when run with --execute. You MUST replace placeholders before executing,
+# and many steps are interactive / manual (per AWS CloudHSM lifecycle).
+#
+# Usage:
+#   ./scripts/cloudhsm/operator_checklist.sh         # prints checklist & commands
+#   ./scripts/cloudhsm/operator_checklist.sh --execute   # executes automated portions (where possible)
+
+set -euo pipefail
+EXEC=false
+if [ "${1:-}" = "--execute" ]; then
+  EXEC=true
+fi
+
+# -------------------------
+# User-editable variables
+# -------------------------
+AWS_REGION="${AWS_REGION:-us-west-2}"
+TF_VAR_VPC_ID="${TF_VAR_VPC_ID:-<VPC_ID>}"
+TF_VAR_SUBNET_IDS="${TF_VAR_SUBNET_IDS:-\"[\\\"<SUBNET_ID_1>\\\",\\\"<SUBNET_ID_2>\\\"]\"}"
+ADMIN_HOST="${ADMIN_HOST:-admin-host.example.com}"
+PKCS11_MODULE_PATH="${PKCS11_MODULE_PATH:-/usr/lib/libcloudhsm_pkcs11.so}"
+PKCS11_LABEL="${PKCS11_LABEL:-cosign-key}"
+MANIFEST_PATH="${MANIFEST_PATH:-/tmp/test_manifest.json}"
+GITHUB_REPO="${GITHUB_REPO:-owner/repo}"   # used for updating CI secrets via gh
+CI_SECRET_NAME="${CI_SECRET_NAME:-COSIGN_PKCS11_LABEL}"
+
+echo "CloudHSM operator checklist (dry-run). Variables:"
+echo "  AWS_REGION=${AWS_REGION}"
+echo "  TF_VAR_VPC_ID=${TF_VAR_VPC_ID}"
+echo "  TF_VAR_SUBNET_IDS=${TF_VAR_SUBNET_IDS}"
+echo "  ADMIN_HOST=${ADMIN_HOST}"
+echo "  PKCS11_MODULE_PATH=${PKCS11_MODULE_PATH}"
+echo "  PKCS11_LABEL=${PKCS11_LABEL}"
+echo "  MANIFEST_PATH=${MANIFEST_PATH}"
+echo
+echo "IMPORTANT: Replace placeholder values above with your environment values BEFORE executing."
+echo
+echo "1) Provision CloudHSM cluster via Terraform (or Console). Example (TF variables must be set):"
+echo
+cat <<'CMD'
+# from repo/infra/hsm/
+export AWS_REGION=<your-region>
+export TF_VAR_vpc_id=<VPC_ID>
+export TF_VAR_subnet_ids='["subnet-aaa","subnet-bbb"]'
+terraform init
+terraform apply -var "aws_region=${AWS_REGION}" -var "vpc_id=${TF_VAR_VPC_ID}" -var "subnet_ids=${TF_VAR_SUBNET_IDS}"
+CMD
+
+if $EXEC; then
+  (cd infra/hsm || true) && terraform init && terraform apply -auto-approve -var "aws_region=${AWS_REGION}" -var "vpc_id=${TF_VAR_VPC_ID}" -var "subnet_ids=${TF_VAR_SUBNET_IDS}" || true
+fi
+
+echo
+echo "2) Wait for cluster to be available. List clusters:"
+echo "   aws cloudhsmv2 describe-clusters --region ${AWS_REGION} --query 'Clusters[*].[ClusterId,State]' --output table"
+echo
+if $EXEC; then
+  aws cloudhsmv2 describe-clusters --region "${AWS_REGION}" --output table || true
+fi
+
+echo
+echo "3) Initialize & claim HSMs via AWS Console/CLI (manual steps). Examples/notes:"
+echo "   - Use Console: CloudHSM -> Clusters -> Initialize (follow guided steps)."
+echo "   - After initialization, HSMs will report status INITIALIZED."
+echo "   - You may need to wait several minutes for HSMs to be ready."
+echo
+
+echo "4) On the admin host (${ADMIN_HOST}) install CloudHSM client and pkcs11 tools and configure client."
+echo "   SSH to admin host and run (example Debian/Ubuntu):"
+cat <<'CMD'
+ssh admin@<ADMIN_HOST>
+sudo apt-get update
+sudo apt-get install -y cloudhsm-client libcloudhsm pkcs11-tool
+# Perform client configuration (adjust cluster ip & id per AWS console instructions)
+sudo /opt/cloudhsm/bin/configure -a <cluster-ip> -c <cluster-id>
+# Verify status:
+sudo /opt/cloudhsm/bin/cloudhsm_client_util status
+exit
+CMD
+
+if $EXEC; then
+  echo "Attempting to SSH to admin host to run basic checks (this requires key-based access)."
+  ssh admin@"${ADMIN_HOST}" "sudo /opt/cloudhsm/bin/cloudhsm_client_util status" || true
+fi
+
+echo
+echo "5) Create Crypto User (CU) and generate/signing key in HSM (interactive). Example using key_mgmt_util or pkcs11-tool:"
+cat <<'CMD'
+# On admin host (interactive):
+sudo /opt/cloudhsm/bin/key_mgmt_util
+# inside tool:
+# createUser CUadmin <password>
+# or use pkcs11-tool to generate an RSA keypair and set label
+pkcs11-tool --module ${PKCS11_MODULE_PATH} --keypairgen --key-type rsa:2048 --label ${PKCS11_LABEL} --id 01
+pkcs11-tool --module ${PKCS11_MODULE_PATH} -O   # list objects
+CMD
+
+echo
+echo "6) Prepare a small manifest to test signing:"
+echo "   echo '{\"test\":\"aegis-pkcs11\"}' > ${MANIFEST_PATH}"
+if $EXEC; then
+  echo '{"test":"aegis-pkcs11"}' | ssh admin@"${ADMIN_HOST}" "cat > ${MANIFEST_PATH}" || true
+fi
+
+echo
+echo "7) Test cosign PKCS11 signing and verify (on admin host):"
+cat <<'CMD'
+# set module path and label (adjust PKCS11_MODULE_PATH and PKCS11_LABEL)
+export PKCS11_MODULE=${PKCS11_MODULE_PATH}
+export PKCS11_LABEL=${PKCS11_LABEL}
+# Sign:
+cosign sign --key "pkcs11:token=${PKCS11_LABEL}?module-path=${PKCS11_MODULE}" ${MANIFEST_PATH}
+# Verify:
+cosign verify ${MANIFEST_PATH}
+# Optionally upload to Rekor:
+rekor-cli upload --artifact ${MANIFEST_PATH}
+CMD
+
+if $EXEC; then
+  ssh admin@"${ADMIN_HOST}" "export PKCS11_MODULE='${PKCS11_MODULE_PATH}' PKCS11_LABEL='${PKCS11_LABEL}'; cosign sign --key \"pkcs11:token=${PKCS11_LABEL}?module-path=${PKCS11_MODULE}\" ${MANIFEST_PATH} || true; cosign verify ${MANIFEST_PATH} || true" || true
+fi
+
+echo
+echo "8) Enable CloudTrail and verify sign events. Query example:"
+echo "   aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign --max-results 50 --region ${AWS_REGION}"
+if $EXEC; then
+  aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign --max-results 50 --region "${AWS_REGION}" > /tmp/cloudtrail_kms_events.json || true
+  echo "Saved /tmp/cloudtrail_kms_events.json"
+fi
+
+echo
+echo "9) Rotation drill (KMS or HSM). Examples:"
+cat <<'CMD'
+# KMS rotation (automated):
+NEW_KEY_ID=$(aws kms create-key --description "cosign rotation test" --query KeyMetadata.KeyId --output text --region ${AWS_REGION})
+aws kms create-alias --alias-name alias/aegis-cosign --target-key-id "${NEW_KEY_ID}" --region ${AWS_REGION}
+# update CI secret to awskms:///<NEW_KEY_ID> (example using gh)
+gh secret set COSIGN_KMS_KEY --repo ${GITHUB_REPO} --body "awskms:///${NEW_KEY_ID}"
+# Validate sign with new KMS key:
+cosign sign --key "awskms:///${NEW_KEY_ID}" ${MANIFEST_PATH}
+cosign verify ${MANIFEST_PATH}
+
+# CloudHSM rotation (manual):
+#  - create a new key in HSM with a new label (e.g. cosign-key-rot1)
+#  - update CI secret COSIGN_PKCS11_LABEL to the new label
+#  - run PKCS11 validate script again
+CMD
+
+echo
+echo "10) Acceptance checklist (confirm each):"
+echo "  - CloudHSM cluster INITIALIZED"
+echo "  - Admin host can reach cluster & list slots"
+echo "  - Crypto User created"
+echo "  - PKCS11 key created and labeled: ${PKCS11_LABEL}"
+echo "  - cosign PKCS11 sign/verify succeeded for ${MANIFEST_PATH}"
+echo "  - Rekor entry created"
+echo "  - CloudTrail shows sign events for expected principal(s)"
+echo "  - Rotation drill executed and validated"
+
+echo
+echo "Operator: capture outputs and upload to compliance bucket for audit using script:"
+echo "  python provider/legal/auto_archive_baa.py --file /tmp/baa_evidence.tgz --provider <provider> --bucket <compliance-bucket>"
+
+echo
+echo "End of CloudHSM operator checklist."
+
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: packer/jetson_ubuntu_arm.pkr.hcl
+// Packer HCL example to build a base Ubuntu ARM64 image suitable as a starting point
+// for Jetson-like devices. For physical Jetson devices the image building approach
+// varies; this template demonstrates building an Ubuntu image for cloud/edge.
+variable "aws_region" {
+  type    = string
+  default = "us-west-2"
+}
+source "amazon-ebs" "ubuntu_arm" {
+  ami_name      = "aegis-jetson-base-{{timestamp}}"
+  instance_type = "t4g.medium"   # ARM64 instance builder
+  region        = var.aws_region
+  source_ami_filter {
+    filters = {
+      name                = "ubuntu/images/hvm-ssd/ubuntu-focal-20.04-arm64*"
+      root-device-type    = "ebs"
+      virtualization-type = "hvm"
+    }
+    owners      = ["099720109477"] # Canonical
+    most_recent = true
+  }
+  ssh_username = "ubuntu"
+}
+
+build {
+  sources = ["source.amazon-ebs.ubuntu_arm"]
+
+  provisioner "file" {
+    source      = "../ansible/provision/cloud_init_runner.yaml"
+    destination = "/tmp/cloud_init_runner.yaml"
+  }
+
+  provisioner "shell" {
+    inline = [
+      "sudo apt-get update",
+      "sudo apt-get install -y python3 python3-pip cloud-init",
+      "sudo mkdir -p /opt/aegis",
+      "sudo chown ubuntu:ubuntu /opt/aegis"
+    ]
+  }
+
+  provisioner "shell" {
+    inline = [
+      "echo 'packer build completed.'"
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: cloud-init/jetson-user-data.yaml
+#cloud-config
+hostname: aegis-jetson
+manage_etc_hosts: true
+users:
+  - name: aegis-runner
+    gecos: "Aegis Runner"
+    sudo: ALL=(ALL) NOPASSWD:ALL
+    groups: users,docker
+    shell: /bin/bash
+    lock_passwd: true
+runcmd:
+  - [ sh, -c, 'apt-get update && apt-get install -y docker.io python3-pip jq' ]
+  - [ sh, -c, 'useradd -m -s /bin/bash aegis-runner || true' ]
+  - [ sh, -c, 'mkdir -p /home/aegis-runner/actions-runner' ]
+  - [ sh, -c, 'systemctl enable --now ssh' ]
+  - [ sh, -c, 'curl -fsSL -o /usr/local/bin/validate_tensorrt_local.sh https://raw.githubusercontent.com/<your-repo>/main/scripts/jetson/validate_tensorrt_local.sh && chmod +x /usr/local/bin/validate_tensorrt_local.sh' ]
+  - [ sh, -c, 'echo "Cloud-init complete for Jetson image" > /etc/motd' ]
+final_message: "The system is ready. Use Ansible to finish higher-level provisioning."
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/register_and_validate.yml
+- name: Register device to CI & Prometheus and run hardware validation (idempotent)
+  hosts: runners
+  become: true
+  vars:
+    runner_user: aegis-runner
+    repo_url: "https://github.com/owner/repo"
+    prometheus_pushgateway: "http://pushgateway.monitoring.svc.cluster.local:9091"
+  tasks:
+    - name: Ensure runner user exists
+      user:
+        name: "{{ runner_user }}"
+        create_home: true
+
+    - name: Install runner (download if not present)
+      become_user: "{{ runner_user }}"
+      shell: |
+        set -e
+        ARCH=$(uname -m)
+        RDIR=/home/{{ runner_user }}/actions-runner
+        mkdir -p $RDIR
+        cd $RDIR
+        if [ ! -f ./config.sh ]; then
+          curl -O -L https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-linux-${ARCH}-2.308.0.tar.gz
+          tar xzf actions-runner-linux-${ARCH}-2.308.0.tar.gz
+        fi
+      args:
+        creates: "/home/{{ runner_user }}/actions-runner/config.sh"
+
+    - name: Configure runner with token (token must be provided via Ansible Vault as runner_token)
+      become_user: "{{ runner_user }}"
+      shell: |
+        cd /home/{{ runner_user }}/actions-runner
+        if [ ! -f .credentials ]; then
+          ./config.sh --url "{{ repo_url }}" --token "{{ runner_token }}" --labels "self-hosted,jetson" --unattended
+        fi
+      args:
+        creates: "/home/{{ runner_user }}/actions-runner/.credentials"
+
+    - name: Ensure systemd service exists for runner
+      copy:
+        dest: /etc/systemd/system/github-runner.service
+        content: |
+          [Unit]
+          Description=GitHub Actions Runner
+          After=network.target
+
+          [Service]
+          Type=simple
+          User={{ runner_user }}
+          WorkingDirectory=/home/{{ runner_user }}/actions-runner
+          ExecStart=/home/{{ runner_user }}/actions-runner/run.sh
+          Restart=always
+          RestartSec=5
+
+          [Install]
+          WantedBy=multi-user.target
+      mode: '0644'
+
+    - name: Start runner service
+      systemd:
+        name: github-runner
+        state: started
+        enabled: yes
+
+    - name: Ensure node_exporter installed & running
+      include_tasks: monitoring/node_exporter_install.yml
+
+    - name: Run TensorRT validation and report status
+      shell: |
+        /usr/local/bin/validate_tensorrt_local.sh || exit 2
+      register: tensorrt_result
+      failed_when: tensorrt_result.rc != 0
+
+    - name: Push validation result to pushgateway (optional)
+      uri:
+        url: "{{ prometheus_pushgateway }}/metrics/job/jetson_validation/instance/{{ inventory_hostname }}"
+        method: POST
+        body: "validation_result{device=\"{{ inventory_hostname }}\"} 1\n"
+        headers:
+          Content-Type: "text/plain"
+      ignore_errors: yes
+
+    - name: Debug validation
+      debug:
+        var: tensorrt_result.stdout_lines
+
*** End Patch
*** Begin Patch
*** Add File: scripts/jetson/register_one_local.sh
+#!/usr/bin/env bash
+#
+# Local helper to register the runner and trigger validation (expects Ansible and vault token configured).
+if [ -z "${1:-}" ]; then
+  echo "usage: $0 <inventory_host>"
+  exit 2
+fi
+HOST="$1"
+ansible-playbook -i "hosts,$HOST," ansible/provision/register_and_validate.yml --extra-vars "runner_token=$(ansible-vault view secrets/runner_token.yml)" --connection=ssh
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/provider_live_test_full.yml
+name: Provider Live Test (Orchestrate, Aggregate, Publish)
+on:
+  workflow_dispatch:
+    inputs:
+      jobs:
+        description: Number of distributed harness jobs
+        required: false
+        default: '10'
+      s3-bucket:
+        description: S3 bucket to archive results
+        required: true
+
+jobs:
+  run-and-collect:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+
+      - name: Install deps
+        run: |
+          pip install boto3 kubernetes
+
+      - name: Run provider live test orchestrator
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          DATABASE_URL: ${{ secrets.DATABASE_URL }}
+        run: |
+          python quantum/hardening/provider_live_test_orchestrator.py --jobs ${{ github.event.inputs.jobs }} --s3-bucket "${{ github.event.inputs.s3-bucket }}" || true
+
+      - name: Collect SLA summary artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: qpu-sla-summary
+          path: /tmp/qpu_sla_summary.json
+
+      - name: Collect adapter recommendations
+        uses: actions/upload-artifact@v4
+        with:
+          name: adapter-recommendations
+          path: /tmp/adapter_recommendations.json
+
+      - name: Collect mitigation suggestions
+        uses: actions/upload-artifact@v4
+        with:
+          name: mitigation-suggestions
+          path: /tmp/mitigation_suggestions.json
+
+      - name: Collect billing reconcile report
+        uses: actions/upload-artifact@v4
+        with:
+          name: billing-reconcile-report
+          path: /tmp/reconcile_report.json
+
*** End Patch
*** End Patch
