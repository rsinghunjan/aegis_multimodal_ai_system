*** Begin Patch
*** Add File: pilot/helm/values-pilot.yaml
+# Helm values for staging pilot deployment (aegis-umbrella)
+pduExporter:
+  image:
+    repository: aegis/pdu-exporter
+    tag: latest
+  replicaCount: 1
+
+deviceRegistry:
+  image:
+    repository: aegis/device-registry
+    tag: latest
+  replicaCount: 1
+
+calibration:
+  image:
+    repository: aegis/calibration
+    tag: latest
+
+serviceMonitor:
+  enabled: true
+  namespace: monitoring
+
+prometheus:
+  enabled: true
+
+redis:
+  enabled: true
+  persistence:
+    enabled: false
+
*** End Patch
*** Begin Patch
*** Add File: pilot/playbook_operator.md
+# Aegis Measurement Pilot Playbook (Operator)
+
+Purpose:
+- Run a staging pilot that verifies exporters, mapping, calibration and achieves >=95% measured coverage.
+- Produce audit-ready reports and fail CI/PRs when coverage is insufficient.
+
+Pre-reqs:
+- kubeconfig with access to staging cluster (export KUBECONFIG)
+- AWS CLI configured (if using S3 for compliance bucket)
+- Repository secrets set: COMPLIANCE_BUCKET, DATABASE_URL_STAGING, KUBECONFIG_STAGING
+- Helm and kubectl available locally or in CI runner
+
+High-level steps:
+1. Deploy umbrella chart to staging
+   - helm upgrade --install aegis-umbrella helm/umbrella -n aegis-staging --create-namespace -f pilot/helm/values-pilot.yaml
+   - kubectl rollout status deployment/aegis-device-registry -n aegis-staging --timeout=120s
+
+2. Wait for exporters + daemonsets to be ready
+   - kubectl get daemonset -n aegis-staging
+
+3. Run calibration job (if not automatic)
+   - kubectl create job --from=cronjob/aegis-calibration-job run-calib-$(date +%s) -n aegis-staging
+   - kubectl wait --for=condition=complete job/run-calib-* -n aegis-staging --timeout=600s
+
+4. Run pilot verification (local or CI)
+   - python measurement/pilot_orchestrator.py --namespace aegis-staging --target-pct 95
+   - This script runs mapping_verifier, calibration_runner_improved and coverage_report, uploads summary to COMPLIANCE_BUCKET and exits non-zero if coverage < target.
+
+Acceptance checks:
+- /tmp/coverage_summary.json exists and uploaded to s3://<COMPLIANCE_BUCKET>/pilot_reports/
+- coverage_summary["coverage"]["coverage_pct_measured"] >= 95.0
+- calibration reports and per-device profiles present in /etc/aegis/power_profiles or compliance bucket
+
+If pilot fails:
+- Inspect /tmp/measurement_coverage_*.json and /tmp/calibration_full_report.json
+- Identify unmapped jobs/nodes and remediate device mapping or exporter deployment
+- Re-run pilot after fixes
+
+CI Integration:
+- Add .github/workflows/pilot_validate_coverage.yml (provided) to run this playbook in CI on PRs or manually via workflow_dispatch.
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/pilot_validate_coverage.yml
+name: Pilot Validate Coverage (PR Gate)
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+  workflow_dispatch:
+
+jobs:
+  run-pilot:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup tools
+        run: |
+          sudo apt-get update -y
+          sudo apt-get install -y awscli jq
+          curl -fsSL https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz -o /tmp/helm.tgz
+          tar -xzf /tmp/helm.tgz -C /tmp && sudo mv /tmp/linux-amd64/helm /usr/local/bin/helm
+          curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
+          chmod +x kubectl; sudo mv kubectl /usr/local/bin/
+
+      - name: Configure Kubeconfig
+        if: ${{ secrets.KUBECONFIG_STAGING != '' }}
+        run: |
+          echo "${{ secrets.KUBECONFIG_STAGING }}" > /tmp/kubeconfig_staging
+          export KUBECONFIG=/tmp/kubeconfig_staging
+          echo "KUBECONFIG set"
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+
+      - name: Deploy umbrella to staging
+        env:
+          KUBECONFIG: /tmp/kubeconfig_staging
+        run: |
+          helm upgrade --install aegis-umbrella helm/umbrella -n aegis-staging --create-namespace -f pilot/helm/values-pilot.yaml
+          kubectl rollout status deployment/aegis-device-registry -n aegis-staging --timeout=120s || true
+
+      - name: Run Pilot Orchestrator
+        env:
+          KUBECONFIG: /tmp/kubeconfig_staging
+          DATABASE_URL: ${{ secrets.DATABASE_URL_STAGING }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python measurement/pilot_orchestrator.py --namespace aegis-staging --target-pct 95
+
+      - name: Upload coverage summary
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: coverage-summary
+          path: /tmp/coverage_summary.json
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci_pi_validation_and_post_promotion.yml
+name: CI PI Validation & Post-promotion Stress Test
+on:
+  workflow_dispatch:
+    inputs:
+      regions:
+        description: "Comma separated regions to validate"
+        required: true
+        default: "US"
+      injected_mae:
+        description: "Synthetic MAE to inject for stress test"
+        required: true
+        default: "50"
+
+jobs:
+  validate-pi:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        run: python -m pip install --upgrade pip && pip install boto3 requests prophet
+      - name: Run per-region PI validation
+        env:
+          FORECAST_REGIONS: ${{ github.event.inputs.regions }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python forecast/region_pi_validation.py --regions "${{ github.event.inputs.regions }}"
+          ls /tmp/forecast_region_validation_*.json || true
+      - name: Inject synthetic MAE monitors (staging) and validate rollback
+        env:
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+          MODEL_REGISTRY_API: ${{ secrets.MODEL_REGISTRY_API }}
+        run: |
+          # inject synthetic monitor to simulate degradation
+          python forecast/drift_test_harness.py --mae ${{ github.event.inputs.injected_mae }} || true
+          # give the monitor a moment to be picked up
+          sleep 30
+          # run promotion monitor to check for rollback
+          python model/model_promotion_monitor.py --region $(echo ${{ github.event.inputs.regions }} | cut -d',' -f1) --minutes 10 --mae-mult 1.2 || true
+
+      - name: Upload validation artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: pi-validation
+          path: /tmp/forecast_region_validation_*.json
+
*** End Patch
*** Begin Patch
*** Add File: hsm/vault-secretproviderclass.yaml
+apiVersion: secrets-store.csi.x-k8s.io/v1
+kind: SecretProviderClass
+metadata:
+  name: aigis-cosign-keys
+  namespace: aegis
+spec:
+  provider: vault
+  parameters:
+    vaultAddress: "https://vault.example.internal"
+    roleName: "aegis-operator-role"
+    objects: |
+      - objectName: "cosign_pubkey"
+        secretPath: "secret/data/aegis/cosign/pubkey"
+        secretKey: "pubkey"
+      - objectName: "hsm_meta"
+        secretPath: "secret/data/aegis/hsm/meta"
+        secretKey: "config"
+
+  # The following field is optional and depends on provider implementation
+  # secretObjects:
+  #  - secretName: cosign-pubkey
+  #    type: Opaque
+  #    data:
+  #      - objectName: cosign_pubkey
+
*** End Patch
*** Begin Patch
*** Add File: hsm/pkcs11-daemonset-example.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: pkcs11-agent
+  namespace: aegis
+spec:
+  selector:
+    matchLabels:
+      app: pkcs11-agent
+  template:
+    metadata:
+      labels:
+        app: pkcs11-agent
+    spec:
+      hostPID: true
+      containers:
+        - name: pkcs11-agent
+          image: aegis/pkcs11-agent:latest
+          volumeMounts:
+            - name: pkcs11-socket
+              mountPath: /var/run/pkcs11
+            - name: hsm-lib
+              mountPath: /opt/hsm/lib
+      volumes:
+        - name: pkcs11-socket
+          hostPath:
+            path: /var/run/pkcs11
+            type: DirectoryOrCreate
+        - name: hsm-lib
+          hostPath:
+            path: /opt/hsm/lib
+            type: Directory
+
*** End Patch
*** Begin Patch
*** Add File: hsm/drill_checklist.md
+# HSM Drill Checklist & Signoff (Operator)
+
+Purpose: Validate multi-HSM signing resilience and record operator signoff for audits.
+
+Pre-drill:
+- Confirm HSM_ADMIN_HOSTS is configured and SSH reachable from operator workstation
+- Ensure COMPLIANCE_BUCKET is set in environment for artifact upload
+- Ensure pkcs11-agent DaemonSet is deployed on operator hosts (see pkcs11-daemonset-example.yaml)
+
+Drill steps:
+1. Run multi-HSM recovery validation:
+   - python hsm/multi_hsm_recovery_validator.py
+   - Inspect /tmp/hsm_recovery_*.json and upload to compliance bucket (script does this)
+2. Simulate single-host outage:
+   - Manually take one HSM admin host offline (or simulate via firewall)
+   - Re-run multi_hsm_recovery_validator.py and confirm remaining hosts can sign
+3. Record rotation metadata (if applied):
+   - python hsm/key_rotation_tracker.py --hsm <host> --key <key-id> --operator <operator>
+4. Operator signoff:
+   - python hsm/drill_signoff.py --report /tmp/hsm_recovery_<ts>.json --operator <operator>
+
+Acceptance & signoff:
+- All scenarios in report show at least one successful signing path.
+- Drill signoff JSON present in COMPLIANCE_BUCKET under hsm_recovery_signoffs/
+- Operator records findings in compliance/pen_findings.json if any failures
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/tuning_runbook.md
+# Scheduler Tuning Runbook: map load test results to HPA & backoff parameters
+
+Overview:
+- Use results from load tests (/tmp/stress_results.json) and Prometheus metrics (queue length, throttle rate) to tune autoscaling and backoff.
+
+Key signals:
+- queue_length (aegis_queue_length)
+- throttle_rate (increase(aegis_queue_throttled_total[5m]))
+- processed_rate (increase(aegis_queue_processed_total[5m]))
+- avg_queue_delay (measure via job timestamps)
+
+Guidelines:
+1. HPA replica targets:
+   - If queue_length > 200 for >5m and processed_rate < incoming_rate => scale up
+   - If queue_length < 50 for >10m and CPU utilization < 40% => scale down
+   - Start with minReplicas=2, maxReplicas=10; adjust by +1 replicate per sustained 200 queue items
+
+2. Backoff policy:
+   - Base delay: 30s
+   - Exponential factor: 2
+   - Max delay: 3600s
+   - If THROTTLE spikes (>50 events/5m): increase base delay to 60s and consider reducing concurrency
+
+3. Queue sizing:
+   - Ensure Redis has sufficient memory; set eviction policy to noeviction for critical queues
+   - Monitor zcard and set alert at 75% of operational capacity
+
+4. Guardrails for noisy telemetry:
+   - Smooth instantaneous device measurements with smoothing_service (rolling window of 5)
+   - Use percentile-based decisioning (P95 of recent 30m) to avoid reacting to spikes
+
+5. Post-tuning validation:
+   - Re-run load test and confirm avg latency and p95 latency meet SLOs
+   - Confirm no increase in errors or failed schedules
+
+Automation:
+- Use scheduler/queue_autoscaler.py for horizon autoscaling in absence of HPA, or combine both
+- Example HPA manifest and Grafana dashboard provided in repo
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/analyze_load_results.py
+#!/usr/bin/env python3
+"""
+Analyze /tmp/stress_results.json and propose HPA replica and backoff suggestions.
+Outputs JSON with recommendations.
+"""
+import json, statistics, os
+
+IN = os.environ.get("STRESS_RESULTS", "/tmp/stress_results.json")
+OUT = "/tmp/load_advice.json"
+
+def main():
+    if not os.path.exists(IN):
+        print("No stress results file:", IN); return
+    data = json.load(open(IN))
+    total = len(data)
+    latencies = [r.get("latency") for r in data if isinstance(r.get("latency"), (int,float))]
+    throttles = sum(1 for r in data if r.get("status") == 429)
+    errors = sum(1 for r in data if r.get("status") == 'error' or (isinstance(r.get("status"), int) and r['status'] >= 500))
+    avg_lat = statistics.mean(latencies) if latencies else None
+    p95 = (sorted(latencies)[int(0.95*len(latencies))-1] if latencies and len(latencies)>0 else None)
+
+    advice = {"total_requests": total, "throttles": throttles, "errors": errors, "avg_latency_s": avg_lat, "p95_s": p95}
+    # simple heuristic for replicas
+    if p95 and p95 > 2.0:
+        advice["recommended_min_replicas"] = 4
+        advice["recommended_max_replicas"] = 12
+    elif p95 and p95 > 1.0:
+        advice["recommended_min_replicas"] = 3
+        advice["recommended_max_replicas"] = 8
+    else:
+        advice["recommended_min_replicas"] = 2
+        advice["recommended_max_replicas"] = 6
+
+    # backoff tuning
+    if throttles > total * 0.05:
+        advice["backoff_base_seconds"] = 60
+        advice["backoff_factor"] = 2
+        advice["backoff_max_seconds"] = 3600
+    else:
+        advice["backoff_base_seconds"] = 30
+        advice["backoff_factor"] = 2
+        advice["backoff_max_seconds"] = 1800
+
+    open(OUT,"w").write(json.dumps(advice, indent=2))
+    print("Wrote advice to", OUT)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
