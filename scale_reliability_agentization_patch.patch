*** Begin Patch
*** Add File: argo/workflows/distributed_tpu_training.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: distributed-tpu-training
+  namespace: staging
+spec:
+  entrypoint: distributed-hpo
+  templates:
+  - name: distributed-hpo
+    dag:
+      tasks:
+      - name: hpo
+        template: hpo
+  - name: hpo
+    container:
+      image: ghcr.io/yourorg/aegis-train:latest
+      command: ["/bin/sh","-c"]
+      args:
+        - |
+          # HPO controller: spawn parallel trials (example)
+          for LR in 0.01 0.005 0.001; do
+            echo "Starting trial lr=${LR}";
+            python /app/train.py --lr ${LR} --epochs 10 --log-mlflow &
+          done
+          wait
+    resources:
+      limits:
+        cpu: "4"
+        memory: "8Gi"
+    ttlStrategy:
+      secondsAfterCompletion: 3600
+
+  # Distributed train example - heavy job that uses TPU VMs (this is a template)
+  - name: distributed-train
+    inputs:
+      parameters:
+      - name: replicas
+        value: "8"
+    container:
+      image: ghcr.io/yourorg/aegis-train:latest
+      command: ["/bin/sh","-c"]
+      args: ["python /app/distributed_train.py --replicas {{inputs.parameters.replicas}} --dataset s3://my-bucket/datasets/dset1"]
+    retryStrategy:
+      limit: 2
+    resources:
+      limits:
+        cpu: "16"
+        memory: "64Gi"
+    activeDeadlineSeconds: 14400
+
*** End Patch
*** Begin Patch
*** Add File: scripts/scale_test_milvus.py
+#!/usr/bin/env python3
+"""
+Insert many vectors into Milvus to perform scale/throughput testing.
+Measure inserts/sec and search latency.
+"""
+import time
+import numpy as np
+from pymilvus import connections, Collection, utility
+
+MILVUS_HOST = "localhost"
+MILVUS_PORT = 19530
+COLLECTION = "scale_test"
+DIM = 128
+N = 20000
+
+def setup():
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    if utility.has_collection(COLLECTION):
+        col = Collection(COLLECTION)
+    else:
+        from pymilvus import FieldSchema, CollectionSchema, DataType
+        fields = [
+            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
+            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM)
+        ]
+        schema = CollectionSchema(fields)
+        col = Collection(COLLECTION, schema)
+        col.create_index("vector", {"index_type": "IVF_FLAT", "metric_type":"L2", "params": {"nlist": 1024}})
+        col.load()
+    return col
+
+def insert_vectors(col, n=N, dim=DIM):
+    ids = list(range(1, n+1))
+    vecs = np.random.random((n, dim)).astype("float32").tolist()
+    start = time.time()
+    col.insert([ids, vecs])
+    col.flush()
+    dt = time.time() - start
+    print(f"Inserted {n} vectors in {dt:.2f}s -> {n/dt:.2f} inserts/s")
+
+def search_latency(col, q=10, k=10):
+    import numpy as np
+    query = np.random.random((1, DIM)).astype("float32").tolist()[0]
+    start = time.time()
+    res = col.search([query], "vector", param={"metric_type":"L2","params":{"nprobe":10}}, limit=k)
+    dt = time.time() - start
+    print(f"Search latency: {dt:.4f}s")
+    return dt
+
+if __name__ == "__main__":
+    c = setup()
+    insert_vectors(c, n=N)
+    for _ in range(5):
+        search_latency(c)
+
*** End Patch
*** Begin Patch
*** Add File: services/loadtest/locustfile.py
+from locust import HttpUser, task, between
+
+class InferenceUser(HttpUser):
+    wait_time = between(0.5, 2)
+
+    @task(5)
+    def predict(self):
+        payload = {"inputs": ["sample text"]*4}
+        self.client.post("/predict", json=payload, timeout=60)
+
+    @task(1)
+    def health(self):
+        self.client.get("/health")
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/load_test.yml
+name: Inference Load Test
+on:
+  workflow_dispatch:
+    inputs:
+      host:
+        description: "Inference host e.g., http://localhost:8085"
+        required: true
+
+jobs:
+  load:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+      - name: Install locust
+        run: python -m pip install locust
+      - name: Run locust headless
+        run: locust -f services/loadtest/locustfile.py --headless -u 200 -r 20 --run-time 2m --host ${{ github.event.inputs.host }} --csv=loadtest || true
+      - name: Upload results
+        uses: actions/upload-artifact@v4
+        with:
+          name: locust-results
+          path: |
+            loadtest_stats_history.csv
+            loadtest_stats.csv
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/model_serving_rollout_analysis.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: AnalysisTemplate
+metadata:
+  name: model-slo-analysis
+  namespace: staging
+spec:
+  metrics:
+  - name: p95-latency
+    interval: 30s
+    count: 3
+    successCondition: result[0] < 1.0
+    failureCondition: result[0] > 2.0
+    provider:
+      prometheus:
+        address: http://prometheus.monitoring.svc.cluster.local
+        query: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le))
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/rollout_with_analysis.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: model-serving-rollout
+  namespace: staging
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: model-serving
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+          pause:
+            duration: 60
+            analysis:
+              templates:
+                - templateName: model-slo-analysis
+        - setWeight: 50
+          pause:
+            duration: 120
+            analysis:
+              templates:
+                - templateName: model-slo-analysis
+  template:
+    metadata:
+      labels:
+        app: model-serving
+    spec:
+      containers:
+      - name: serving
+        image: ghcr.io/yourorg/model-serving:stable@sha256:REPLACE_ME
+
*** End Patch
*** Begin Patch
*** Add File: scripts/budget_watcher.py
+#!/usr/bin/env python3
+"""
+Budget watcher for cloud accelerator spend (GCP example).
+ - polls Cloud Billing Budget API and triggers auto-teardown for active TPU workflows if budget exceeded.
+ - requires GCP service account key set in GOOGLE_APPLICATION_CREDENTIALS
+"""
+import time
+import os
+import requests
+import subprocess
+
+THRESHOLD_PERCENT = float(os.environ.get("BUDGET_THRESHOLD_PERCENT", "90"))
+BUDGET_NAME = os.environ.get("GCP_BUDGET_NAME", "")
+CHECK_INTERVAL = int(os.environ.get("CHECK_INTERVAL", "300"))
+
+def query_budget_percent(budget_name):
+    # Placeholder: Real implementation should call Cloud Billing API. Here we simulate.
+    # Return a float 0-100
+    return 50.0
+
+def teardown_tpu_workflows():
+    # Example: call Argo to terminate workflows labeled tpu=true
+    subprocess.run(["kubectl","-n","staging","delete","workflow","-l","aegis/tpu=true"], check=False)
+
+if __name__ == "__main__":
+    while True:
+        pct = query_budget_percent(BUDGET_NAME)
+        print("Budget usage:", pct)
+        if pct >= THRESHOLD_PERCENT:
+            print("Budget exceeded threshold; tearing down TPU workflows")
+            teardown_tpu_workflows()
+        time.sleep(CHECK_INTERVAL)
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_worker/langchain_agent_full.py
+"""
+LangChain / AutoGen-style agent worker with:
+ - RAG retriever (calls RAG service)
+ - Tool execution via Controller execute API
+ - Memory: short-term in Redis, long-term embedded in Milvus
+ - Summarization hook for long-term memory compaction
+"""
+import os
+import requests
+import redis
+from pymilvus import connections, Collection
+from sentence_transformers import SentenceTransformer
+from langchain import OpenAI
+from langchain.agents import initialize_agent, Tool
+from langchain.llms import OpenAI as LCOpenAI
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis.ops.svc.cluster.local:6379/0")
+MILTUS_HOST = os.environ.get("MILVUS_HOST", "milvus")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", 19530))
+COLLECTION = os.environ.get("MILVUS_COLLECTION", "agent_memory")
+RAG_URL = os.environ.get("RAG_URL", "http://rag-service:8000/query")
+CONTROLLER_URL = os.environ.get("CONTROLLER_URL", "http://agent-controller:8200")
+OPENAI_KEY = os.environ.get("OPENAI_API_KEY")
+
+redis_client = redis.from_url(REDIS_URL)
+connections.connect(host=MILTUS_HOST, port=str(MILVUS_PORT))
+emb_model = SentenceTransformer("all-MiniLM-L6-v2")
+
+def rag_retriever(query):
+    r = requests.post(RAG_URL, json={"q": query, "k": 5}, timeout=20)
+    if r.status_code != 200:
+        return ""
+    return r.json().get("answer","")
+
+def execute_tool(action):
+    r = requests.post(f"{CONTROLLER_URL}/execute", json={"actor":"agent","action":action}, timeout=30)
+    return r.json()
+
+def persist_short_memory(agent_id, text):
+    key = f"agent:{agent_id}:context"
+    redis_client.rpush(key, text)
+    redis_client.expire(key, 60*60*24)
+
+def persist_long_memory(agent_id, text):
+    col = Collection(COLLECTION)
+    emb = emb_model.encode([text])[0].tolist()
+    new_id = int(redis_client.incr("agent_long_id"))
+    col.insert([[new_id],[text],[emb]])
+    col.flush()
+
+def create_agent():
+    llm = LCOpenAI(openai_api_key=OPENAI_KEY, temperature=0.0)
+    tools = [
+        Tool(name="rag_search", func=rag_retriever, description="Retrieve context via RAG"),
+        Tool(name="execute_tool", func=execute_tool, description="Execute orchestrator tool")
+    ]
+    agent = initialize_agent(tools, llm, agent="structured-chat", verbose=True)
+    return agent
+
+def run_agent(agent_id, prompt):
+    agent = create_agent()
+    persist_short_memory(agent_id, prompt)
+    result = agent.run(prompt)
+    persist_short_memory(agent_id, result)
+    persist_long_memory(agent_id, result)
+    return result
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_worker/summarizer.py
+"""
+Periodic summarizer that reads short-term context from Redis, summarizes, stores summary in Milvus,
+and evicts older messages. This reduces long-term memory footprint and keeps important context.
+"""
+import os, redis, time, requests
+from sentence_transformers import SentenceTransformer
+from pymilvus import connections, Collection
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis.ops.svc.cluster.local:6379/0")
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "milvus")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", 19530))
+COLLECTION = os.environ.get("MILVUS_COLLECTION", "agent_memory")
+SUMMARIZER_API = os.environ.get("SUMMARIZER_API", "http://local-summarizer:8000/summarize")
+
+redis_client = redis.from_url(REDIS_URL)
+connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+emb = SentenceTransformer("all-MiniLM-L6-v2")
+
+def summarize_texts(texts):
+    # call external summarizer service (could be small LLM)
+    try:
+        r = requests.post(SUMMARIZER_API, json={"texts": texts}, timeout=30)
+        if r.status_code == 200:
+            return r.json().get("summary","")
+    except Exception:
+        pass
+    # fallback: naive concat
+    return " ".join(texts[:10])
+
+def run_once(max_messages=50):
+    # for each agent key, summarize
+    for key in redis_client.scan_iter("agent:*:context"):
+        msgs = redis_client.lrange(key, 0, -1)
+        if len(msgs) < 5:
+            continue
+        texts = [m.decode() for m in msgs]
+        summary = summarize_texts(texts)
+        # persist summary to Milvus
+        col = Collection(COLLECTION)
+        embedding = emb.encode([summary])[0].tolist()
+        new_id = int(redis_client.incr("agent_long_id"))
+        col.insert([[new_id],[summary],[embedding]])
+        col.flush()
+        # evict oldest messages
+        redis_client.ltrim(key, -10, -1)
+
+if __name__ == "__main__":
+    while True:
+        run_once()
+        time.sleep(60)
+
*** End Patch
*** Begin Patch
*** Add File: services/approval/approval_service.py
+"""
+Human approval / multi-sig runtime gate service.
+ - Create approval requests, track approvers, require N approvals to mark approved.
+ - Exposes endpoints for creating requests and approving them; emits audit events.
+ - Can be wired to GitHub webhooks or used standalone.
+"""
+from flask import Flask, request, jsonify
+import os, json, time, uuid
+
+APP_DB = "/data/approvals.json"
+
+app = Flask("approval")
+
+def load():
+    if os.path.exists(APP_DB):
+        return json.load(open(APP_DB))
+    return {}
+
+def save(db):
+    with open(APP_DB, "w") as f:
+        json.dump(db, f, indent=2)
+
+@app.post("/request")
+def request_approval():
+    body = request.get_json()
+    req_id = f"apr-{int(time.time())}-{uuid.uuid4().hex[:6]}"
+    db = load()
+    db[req_id] = {"id": req_id, "action": body.get("action"), "required": int(body.get("required",2)), "approvers": [], "status": "pending", "ts": int(time.time())}
+    save(db)
+    return jsonify(db[req_id])
+
+@app.post("/approve")
+def approve():
+    body = request.get_json()
+    req_id = body.get("id")
+    approver = body.get("approver")
+    db = load()
+    if req_id not in db:
+        return jsonify({"error":"not_found"}), 404
+    if approver in db[req_id]["approvers"]:
+        return jsonify({"error":"already_approved"}), 400
+    db[req_id]["approvers"].append(approver)
+    if len(db[req_id]["approvers"]) >= db[req_id]["required"]:
+        db[req_id]["status"] = "approved"
+    save(db)
+    return jsonify(db[req_id])
+
+@app.get("/status/<req_id>")
+def status(req_id):
+    db = load()
+    return jsonify(db.get(req_id, {}))
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8096)))
+
*** End Patch
*** Begin Patch
*** Add File: services/feature_store/feast_materialize_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: feast-materialize
+  namespace: staging
+spec:
+  schedule: "0 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+          - name: materialize
+            image: ghcr.io/yourorg/feast-client:latest
+            command: ["/bin/sh","-c"]
+            args:
+              - |
+                echo "Running Feast materialize (stub)"; exit 0
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: services/lineage/search_api.py
+"""
+Searchable dataset catalog API
+ - Query datasets by name, SBOM content, timestamp
+ - Simple REST API backed by the lineage store created earlier (lineage_service)
+"""
+from flask import Flask, request, jsonify
+import json, os
+
+LINEAGE_DB = os.environ.get("LINEAGE_DB", "/data/lineage.json")
+app = Flask("dataset-search")
+
+def load_db():
+    if os.path.exists(LINEAGE_DB):
+        return json.load(open(LINEAGE_DB))
+    return {"datasets":{},"transforms":{}}
+
+@app.get("/datasets")
+def list_datasets():
+    q = request.args.get("q","")
+    db = load_db()
+    res = []
+    for k,v in db.get("datasets",{}).items():
+        if q in k or q in json.dumps(v):
+            res.append({"name":k,"meta":v})
+    return jsonify(res)
+
+@app.get("/datasets/<name>")
+def get_dataset(name):
+    db = load_db()
+    return jsonify(db.get("datasets",{}).get(name, {}))
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0",port=int(os.environ.get("PORT", 8091)))
+
*** End Patch
*** Begin Patch
*** Add File: services/retrain/retrain_trigger.py
+"""
+Monitors model quality/drift metrics via Prometheus and triggers an Argo training workflow when thresholds breached.
+"""
+import os, time, requests, subprocess
+
+PROMETHEUS = os.environ.get("PROMETHEUS_URL", "http://prometheus.monitoring.svc.cluster.local")
+DRIFT_QUERY = 'increase(model_drift_events_total[1h])'
+THRESHOLD = int(os.environ.get("DRIFT_THRESHOLD", "1"))
+ARGO_WORKFLOW = os.environ.get("RETRAIN_WORKFLOW", "distributed-tpu-training")
+
+def query_prometheus(q):
+    r = requests.get(f"{PROMETHEUS}/api/v1/query", params={"query": q}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def trigger_retrain():
+    # Use Argo CLI to submit workflow (assumes kubectl/argo auth present)
+    subprocess.run(["argo","submit","-n","staging","--wait",ARGO_WORKFLOW], check=False)
+
+if __name__ == "__main__":
+    while True:
+        try:
+            resp = query_prometheus(DRIFT_QUERY)
+            val = 0
+            if resp["status"] == "success":
+                results = resp.get("data",{}).get("result",[])
+                if results:
+                    val = sum([float(r["value"][1]) for r in results])
+            if val >= THRESHOLD:
+                print("Drift threshold exceeded; triggering retrain")
+                trigger_retrain()
+        except Exception as e:
+            print("Error querying/prometheus or triggering retrain:", e)
+        time.sleep(int(os.environ.get("CHECK_INTERVAL", "300")))
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/slo_and_drift_rules.yaml
+groups:
+- name: aegis.slo.drift
+  rules:
+  - alert: ModelDriftDetected
+    expr: increase(model_drift_events_total[1h]) > 0
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Model drift events detected in the past hour"
+
+  - alert: ServingP95LatencyViolation
+    expr: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le)) > 1.0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Serving p95 latency >1s"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/pod_security_restrictive.yaml
+apiVersion: policy/v1beta1
+kind: PodSecurityPolicy
+metadata:
+  name: aegis-restrictive-psp
+spec:
+  privileged: false
+  allowPrivilegeEscalation: false
+  requiredDropCapabilities:
+    - ALL
+  volumes:
+    - 'configMap'
+    - 'emptyDir'
+    - 'persistentVolumeClaim'
+  hostNetwork: false
+  hostIPC: false
+  hostPID: false
+  readOnlyRootFilesystem: true
+  runAsUser:
+    rule: 'MustRunAsNonRoot'
+  seLinux:
+    rule: RunAsAny
+  fsGroup:
+    rule: RunAsAny
+
*** End Patch
*** Begin Patch
*** Add File: tests/scale_e2e_checklist.md
+# Scale & Reliability E2E Checklist
+
+1. Distributed TPU training
+ - Launch argo/workflows/distributed_tpu_training.yaml with a real dataset and N replicas.
+ - Verify job completes, checkpoints uploaded to COMPLIANCE_BUCKET, and Argo cleanup executed.
+
+2. Milvus & embedding scale
+ - Run scripts/scale_test_milvus.py against Milvus cluster and verify inserts/sec & search latency acceptable.
+
+3. Inference load
+ - Run .github/workflows/load_test.yml against inference endpoint; verify p95 < SLO and no errors.
+
+4. Canary analysis & rollback
+ - Deploy a canary using k8s/manifests/rollout_with_analysis.yaml; intentionally violate SLO to confirm analysis fails and Rollout aborts / rollbacks.
+
+5. Budget control
+ - Simulate budget high and verify scripts/budget_watcher.py triggers teardown of TPU workflows.
+
+6. Agents
+ - Run agent worker (services/agent_worker/langchain_agent_full.py) with long-running plan; confirm tools run as Jobs via services/agent_controller/k8s_executor_projected.py and that approvals via services/approval/approval_service.py are honored.
+
+7. Retrain on drift
+ - Emit model_drift_events_total metric and confirm services/retrain/retrain_trigger.py triggers Argo retrain.
+
*** End Patch
*** End Patch
