*** Begin Patch
*** Add File:legal/msasign/README.md
+MSA Sign-off & e-Signature Integration
+======================================
+
+This folder contains templates and example scripts to automate sending the MSA to customers for e-signature,
+tracking signature status, and recording signed copies into the compliance evidence store.
+
+Supported providers:
+- DocuSign (example)
+- HelloSign (example)
+
+Notes:
+- These are examples — you must create API keys in your vendor account and set them as repository secrets.
+- Legal must still review and approve final text before sending.
+
*** End Patch
*** Begin Patch
*** Add File:legal/msasign/docusign_send_template.py
+#!/usr/bin/env python3
+"""
+Example: Send MSA for signature using DocuSign API (needs DOCUSIGN_ACCOUNT_ID, DOCUSIGN_TOKEN env vars).
+This script uploads the MSA (legal/msa_final.md or converted PDF) and creates an envelope to the customer.
+"""
+import os, requests, json, sys
+
+API_BASE = os.environ.get("DOCUSIGN_BASE", "https://demo.docusign.net/restapi")
+ACCOUNT_ID = os.environ.get("DOCUSIGN_ACCOUNT_ID", "")
+TOKEN = os.environ.get("DOCUSIGN_TOKEN", "")
+
+def send_envelope(pdf_path, signer_email, signer_name, subject="Aegis MSA", message="Please sign"):
+    if not ACCOUNT_ID or not TOKEN:
+        print("Set DOCUSIGN_ACCOUNT_ID and DOCUSIGN_TOKEN in env")
+        return
+    url = f"{API_BASE}/v2.1/accounts/{ACCOUNT_ID}/envelopes"
+    with open(pdf_path, "rb") as fh:
+        pdf_bytes = fh.read()
+    # Minimal envelope creation (base64 inline) — for demo only
+    import base64
+    document_base64 = base64.b64encode(pdf_bytes).decode("ascii")
+    data = {
+        "emailSubject": subject,
+        "status": "sent",
+        "documents": [
+            {
+                "documentBase64": document_base64,
+                "name": "MSA.pdf",
+                "documentId": "1"
+            }
+        ],
+        "recipients": {
+            "signers": [
+                {
+                    "email": signer_email,
+                    "name": signer_name,
+                    "recipientId": "1",
+                    "routingOrder": "1",
+                    "tabs": {
+                        "signHereTabs": [{"xPosition":"100","yPosition":"100","documentId":"1","pageNumber":"1"}]
+                    }
+                }
+            ]
+        }
+    }
+    headers = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}
+    r = requests.post(url, headers=headers, data=json.dumps(data))
+    if r.status_code >= 400:
+        print("Error sending envelope:", r.status_code, r.text)
+    else:
+        print("Envelope created:", r.json().get("envelopeId"))
+
+if __name__ == "__main__":
+    if len(sys.argv) < 4:
+        print("Usage: docusign_send_template.py <pdf-path> <signer-email> <signer-name>")
+        sys.exit(1)
+    send_envelope(sys.argv[1], sys.argv[2], sys.argv[3])
+
*** End Patch
*** Begin Patch
*** Add File:productization/case_studies/publish_automation.py
+#!/usr/bin/env python3
+"""
+Automate publishing an approved case study to the public/docs site.
+- Verifies legal approval record exists.
+- Copies redacted markdown to public/case-studies/ with date and metadata.
+"""
+import os, shutil, sys, datetime, json
+
+APPROVAL_DB = "productization/case_studies/approvals.json"  # simple JSON record of approvals
+
+def is_approved(case_id):
+    if not os.path.exists(APPROVAL_DB):
+        return False
+    data = json.load(open(APPROVAL_DB))
+    return data.get(case_id, {}).get("approved", False)
+
+def publish(case_md, case_id):
+    if not is_approved(case_id):
+        print("Case not approved by legal/PO. Add entry to approvals.json")
+        return
+    if not os.path.exists(case_md):
+        print("Case file missing:", case_md)
+        return
+    pub_dir = "public/case-studies"
+    os.makedirs(pub_dir, exist_ok=True)
+    stamp = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
+    dst = os.path.join(pub_dir, f"{case_id}-{stamp}.md")
+    shutil.copy(case_md, dst)
+    print("Published case study to", dst)
+
+if __name__ == "__main__":
+    if len(sys.argv) != 3:
+        print("Usage: publish_automation.py <case-md> <case-id>")
+        sys.exit(1)
+    publish(sys.argv[1], sys.argv[2])
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/scale/README.md
+Sandbox Scale & TTL Automation
+=============================
+
+This module contains automation to provision ephemeral student sandboxes at scale (cloud module) and
+workers to auto-destroy expired sandboxes.
+
+Components:
+- terraform/managed/sandbox: TF module template to create per-student EKS clusters (or lightweight k3d clusters in cloud)
+- provisioning worker: training/hosted_sandbox/provision_worker.sh (already included)
+- destroy worker: training/hosted_sandbox/auto_destroy_worker_improved.sh (below)
+
+Notes:
+- Running many sandboxes in cloud can be costly. Use quotas and budget alarms.
+- Use per-student tags for automated cleanup.
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/auto_destroy_worker_improved.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# auto_destroy_worker_improved.sh
+# This script scans the sandbox sqlite DB for provisioned sandboxes older than TTL and attempts to destroy them.
+DB=${1:-sandbox.db}
+TFVARS_DIR=./training/hosted_sandbox/tfvars_students
+
+python3 - <<PY
+import sqlite3, time, os, subprocess, json
+db="${DB}"
+conn=sqlite3.connect(db)
+cur=conn.cursor()
+rows = cur.execute("SELECT id, student, ttl_hours, status, created_at FROM sandboxes WHERE status='provisioned'").fetchall()
+now=time.time()
+for r in rows:
+    id, student, ttl, status, created_at = r
+    # created_at is in ISO format; parse naive
+    try:
+        import datetime
+        created_ts = datetime.datetime.strptime(created_at, "%Y-%m-%dT%H:%M:%SZ").timestamp()
+    except:
+        created_ts = now
+    if now - created_ts > (ttl * 3600):
+        print("Destroying sandbox id", id, student)
+        tfvars = f"{TFVARS_DIR}/{student.replace('@','_').replace(' ','_')}.tfvars"
+        if os.path.exists(tfvars):
+            mod = "terraform/managed/sandbox"
+            cmd = ["bash","-lc", f"cd {mod} && terraform destroy -var-file={tfvars} -auto-approve"]
+            print("Running:", " ".join(cmd))
+            subprocess.call(" ".join(cmd), shell=True)
+            cur.execute("UPDATE sandboxes SET status='destroyed' WHERE id=?", (id,))
+            conn.commit()
+        else:
+            print("tfvars not found for", student, tfvars)
+conn.close()
+PY
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/grader/dashboard_app.py
+#!/usr/bin/env python3
+"""
+Grading dashboard (minimal Flask app) that shows students and lab scores and exposes API for grader jobs to POST results.
+"""
+from flask import Flask, request, jsonify, render_template
+import sqlite3, os, time
+
+DB = os.environ.get("SANDBOX_DB", "sandbox.db")
+app = Flask(__name__)
+
+def query(sql, args=()):
+    conn = sqlite3.connect(DB)
+    cur = conn.cursor()
+    cur.execute(sql, args)
+    rows = cur.fetchall()
+    conn.close()
+    return rows
+
+@app.route("/")
+def index():
+    sandboxes = query("SELECT id, student, ttl_hours, status, created_at FROM sandboxes ORDER BY id DESC LIMIT 100")
+    grades = query("SELECT id, student, lab, score, created_at FROM grades ORDER BY id DESC LIMIT 200")
+    return render_template("grader_index.html", sandboxes=sandboxes, grades=grades)
+
+@app.route("/api/grade", methods=["POST"])
+def api_grade():
+    data = request.json
+    student = data.get("student")
+    lab = data.get("lab")
+    score = float(data.get("score", 0))
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    now = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+    c.execute("INSERT INTO grades (student, lab, score, created_at) VALUES (?,?,?,?)", (student, lab, score, now))
+    conn.commit()
+    conn.close()
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8082)
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/grader/templates/grader_index.html
+<!doctype html>
+<html>
+  <head><title>Aegis Grader Dashboard</title></head>
+  <body>
+    <h1>Grader Dashboard</h1>
+    <h2>Sandboxes</h2>
+    <table border="1"><tr><th>ID</th><th>Student</th><th>TTL</th><th>Status</th><th>Created</th></tr>
+    {% for s in sandboxes %}
+    <tr><td>{{ s[0] }}</td><td>{{ s[1] }}</td><td>{{ s[2] }}h</td><td>{{ s[3] }}</td><td>{{ s[4] }}</td></tr>
+    {% endfor %}
+    </table>
+    <h2>Grades</h2>
+    <pre>{{ grades }}</pre>
+  </body>
+</html>
+
*** End Patch
*** Begin Patch
*** Add File:infra/cicd/terraform_apply_workflow.yml
+name: Terraform Plan & Apply (with approval)
+on:
+  workflow_dispatch:
+    inputs:
+      module-dir:
+        required: true
+        description: 'Terraform module directory (relative path)'
+      tfvars-file:
+        required: true
+        description: 'tfvars file path (relative)'
+
+jobs:
+  plan:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Terraform
+        uses: hashicorp/setup-terraform@v2
+      - name: Terraform init & plan
+        run: |
+          cd ${{ github.event.inputs.module-dir }}
+          terraform init -input=false
+          terraform plan -var-file="${{ github.event.inputs.tfvars-file }}" -out=plan.out -input=false
+      - name: Upload plan output
+        uses: actions/upload-artifact@v4
+        with:
+          name: tfplan
+          path: ${{ github.event.inputs.module-dir }}/plan.out
+
+  apply:
+    needs: plan
+    runs-on: ubuntu-latest
+    environment:
+      name: staging
+      url: https://example.com
+    permissions:
+      contents: read
+    steps:
+      - name: Approval wait
+        run: echo "This job requires manual approval in the Actions UI (environment protection rules)."
+      - uses: actions/checkout@v4
+      - name: Apply terraform
+        run: |
+          cd ${{ github.event.inputs.module-dir }}
+          terraform apply -auto-approve plan.out
+      - name: Run smoke tests
+        run: |
+          ./infra/validation/failover_validate.sh || true
+
*** End Patch
*** Begin Patch
*** Add File:infra/iam/iam_policy_audit.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Scan Terraform files and tfvars for wildcard ARNs and produce a report.
+DIR=${1:-terraform}
+OUT=${2:-/tmp/aegis-iam-audit.txt}
+echo "Scanning $DIR for wildcard ARNs..."
+grep -R --exclude-dir=.terraform -n "arn:aws:iam::.*:\\*\\|\"\\*\"" $DIR || true
+echo "Scanning for aws_iam_policy documents with '*' actions..."
+python3 - <<PY
+import re,sys,os
+out=[]
+for root,dirs,files in os.walk("${DIR}"):
+    for f in files:
+        if f.endswith(".tf") or f.endswith(".tfvars"):
+            p=os.path.join(root,f)
+            s=open(p).read()
+            if re.search(r'\"\\*\"',s) or re.search(r'arn:aws:[^\\s]*:\\*',s):
+                out.append(p)
+if out:
+    print("Potential wildcard usage found in:")
+    for x in out:
+        print(" -", x)
+else:
+    print("No obvious wildcard patterns found.")
+PY
+echo "Report written to $OUT"
+
*** End Patch
*** Begin Patch
*** Add File:rl/rlhf/Dockerfile
+FROM python:3.10-slim
+WORKDIR /opt/rl
+RUN apt-get update && apt-get install -y git build-essential && rm -rf /var/lib/apt/lists/*
+COPY rl/rlhf /opt/rl
+RUN pip install --no-cache-dir transformers datasets accelerate trl[safe] boto3
+CMD ["python3","/opt/rl/train_sft.py","--help"]
+
*** End Patch
*** Begin Patch
*** Add File:rl/rlhf/requirements.txt
+transformers>=4.30.0
+datasets>=2.10.0
+accelerate>=0.20.0
+trl>=0.5.0
+boto3
+
*** End Patch
*** Begin Patch
*** Add File:rl/rlhf/README_PRODUCTION.md
+Productionizing RLHF
+====================
+
+This document lists steps to move the RLHF scaffolds to production:
+1. Replace placeholder SFT/PPO training scripts with full HF Trainer/TRL configurations tuned for your model(s).
+2. Use multi-node training with accelerate or DeepSpeed (configure cluster and launch).
+3. Ensure checkpoint_handler uploads to encrypted S3 bucket with proper key policies.
+4. Add monitoring for GPU utilization, training throughput and checkpoint integrity.
+5. Add pre-commit/CI to build and scan the RLHF Docker image.
+6. Limit data (PII) and ensure consent/retention metadata in dataset manifests.
+
*** End Patch
*** Begin Patch
*** Add File:managed/ops/operations_playbook.md
+# Managed Operations Playbook
+
+Overview:
+- This playbook describes runbooks and staffing for managed Aegis operations.
+
+On-call procedures:
+- Pager triage: severity mapping and initial response templates
+- Escalation matrix: 1st line SRE -> Security -> Platform Lead
+
+Operational tasks:
+- Weekly checks: cluster health, backups, cost reports
+- Monthly: DR drill (dry), quarterly: full external pentest
+- Incident capture: use runbooks/incident_report_template.md
+
*** End Patch
*** Begin Patch
*** Add File:compliance/soc2/auditor_handoff_template.md
+Auditor Handoff Template
+========================
+
+Prepare the following and share via secure transfer:
+- SOC2 evidence bundle (run security/soc2/collect_evidence.sh)
+- Recent pentest report
+- DR drill report and RTO/RPO evidence
+- Policies & runbooks (RBAC, change management)
+- Contact POC for live Q&A during audit window
+
+Include a short README with temporary credentials and expiry windows.
+
*** End Patch
*** Begin Patch
*** Add File:productization/managed_offering/ops_onboarding_checklist.md
+# Managed Ops Onboarding Checklist
+
+1. Staffing: confirm SRE & Security engineer assignments and on-call rotation
+2. Access: create service accounts and vault policies for managed ops
+3. Monitoring: connect tenant to central Prometheus/Grafana and Alertmanager
+4. Backup: configure lakeFS backup schedule and Vault snapshot CronJobs
+5. Runbooks: ensure runbooks are accessible and training completed
+
*** End Patch
*** Begin Patch
*** Add File:pilots/pilot_examples/customer_pilot_publish_workflow.sh
+#!/usr/bin/env bash
+set -euo pipefail
+CASE_MD=${1:-productization/case_studies/published_case_study_retail.md}
+CASE_ID=${2:-retail-pilot-001}
+# mark approved in approvals.json (operator must confirm)
+APPROVAL_DB=productization/case_studies/approvals.json
+if [ ! -f $APPROVAL_DB ]; then
+  echo "{}" > $APPROVAL_DB
+fi
+python3 - <<PY
+import json
+db="$APPROVAL_DB"
+d=json.load(open(db))
+d["$CASE_ID"]={"approved": True, "approved_by": "legal@example.com", "approved_at":"$(date -u +"%Y-%m-%dT%H:%M:%SZ")"}
+open(db,"w").write(json.dumps(d,indent=2))
+print("Set approval for $CASE_ID in $db")
+PY
+python3 productization/case_studies/publish_automation.py "$CASE_MD" "$CASE_ID"
+
*** End Patch
*** Begin Patch
*** Add File:tests/ci/smoke_apply_and_smoke_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example smoke test to run after terraform/helm apply.
+echo "Running smoke checks..."
+# Check KServe CRD
+kubectl get crd inferenceservices.serving.kubeflow.org || true
+# Check MLflow reachable (use MLFLOW_TRACKING_URI env)
+if [ -n "${MLFLOW_TRACKING_URI:-}" ]; then
+  curl -fsS "$MLFLOW_TRACKING_URI" || echo "MLflow not reachable"
+fi
+echo "Smoke test completed."
+
*** End Patch
*** Begin Patch
*** Add File:automation/esign/hs_sign_template.sh
+#!/usr/bin/env bash
+#
+# HelloSign one-off send example (requires HELLOSIGN_API_KEY)
+# This is a minimal example to send a signature request via REST API (for small customers)
+API_KEY=${HELLOSIGN_API_KEY:-}
+if [ -z "$API_KEY" ]; then
+  echo "Set HELLOSIGN_API_KEY env var"
+  exit 1
+fi
+PDF=${1:-legal/msa_final.md}
+EMAIL=${2:-signer@example.com}
+NAME=${3:-Signer Name}
+curl -u "$API_KEY:" -X POST https://api.hellosign.com/v3/signature_request/send \
+  -F "file[0]=@$PDF" \
+  -F "title=Aegis MSA" \
+  -F "subject=Please sign Aegis MSA" \
+  -F "message=Please sign the attached MSA" \
+  -F "signers[0][email_address]=$EMAIL" \
+  -F "signers[0][name]=$NAME"
+
*** End Patch
*** End Patch
