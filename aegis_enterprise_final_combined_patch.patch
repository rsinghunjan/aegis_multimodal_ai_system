*** Begin Patch
*** Add File:.github/workflows/model_validation_gate.yml
+name: Model Profile Validation Gate
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 8 * * 1' # weekly Monday 08:00 UTC
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  EVIDENCE_RECON_PREFIX: reconciliations/
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+  COSIGN_KMS_KEY_ARN: ${{ secrets.COSIGN_KMS_KEY_ARN }}
+  MEDIAN_ERROR_THRESHOLD: "0.10"
+
+jobs:
+  validate-model:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout repo
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install boto3 numpy
+
+      - name: Run model validation gate
+        env:
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          python3 ci/validate_model_gating.py \
+            --bucket "${EVIDENCE_BUCKET}" \
+            --prefix "${EVIDENCE_RECON_PREFIX}" \
+            --threshold "${MEDIAN_ERROR_THRESHOLD}" \
+            --upload-report "true"
+
*** End Patch
*** Begin Patch
*** Add File:ci/validate_model_gating.py
+#!/usr/bin/env python3
+"""
+Validate a candidate power profile / runtime model by computing median relative error
+from reconciliation reports stored in S3. Exit non-zero if median error > threshold.
+Optionally upload a signed summary to S3 for audit.
+"""
+import argparse, boto3, json, statistics, os, tempfile, subprocess
+
+def list_reconciliations(s3, bucket, prefix, max_keys=500):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix, PaginationConfig={"MaxItems": max_keys}):
+        for o in page.get("Contents", []):
+            yield o["Key"]
+
+def fetch_json(s3, bucket, key):
+    obj = s3.get_object(Bucket=bucket, Key=key)
+    return json.loads(obj["Body"].read().decode("utf-8"))
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--bucket", required=True)
+    p.add_argument("--prefix", default="reconciliations/")
+    p.add_argument("--threshold", type=float, default=0.10)
+    p.add_argument("--upload-report", choices=("true","false"), default="false")
+    args = p.parse_args()
+
+    s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION"))
+    keys = list(list_reconciliations(s3, args.bucket, args.prefix, max_keys=1000))
+    rels = []
+    for k in keys:
+        try:
+            j = fetch_json(s3, args.bucket, k)
+            est = j.get("estimate", {}).get("estimated_emissions_kg")
+            meas = j.get("measured", {}).get("measured_emissions_kg")
+            if est is None or meas is None or meas == 0:
+                continue
+            rel = abs(meas - est) / float(meas)
+            rels.append(rel)
+        except Exception as e:
+            print("skip", k, e)
+    if not rels:
+        print("No valid reconciliation reports found; failing pipeline")
+        raise SystemExit(2)
+    median = statistics.median(rels)
+    report = {"count": len(rels), "median_relative_error": median}
+    print("Validation report:", report)
+    if args.upload_report == "true":
+        tmp = tempfile.mktemp(suffix=".json")
+        with open(tmp, "w") as f:
+            json.dump(report, f, indent=2)
+        s3_key = f"calibration/validation_reports/validation_{os.path.basename(tmp)}"
+        s3.upload_file(tmp, args.bucket, s3_key)
+        # optional cosign sign (placeholder using cosign CLI + AWS KMS)
+        cosign_key = os.environ.get("COSIGN_KMS_KEY_ARN")
+        if cosign_key:
+            subprocess.run(["cosign","sign","--key",f"awskms://{cosign_key}", tmp], check=False)
+            subprocess.run(["aws","s3","cp",tmp,f"s3://{args.bucket}/{s3_key}"], check=False)
+    if median > args.threshold:
+        print(f"Median relative error {median:.3f} exceeds threshold {args.threshold:.3f} - failing")
+        raise SystemExit(3)
+    print("Median relative error within threshold - pass")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/enforcement_test_harness.py
+#!/usr/bin/env python3
+"""
+Enforcement test harness:
+ - Submits synthetic pods/workflow CRs annotated with aegis.carbon.hint to test enforcement coverage.
+ - Reads enforcement counters from Redis (as implemented in enforcer/controller) to compute coverage.
+ - Reports a simple CSV/JSON summary.
+"""
+import time, json, os
+from kubernetes import client, config
+import redis
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://aegis-redis.aegis.svc:6379/0")
+NAMESPACE = os.environ.get("TEST_NAMESPACE", "aegis-test")
+COUNT = int(os.environ.get("TEST_COUNT", "50"))
+
+def create_test_pod(api, i, prefer_spot=False):
+    name = f"enf-test-{i}"
+    annotations = {"aegis.carbon.hint": json.dumps({"action":"run","prefer_spot": prefer_spot})}
+    pod = client.V1Pod(metadata=client.V1ObjectMeta(name=name, namespace=NAMESPACE, annotations=annotations),
+                       spec=client.V1PodSpec(containers=[client.V1Container(name="busybox", image="busybox", command=["sleep","3600"])]))
+    try:
+        api.create_namespaced_pod(NAMESPACE, pod)
+    except Exception as e:
+        print("create pod failed", e)
+
+def main():
+    config.load_kube_config()
+    v1 = client.CoreV1Api()
+    r = redis.Redis.from_url(REDIS_URL, decode_responses=True)
+    # ensure test namespace
+    try:
+        client.CoreV1Api().create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE)))
+    except Exception:
+        pass
+    # submit pods
+    for i in range(COUNT):
+        create_test_pod(v1, i, prefer_spot=(i%2==0))
+        time.sleep(0.2)
+    print("Submitted test pods, waiting for enforcement to act...")
+    time.sleep(30)
+    # read counters
+    triggered = int(r.get("aegis:policy:triggered_total") or 0)
+    enforced = int(r.get("aegis:enforcer:resume_jobs_created") or 0) + int(r.get("aegis:enforcer:patched_node_selector_total") or 0)
+    report = {"submitted": COUNT, "triggered": triggered, "enforced": enforced, "coverage": enforced / triggered if triggered else None}
+    print(json.dumps(report, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:security/rbac/minimal_webhook_controller_rbac.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-webhook-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-webhook-role
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["list","watch"]
+  - apiGroups: ["admissionregistration.k8s.io"]
+    resources: ["mutatingwebhookconfigurations"]
+    verbs: ["get"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: aegis-webhook-binding
+subjects:
+  - kind: ServiceAccount
+    name: aegis-webhook-sa
+    namespace: aegis
+roleRef:
+  kind: ClusterRole
+  name: aegis-webhook-role
+  apiGroup: rbac.authorization.k8s.io
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-enforcer-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-enforcer-role
+rules:
+  - apiGroups: ["argoproj.io"]
+    resources: ["workflows"]
+    verbs: ["get","list","watch","patch"]
+  - apiGroups: [""]
+    resources: ["jobs"]
+    verbs: ["create","get","list"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: aegis-enforcer-binding
+subjects:
+  - kind: ServiceAccount
+    name: aegis-enforcer-sa
+    namespace: aegis
+roleRef:
+  kind: ClusterRole
+  name: aegis-enforcer-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign/README_SIGNING_POLICY.md
+# Cosign signing policy (AWS KMS)
+
+Summary
+- Use AWS KMS to hold the private key for cosign. Restrict usage with a key policy.
+- Use Rekor for transparency logging. All signed artifacts are uploaded to the evidence S3 bucket and Rekor entries are recorded.
+
+Example signing flow (operator CI):
+1. CI builds artifact (e.g., /tmp/power_profiles.yaml).
+2. CI calls: cosign sign --key "awskms://arn:aws:kms:...:key/..." /tmp/power_profiles.yaml
+3. CI uploads artifact + signature to s3://<EVIDENCE_BUCKET>/calibration/
+4. CI records Rekor entry (cosign does this automatically if REKOR env configured).
+
+Key governance
+- KMS key policy must restrict who (principals) can use GenerateDataKey/Sign and require CloudTrail logging.
+- Rotate signing key per org policy; update CI secrets.
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign/cosign_sign.sh
+#!/usr/bin/env bash
+set -euo pipefail
+if [ $# -lt 2 ]; then
+  echo "usage: $0 <file> <kms_key_arn>"
+  exit 2
+fi
+FILE=$1
+KMS=$2
+cosign sign --key "awskms://${KMS}" "${FILE}"
+echo "Signed ${FILE} with KMS ${KMS}"
+
*** End Patch
*** Begin Patch
*** Add File:terraform/redshift/iam_role_for_copy.tf
+resource "aws_iam_role" "redshift_s3_access" {
+  name = "aegis-redshift-s3-access"
+  assume_role_policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Effect = "Allow"
+        Principal = {
+          Service = "redshift.amazonaws.com"
+        }
+        Action = "sts:AssumeRole"
+      }
+    ]
+  })
+}
+
+resource "aws_iam_policy" "redshift_s3_policy" {
+  name = "aegis-redshift-s3-policy"
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Effect = "Allow"
+        Action = [
+          "s3:GetObject",
+          "s3:ListBucket"
+        ]
+        Resource = [
+          "arn:aws:s3:::${var.evidence_bucket}",
+          "arn:aws:s3:::${var.evidence_bucket}/${var.parquet_prefix}/*"
+        ]
+      }
+    ]
+  })
+}
+
+resource "aws_iam_role_policy_attachment" "attach" {
+  role       = aws_iam_role.redshift_s3_access.name
+  policy_arn = aws_iam_policy.redshift_s3_policy.arn
+}
+
+output "redshift_s3_role_arn" {
+  value = aws_iam_role.redshift_s3_access.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:ingest/copy_manifest_generator.py
+#!/usr/bin/env python3
+"""
+Generate a Redshift COPY manifest for parquet files in an S3 prefix (simple listing).
+The manifest can be used with COPY ... FORMAT AS PARQUET.
+"""
+import boto3, json, os
+s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))
+
+def list_parquet(bucket, prefix):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            if obj["Key"].endswith(".parquet"):
+                yield f"s3://{bucket}/{obj['Key']}"
+
+def main():
+    bucket = os.environ["EVIDENCE_BUCKET"]
+    prefix = os.environ.get("PARQUET_OUT_PREFIX", "parquet/evidence")
+    manifest = {"entries": []}
+    for uri in list_parquet(bucket, prefix):
+        manifest["entries"].append({"url": uri, "mandatory": True})
+    out = "/tmp/copy_manifest.json"
+    with open(out, "w") as f:
+        json.dump(manifest, f, indent=2)
+    print("Wrote", out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:airflow/dags/aegis_bulk_load_dag.py
+from datetime import datetime, timedelta
+from airflow import DAG
+from airflow.operators.python import PythonOperator
+from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
+import os, subprocess, requests
+
+DEFAULT_ARGS = {
+    'owner': 'aegis',
+    'depends_on_past': False,
+    'email_on_failure': False,
+    'retries': 1,
+    'retry_delay': timedelta(minutes=10),
+}
+
+def on_failure(context):
+    webhook = os.environ.get("SLACK_WEBHOOK_URL")
+    if webhook:
+        requests.post(webhook, json={"text": f"ETL failed: {context.get('task_instance').task_id}"})
+
+def generate_manifest():
+    cmd = "python3 /opt/ingest/copy_manifest_generator.py"
+    subprocess.check_call(cmd, shell=True)
+
+with DAG(
+    dag_id="aegis_bulk_load",
+    default_args=DEFAULT_ARGS,
+    schedule_interval="@daily",
+    start_date=datetime(2025,1,1),
+    catchup=False,
+    max_active_runs=1,
+    on_failure_callback=on_failure
+) as dag:
+
+    gen_manifest = PythonOperator(
+        task_id="generate_manifest",
+        python_callable=generate_manifest
+    )
+
+    bulk_loader = KubernetesPodOperator(
+        task_id="run_redshift_copy",
+        name="redshift-copy",
+        namespace="aegis",
+        image="ghcr.io/yourorg/aegis-redshift-loader:latest",
+        cmds=["/bin/sh","-c"],
+        arguments=["/opt/scripts/redshift_bulk_loader.sh"],
+        get_logs=True,
+        is_delete_operator_pod=True,
+        in_cluster=True
+    )
+
+    gen_manifest >> bulk_loader
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/alertmanager/alerts_bi.yaml
+groups:
+- name: aegis-bi
+  rules:
+  - alert: ETLParquetFailure
+    expr: increase(aegis_etl_parquet_failed[15m]) > 0
+    for: 5m
+    labels:
+      severity: page
+    annotations:
+      summary: "Parquet pipeline failure in last 15m"
+
+  - alert: ETLBulkLoadFailure
+    expr: increase(aegis_etl_bulk_load_failed[15m]) > 0
+    for: 5m
+    labels:
+      severity: page
+    annotations:
+      summary: "Bulk load failure in last 15m"
+
+  - alert: DataQualityFailure
+    expr: aegis_ge_latest_success == 0
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Great Expectations validation failing"
+
+  - alert: EnforcementCoverageLow
+    expr: (aegis_enforcer_enforced_total / max(aegis_policy_triggered_total,1)) < 0.95
+    for: 30m
+    labels:
+      severity: page
+    annotations:
+      summary: "Enforcement coverage below 95%"
+
*** End Patch
*** Begin Patch
*** Add File:governance/superset_rls_example.sql
+-- Example RLS policy and pre-query for Superset to enforce per-team visibility
+-- Create policy in Postgres (marts.core_job_runs)
+ALTER TABLE marts.core_job_runs ENABLE ROW LEVEL SECURITY;
+
+CREATE POLICY rls_team_policy ON marts.core_job_runs
+USING (team = current_setting('app.team', true)::text OR current_setting('app.team', true) = '');
+
+-- Superset pre-query (set per connection or dataset)
+-- SET app.team = '{{ current_user.extra.team if current_user.extra.team else \"\" }}';
+
*** End Patch
*** Begin Patch
*** Add File:reports/sign_and_upload_report.sh
+#!/usr/bin/env bash
+set -euo pipefail
+REPORT_FILE=${1:-/tmp/daily_team_report.csv}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-aegis-evidence}
+KMS_KEY=${COSIGN_KMS_KEY_ARN:-}
+
+if [ ! -f "${REPORT_FILE}" ]; then
+  echo "Report file not found: ${REPORT_FILE}"
+  exit 2
+fi
+
+if [ -n "${KMS_KEY}" ]; then
+  cosign sign --key "awskms://${KMS_KEY}" "${REPORT_FILE}" || true
+fi
+
+aws s3 cp "${REPORT_FILE}" "s3://${EVIDENCE_BUCKET}/reports/$(basename ${REPORT_FILE})"
+echo "Uploaded signed report to s3://${EVIDENCE_BUCKET}/reports/$(basename ${REPORT_FILE})"
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/produce_combined_patch_NOTES.md
+# Notes about the combined artifacts produced
+
+What is included:
+- CI workflow and validation script to gate model/profile promotion using median relative error.
+- Enforcement test harness to measure enforcement coverage end-to-end.
+- Minimal least-privilege RBAC for webhook & enforcer service accounts.
+- Cosign signing policy doc + helper script using AWS KMS.
+- Terraform IAM role skeleton for Redshift COPY access and copy manifest generator.
+- Airflow DAG using KubernetesPodOperator for idempotent bulk load with manifest generation.
+- Alertmanager rules for ETL/DQ/enforcement monitoring.
+- Superset RLS SQL example and pre-query snippet.
+- Signed report helper for scheduling and audit.
+
+Security reminders:
+- Replace placeholders (bucket names, ARNs, secrets) with your real values before applying.
+- Limit IAM role S3 permissions to the exact prefix used by parquet artifacts.
+- Ensure cosign KMS keys and Rekor server are configured and audited.
+
*** End Patch
*** End Patch
