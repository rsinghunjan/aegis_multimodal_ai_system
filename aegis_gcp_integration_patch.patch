*** Begin Patch
*** Add File: scripts/gcp_enable_and_setup.sh
+#!/usr/bin/env bash
+#
+# Bootstrap script to enable required GCP services/APIs and create a service account for Aegis TPU workflows.
+# Usage:
+#   PROJECT=your-gcp-project REGION=us-central1 ZONE=us-central1-b ./scripts/gcp_enable_and_setup.sh
+set -euo pipefail
+
+PROJECT="${PROJECT:-}"
+REGION="${REGION:-us-central1}"
+ZONE="${ZONE:-us-central1-b}"
+SA_NAME="${SA_NAME:-aegis-trainer-sa}"
+K8S_NAMESPACE="${K8S_NAMESPACE:-aegis}"
+
+if [ -z "$PROJECT" ]; then
+  echo "Set PROJECT environment variable to your GCP project id" >&2
+  exit 2
+fi
+
+echo "Enabling required APIs in project: $PROJECT"
+gcloud config set project "$PROJECT"
+gcloud services enable \
+  compute.googleapis.com \
+  tpu.googleapis.com \
+  container.googleapis.com \
+  iam.googleapis.com \
+  storage.googleapis.com \
+  monitoring.googleapis.com \
+  logging.googleapis.com \
+  --project "$PROJECT"
+
+echo "Creating service account: $SA_NAME"
+gcloud iam service-accounts create "$SA_NAME" \
+  --display-name="Aegis Trainer Service Account" \
+  --project "$PROJECT"
+
+SA_EMAIL="${SA_NAME}@${PROJECT}.iam.gserviceaccount.com"
+echo "Granting storage.objectAdmin to $SA_EMAIL (least-privilege adjust as needed)"
+gcloud projects add-iam-policy-binding "$PROJECT" \
+  --member="serviceAccount:${SA_EMAIL}" \
+  --role="roles/storage.objectAdmin"
+
+echo "Granting minimal roles for workload identity mapping (if using Workload Identity)"
+# No blanket binding here; mapping to k8s SA happens below via IAM binding.
+
+echo "Generating JSON key for the service account (OPTIONAL - prefer Workload Identity)"
+KEY_FILE="./${SA_NAME}-key.json"
+gcloud iam service-accounts keys create "$KEY_FILE" \
+  --iam-account "$SA_EMAIL" --project "$PROJECT"
+echo "Wrote service account key to $KEY_FILE (store securely or delete if using Workload Identity)"
+
+cat <<EOF
+Next steps (choose one):
+1) Workload Identity (recommended for GKE):
+   - Create a Kubernetes ServiceAccount (in namespace ${K8S_NAMESPACE}) and bind it:
+     kubectl create serviceaccount aegis-trainer -n ${K8S_NAMESPACE}
+     gcloud iam service-accounts add-iam-policy-binding ${SA_EMAIL} \\
+       --member="serviceAccount:${PROJECT}.svc.id.goog[${K8S_NAMESPACE}/aegis-trainer]" \\
+       --role="roles/iam.workloadIdentityUser" --project=${PROJECT}
+     kubectl annotate serviceaccount -n ${K8S_NAMESPACE} aegis-trainer \\
+       iam.gke.io/gcp-service-account=${SA_EMAIL}
+
+2) If not using Workload Identity, you can mount the key into a k8s secret:
+   kubectl create secret generic gcp-sa-key -n ${K8S_NAMESPACE} --from-file=key.json=${KEY_FILE}
+   (then mount or expose via env in pods)
+
+3) Setup Artifact Registry / Container Registry to push trainer images:
+   gcloud auth configure-docker --project ${PROJECT}
+   # See scripts/push_to_artifact_registry.sh in repo for image push helper
+
+EOF
+
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: scripts/push_to_artifact_registry.sh
+#!/usr/bin/env bash
+#
+# Build and push a docker image to Artifact Registry (Docker repository).
+# Usage:
+#  PROJECT=my-proj REPO=my-repo REGION=us-central1 IMAGE=yourimage:tag ./scripts/push_to_artifact_registry.sh
+set -euo pipefail
+
+PROJECT="${PROJECT:-}"
+REPO="${REPO:-aegis-repo}"
+REGION="${REGION:-us-central1}"
+IMAGE="${IMAGE:-aegis-tpu-trainer:latest}"
+DOCKERFILE="${DOCKERFILE:-Dockerfile}"
+
+if [ -z "$PROJECT" ]; then
+  echo "Set PROJECT environment variable" >&2
+  exit 2
+fi
+
+AR_HOST="${REGION}-docker.pkg.dev"
+FULL_NAME="${AR_HOST}/${PROJECT}/${REPO}/${IMAGE}"
+
+echo "Ensure Artifact Registry repo exists (create if needed):"
+echo "gcloud artifacts repositories create ${REPO} --repository-format=docker --location=${REGION} || true"
+
+echo "Authenticating docker to gcloud/AR"
+gcloud auth configure-docker "${AR_HOST}" --project "${PROJECT}"
+
+echo "Building image ${FULL_NAME} from ${DOCKERFILE}"
+docker build -t "${FULL_NAME}" -f "${DOCKERFILE}" .
+
+echo "Pushing ${FULL_NAME}"
+docker push "${FULL_NAME}"
+
+echo "Pushed image: ${FULL_NAME}"
+echo "Use this image in your Argo workflows / k8s manifests"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/gcp/workload_identity_sa.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-trainer
+  namespace: aegis
+  annotations:
+    # Set this to the GCP service account you created:
+    # iam.gke.io/gcp-service-account: "aegis-trainer@PROJECT_ID.iam.gserviceaccount.com"
+    iam.gke.io/gcp-service-account: "REPLACE_WITH_SA_EMAIL"
+
+---
+# Example Kubernetes secret for mounting JSON key (only if NOT using Workload Identity)
+apiVersion: v1
+kind: Secret
+metadata:
+  name: gcp-sa-key
+  namespace: aegis
+type: Opaque
+stringData:
+  # Create this secret with: kubectl create secret generic gcp-sa-key -n aegis --from-file=key.json=/path/to/key.json
+  key.json: ""
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/tpu_training_gcp.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-tpu-train-gcp-
+  namespace: aegis
+spec:
+  entrypoint: tpu-train-gcp
+  templates:
+  - name: tpu-train-gcp
+    container:
+      image: "REGION-docker.pkg.dev/PROJECT/REPO/tpu-trainer:latest" # replace with artifact registry image name
+      command: [sh, -c]
+      args:
+        - |
+          echo "Starting TPU-aware training job (GCP variant)"
+          python train_on_tpu.py --epochs 1 --model-out /tmp/saved_model
+          tar -czf /tmp/artifact.tar.gz -C /tmp saved_model
+          # Upload to GCS instead of S3
+          gsutil cp /tmp/artifact.tar.gz gs://$OBJECT_STORE_BUCKET/model-archives/tpu-artifact-$(date +%s).tar.gz
+      resources:
+        limits:
+          cpu: "4"
+          memory: "16Gi"
+    nodeSelector:
+      aegis.cloud/tpu: "true"   # If using TPU-enabled node pool; alternative is to provision TPU VM and run externally
+    tolerations:
+      - key: "aegis.cloud/tpu"
+        operator: "Equal"
+        value: "true"
+        effect: "NoSchedule"
+    volumes:
+      - name: gcp-creds
+        secret:
+          secretName: gcp-sa-key # optional, prefer Workload Identity
+
+  # Notes:
+  # - Replace REGION/PROJECT/REPO placeholders with your Artifact Registry values.
+  # - If using Workload Identity, do not mount JSON key; the k8s SA -> GCP SA mapping provides credentials.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/check_tf_tpu_compat.sh
+#!/usr/bin/env bash
+#
+# Small helper to inspect a trainer image's TensorFlow version and print suggestions for TPU runtime.
+# Usage:
+#   ./scripts/check_tf_tpu_compat.sh IMAGE=us-docker.pkg.dev/myproj/repo/tpu-trainer:latest
+set -euo pipefail
+
+IMAGE="${IMAGE:-}"
+if [ -z "$IMAGE" ]; then
+  echo "Set IMAGE env var (trainer image in registry)" >&2
+  exit 2
+fi
+
+echo "Running a container to inspect tensorflow version (best-effort, requires docker access)"
+docker run --rm "${IMAGE}" python - <<PY
+import tensorflow as tf, sys
+v = tf.__version__
+print("tensorflow.version:", v)
+major = int(v.split('.')[0])
+print("Suggested TPU runtime mapping notes:")
+if major >= 2:
+    print("- For Cloud TPU v2/v3/v4, ensure TF runtime is a TPU-compatible build. See: https://cloud.google.com/tpu/docs/tensorflow-versions")
+else:
+    print("- TensorFlow older than 2.x may not be supported on modern TPU runtimes.")
+sys.exit(0)
+PY
+
*** End Patch
*** Begin Patch
*** Add File: scripts/replace_s3_with_gsutil.sh
+#!/usr/bin/env bash
+#
+# Convenience script to patch Argo workflow templates that use `aws s3 cp` and replace with `gsutil cp`.
+# Usage: ./scripts/replace_s3_with_gsutil.sh <path-to-workflow.yaml>
+set -euo pipefail
+
+FILE="${1:-}"
+if [ -z "$FILE" ] || [ ! -f "$FILE" ]; then
+  echo "Usage: $0 <workflow.yaml>" >&2
+  exit 2
+fi
+
+sed -e 's|aws s3 cp|gsutil cp|g' "$FILE" > "${FILE}.gs"
+echo "Wrote patched workflow to ${FILE}.gs (review and apply as needed)"
+
*** End Patch
*** Begin Patch
*** Add File: docs/GCP_TPU_WORKLOAD_IDENTITY_AND_RUNBOOK.md
+````markdown
+```text
+GCP TPU Integration & Workload Identity Runbook (Aegis)
+
+Overview
+- This runbook shows commands to provision GCP service account, configure Workload Identity, push trainer images to Artifact Registry,
+  run a TPU training workflow that uploads to GCS, and validate basic TPU functionality.
+
+Assumptions
+- gcloud SDK installed and authenticated with a user that has project/owner rights to the target PROJECT.
+- kubectl configured to point to the GKE cluster where Aegis runs (or you have cluster admin rights).
+
+Variables (example)
+PROJECT=my-gcp-project
+REGION=us-central1
+ZONE=us-central1-b
+REPO=aegis-repo
+NAMESPACE=aegis
+SA_NAME=aegis-trainer-sa
+
+1) Enable APIs (run from scripts/gcp_enable_and_setup.sh)
+  PROJECT=$PROJECT REGION=$REGION ZONE=$ZONE ./scripts/gcp_enable_and_setup.sh
+
+2) Create Workload Identity mapping (recommended)
+  kubectl create serviceaccount aegis-trainer -n $NAMESPACE
+  gcloud iam service-accounts add-iam-policy-binding ${SA_NAME}@${PROJECT}.iam.gserviceaccount.com \\
+    --member="serviceAccount:${PROJECT}.svc.id.goog[${NAMESPACE}/aegis-trainer]" \\
+    --role="roles/iam.workloadIdentityUser" --project=${PROJECT}
+  kubectl annotate serviceaccount -n $NAMESPACE aegis-trainer \\
+    iam.gke.io/gcp-service-account=${SA_NAME}@${PROJECT}.iam.gserviceaccount.com
+
+3) Build and push trainer image to Artifact Registry
+  # Ensure a docker repo exists:
+  gcloud artifacts repositories create ${REPO} --repository-format=docker --location=${REGION} || true
+  # Use helper:
+  PROJECT=${PROJECT} REPO=${REPO} REGION=${REGION} IMAGE=tpu-trainer:latest DOCKERFILE=tpu/Dockerfile.tpu_trainer ./scripts/push_to_artifact_registry.sh
+
+4) Update Argo workflow image and storage
+  - Edit argo/workflows/tpu_training_gcp.yaml and replace REGION/PROJECT/REPO and OBJECT_STORE_BUCKET (GCS bucket).
+  - Ensure OBJECT_STORE_BUCKET points to a GCS bucket accessible by the trainer SA.
+
+5) Run TPU workflow in Argo (staging)
+  kubectl apply -f argo/workflows/tpu_training_gcp.yaml -n aegis
+
+6) Validate:
+  - Check Argo workflow logs (argo CLI or kubectl logs)
+  - Confirm GCS object exists: gsutil ls gs://$OBJECT_STORE_BUCKET/model-archives/
+  - Inspect TF logs for TPUStrategy initialization
+
+Notes on keys vs Workload Identity
+- Prefer Workload Identity: no JSON keys to manage, short-lived tokens via metadata server.
+- If you must use a JSON key, store it as a k8s secret (kubectl create secret generic gcp-sa-key -n aegis --from-file=key.json=/path/to/key.json) and mount into pods, but take care to rotate and restrict access.
+
+Monitoring / Logs
+- Enable Cloud Monitoring and Logging APIs; link Stackdriver logs to your monitoring workflow.
+- Optionally export relevant metrics into Prometheus via scraping or use Cloud Monitoring exporters.
+
+Costs & Quotas
+- Request TPU quota before large tests:
+  gcloud compute regions describe ${REGION} --project=${PROJECT}
+  gcloud alpha compute tpus tpu-vm list-accelerator-types --zone ${ZONE}
+- Create budget alerts in Cloud Billing to avoid surprises.
+
+Troubleshooting
+- TPU initialization errors: check TF runtime compatibility; verify TPU is reachable from the node or VM.
+- GCS upload failures: check SA permissions (storage.objectAdmin or least-privilege role that allows write).
+
+```
+````

*** End Patch
*** End Patch
