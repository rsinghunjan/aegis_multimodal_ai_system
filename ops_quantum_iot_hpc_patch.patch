*** Begin Patch
*** Add File:ops/quantum/qpu_adapter.py
+#!/usr/bin/env python3
+"""
+QPU Adapter service
+- Provides a simple HTTP API to submit quantum jobs to multiple providers (AWS Braket, IonQ, Rigetti, etc.)
+- Normalizes job submission and returns a job_id. Integrates with cost estimator and provenance signing.
+Config (env):
+  QPU_PROVIDERS: comma-separated provider names e.g. "braket,ionq"
+  EVIDENCE_BUCKET: S3 bucket for provenance artifacts
+  SIGNING_KEY (optional): HMAC key for signing provenance records (store in ExternalSecrets)
+Note: This adapter is a gateway; operator must install provider SDKs and credentials on the host/cluster.
+"""
+import os
+import json
+import time
+import hmac
+import hashlib
+import uuid
+from flask import Flask, request, jsonify
+import boto3
+
+APP = Flask(__name__)
+S3 = boto3.client("s3")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+SIGNING_KEY = os.environ.get("SIGNING_KEY", "")
+
+def sign_record(data: dict) -> str:
+    if not SIGNING_KEY:
+        return ""
+    payload = json.dumps(data, sort_keys=True).encode("utf-8")
+    return hmac.new(SIGNING_KEY.encode("utf-8"), payload, hashlib.sha256).hexdigest()
+
+def store_provenance(record: dict) -> str:
+    if not EVIDENCE_BUCKET:
+        return ""
+    key = f"evidence/qpu/{int(time.time())}-{uuid.uuid4().hex}.json"
+    S3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(record).encode("utf-8"))
+    return key
+
+def submit_to_provider(provider: str, payload: dict) -> dict:
+    # Operator must implement provider-specific submission here.
+    # This stub returns a fake job id and echoes payload for testing.
+    jid = f"{provider}-{uuid.uuid4().hex[:8]}"
+    return {"provider_job_id": jid, "status": "submitted", "submitted_at": time.time()}
+
+@APP.route("/v1/qpu/submit", methods=["POST"])
+def submit():
+    body = request.json or {}
+    provider = body.get("provider", "braket")
+    job_spec = body.get("job_spec", {})
+    # Run cost estimator hook (operator can implement a call-out)
+    provenance = {"job_spec": job_spec, "provider": provider, "submitted_by": body.get("user", "unknown"), "ts": time.time()}
+    submit_result = submit_to_provider(provider, job_spec)
+    provenance["submit_result"] = submit_result
+    provenance["signature"] = sign_record(provenance)
+    key = store_provenance(provenance)
+    return jsonify({"job_id": submit_result["provider_job_id"], "provenance_key": key})
+
+@APP.route("/v1/qpu/status/<job_id>", methods=["GET"])
+def status(job_id):
+    # Stub: in real impl call provider SDK for status
+    return jsonify({"job_id": job_id, "status": "COMPLETED", "result": {"shots": 100}})
+
+if __name__ == "__main__":
+    APP.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8088)))
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantum/qpu_submit.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# CLI convenience wrapper to submit QPU jobs through the QPU adapter
+# Usage:
+#   ./ops/quantum/qpu_submit.sh --provider braket --job-spec job.json
+
+PROVIDER=""
+JOB_SPEC=""
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --provider) PROVIDER="$2"; shift 2;;
+    --job-spec) JOB_SPEC="$2"; shift 2;;
+    *) shift ;;
+  esac
+done
+
+if [ -z "${PROVIDER}" ] || [ -z "${JOB_SPEC}" ]; then
+  echo "Usage: $0 --provider <provider> --job-spec path/to/json"
+  exit 2
+fi
+
+URL=${QPU_ADAPTER_URL:-http://qpu-adapter.aegis.svc.cluster.local:8088/v1/qpu/submit}
+curl -s -X POST -H "Content-Type: application/json" -d "{\"provider\":\"${PROVIDER}\",\"job_spec\":$(cat ${JOB_SPEC})}" "${URL}" | jq .
+
*** End Patch
*** Begin Patch
*** Add File:argo/qpu/qpu_job_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: qpu-job-
+  namespace: argo
+spec:
+  entrypoint: qpu-run
+  templates:
+    - name: qpu-run
+      steps:
+        - - name: submit-qpu
+            template: submit-qpu
+        - - name: wait-and-fetch
+            template: wait-and-fetch
+
+    - name: submit-qpu
+      script:
+        image: curlimages/curl:7.85
+        command: [sh]
+        source: |
+          JOB_SPEC=$(cat <<'JSON'
+          {"circuit":"H 0; CNOT 0 1; MEASURE 0 1","shots":1000}
+          JSON
+          )
+          curl -s -X POST -H "Content-Type: application/json" -d "{\"provider\":\"braket\",\"job_spec\":${JOB_SPEC},\"user\":\"argo\"}" http://qpu-adapter.aegis.svc.cluster.local:8088/v1/qpu/submit > /tmp/sub.json
+          cat /tmp/sub.json
+
+    - name: wait-and-fetch
+      script:
+        image: curlimages/curl:7.85
+        command: [sh]
+        source: |
+          JOB_ID=$(jq -r .job_id /tmp/sub.json)
+          # Poll for status
+          for i in 1 2 3 4 5; do
+            curl -s http://qpu-adapter.aegis.svc.cluster.local:8088/v1/qpu/status/${JOB_ID} | jq .
+            sleep 10
+          done
+
*** End Patch
*** Begin Patch
*** Add File:ops/iot/edge_agent/Dockerfile
+FROM python:3.11-slim
+WORKDIR /app
+COPY agent.py /app/agent.py
+RUN pip install --no-cache-dir paho-mqtt requests boto3
+CMD ["python", "agent.py"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/iot/edge_agent/agent.py
+#!/usr/bin/env python3
+"""
+Lightweight edge agent
+- Connects to an MQTT broker (or AWS IoT Core), gathers sensor data, buffers locally, and forwards to a collector or S3.
+- Supports local persistence & signed uploads for provenance.
+Config via env:
+  MQTT_BROKER, MQTT_TOPIC, COLLECTOR_URL, DEVICE_ID, EVIDENCE_BUCKET
+"""
+import os
+import json
+import time
+import uuid
+import threading
+import requests
+import paho.mqtt.client as mqtt
+import boto3
+
+MQTT_BROKER = os.environ.get("MQTT_BROKER", "mqtt.example.local")
+MQTT_TOPIC = os.environ.get("MQTT_TOPIC", "aegis/edge")
+COLLECTOR_URL = os.environ.get("COLLECTOR_URL", "http://edge-collector.aegis.svc.cluster.local:8099/ingest")
+DEVICE_ID = os.environ.get("DEVICE_ID", f"edge-{uuid.uuid4().hex[:8]}")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+s3 = boto3.client("s3")
+
+buffer = []
+
+def upload_buffer():
+    global buffer
+    while True:
+        if buffer:
+            payload = {"device_id": DEVICE_ID, "records": buffer}
+            try:
+                resp = requests.post(COLLECTOR_URL, json=payload, timeout=10)
+                if resp.status_code == 200:
+                    buffer = []
+                else:
+                    # fallback to S3
+                    if EVIDENCE_BUCKET:
+                        key = f"edge/{DEVICE_ID}/{int(time.time())}.json"
+                        s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(payload).encode())
+                        buffer = []
+            except Exception:
+                pass
+        time.sleep(5)
+
+def on_message(client, userdata, msg):
+    try:
+        rec = json.loads(msg.payload.decode())
+    except Exception:
+        rec = {"raw": msg.payload.decode()}
+    rec["ts"] = int(time.time())
+    rec["topic"] = msg.topic
+    buffer.append(rec)
+
+def main():
+    t = threading.Thread(target=upload_buffer, daemon=True)
+    t.start()
+    client = mqtt.Client()
+    client.on_message = on_message
+    client.connect(MQTT_BROKER, 1883, 60)
+    client.subscribe(MQTT_TOPIC)
+    client.loop_forever()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/edge/edge-collector-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: edge-collector
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: edge-collector
+  template:
+    metadata:
+      labels:
+        app: edge-collector
+    spec:
+      containers:
+        - name: collector
+          image: REPLACE_IMAGE_REGISTRY/edge-collector:latest
+          ports:
+            - containerPort: 8099
+          env:
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-runtime-secrets
+                  key: EVIDENCE_BUCKET
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: edge-collector
+  namespace: aegis
+spec:
+  selector:
+    app: edge-collector
+  ports:
+    - port: 8099
+      targetPort: 8099
+
*** End Patch
*** Begin Patch
*** Add File:ops/hpc/slurm_submit.py
+#!/usr/bin/env python3
+"""
+Simple Slurm submitter wrapper for HPC integration.
+- Accepts a job spec JSON and writes a Slurm batch script, then runs sbatch.
+Requires slurm client tools on the host (sbatch).
+"""
+import os
+import json
+import subprocess
+import tempfile
+import argparse
+
+def create_slurm_script(spec):
+    lines = [
+        "#!/bin/bash",
+        f"#SBATCH --job-name={spec.get('name','aegis-job')}",
+        f"#SBATCH --cpus-per-task={spec.get('cpus',4)}",
+        f"#SBATCH --mem={spec.get('mem','16G')}",
+        f"#SBATCH --time={spec.get('time','01:00:00')}",
+    ]
+    if spec.get("gpus"):
+        lines.append(f"#SBATCH --gres=gpu:{spec.get('gpus')}")
+    lines.append("set -euo pipefail")
+    lines.append(spec.get("command","echo Hello HPC"))
+    return "\n".join(lines)
+
+def submit(spec):
+    script = create_slurm_script(spec)
+    with tempfile.NamedTemporaryFile("w", delete=False) as fh:
+        fh.write(script)
+        path = fh.name
+    res = subprocess.run(["sbatch", path], capture_output=True, text=True)
+    return res.stdout, res.stderr, res.returncode
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--spec", required=True)
+    args = p.parse_args()
+    spec = json.loads(open(args.spec).read())
+    out, err, rc = submit(spec)
+    print(out)
+    if rc != 0:
+        print(err)
+        raise SystemExit(rc)
+
*** End Patch
*** Begin Patch
*** Add File:ops/hpc/mpi_k8s_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: mpi-job-example
+  namespace: aegis
+spec:
+  parallelism: 4
+  template:
+    spec:
+      containers:
+        - name: mpi-worker
+          image: REPLACE_IMAGE_REGISTRY/mpi-worker:latest
+          command: ["mpirun", "-np", "4", "python", "worker.py"]
+          resources:
+            limits:
+              cpu: "4000m"
+              memory: "8Gi"
+      restartPolicy: Never
+
*** End Patch
*** Begin Patch
*** Add File:ops/integration/qpu_cost_estimator_integration.py
+#!/usr/bin/env python3
+"""
+Wrapper to integrate QPU cost estimator into CI and admission gates.
+Provides a CLI to estimate cost and emit JSON for further gating.
+"""
+import json
+import argparse
+def estimate_from_spec(spec):
+    # operator to replace with real provider estimator
+    # simple heuristic: shots * 1e-6 USD
+    return float(spec.get("shots", 1000)) * 1e-6
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--spec", required=True, help="path to job spec json")
+    args = p.parse_args()
+    s = json.load(open(args.spec))
+    est = estimate_from_spec(s)
+    print(json.dumps({"estimate_usd": est}))
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/hpc-resource-quota.yaml
+apiVersion: v1
+kind: ResourceQuota
+metadata:
+  name: hpc-quota
+  namespace: aegis
+spec:
+  hard:
+    requests.cpu: "200"
+    requests.memory: 500Gi
+    limits.cpu: "400"
+    limits.memory: 1000Gi
+    requests.nvidia.com/gpu: "10"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/egress-restriction-policy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: deny-egress-unless-approved
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels: {}
+  policyTypes:
+    - Egress
+  egress:
+    - to:
+        - podSelector:
+            matchLabels:
+              app: edge-collector
+      ports:
+        - protocol: TCP
+          port: 8099
+    - to:
+        - ipBlock:
+            cidr: 10.0.0.0/8
+
*** End Patch
*** Begin Patch
*** Add File:ops/provenance/qpu_sign_and_record.py
+#!/usr/bin/env python3
+"""
+Signs QPU job provenance and uploads to S3 and Rekor via cosign.
+Operator must configure COSIGN_KMS_ARN and have cosign available in PATH.
+"""
+import os, json, subprocess, sys, tempfile
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+COSIGN_KMS_ARN = os.environ.get("COSIGN_KMS_ARN", "")
+
+def upload_to_s3(path):
+    if not EVIDENCE_BUCKET:
+        return
+    subprocess.check_call(["aws","s3","cp",path,f"s3://{EVIDENCE_BUCKET}/evidence/"])
+
+def sign_and_upload(record: dict):
+    if not COSIGN_KMS_ARN:
+        raise RuntimeError("COSIGN_KMS_ARN not set")
+    with tempfile.NamedTemporaryFile("w", delete=False) as fh:
+        fh.write(json.dumps(record))
+        p = fh.name
+    # cosign sign-blob using KMS
+    subprocess.check_call(["cosign","sign-blob","--kms",COSIGN_KMS_ARN,"-key",COSIGN_KMS_ARN,p])
+    upload_to_s3(p)
+    print("Signed and uploaded", p)
+
+if __name__ == "__main__":
+    rec = json.load(sys.stdin)
+    sign_and_upload(rec)
+
*** End Patch
*** Begin Patch
*** Add File:docs/quantum_iot_hpc_README.md
+# Quantum / IoT / HPC Integration for Aegis
+
+This patch adds integration scaffolding to make Aegis ready to handle QPU jobs, IoT/Edge device ingestion, and HPC workloads.
+
+What was added
+- QPU Adapter (ops/quantum/qpu_adapter.py) and CLI (ops/quantum/qpu_submit.sh)
+- Argo workflow example to submit QPU jobs (argo/qpu/qpu_job_workflow.yaml)
+- QPU cost estimator integration (ops/integration/qpu_cost_estimator_integration.py)
+- Provenance signing for QPU jobs (ops/provenance/qpu_sign_and_record.py)
+- Edge agent (Dockerfile + agent.py) and edge-collector deployment (k8s/edge/edge-collector-deployment.yaml)
+- HPC integration:
+  - Slurm submit wrapper (ops/hpc/slurm_submit.py)
+  - MPI job example (ops/hpc/mpi_k8s_job.yaml)
+  - ResourceQuota for GPU/CPU (k8s/security/hpc-resource-quota.yaml)
+- Security & networking:
+  - NetworkPolicies to restrict egress and edge collector access (k8s/security)
+  - Secured admission webhook (earlier patch) can be used to gate QPU/HPC jobs via carbon/cost logic
+
+Operator steps (high level)
+1) Wire provider credentials into Secrets Manager and ExternalSecrets (see ops/wire_providers_and_secrets.sh)
+2) Deploy QPU adapter: build image and deploy as a service (or run locally)
+3) Configure provider-specific SDKs/credentials on the host that will call provider APIs (Braket SDK, IonQ)
+4) Deploy Whisper/CLIP and GPU model servers for on-prem inference if required
+5) Deploy Weaviate/Milvus for vector indexing and Argo embed workflows for backfills
+6) Deploy Edge collector and build/push edge-agent image to your edge device fleet
+7) Configure Slurm endpoints or use the Slurm submit wrapper on HPC login nodes
+8) Hook provenance signing to cosign/KMS and Rekor (ops/provenance/qpu_sign_and_record.py and ops/provenance/cosign_sign_and_upload.sh)
+
+Notes & cautions
+- This patch is intended to provide production-ready scaffolding, but operator work is required:
+  - Provider-specific setup (QPU provider accounts, keys, and SDKs)
+  - Edge fleet management (device auth, fleet OTA updates, and network/firewall configuration)
+  - HPC scheduler integration testing and security validation
+  - Hardening of the QPU adapter to ensure credentials aren't leaked
+  - Ensure cosign and Rekor flows are available and permitted in your environment
+
+Next steps I can take
+- Generate Kubernetes manifests for a managed Weaviate/Milvus cluster with HA and persistence.
+- Build example Docker images and a GitHub Actions workflow to build/push them to your ECR/GCR.
+- Create an operator-ready helm chart to install the QPU adapter and edge-collector with values for your environment.
+- Generate a sample device provisioning script (AWS IoT or other) to bootstrap edge devices.
+
*** End Patch
*** End Patch
