*** Begin Patch
*** Add File:quantum/hsm/vendor_validation/run_hsm_validation.sh
+#!/usr/bin/env bash
+# Run an end-to-end HSM validation against a vendor PKCS#11 module (or SoftHSM)
+# Usage:
+#   ./run_hsm_validation.sh \
+#     --artifact /tmp/test.bin \
+#     --pkcs11-lib /usr/lib/softhsm/libsofthsm2.so \
+#     --pkcs11-slot 0 \
+#     --pkcs11-pin 1234 \
+#     --pkcs11-keylabel pqkey \
+#     --s3-bucket my-hsm-audit-bucket \
+#     --rekor
+set -euo pipefail
+
+ARGS=()
+S3_BUCKET=""
+REKOR=false
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --artifact) ART="$2"; shift 2;;
+    --pkcs11-lib) PKCS11_LIB="$2"; shift 2;;
+    --pkcs11-slot) PKCS11_SLOT="$2"; shift 2;;
+    --pkcs11-pin) PKCS11_PIN="$2"; shift 2;;
+    --pkcs11-keylabel) PKCS11_KEYLABEL="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --rekor) REKOR=true; shift 1;;
+    *) ARGS+=("$1"); shift;;
+  esac
+done
+
+: "${ART:?--artifact is required}"
+: "${PKCS11_LIB:?--pkcs11-lib is required}"
+: "${PKCS11_SLOT:?--pkcs11-slot is required}"
+: "${PKCS11_PIN:?--pkcs11-pin is required}"
+: "${PKCS11_KEYLABEL:?--pkcs11-keylabel is required}"
+
+OUTDIR="$(mktemp -d /tmp/aegis_hsm_val.XXXX)"
+echo "Using OUTDIR=$OUTDIR"
+
+echo "Signing artifact with hybrid_signer_hsm (PKCS#11)..."
+python3 quantum/crypto/hybrid_signer_hsm.py sign --artifact "${ART}" --outdir "${OUTDIR}" --use-pkcs11 --pkcs11-lib "${PKCS11_LIB}" --pkcs11-slot "${PKCS11_SLOT}" --pkcs11-pin "${PKCS11_PIN}" --pkcs11-keylabel "${PKCS11_KEYLABEL}" $( $REKOR && echo "--rekor" || echo "" )
+
+echo "Hybrid metadata:"
+cat "${OUTDIR}/hybrid-signature.json" || true
+
+if [ -n "${S3_BUCKET:-}" ]; then
+  echo "Checking HSM audit upload to s3://${S3_BUCKET}/hsm-audit/ (may require eventual consistency)..."
+  echo "Waiting up to 120s for any audit files..."
+  for i in $(seq 1 24); do
+    aws s3 ls "s3://${S3_BUCKET}/hsm-audit/" && break || sleep 5
+  done
+  echo "Listing audit objects:"
+  aws s3 ls "s3://${S3_BUCKET}/hsm-audit/" || echo "No audit objects found"
+fi
+
+echo "Validation run complete. OUTDIR=${OUTDIR}"
+echo "Operator: collect ${OUTDIR} and, if S3 audit objects exist, include them in compliance bundle."
+
*** End Patch
*** Begin Patch
*** Add File:quantum/hsm/vendor_validation/verify_hsm_audit.py
+#!/usr/bin/env python3
+"""
+Poll S3 for HSM audit files and verify presence within a timeout window.
+Usage:
+  python3 verify_hsm_audit.py --s3-bucket my-bucket --prefix hsm-audit/ --timeout 300
+"""
+import argparse, boto3, time, sys
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--prefix", default="hsm-audit/")
+    p.add_argument("--timeout", type=int, default=300)
+    args = p.parse_args()
+    s3 = boto3.client("s3")
+    deadline = time.time() + args.timeout
+    while time.time() < deadline:
+        resp = s3.list_objects_v2(Bucket=args.s3_bucket, Prefix=args.prefix, MaxKeys=10)
+        if resp.get("KeyCount",0) > 0:
+            print("Found audit objects:")
+            for o in resp.get("Contents", []):
+                print(" -", o["Key"])
+            sys.exit(0)
+        time.sleep(5)
+    print("Timeout waiting for audit objects in s3://{}/{}".format(args.s3_bucket, args.prefix))
+    sys.exit(2)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/vault/scripts/publish_provider_creds.sh
+#!/usr/bin/env bash
+#
+# Publish provider credentials into Vault KV v2 for pilots
+# Usage:
+#   ./publish_provider_creds.sh --vault-path secret/data/quantum/providers --key braket_device --value "arn:aws:braket:..."
+set -euo pipefail
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --vault-path) VAULT_PATH="$2"; shift 2;;
+    --key) KEY="$2"; shift 2;;
+    --value) VALUE="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${VAULT_PATH:?--vault-path required}"
+: "${KEY:?--key required}"
+: "${VALUE:?--value required}"
+
+echo "Writing to Vault: $VAULT_PATH -> $KEY"
+vault kv put "${VAULT_PATH}" "${KEY}=${VALUE}"
+echo "Done"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/run_pilots_staging.yml
+name: Run QPU Pilots (Staging)
+on:
+  workflow_dispatch:
+jobs:
+  run-pilots:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          pip install boto3 mlflow qiskit qiskit-ibm-runtime hvac
+      - name: Run pilots via orchestrator
+        env:
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }}
+          S3_BUCKET: ${{ secrets.STAGING_S3_BUCKET }}
+        run: |
+          python3 providers/pilot/orchestrator.py --provider braket --vault-path "${{ secrets.VAULT_PROVIDER_PATH }}" --program "${{ secrets.PILOT_PROGRAM_PATH }}" --s3-bucket "${{ secrets.STAGING_S3_BUCKET }}"
+          python3 providers/pilot/orchestrator.py --provider ibm --vault-path "${{ secrets.VAULT_PROVIDER_PATH }}" --program "${{ secrets.PILOT_PROGRAM_PATH }}" --s3-bucket "${{ secrets.STAGING_S3_BUCKET }}" --backend "${{ secrets.IBM_BACKEND }}"
+
*** End Patch
*** Begin Patch
*** Add File:broker/scripts/deploy_broker_staging.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy broker helm chart to staging and create required secrets
+# Usage:
+#  ./deploy_broker_staging.sh --namespace aegis --job-db-url "postgresql://..." --jwt-secret "..." --helm-release aegis-quantum-broker
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --namespace) NS="$2"; shift 2;;
+    --job-db-url) JOB_DB="$2"; shift 2;;
+    --jwt-secret) JWT="$2"; shift 2;;
+    --helm-release) RELEASE="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${NS:=aegis}"
+: "${JOB_DB:?--job-db-url required}"
+: "${JWT:?--jwt-secret required}"
+: "${RELEASE:=aegis-quantum-broker}"
+
+kubectl create ns "${NS}" --dry-run=client -o yaml | kubectl apply -f -
+kubectl create secret generic aegis-db-secret -n "${NS}" --from-literal=JOB_DATABASE_URL="${JOB_DB}" --dry-run=client -o yaml | kubectl apply -f -
+kubectl create secret generic aegis-broker-secret -n "${NS}" --from-literal=BROKER_API_JWT_SECRET="${JWT}" --dry-run=client -o yaml | kubectl apply -f -
+
+echo "Ensure cert-manager is installed, then apply cert manifests (broker/k8s/cert-manager-issuer.yaml)"
+kubectl apply -f broker/k8s/cert-manager-issuer.yaml || true
+
+helm upgrade --install "${RELEASE}" broker/helm -n "${NS}" --set image.tag=staging --values broker/helm/values-production.yaml
+echo "Broker helm deploy triggered. Check pods: kubectl get pods -n ${NS}"
+
*** End Patch
*** Begin Patch
*** Add File:broker/scripts/validate_mtls.sh
+#!/usr/bin/env bash
+#
+# Validate mTLS on broker ingress endpoint by performing a client-certificate curl.
+# Usage:
+#   ./validate_mtls.sh --url https://aegis-quantum-broker.aegis.svc.cluster.local/health --cert client.crt --key client.key --cacert ca.crt
+set -euo pipefail
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --url) URL="$2"; shift 2;;
+    --cert) CERT="$2"; shift 2;;
+    --key) KEY="$2"; shift 2;;
+    --cacert) CACERT="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${URL:?--url required}"
+: "${CERT:?--cert required}"
+: "${KEY:?--key required}"
+: "${CACERT:?--cacert required}"
+
+curl --cert "${CERT}" --key "${KEY}" --cacert "${CACERT}" -sSf "${URL}" && echo "mTLS endpoint OK"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/rekor_enforce_strict.yml
+name: Rekor Enforcement & Auditor Report
+on:
+  workflow_dispatch:
+jobs:
+  rekor-check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          pip install mlflow boto3
+      - name: Run Rekor MLflow Checker
+        env:
+          MLFLOW_URL: ${{ secrets.MLFLOW_URL }}
+        run: |
+          python3 quantum/rekor/check_mlflow_rekor.py --mlflow-url "${MLFLOW_URL}" --experiment quantum-pilots --threshold 20
+      - name: Produce auditor report (placeholder)
+        run: |
+          echo "If the above succeeded, MLflow runs have rekor.entry tags. For auditors, collect evidence using compliance/packager.py"
+
*** End Patch
*** Begin Patch
*** Add File:observability/billing/aws_cur_placeholder.md
+AWS CUR (Cost & Usage Report) Integration — placeholder
+-----------------------------------------------------
+Operator steps (high level):
+1. In AWS Billing console, create a Cost & Usage Report that writes to an S3 bucket.
+2. Ensure report is configured with hourly or daily granularity and includes resource tags.
+3. Use a scheduled job (lambda, ECS task or on-prem cron) to convert CSV/Parquet into simplified billing CSV consumed by broker/quotas/chargeback_connector.py
+4. Example ingestion script is broker/quotas/chargeback_connector.py — adapt CSV mapping accordingly.
+
+Note: CUR setup requires IAM permissions and cross-account S3 access if using centralized billing.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/audit_packager_enhanced.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUT=${1:-/tmp/aegis_audit_bundle}
+mkdir -p "$OUT"
+echo "Collecting broker logs..."
+kubectl logs -n aegis deployment/aegis-quantum-broker > "$OUT/broker.log" || true
+echo "Collecting HSM audit S3 objects (if configured)..."
+if [ -n "${HSM_AUDIT_BUCKET:-}" ]; then
+  aws s3 cp "s3://${HSM_AUDIT_BUCKET}/hsm-audit/" "$OUT/hsm-audit/" --recursive || true
+fi
+echo "Exporting Rekor entries for MLflow artifacts (if present)..."
+# This requires a list of artifact hashes; placeholder: operator to supply.
+tar czf "${OUT}.tar.gz" -C "$(dirname "$OUT")" "$(basename "$OUT")"
+echo "Auditor bundle: ${OUT}.tar.gz"
+
*** End Patch
*** Begin Patch
*** Add File:docs/legal/export_control_runbook.md
+# Export Control Runbook (operators & legal)
+
+1. Inventory cryptographic algorithms used (classical & PQ). Record in compliance/crypto_inventory.md.
+2. Identify where artifacts and keys are stored and which jurisdictions they may traverse (S3 buckets, QPU provider endpoints).
+3. Engage legal: provide vendor list (Braket, IBM, Azure) and intended data flows for export-control assessment.
+4. If export licenses required, document requests and approvals; store decisions in compliance/evidence/.
+5. Maintain a change log for any cryptographic algorithm additions or vendor onboarding.
+
*** End Patch
*** End Patch
