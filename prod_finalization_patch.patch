*** Begin Patch
*** Add File:infra/milvus/helm-production-readme.md
+Milvus Production Deployment (Helm) â€” quick steps
+
+1) Add the Milvus Helm repo and install with production values:
+   helm repo add milvus https://milvus-helm-repo.example
+   helm repo update
+   helm upgrade --install milvus milvus/milvus -n milvus --create-namespace -f values-production.yaml
+
+2) Ensure PVCs are available (see pvc-production-example.yaml) and that StorageClass matches your cloud provider.
+3) After install, create index via multimodal_collection.create_index or use the CLI.
+
+Notes:
+- Tune indexer/compute resource sizes for your expected embedding dim and QPS.
+- Enable metrics scraping (prometheus) and configure ServiceMonitor for milvus.
+
*** End Patch
*** Begin Patch
*** Add File:infra/mlflow/deployment-mlflow.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: mlflow
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: mlflow
+  template:
+    metadata:
+      labels:
+        app: mlflow
+    spec:
+      containers:
+        - name: mlflow
+          image: ghcr.io/yourorg/mlflow:latest
+          env:
+            - name: MLFLOW_S3_ENDPOINT_URL
+              value: "https://s3.amazonaws.com"
+            - name: AWS_REGION
+              value: "REPLACE_WITH_AWS_REGION"
+          ports:
+            - containerPort: 5000
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "1Gi"
+            limits:
+              cpu: "2"
+              memory: "4Gi"
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: mlflow
+  namespace: aegis
+spec:
+  selector:
+    app: mlflow
+  ports:
+    - port: 5000
+      targetPort: 5000
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File:infra/redis/helm-install.md
+Install Redis (production)
+
+Recommended: use managed Redis (ElastiCache) or Bitnami chart with replication and persistence.
+
+Example Helm (bitnami):
+  helm repo add bitnami https://charts.bitnami.com/bitnami
+  helm install redis bitnami/redis -n aegis --create-namespace -f values-production.yaml
+
+Set auth and use TLS for external connections when required.
+
*** End Patch
*** Begin Patch
*** Add File:infra/seldon/triton-nodepool-annotations.yaml
+# NodeGroup annotation example for Triton/Seldon GPU nodes
+# Apply these labels/taints to GPU nodepool created via TF/Terraform or cloud console
+labels:
+  workload: "inference"
+  purpose: "triton"
+taints:
+  - key: aegis/inference
+    value: "true"
+    effect: NO_SCHEDULE
+
+# Ensure your autoscaler is configured to scale the underlying ASG/instance pool for this nodegroup.
+
*** End Patch
*** Begin Patch
*** Add File:infra/terraform/gpu_nodegroup_example.tf
+// Example Terraform snippet for Azure/AWS/GCP GPU node group
+// See earlier aws_eks_node_group example; add autoscaling tags and taints.
+// This file is a placeholder: adapt to your cloud provider.
+
*** End Patch
*** Begin Patch
*** Add File:security/external-secrets/secretstore-aws.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: SecretStore
+metadata:
+  name: aws-secrets-manager
+  namespace: aegis
+spec:
+  provider:
+    aws:
+      service: SecretsManager
+      region: REPLACE_WITH_AWS_REGION
+      auth:
+        jwt:
+          serviceAccountRef:
+            name: external-secrets-sa
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: external-secrets-sa
+  namespace: aegis
+  annotations:
+    eks.amazonaws.com/role-arn: arn:aws:iam::REPLACE_ACCOUNT:role/aegis-external-secrets-role
+
+# Operator: create IAM role with trust to EKS OIDC and permission to read specific secret ARNs only.
+
*** End Patch
*** Begin Patch
*** Add File:security/kms/cosign_kms_complete.tf
+// Terraform: create cosign KMS key + grant minimal use to operator principals
+resource "aws_kms_key" "cosign" {
+  description             = "Cosign key for Aegis artifact signing - production"
+  deletion_window_in_days = 30
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign-prod"
+  target_key_id = aws_kms_key.cosign.id
+}
+
+resource "aws_iam_policy" "cosign_use_policy" {
+  name   = "aegis-cosign-use"
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Effect = "Allow"
+        Action = [
+          "kms:Sign",
+          "kms:Verify",
+          "kms:Encrypt",
+          "kms:Decrypt",
+          "kms:GenerateDataKey*",
+          "kms:DescribeKey"
+        ]
+        Resource = aws_kms_key.cosign.arn
+        Condition = {
+          "StringEquals" = {
+            "aws:PrincipalTag/role" = "aegis-cosign"
+          }
+        }
+      }
+    ]
+  })
+}
+
+// You must attach this policy only to the operator principal(s) (CI, security roles).
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign/rotate_and_verify.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Rotate cosign KMS key and verify Rekor entry on a sample artifact
+NEW_KEY_ARN=${1:-}
+ARTIFACT=${2:-}
+REKOR_URL=${REKOR_URL:-""}
+
+if [ -z "$NEW_KEY_ARN" ] || [ -z "$ARTIFACT" ]; then
+  echo "Usage: $0 <awskms_key_arn> <artifact-path>"
+  exit 2
+fi
+
+echo "Signing $ARTIFACT with $NEW_KEY_ARN"
+cosign sign --key "awskms://$NEW_KEY_ARN" "$ARTIFACT"
+echo "Signature created: $ARTIFACT.sig"
+
+if [ -n "$REKOR_URL" ]; then
+  echo "Posting small Rekor note (best-effort)"
+  curl -sS -X POST -H "Content-Type: application/json" --data "{\"artifact\":\"$ARTIFACT\",\"key\":\"$NEW_KEY_ARN\"}" "$REKOR_URL/api/v1/log/entries" || true
+fi
+
+echo "Done. Operator: update ExternalSecrets COSIGN_KMS_KEY_ARN to awskms://$NEW_KEY_ARN"
+
*** End Patch
*** Begin Patch
*** Add File:security/policies/podsecurity-seccomp.rego
+package kubernetes.admission
+
+# Deny pods without seccompProfile RuntimeDefault
+deny[msg] {
+  input.request.kind.kind == "Pod"
+  not input.request.object.spec.securityContext.seccompProfile
+  msg = "Pod must set a seccompProfile (RuntimeDefault)"
+}
+
+deny[msg] {
+  input.request.kind.kind == "Pod"
+  container := input.request.object.spec.containers[_]
+  not container.securityContext
+  msg = "Containers must have securityContext"
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/network/toolrunner-strict-networkpolicy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: tool-runner-strict-egress
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: tool-runner
+  policyTypes:
+    - Egress
+  egress:
+    # allow to S3 endpoints (CIDR example) - replace with provider ranges or VPC endpoints
+    - to:
+        - ipBlock:
+            cidr: 10.0.0.0/8
+      ports:
+        - protocol: TCP
+          port: 443
+    # allow to Milvus internal service
+    - to:
+        - podSelector:
+            matchLabels:
+              app: milvus
+      ports:
+        - protocol: TCP
+          port: 19530
+
+---
+# Disallow hostNetwork and hostPath mounts via Gatekeeper / PodSecurity (see constraints)
+
*** End Patch
*** Begin Patch
*** Add File:security/iam/assume_role_ephemeral_sample.py
+#!/usr/bin/env python3
+"""
+Tool-runner ephemeral credential sample:
+ - Demonstrates assume-role-with-web-identity or STS assume-role to mint short-lived creds
+ - Runner uses temporary creds to access S3 for upload only
+"""
+import os, boto3
+from botocore.exceptions import ClientError
+
+ROLE_ARN = os.environ.get("TOOL_RUNNER_ROLE_ARN", "arn:aws:iam::REPLACE_ACCOUNT:role/aegis-tool-ephemeral")
+SESSION_NAME = "tool-runner-session"
+
+def assume_role():
+    sts = boto3.client("sts")
+    resp = sts.assume_role(RoleArn=ROLE_ARN, RoleSessionName=SESSION_NAME, DurationSeconds=900)
+    creds = resp["Credentials"]
+    return creds
+
+def use_temporary_creds(creds):
+    s3 = boto3.client("s3", aws_access_key_id=creds["AccessKeyId"],
+                      aws_secret_access_key=creds["SecretAccessKey"],
+                      aws_session_token=creds["SessionToken"])
+    # Example: put small result artifact
+    s3.put_object(Bucket=os.environ.get("EVIDENCE_BUCKET"), Key="agents/tmp/test.txt", Body=b"ok")
+
+if __name__ == "__main__":
+    c = assume_role()
+    use_temporary_creds(c)
+
*** End Patch
*** Begin Patch
*** Add File:security/hardening/toolrunner_confinement_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Confinement test for tool-runner: attempts outbound connection to public host and writing to hostPath.
+# Expected behavior: network egress blocked to external addresses; hostPath write denied.
+#
+TOOL_RUNNER_POD=${1:-tool-runner-test-pod}
+NAMESPACE=${2:-aegis}
+
+cat > pod.yaml <<EOF
+apiVersion: v1
+kind: Pod
+metadata:
+  name: $TOOL_RUNNER_POD
+  namespace: $NAMESPACE
+  labels:
+    app: tool-runner
+spec:
+  containers:
+  - name: test
+    image: curlimages/curl:8.2.1
+    command: ["/bin/sh","-c","sleep 3600"]
+    securityContext:
+      runAsNonRoot: true
+      allowPrivilegeEscalation: false
+EOF
+
+kubectl apply -f pod.yaml
+sleep 2
+POD=$(kubectl -n $NAMESPACE get pod -l app=tool-runner -o jsonpath='{.items[0].metadata.name}')
+echo "Running egress test from pod $POD"
+kubectl -n $NAMESPACE exec $POD -- curl -sS https://example.com -m 5 || echo "Egress blocked or timed out (expected)"
+
+echo "Attempting hostPath write (should fail if no hostPath allowed)"
+kubectl -n $NAMESPACE exec $POD -- sh -c "echo test > /hostpath_test || echo 'hostPath write blocked (expected)'"
+
+kubectl -n $NAMESPACE delete pod $POD || true
+rm pod.yaml
+
*** End Patch
*** Begin Patch
*** Add File:safety/nsfw/pii_service_stub/Dockerfile
+FROM python:3.11-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN pip install -r requirements.txt
+COPY nsfw_pii_service.py /app/nsfw_pii_service.py
+CMD ["python","/app/nsfw_pii_service.py"]
+
*** End Patch
*** Begin Patch
*** Add File:safety/nsfw/pii_service_stub/requirements.txt
+flask
+pillow
+opencv-python
+numpy
+
*** End Patch
*** Begin Patch
*** Add File:safety/nsfw/pii_service_stub/nsfw_pii_service.py
+from flask import Flask, request, jsonify
+from safety.image_privacy import nsfw_check_placeholder, detect_faces_and_redact
+import base64, tempfile
+
+app = Flask("nsfw-pii-service")
+
+@app.post("/analyze_image")
+def analyze_image():
+    j = request.get_json()
+    img_b64 = j.get("image_base64")
+    if not img_b64:
+        return jsonify({"error":"no image provided"}), 400
+    img_bytes = base64.b64decode(img_b64)
+    tmp = tempfile.mktemp(suffix=".jpg")
+    with open(tmp,"wb") as f:
+        f.write(img_bytes)
+    nsfw = nsfw_check_placeholder(tmp)
+    redacted, faces = detect_faces_and_redact(tmp)
+    return jsonify({"nsfw": nsfw, "faces_redacted": faces})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8082)
+
*** End Patch
*** Begin Patch
*** Add File:ops/scale_tests/dw_idempotency_test.py
+#!/usr/bin/env python3
+"""
+Simple DW idempotency test: writes a batch of records, replays the same batch multiple times and ensures deduplication/upsert invariants.
+Adapt to your DW (Redshift/Postgres/etc.) by changing the insert/upsert logic.
+"""
+import os, hashlib, time
+import psycopg2
+
+DB_CONN = os.environ.get("DW_CONN", "dbname=dw user=dwuser host=dw-host")
+
+def make_rows(n):
+    rows = []
+    for i in range(n):
+        data = f"record-{i}"
+        pk = hashlib.sha256(data.encode()).hexdigest()[:16]
+        rows.append((pk,data))
+    return rows
+
+def upsert_rows(rows):
+    conn = psycopg2.connect(DB_CONN)
+    cur = conn.cursor()
+    for pk, data in rows:
+        cur.execute("INSERT INTO idempotent_test (id, data) VALUES (%s,%s) ON CONFLICT (id) DO UPDATE SET data = EXCLUDED.data", (pk,data))
+    conn.commit()
+    cur.close()
+    conn.close()
+
+if __name__ == "__main__":
+    rows = make_rows(1000)
+    for i in range(3):
+        t0 = time.time()
+        upsert_rows(rows)
+        print("Iteration", i, "elapsed", time.time()-t0)
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/slo_rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-slo-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: slos
+      rules:
+        - alert: RAGHighErrorRate
+          expr: increase(aegis_rag_requests_total{status="error"}[5m]) > 10
+          for: 10m
+          labels:
+            severity: critical
+          annotations:
+            summary: "High RAG error rate"
+        - alert: AgentTokenSpendHigh
+          expr: increase(aegis_llm_tokens_total[1h]) > 1000000
+          for: 30m
+          labels:
+            severity: warning
+          annotations:
+            summary: "High token spend in last hour"
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/alertmanager/alertmanager-prod.yaml
+global:
+  resolve_timeout: 5m
+route:
+  receiver: 'pagerduty'
+  group_wait: 20s
+  group_interval: 5m
+  repeat_interval: 2h
+receivers:
+  - name: 'slack'
+    slack_configs:
+      - api_url: 'https://hooks.slack.com/services/REPLACE/SLACK/WEBHOOK'
+        channel: '#aegis-alerts'
+  - name: 'pagerduty'
+    pagerduty_configs:
+      - service_key: 'REPLACE_PAGERDUTY_SERVICE_KEY'
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana/production_agent_dashboard.json
+{
+  "dashboard": {
+    "title": "Aegis Agent Operations (prod)",
+    "panels": [
+      {"title":"Agent Runs per minute","type":"graph","targets":[{"expr":"rate(aegis_agent_runs_total[1m])"}]},
+      {"title":"Agent Token Spend (1h)","type":"graph","targets":[{"expr":"increase(aegis_llm_tokens_total[1h])"}]},
+      {"title":"RAG p95 Latency","type":"graph","targets":[{"expr":"histogram_quantile(0.95, sum(rate(aegis_rag_latency_seconds_bucket[5m])) by (le))"}]}
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:compliance/telemetry/collector_readme.md
+Telemetry collection & calibration
+
+1) Deploy telemetry collector CronJob (compliance/telemetry collector already provided) to gather:
+   - Per-model/pipeline token and GPU usage
+   - RAG latency percentiles
+   - Milvus index stats: collection sizes & p95 query latency
+
+2) Run calibration runner (compliance/calibration CronJob) nightly to compute median_relative_error metrics and write reports to s3://<EVIDENCE_BUCKET>/calibration/
+
+3) Keep telemetry for 14 days as acceptance window.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/audit/audit_package_runbook.md
+# Audit Package Runbook (operator)
+
+Purpose: collect all evidence for a representative model/agent run and produce a signed audit package.
+
+Steps:
+1) Identify run id and relevant S3 prefixes (agents/<run_id>/, llm/checkpoints/<model>/<run_id>/, mlflow/runs/<run_id>/, milvus/<collection>/snapshot_<ts>.json).
+2) Run collector:
+   python3 compliance/audit/audit_package_collector.py <run_id>
+3) Verify tar uploaded to s3://<EVIDENCE_BUCKET>/audit-packages/
+4) Verify signature: cosign verify --key awskms://<COSIGN_KMS_ARN> s3://<EVIDENCE_BUCKET>/audit-packages/<tar>
+5) Optional: post to Rekor via the audit script (best-effort).
+
+Attach the package and manifest to the auditor ticket.
+
*** End Patch
*** End Patch
