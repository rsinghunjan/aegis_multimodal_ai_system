*** Begin Patch
*** Add File: provider/legal/enforce_baa_precheck.py
+#!/usr/bin/env python3
+"""
+CI precheck to enforce that a BAA exists for a provider before permitting
+promotions of regulated=true artifacts.
+
+Exit codes:
+ 0 -> OK (BAA present or regulated=false)
+ 2 -> BAA missing (failure)
+"""
+import os
+import sys
+from provider.legal.check_baa import check_baa_for_provider
+
+def main():
+    provider = os.environ.get("PROVIDER_NAME", sys.argv[1] if len(sys.argv)>1 else "")
+    regulated = os.environ.get("REGULATED", "false").lower() in ("1","true","yes")
+    if not provider:
+        print("PROVIDER_NAME required", file=sys.stderr); return 2
+    if not regulated:
+        print(f"Not regulated for provider={provider} — OK")
+        return 0
+    ok = check_baa_for_provider(provider)
+    if ok:
+        print(f"BAA present for provider={provider} — OK")
+        return 0
+    print(f"ERROR: BAA missing for provider={provider}. Promotion blocked.", file=sys.stderr)
+    return 2
+
+if __name__=="__main__":
+    sys.exit(main())
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/baa_dashboard.py
+#!/usr/bin/env python3
+"""
+Simple dashboard generator listing providers without signed BAAs and
+optionally emailing or notifying Slack (stub).
+"""
+import csv, json, os
+from pathlib import Path
+
+CSV = Path("provider/legal/baas_tracker.csv")
+
+def list_missing():
+    missing=[]
+    if not CSV.exists():
+        print("BAA tracker not found:", CSV); return missing
+    with CSV.open() as fh:
+        for r in csv.DictReader(fh):
+            if r.get("baa_signed","").lower() != "true":
+                missing.append(r)
+    return missing
+
+def main():
+    miss = list_missing()
+    print(json.dumps({"missing_count":len(miss), "missing": miss}, indent=2))
+    # TODO: integrate with Slack/email
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/auto_init_helper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Helper to automate CloudHSM provisioning & validation steps where possible.
+# This script prints the commands to run and will optionally execute automated
+# AWS CLI steps when run with --execute (manual operator interaction still required).
+#
+EXEC=false
+if [ "${1:-}" = "--execute" ]; then EXEC=true; fi
+
+AWS_REGION="${AWS_REGION:-us-west-2}"
+CLUSTER_ID="${CLUSTER_ID:-<fill-cluster-id-if-known>}"
+
+echo "CloudHSM auto-init helper (region=${AWS_REGION})"
+
+echo
+echo "1) Create cluster via Terraform (recommended):"
+echo "   cd infra/hsm && terraform init && terraform apply -var 'aws_region=${AWS_REGION}' -var 'vpc_id=<VPC>' -var 'subnet_ids=[\"subnet-1\",\"subnet-2\"]'"
+
+if $EXEC; then
+  (cd infra/hsm && terraform init && terraform apply -auto-approve -var "aws_region=${AWS_REGION}" ) || true
+fi
+
+echo
+echo "2) After creation, claim & initialize HSMs in AWS Console; record cluster id."
+echo "   Example list: aws cloudhsmv2 describe-clusters --region ${AWS_REGION} --output json"
+if $EXEC && [ "${CLUSTER_ID}" != "<fill-cluster-id-if-known>" ]; then
+  aws cloudhsmv2 describe-clusters --cluster-id "${CLUSTER_ID}" --region "${AWS_REGION}" || true
+fi
+
+echo
+echo "3) On admin host: install client & configure. Example (Debian):"
+cat <<'CMD'
+sudo apt-get update
+sudo apt-get install -y cloudhsm-client libcloudhsm pkcs11-tool
+sudo /opt/cloudhsm/bin/configure -a <hsm-ip> -c <cluster-id>
+sudo /opt/cloudhsm/bin/cloudhsm_client_util status
+CMD
+
+echo
+echo "4) Create CU and HSM key (manual/interactive). Example:"
+cat <<'CMD'
+sudo /opt/cloudhsm/bin/key_mgmt_util
+# inside: createUser CUadmin <password>
+pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so --keypairgen --key-type rsa:2048 --label cosign-key --id 01
+pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so -O
+CMD
+
+echo
+echo "5) Validate cosign PKCS11 signing on admin host:"
+cat <<'CMD'
+echo '{"test":"aegis"}' > /tmp/test_manifest.json
+export PKCS11_MODULE=/usr/lib/libcloudhsm_pkcs11.so
+cosign sign --key "pkcs11:token=cosign-key?module-path=${PKCS11_MODULE}" /tmp/test_manifest.json
+cosign verify /tmp/test_manifest.json
+rekor-cli upload --artifact /tmp/test_manifest.json
+CMD
+
+echo
+echo "6) Run rotation drill (KMS example automated; HSM example manual):"
+cat <<'CMD'
+# KMS rotation (example):
+NEW_KEY_ID=$(aws kms create-key --description "cosign rotation drill" --query KeyMetadata.KeyId --output text --region ${AWS_REGION})
+aws kms create-alias --alias-name alias/aegis-cosign --target-key-id $NEW_KEY_ID --region ${AWS_REGION}
+# update CI secret manual step (gh secret set ...)
+CMD
+
+echo
+echo "7) Audit CloudTrail for Sign events:"
+echo "   aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign --region ${AWS_REGION} --max-results 50"
+
+echo
+echo "Output: after running, run hsm/cloudtrail/generate_audit_report.py with COSIGN_KEY_ID set to produce audit json"
+
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/cloudhsm_admin_playbook_more.yml
+- name: CloudHSM admin extra tasks (scaffold for key import / validation)
+  hosts: cloudhsm_admin
+  become: true
+  tasks:
+    - name: Ensure pkcs11-tool present
+      package:
+        name: pkcs11-tool
+        state: present
+      ignore_errors: yes
+
+    - name: Generate HSM key placeholder note
+      debug:
+        msg: |
+          This playbook cannot create keys without HSM interactive steps.
+          Use the provided helper: hsm/pkcs11/import_key.sh on admin host to guide key import.
+
+    - name: Copy cosign validation helper
+      copy:
+        src: hsm/pkcs11/cosign_pkcs11_validation.sh
+        dest: /usr/local/bin/cosign_pkcs11_validation.sh
+        mode: '0755'
+
+    - name: Run pkcs11-tool list (non-fatal)
+      command: pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so -O
+      register: pklist
+      ignore_errors: yes
+
+    - name: Show pkcs11-tool output
+      debug:
+        var: pklist.stdout_lines
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/jetson_pilot_full.yml
+- name: Jetson pilot full onboarding and validation (3-10 nodes)
+  hosts: runners
+  become: true
+  serial: 5
+  vars:
+    runner_user: aegis-runner
+    repo_url: "https://github.com/owner/repo"
+  tasks:
+    - name: Ensure base packages and runner user
+      apt:
+        update_cache: yes
+        name:
+          - python3-pip
+          - docker.io
+        state: present
+
+    - name: Ensure runner user exists
+      user:
+        name: "{{ runner_user }}"
+        create_home: yes
+
+    - name: Deploy and configure runner, node_exporter and mdm (reuses earlier roles)
+      include_tasks: register_and_validate.yml
+
+    - name: Wait for node_exporter metric to appear (prometheus scrape)
+      pause:
+        minutes: 1
+
+    - name: Run end-to-end smoke job to register and run validation
+      delegate_to: localhost
+      shell: |
+        echo "Triggering a hardware-validation job via CI or controller API (operator step)"
+      changed_when: false
+
+    - name: Mark node done (debug)
+      debug:
+        msg: "Node {{ inventory_hostname }} onboarded and validated (check CI & Prometheus)"
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/adapter_throttle.py
+#!/usr/bin/env python3
+"""
+Generate per-provider token-bucket throttle configs and write to a k8s ConfigMap
+that adapters will consume. This script can be run after measuring provider rates.
+"""
+import json, subprocess, sys
+RECOMMEND_FILE="/tmp/adapter_throttle_recommendations.json"
+
+def make_recommendations():
+    # Example recommendations, in real use replace with measured rates
+    recs = {
+        "braket": {"rate_per_sec": 2.0, "burst": 10, "retries": 6, "backoff_s": 2.0},
+        "ibm": {"rate_per_sec": 1.0, "burst": 5, "retries": 8, "backoff_s": 2.5}
+    }
+    open(RECOMMEND_FILE,"w").write(json.dumps(recs,indent=2))
+    return recs
+
+def apply_to_configmap(recs):
+    cm={
+        "apiVersion":"v1","kind":"ConfigMap","metadata":{"name":"aegis-adapter-throttle","namespace":"aegis"},
+        "data":{"throttle_config": json.dumps(recs)}
+    }
+    p = subprocess.run(["kubectl","apply","-f","-"], input=json.dumps(cm).encode())
+    print("kubectl apply returned", p.returncode)
+
+if __name__=="__main__":
+    recs = make_recommendations()
+    apply_to_configmap(recs)
+    print("Wrote recommendations to", RECOMMEND_FILE)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/tuning/auto_apply_adapter_recs.py
+#!/usr/bin/env python3
+"""
+Consume adapter recommendations and apply them into adapter config (k8s ConfigMap).
+This automates the apply step after measure_and_tune produces /tmp/adapter_recommendations.json.
+"""
+import os, json, subprocess, sys
+REC_FILE="/tmp/adapter_recommendations.json"
+
+if not os.path.exists(REC_FILE):
+    print("No recommendations file:", REC_FILE); sys.exit(2)
+
+recs = json.load(open(REC_FILE))
+cm={
+    "apiVersion":"v1","kind":"ConfigMap","metadata":{"name":"aegis-adapter-config","namespace":"aegis"},
+    "data":{"config": json.dumps(recs)}
+}
+subprocess.run(["kubectl","apply","-f","-"], input=json.dumps(cm).encode(), check=False)
+print("Applied adapter recommendations ConfigMap")
+
*** End Patch
*** Begin Patch
*** Add File: mitigation/auto_apply_mitigation.py
+#!/usr/bin/env python3
+"""
+Apply mitigation suggestions produced by mitigation/run_mitigation_batch.py
+to the adapters/configstore as per-device policy toggles (e.g., apply_readout_mitigation).
+"""
+import json, os, subprocess
+SUG_FILE="/tmp/mitigation_suggestions.json"
+
+def load_suggestions():
+    if not os.path.exists(SUG_FILE):
+        print("No suggestions found:", SUG_FILE); return []
+    return json.load(open(SUG_FILE))
+
+def build_map(sugg):
+    out={}
+    for s in sugg:
+        dev=s.get("device")
+        out[dev]=s.get("suggestion",{})
+    return out
+
+def apply_map(dev_map):
+    cm={
+        "apiVersion":"v1","kind":"ConfigMap","metadata":{"name":"aegis-mitigation-config","namespace":"aegis"},
+        "data":{"mitigation": json.dumps(dev_map)}
+    }
+    subprocess.run(["kubectl","apply","-f","-"], input=json.dumps(cm).encode(), check=False)
+    print("Applied mitigation ConfigMap")
+
+if __name__=="__main__":
+    sug = load_suggestions()
+    mapping = build_map(sug)
+    apply_map(mapping)
+    print("Mitigation map applied for", len(mapping), "devices")
+
*** End Patch
*** Begin Patch
*** Add File: billing/scheduler_tag_enforcer.py
+#!/usr/bin/env python3
+"""
+Middleware hook to ensure provider job creation always includes required tags
+('aegis:tenant' and 'aegis:job_id'). Integrate this into scheduler code-path
+that calls provider SDKs when creating resources.
+"""
+import os, uuid
+
+def ensure_tags(job_payload, tenant=None, job_id=None):
+    tenant = tenant or job_payload.get("owner") or os.environ.get("DEFAULT_TENANT","unknown")
+    job_id = job_id or str(uuid.uuid4())
+    tags = job_payload.get("tags", {})
+    tags["aegis:tenant"] = tenant
+    tags["aegis:job_id"] = job_id
+    job_payload["tags"] = tags
+    return job_payload
+
+# Example usage:
+if __name__=="__main__":
+    jp={"owner":"tenant-abc"}
+    print(ensure_tags(jp))
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile_alerts.py
+#!/usr/bin/env python3
+"""
+Wrapper around reconcile_automated.py to run daily and send rich alerts on exceptions.
+This script runs reconciliation and posts a short summary to SNS/Slack (stub).
+"""
+import subprocess, json, os
+from datetime import date, timedelta
+SNS_TOPIC=os.environ.get("BILLING_SNS_TOPIC","")
+
+def run_reconcile():
+    subprocess.run(["python","billing/reconcile_automated.py"], check=False)
+    if os.path.exists("/tmp/reconcile_report.json"):
+        r=json.load(open("/tmp/reconcile_report.json"))
+        exceptions=[e for e in r if not e.get("matched")]
+        summary={"total":len(r),"exceptions":len(exceptions)}
+        print("Reconcile summary:", summary)
+        if exceptions and SNS_TOPIC:
+            import boto3
+            sns=boto3.client("sns")
+            sns.publish(TopicArn=SNS_TOPIC, Subject="Billing reconcile exceptions", Message=json.dumps(exceptions))
+
+if __name__=="__main__":
+    run_reconcile()
+
*** End Patch
*** Begin Patch
*** Add File: sre/locust_scale_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Launch large Locust worker fleet as k8s Jobs and collect a simple Prometheus snapshot.
+N="${1:-300}"
+MASTER_SVC="${2:-locust-master.aegis.svc.cluster.local}"
+echo "Launching ${N} locust workers as k8s jobs"
+for i in $(seq 1 $N); do
+  kubectl create job locust-worker-$i --image=locustio/locust -- /bin/sh -c "locust -f /mnt/locust/locustfile.py --worker --master-host ${MASTER_SVC}" || true
+done
+echo "Sleeping 10s to allow scheduling"
+sleep 10
+echo "Collect simple metric snapshot from Prometheus (requires port-forwardable prometheus svc)"
+kubectl port-forward svc/prometheus-operated -n monitoring 9090:9090 >/dev/null 2>&1 &
+PF=$!
+sleep 2
+curl -s "http://127.0.0.1:9090/api/v1/query?query=sum(rate(http_requests_total[5m]))" -o /tmp/locust_prom_snapshot.json || true
+kill $PF || true
+echo "Saved snapshot /tmp/locust_prom_snapshot.json"
+
*** End Patch
*** Begin Patch
*** Add File: db/partitioning/auto_partition_creator.py
+#!/usr/bin/env python3
+"""
+Create today's partition for device_telemetry and prune old partitions
+This is intended to be run daily as a cron or GitHub Actions job.
+"""
+import os, sys, psycopg2
+from datetime import date, timedelta
+
+DB_URL=os.environ.get("DATABASE_URL","postgresql://aegis:aegis@postgres:5432/aegis")
+conn = psycopg2.connect(DB_URL)
+
+def create_partition(conn, p_date):
+    cur = conn.cursor()
+    pname = f"device_telemetry_{p_date.strftime('%Y%m%d')}"
+    create_sql = f"CREATE TABLE IF NOT EXISTS {pname} PARTITION OF device_telemetry FOR VALUES FROM ('{p_date}') TO ('{p_date + timedelta(days=1)}');"
+    cur.execute(create_sql)
+    conn.commit()
+    cur.close()
+
+def prune_old(conn, keep_days=90):
+    # Optional: drop partitions older than keep_days
+    cutoff = date.today() - timedelta(days=keep_days)
+    # implementation depends on existing naming; operator review recommended
+
+if __name__=="__main__":
+    today = date.today()
+    create_partition(conn, today)
+    print("Created partition for", today)
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/tabletop_cloudhsm.md
+# CloudHSM Tabletop Exercise — Runbook
+
+Objective:
+- Validate operational readiness for CloudHSM init, key import, cosign PKCS11 signing, rotation drill and audit procedures.
+
+Participants:
+- CloudOps, Security, SRE, Legal, Incident Manager.
+
+Agenda (approx 90 minutes)
+1. Intro & goals (5m)
+2. Pre-check: confirm cluster exists & admin host prepared (5m)
+3. Step through operator_checklist.sh (20m) — CloudOps runs through each step
+4. Simulate a failed key import & incident response (20m)
+5. Perform rotation drill simulation (20m)
+6. Audit verification & evidence capture (15m)
+
+Artifacts to produce:
+- CloudTrail audit JSON, cosign verification logs, Rekor entries, compliance bucket link to evidence tarball.
+
+Success criteria:
+- Teams can perform key import/validation and rotation drill within SLA and capture audit evidence to compliance bucket.
+
*** End Patch
*** End Patch
