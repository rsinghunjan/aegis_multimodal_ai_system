*** Begin Patch
*** Add File: .github/workflows/triage_on_readiness_failure.yml
+name: Triage Readiness Failure
+
+on:
+  workflow_run:
+    workflows: ["Fully AI Readiness Gate (Auto-evaluate)"]
+    types:
+      - completed
+
+permissions:
+  contents: read
+  issues: write
+
+jobs:
+  triage:
+    if: ${{ github.event.workflow_run.conclusion != 'success' }}
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests || true
+
+      - name: Run readiness artifact analyzer
+        id: analyze
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          GITHUB_REPOSITORY: ${{ github.repository }}
+          WORKFLOW_RUN_ID: ${{ github.event.workflow_run.id }}
+        run: |
+          python3 scripts/analyze_readiness_artifacts.py --download > /tmp/readiness_analyze.txt || true
+          echo "ANALYSIS_OUTPUT<<EOF" >> $GITHUB_OUTPUT
+          cat /tmp/readiness_analyze.txt >> $GITHUB_OUTPUT
+          echo "EOF" >> $GITHUB_OUTPUT
+
+      - name: Create triage issue
+        if: always()
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          GITHUB_REPOSITORY: ${{ github.repository }}
+        run: |
+          BODY="$(cat /tmp/readiness_analyze.txt || echo 'No analyzer output')"
+          TITLE="Readiness Gate Failed - Run ${{ github.event.workflow_run.id }} - Triage"
+          # Use gh if available, otherwise use API
+          if command -v gh >/dev/null 2>&1; then
+            echo "$BODY" | gh issue create --title "$TITLE" --body-file - --label "readiness" --label "triage"
+          else
+            PAYLOAD=$(jq -n --arg t "$TITLE" --arg b "$BODY" '{title:$t, body:$b, labels:["readiness","triage"]}')
+            curl -s -X POST -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/${GITHUB_REPOSITORY}/issues -d "$PAYLOAD"
+          fi
+
*** End Patch
*** Begin Patch
*** Add File: scripts/create_prefilled_signoff_from_latest_artifacts.py
+#!/usr/bin/env python3
+"""
+create_prefilled_signoff_from_latest_artifacts.py
+
+Find the latest successful "Fully AI Readiness Gate (Auto-evaluate)" workflow run,
+download KPI artifact (if present), and create a pre-filled signoff issue that
+references the artifacts and includes a short summary.
+
+Requires:
+ - GITHUB_TOKEN env var
+ - GITHUB_REPOSITORY env var (owner/repo)
+
+Usage:
+  GITHUB_TOKEN=... GITHUB_REPOSITORY=owner/repo python3 scripts/create_prefilled_signoff_from_latest_artifacts.py
+"""
+import os
+import sys
+import requests
+import tempfile
+import json
+
+GITHUB_API = "https://api.github.com"
+
+def gh_headers():
+    token = os.environ.get("GITHUB_TOKEN")
+    if not token:
+        print("GITHUB_TOKEN required", file=sys.stderr)
+        sys.exit(2)
+    return {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3+json"}
+
+def find_workflow_id(repo, workflow_name):
+    url = f"{GITHUB_API}/repos/{repo}/actions/workflows"
+    r = requests.get(url, headers=gh_headers(), timeout=30)
+    r.raise_for_status()
+    wf_list = r.json().get("workflows", [])
+    for wf in wf_list:
+        if wf.get("name") == workflow_name:
+            return wf.get("id")
+    return None
+
+def latest_success_run(repo, workflow_id):
+    url = f"{GITHUB_API}/repos/{repo}/actions/workflows/{workflow_id}/runs?status=completed&per_page=10"
+    r = requests.get(url, headers=gh_headers(), timeout=30)
+    r.raise_for_status()
+    runs = r.json().get("workflow_runs", [])
+    for run in runs:
+        if run.get("conclusion") == "success":
+            return run
+    return None
+
+def list_artifacts(repo, run_id):
+    url = f"{GITHUB_API}/repos/{repo}/actions/runs/{run_id}/artifacts"
+    r = requests.get(url, headers=gh_headers(), timeout=30)
+    r.raise_for_status()
+    return r.json().get("artifacts", [])
+
+def download_artifact(artifact):
+    url = artifact["archive_download_url"]
+    r = requests.get(url, headers=gh_headers(), stream=True, timeout=60)
+    r.raise_for_status()
+    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
+    with open(tmp.name, "wb") as fh:
+        for chunk in r.iter_content(chunk_size=8192):
+            fh.write(chunk)
+    return tmp.name
+
+def create_issue(repo, title, body):
+    url = f"{GITHUB_API}/repos/{repo}/issues"
+    resp = requests.post(url, headers=gh_headers(), json={"title": title, "body": body, "labels": ["readiness","signoff"]}, timeout=30)
+    resp.raise_for_status()
+    return resp.json()
+
+def main():
+    repo = os.environ.get("GITHUB_REPOSITORY")
+    if not repo:
+        print("GITHUB_REPOSITORY required", file=sys.stderr)
+        sys.exit(2)
+
+    workflow_name = "Fully AI Readiness Gate (Auto-evaluate)"
+    wf_id = find_workflow_id(repo, workflow_name)
+    if not wf_id:
+        print("Workflow not found:", workflow_name, file=sys.stderr)
+        sys.exit(2)
+
+    run = latest_success_run(repo, wf_id)
+    if not run:
+        print("No successful runs found for workflow:", workflow_name, file=sys.stderr)
+        sys.exit(2)
+
+    run_id = run["id"]
+    artifacts = list_artifacts(repo, run_id)
+    kpi_art = None
+    for a in artifacts:
+        if "kpi" in a["name"].lower() or "kpi_snapshot" in a["name"].lower():
+            kpi_art = a
+            break
+
+    body_lines = []
+    body_lines.append(f"Automated signoff draft for readiness run #{run_id}")
+    body_lines.append("")
+    body_lines.append(f"Workflow run: {run.get('html_url')}")
+    if kpi_art:
+        kpi_dl = download_artifact(kpi_art)
+        body_lines.append(f"KPI artifact downloaded to artifacts attached to this issue by reviewer: {kpi_dl}")
+    else:
+        body_lines.append("No KPI artifact found in the run artifacts. Please inspect readiness artifacts manually.")
+
+    body_lines.append("")
+    body_lines.append("## Readiness Checklist (pre-filled)")
+    body_lines.extend([
+        "- [ ] Milvus HA & TLS validated",
+        "- [ ] Backups verified and restorable",
+        "- [ ] LLM gateway p95 & error rate within SLO",
+        "- [ ] Gatekeeper policy enforcement validated",
+        "- [ ] Operator rollback >= 99% and FP rate acceptable",
+        "- [ ] Audit artifacts uploaded & retention verified",
+        "- [ ] Identity migration complete (no long-lived keys)",
+    ])
+
+    body = "\n".join(body_lines)
+    issue = create_issue(repo, f"Aegis Readiness Signoff Draft - Run {run_id}", body)
+    print("Created signoff issue:", issue.get("html_url"))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: docs/PRIORITIZED_ISSUES.yaml
+---
+- title: "Milvus HA: Deploy and validate in staging"
+  body: |
+    Tasks:
+    - Deploy Milvus in HA mode via Helm in staging
+    - Enable TLS and validate certificates
+    - Run backup CronJob and verify backup object creation
+    Acceptance:
+    - Pods Ready, TLS certs present, backup object exists and can be downloaded
+  labels:
+    - infra
+    - priority:high
+  assignees:
+    - rsinghunjan
+
+- title: "LLM Upstream: Deploy representative model and tune SLOs"
+  body: |
+    Tasks:
+    - Deploy a representative upstream model behind the gateway
+    - Run llm_load_test to measure p95 and error rate
+    - Tune batching, replica counts
+    Acceptance:
+    - p95 <= target and error rate < 1% under representative load
+  labels:
+    - ml
+    - priority:high
+  assignees:
+    - ml-team
+
+- title: "Operator E2E: Run large-scale runs and validate rollback / FP"
+  body: |
+    Tasks:
+    - Run operator_e2e_repeat for N >= 500 runs in staging
+    - Export per-run metrics to Pushgateway
+    - Compute rollback success and FP rate
+    Acceptance:
+    - Rollback success >= 99%, FP rate <= 1%
+  labels:
+    - platform
+    - priority:high
+  assignees:
+    - platform-team
+
+- title: "Identity Migration: Migrate KSAs to Workload Identity/IRSA"
+  body: |
+    Tasks:
+    - Migrate operator, gateway, backup uploader KSAs
+    - Remove long-lived keys from CI and repos
+    - Run secret-scan to verify no long-lived keys remain
+    Acceptance:
+    - All critical KSAs annotated & tested; secret-scan clean
+  labels:
+    - security
+    - priority:high
+  assignees:
+    - security-team
+
+- title: "Audit Immutability: Enable AUTO_UPLOAD_AUDIT and verify retention"
+  body: |
+    Tasks:
+    - Enable AUTO_UPLOAD_AUDIT in staging
+    - Configure object-lock / retention on audit bucket
+    - Run verify_audit_retention.py to confirm immutability
+    Acceptance:
+    - Audit artifacts exist and retention/object-lock verified programmatically
+  labels:
+    - security
+    - priority:medium
+  assignees:
+    - security-team
+
+- title: "Gatekeeper: Validate rego rules against real workloads"
+  body: |
+    Tasks:
+    - Apply Gatekeeper constraints in staging
+    - Run policy CI and real traffic tests
+    - Tune rego constraints based on false positives
+    Acceptance:
+    - No critical false positives; unsafe SuggestedActions are denied
+  labels:
+    - security
+    - priority:medium
+  assignees:
+    - platform-team
+
*** End Patch
*** Begin Patch
*** Add File: scripts/create_prioritized_issues.py
+#!/usr/bin/env python3
+"""
+create_prioritized_issues.py
+
+Reads docs/PRIORITIZED_ISSUES.yaml and creates GitHub issues for each entry.
+Requires GITHUB_TOKEN and GITHUB_REPOSITORY environment variables.
+
+Usage:
+  GITHUB_TOKEN=... GITHUB_REPOSITORY=owner/repo python3 scripts/create_prioritized_issues.py
+"""
+import os
+import sys
+import yaml
+import requests
+
+GITHUB_API = "https://api.github.com"
+
+def gh_headers():
+    token = os.environ.get("GITHUB_TOKEN")
+    if not token:
+        print("GITHUB_TOKEN required", file=sys.stderr)
+        sys.exit(2)
+    return {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3+json"}
+
+def create_issue(repo, title, body, labels=None, assignees=None):
+    url = f"{GITHUB_API}/repos/{repo}/issues"
+    payload = {"title": title, "body": body}
+    if labels:
+        payload["labels"] = labels
+    if assignees:
+        payload["assignees"] = assignees
+    resp = requests.post(url, headers=gh_headers(), json=payload, timeout=30)
+    resp.raise_for_status()
+    return resp.json()
+
+def main():
+    repo = os.environ.get("GITHUB_REPOSITORY")
+    if not repo:
+        print("GITHUB_REPOSITORY required", file=sys.stderr)
+        sys.exit(2)
+    path = "docs/PRIORITIZED_ISSUES.yaml"
+    if not os.path.exists(path):
+        print("Prioritized issues file not found:", path, file=sys.stderr)
+        sys.exit(2)
+    with open(path) as f:
+        data = yaml.safe_load(f)
+    created = []
+    for item in data:
+        title = item.get("title")
+        body = item.get("body", "")
+        labels = item.get("labels", [])
+        assignees = item.get("assignees", [])
+        print("Creating issue:", title)
+        issue = create_issue(repo, title, body, labels, assignees)
+        print(" ->", issue.get("html_url"))
+        created.append(issue.get("html_url"))
+    print("Created", len(created), "issues.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
