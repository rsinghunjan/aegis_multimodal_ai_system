*** Begin Patch
*** Add File: terraform/vault/README_admin.md
+Vault AppRole & Provider Onboarding (Administrator instructions)
+===============================================================
+
+This document describes how a Vault administrator can run the Terraform snippets
+in terraform/vault/ to create a least-privilege policy and an AppRole for the
+quantum controller, and how to securely deliver the AppRole secret to the
+operator that bootstraps the controller token rotator.
+
+Prerequisites
+-------------
+- Terraform v1.3+ installed.
+- Vault CLI installed (optional, for verification).
+- Vault server reachable and you have a bootstrap admin token (do not store it in CI).
+
+Steps
+-----
+1. Copy repo to your operator workstation:
+   git clone <repo> && cd <repo>/terraform/vault
+
+2. Initialize Terraform:
+   terraform init
+
+3. Create a terraform.tfvars file containing:
+   vault_addr = "https://vault.example.com"
+   approle_name = "quantum-controller"
+   provider_name = "ibm"
+
+4. Export VAULT_TOKEN in your shell (admin token):
+   export VAULT_TOKEN="s.xxxxx_admin_token"
+
+5. Apply Terraform plan (review output before applying):
+   terraform plan -out=tfplan
+   terraform apply tfplan
+
+6. After apply, Terraform will output:
+   - approle_role_id
+   - approle_secret_id (sensitive)
+
+   Copy these values securely and hand them to the operator (out-of-band) to bootstrap
+   the vault rotator or controller. Do NOT commit secrets to git or CI.
+
+7. Optionally verify with Vault CLI:
+   vault read auth/approle/role/quantum-controller/role-id
+   vault write -f auth/approle/role/quantum-controller/secret-id
+
+Delivering credentials securely
+------------------------------
+The operator should create a Kubernetes Secret named `vault-credentials` in namespace `aegis`
+containing:
+  - addr: https://vault.example.com
+  - role_id: <approle_role_id>
+  - secret_id: <approle_secret_id>
+
+For example (operator-run, not Terraform):
+  kubectl create secret generic vault-credentials -n aegis \
+    --from-literal=addr=https://vault.example.com \
+    --from-literal=role_id='<ROLE_ID>' \
+    --from-literal=secret_id='<SECRET_ID>'
+
+Important security notes
+------------------------
+- The approle secret_id is one-time or limited-use depending on configuration. Treat it
+  as highly sensitive.
+- Prefer using the vault_token_rotator (utils/vault_token_rotator.py) run by an operator
+  with admin privileges to create short-lived client tokens and write them into a
+  Kubernetes secret `vault-credentials` for the controller to consume. The controller
+  should NOT have the admin token.
+- Audit logs in Vault should be enabled and monitored for secret reads and token creation.
+
+If you want I can generate a small wrapper script that the operator can run to create
+the Kubernetes secret from Terraform outputs; tell me and I'll add it.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/cronjobs/vault-token-rotator-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: vault-token-rotator
+  namespace: aegis
+spec:
+  schedule: "*/30 * * * *" # every 30 minutes (tune as desired)
+  concurrencyPolicy: Forbid
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: quantum-controller
+          containers:
+            - name: rotator
+              image: ghcr.io/yourorg/aegis-tools:latest
+              imagePullPolicy: IfNotPresent
+              command: ["python", "utils/vault_token_rotator.py", "--policies", "quantum-provider-read", "--ttl", "1h", "--namespace", "aegis", "--secret-name", "vault-credentials"]
+              env:
+                - name: VAULT_ADDR
+                  valueFrom:
+                    secretKeyRef:
+                      name: vault-admin-credentials
+                      key: addr
+                - name: VAULT_TOKEN
+                  valueFrom:
+                    secretKeyRef:
+                      name: vault-admin-credentials
+                      key: token
+              resources:
+                requests:
+                  cpu: "50m"
+                  memory: "64Mi"
+                limits:
+                  cpu: "200m"
+                  memory: "256Mi"
+          restartPolicy: OnFailure
+
+Notes:
+- This CronJob runs the rotator with the admin token stored in `vault-admin-credentials` (namespace aegis).
+- The admin token is required to create scoped tokens; protect this secret tightly and restrict access via RBAC.
+- Alternatively, run the rotator as a one-off operator job executed by a human operator rather than a CronJob if you prefer.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/argo_sync_and_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Apply ArgoCD Application manifests (argo/apps/*), wait for sync, then run Redis failover & provider stress tests.
+#
+RESULTS_DIR="${1:-provider_staging_results}"
+mkdir -p "${RESULTS_DIR}"
+
+echo "Applying ArgoCD Application manifests to register apps with ArgoCD..."
+kubectl apply -f argo/apps/ || true
+
+echo "Waiting for ArgoCD Application CRs to be created..."
+sleep 5
+
+# If argocd CLI is available and configured, we can trigger sync. Otherwise assume ArgoCD auto-sync will handle it.
+if command -v argocd >/dev/null 2>&1; then
+  for app in $(ls argo/apps | sed -e 's/.yaml$//g'); do
+    echo "Attempting to sync Argo app: ${app}"
+    argocd app sync "${app}" || true
+  done
+fi
+
+echo "Waiting for core pods to become Ready (redis, prometheus, grafana, oauth2-proxy, fluentd)..."
+kubectl wait --for=condition=available --timeout=300s deployment/quantum-controller -n aegis || true
+kubectl wait --for=condition=ready --timeout=300s statefulset/redis -n aegis || true || echo "Redis readiness wait timed out"
+
+echo "Running Redis failover test..."
+python3 scripts/redis_failover_test.py || echo "Redis failover test failed" > "${RESULTS_DIR}/redis_failover_fail.txt"
+
+echo "Running provider stress test (PoC)..."
+python3 scripts/provider_stress_test.py --provider ibm --count 20 --concurrency 4 || echo "Provider stress test errors" > "${RESULTS_DIR}/provider_stress_fail.txt"
+
+echo "Collecting logs and results..."
+kubectl get pods -n aegis -l app=redis -o wide > "${RESULTS_DIR}/redis_pods.txt" || true
+kubectl get pods -n aegis -l app=quantum-controller -o wide > "${RESULTS_DIR}/controller_pods.txt" || true
+kubectl logs -n aegis -l app=quantum-controller --tail=200 > "${RESULTS_DIR}/controller_logs.txt" || true
+
+echo "Done. Results are in ${RESULTS_DIR}"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/staging-deploy-and-test.yml
+name: Staging Deploy & Run Tests (self-hosted)
+on:
+  workflow_dispatch:
+
+jobs:
+  deploy-and-test:
+    runs-on: [self-hosted, qpu-adjacent]
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set KUBECONFIG (optional)
+        # If using a KUBECONFIG content secret, write it to $HOME/.kube/config
+        if: ${{ secrets.KUBECONFIG_CONTENT != '' }}
+        run: |
+          mkdir -p $HOME/.kube
+          echo "${{ secrets.KUBECONFIG_CONTENT }}" > $HOME/.kube/config
+
+      - name: Ensure kubectl
+        run: |
+          if ! command -v kubectl >/dev/null 2>&1; then
+            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
+            chmod +x kubectl
+            sudo mv kubectl /usr/local/bin/
+          fi
+
+      - name: Apply Argo apps & run tests
+        env:
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+          BRK_S3_BUCKET: ${{ secrets.BRK_S3_BUCKET }}
+        run: |
+          chmod +x scripts/argo_sync_and_test.sh
+          ./scripts/argo_sync_and_test.sh provider_staging_results
+
+      - name: Upload results
+        uses: actions/upload-artifact@v4
+        with:
+          name: staging-test-results
+          path: provider_staging_results/**
+
*** End Patch
*** Begin Patch
*** Add File: docs/provider_credentials.md
+Provider Credentials & Self-hosted Runner Guide
+=============================================
+
+This document explains how to provide sandbox credentials (IBM / Braket) securely
+and how to prepare a secure self-hosted runner to execute provider E2E verification.
+
+Secrets required (examples)
+---------------------------
+- For IBM Qiskit Runtime:
+  - IBM_TOKEN: runtime API token (or store in Vault at secret/data/quantum/providers/ibm)
+
+- For AWS Braket:
+  - AWS_ACCESS_KEY_ID
+  - AWS_SECRET_ACCESS_KEY
+  - AWS_REGION (e.g., us-west-1)
+  - BRK_S3_BUCKET (S3 bucket for program payloads)
+
+- Rekor (optional):
+  - REKOR_URL
+
+- Signing (if using local key):
+  - LOCAL_SIGNING_KEY_PATH (path on self-hosted runner where private key is available; do NOT store private key in GitHub secrets)
+
+Recommended secret distribution approaches
+----------------------------------------
+1) Vault (recommended)
+   - Store provider credentials under secret/data/quantum/providers/<provider>.
+   - Provide the controller with a scoped token (AppRole secret_id & role_id) or let the rotator populate vault-credentials secret.
+
+2) Self-hosted runner: local secrets
+   - Place sensitive private keys (signing key) and kubeconfig on the runner VM's filesystem with strict permissions (600).
+   - Configure GitHub Actions runner with labels [self-hosted, qpu-adjacent] and ensure it is network-isolated and access-controlled.
+   - Do NOT put provider private keys in GitHub secrets. Use GitHub secrets only for ephemeral tokens if unavoidable.
+
+Preparing a secure self-hosted runner
+------------------------------------
+1) Provision a VM in a secure network (no public access except as required).
+2) Install GitHub Actions Runner and register with your repository / organization.
+3) Install required tools: Python 3.11, kubectl, docker (if running containerized tests), argocd CLI (optional).
+4) Place signing private key at: /opt/aegis/secrets/signing_key.pem (chmod 600).
+5) When running workflows, set LOCAL_SIGNING_KEY_PATH to that path via repository or organization secret (only used in self-hosted runner).
+
+Example environment variables to set on the runner (do not export to untrusted shells):
+  export LOCAL_SIGNING_KEY_PATH=/opt/aegis/secrets/signing_key.pem
+  export BRK_S3_BUCKET=aegis-test-bucket
+  export VAULT_ADDR=https://vault.example.com
+  export VAULT_TOKEN=<scoped-token>   # Prefer to use rotator to populate k8s secret instead
+
+Notes
+-----
+- Always use provider sandboxes (free tiers) for initial testing to avoid billing.
+- Rotate any tokens or keys after tests. Use vault_token_rotator to manage tokens automatically.
+
*** End Patch
*** End Patch
