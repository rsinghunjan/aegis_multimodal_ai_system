*** Begin Patch
*** Add File: terraform/aws/complete_infra.tf
+// Terraform: Complete AWS infra for Aegis production readiness
+// Provision KMS for cosign, GitHub OIDC role, S3 compliance bucket (encrypted),
+// RDS Postgres Multi-AZ, ElastiCache Redis replication, IAM minimal policies.
+// Usage:
+//  - populate terraform.tfvars and run `terraform init && terraform apply -auto-approve`
+
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws", version = ">= 4.0" }
+    random = { source = "hashicorp/random" }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+variable "aws_region" { type = string }
+variable "github_org" { type = string }
+variable "github_repo" { type = string }
+variable "oidc_provider_arn" { type = string }
+variable "oidc_provider_url" { type = string }
+variable "db_username" { type = string, default = "aegis" }
+variable "db_password" { type = string }
+
+data "aws_caller_identity" "current" {}
+
+data "aws_availability_zones" "available" {}
+
+resource "random_id" "bucket_id" {
+  byte_length = 4
+}
+
+resource "aws_kms_key" "cosign_key" {
+  description             = "KMS key for cosign signing by GitHub Actions"
+  deletion_window_in_days = 30
+  enable_key_rotations    = true
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign"
+  target_key_id = aws_kms_key.cosign_key.key_id
+}
+
+data "aws_iam_policy_document" "github_oidc_assume" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(var.oidc_provider_url, "https://", "")}:sub"
+      values   = ["repo:${var.github_org}/${var.github_repo}:ref:refs/heads/main"]
+    }
+  }
+}
+
+resource "aws_iam_role" "github_oidc_role" {
+  name               = "aegis-github-oidc-role"
+  assume_role_policy = data.aws_iam_policy_document.github_oidc_assume.json
+}
+
+resource "aws_iam_role_policy" "allow_kms_sign" {
+  name = "allow-kms-sign"
+  role = aws_iam_role.github_oidc_role.id
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Sid = "AllowCosignSign"
+        Effect = "Allow"
+        Action = [
+          "kms:Sign",
+          "kms:GetPublicKey",
+          "kms:DescribeKey"
+        ]
+        Resource = aws_kms_key.cosign_key.arn
+      }
+    ]
+  })
+}
+
+resource "aws_s3_bucket" "compliance" {
+  bucket = "aegis-compliance-${random_id.bucket_id.hex}"
+  acl    = "private"
+  versioning { enabled = true }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm     = "aws:kms"
+        kms_master_key_id = aws_kms_key.cosign_key.arn
+      }
+    }
+  }
+  lifecycle_rule {
+    enabled = true
+    expiration {
+      days = 3650
+    }
+  }
+}
+
+resource "aws_db_subnet_group" "rds_subnet_group" {
+  name       = "aegis-rds-subnet"
+  subnet_ids = slice(data.aws_subnet_ids.private.ids, 0, 2)
+  description = "Subnet group for Aegis RDS"
+}
+
+data "aws_vpc" "default" {
+  default = true
+}
+
+data "aws_subnet_ids" "private" {
+  vpc_id = data.aws_vpc.default.id
+}
+
+resource "aws_db_instance" "postgres" {
+  identifier              = "aegis-postgres"
+  engine                  = "postgres"
+  engine_version          = "15.3"
+  instance_class          = "db.m6g.large"
+  allocated_storage       = 100
+  username                = var.db_username
+  password                = var.db_password
+  multi_az                = true
+  skip_final_snapshot     = true
+  db_subnet_group_name    = aws_db_subnet_group.rds_subnet_group.name
+  publicly_accessible     = false
+}
+
+resource "aws_elasticache_subnet_group" "redis_subnet" {
+  name       = "aegis-redis-subnet"
+  subnet_ids = data.aws_subnet_ids.private.ids
+  description = "Subnet group for Aegis Redis"
+}
+
+resource "aws_elasticache_replication_group" "redis" {
+  replication_group_id          = "aegis-redis"
+  replication_group_description = "Aegis Redis cluster"
+  node_type                     = "cache.m6g.large"
+  number_cache_clusters         = 3
+  automatic_failover_enabled    = true
+  subnet_group_name             = aws_elasticache_subnet_group.redis_subnet.name
+}
+
+output "kms_key_arn" {
+  value = aws_kms_key.cosign_key.arn
+}
+output "github_oidc_role_arn" {
+  value = aws_iam_role.github_oidc_role.arn
+}
+output "s3_compliance_bucket" {
+  value = aws_s3_bucket.compliance.bucket
+}
+output "rds_endpoint" {
+  value = aws_db_instance.postgres.address
+}
+output "redis_endpoint" {
+  value = aws_elasticache_replication_group.redis.primary_endpoint_address
+}
+
*** End Patch
*** Begin Patch
*** Add File: terraform/gcp/complete_infra.tf
+// Terraform: Complete GCP infra for Aegis production readiness
+// Provision KMS key, GitHub service account helper, GCS bucket, Cloud SQL (regional), Memorystore Redis.
+
+provider "google" {
+  project = var.gcp_project
+  region  = var.gcp_region
+}
+
+variable "gcp_project" { type = string }
+variable "gcp_region" { type = string }
+variable "db_user" { type = string, default = "aegis" }
+variable "db_password" { type = string }
+
+resource "random_id" "bucket_id" {
+  byte_length = 4
+}
+
+resource "google_storage_bucket" "compliance" {
+  name     = "aegis-compliance-${random_id.bucket_id.hex}"
+  location = var.gcp_region
+  versioning { enabled = true }
+  uniform_bucket_level_access = true
+}
+
+resource "google_kms_key_ring" "cosign_ring" {
+  name     = "aegis-cosign-ring"
+  location = var.gcp_region
+}
+
+resource "google_kms_crypto_key" "cosign_key" {
+  name     = "aegis-cosign-key"
+  key_ring = google_kms_key_ring.cosign_ring.id
+}
+
+resource "google_service_account" "github_actions" {
+  account_id   = "aegis-github-actions"
+  display_name = "Aegis GitHub Actions"
+}
+
+resource "google_kms_crypto_key_iam_member" "binding" {
+  crypto_key_id = google_kms_crypto_key.cosign_key.id
+  role          = "roles/cloudkms.signerVerifier"
+  member        = "serviceAccount:${google_service_account.github_actions.email}"
+}
+
+resource "google_sql_database_instance" "postgres" {
+  name             = "aegis-postgres"
+  database_version = "POSTGRES_15"
+  region           = var.gcp_region
+  settings {
+    tier = "db-custom-2-8192"
+    availability_type = "REGIONAL"
+    backup_configuration { enabled = true }
+  }
+}
+
+resource "google_sql_user" "postgres_user" {
+  name     = var.db_user
+  instance = google_sql_database_instance.postgres.name
+  password = var.db_password
+}
+
+resource "google_redis_instance" "redis" {
+  name           = "aegis-redis"
+  tier           = "STANDARD_HA"
+  memory_size_gb = 4
+  region         = var.gcp_region
+  redis_version  = "6.x"
+}
+
+output "gcp_kms_key_ref" {
+  value = "gcpkms://projects/${var.gcp_project}/locations/${var.gcp_region}/keyRings/${google_kms_key_ring.cosign_ring.name}/cryptoKeys/${google_kms_crypto_key.cosign_key.name}"
+}
+output "github_actions_sa_email" {
+  value = google_service_account.github_actions.email
+}
+output "gcs_compliance_bucket" {
+  value = google_storage_bucket.compliance.url
+}
+output "cloudsql_instance_name" {
+  value = google_sql_database_instance.postgres.name
+}
+output "memorystore_addr" {
+  value = google_redis_instance.redis.host
+}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci_sign_kms_rekor_production.yml
+name: CI: Build, SBOM, SCA, Sign with Cloud KMS, Rekor verify and publish artifacts
+on:
+  push:
+    branches: [ main ]
+
+permissions:
+  contents: read
+  id-token: write
+  packages: write
+
+jobs:
+  build-scan-sign:
+    runs-on: ubuntu-latest
+    env:
+      IMAGE: ${{ secrets.REGISTRY_HOST }}/aegis:${{ github.sha }}
+      REKOR_URL: ${{ secrets.REKOR_URL }}
+      COSIGN_EXPERIMENTAL: "1"
+    steps:
+    - uses: actions/checkout@v4
+    - name: Set up Docker Buildx
+      uses: docker/setup-buildx-action@v2
+    - name: Build image
+      run: docker build -t "$IMAGE" -f services/serving/Dockerfile .
+    - name: Generate SBOM (syft)
+      uses: anchore/syft-action@v1
+      with:
+        image: ${{ env.IMAGE }}
+        output: sbom.json
+    - name: Run Trivy scan
+      uses: aquasecurity/trivy-action@v1
+      with:
+        image-ref: ${{ env.IMAGE }}
+        format: json
+        output: trivy.json
+    - name: Fail on critical CVEs
+      run: |
+        if jq -e '.[] | .Vulnerabilities?[]? | select(.Severity=="CRITICAL")' trivy.json >/dev/null 2>&1; then
+          echo "CRITICAL vulnerabilities found"; jq . trivy.json; exit 1
+        fi
+    - name: Login to registry
+      uses: docker/login-action@v2
+      with:
+        registry: ${{ secrets.REGISTRY_HOST }}
+        username: ${{ secrets.REGISTRY_USER }}
+        password: ${{ secrets.REGISTRY_TOKEN }}
+    - name: Push image
+      run: docker push "$IMAGE"
+    - name: Get image digest
+      id: digest
+      run: |
+        DIG=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE" | awk -F@ '{print $2}')
+        echo "digest=$DIG" >> $GITHUB_OUTPUT
+    - name: Sign image with Cloud KMS
+      env:
+        IMAGE: ${{ env.IMAGE }}
+      run: |
+        if [ "${{ secrets.CLOUD_PROVIDER }}" = "aws" ]; then
+          cosign sign --key "awskms://${{ secrets.AWS_KMS_KEY_ARN }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+        else
+          cosign sign --key "gcpkms://${{ secrets.GCP_KMS_KEY_REF }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+        fi
+    - name: Verify with Rekor
+      run: cosign verify --rekor-url "${{ secrets.REKOR_URL }}" "${{ env.IMAGE }}@${{ steps.digest.outputs.digest }}"
+    - name: Upload artifacts to compliance store
+      if: ${{ secrets.COMPLIANCE_BUCKET != '' }}
+      run: |
+        if [ "${{ secrets.CLOUD_PROVIDER }}" = "aws" ]; then
+          aws s3 cp sbom.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/sbom.json"
+          aws s3 cp trivy.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/trivy.json"
+        else
+          gsutil cp sbom.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/sbom.json" || true
+          gsutil cp trivy.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/trivy.json" || true
+        fi
+
*** End Patch
*** Begin Patch
*** Add File: helm/vault/values-autounseal.yaml
+# Vault Helm values (production) with auto-unseal via AWS KMS (example)
+server:
+  ha:
+    enabled: true
+  dataStorage:
+    enabled: true
+  extraEnvironmentVars:
+    VAULT_LOCAL_CONFIG: |
+      ui = true
+      listener "tcp" {
+        address = "0.0.0.0:8200"
+        tls_disable = 1
+      }
+      storage "raft" {
+        path = "/vault/data"
+      }
+      seal "awskms" {
+        region = "us-west-2"
+        kms_key_id = "arn:aws:kms:REPLACE"
+      }
+  service:
+    type: ClusterIP
+injector:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/bootstrap_vault_auto_unseal.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Bootstraps Vault (post-helm install) and configures Kubernetes auth.
+# Requires: vault CLI, kubectl, VAULT_ADDR and initial root token (or auto-unseal configured).
+#
+VAULT_ADDR=${VAULT_ADDR:-http://vault.ops.svc.cluster.local:8200}
+VAULT_TOKEN=${1:-}
+if [ -z "$VAULT_TOKEN" ]; then
+  echo "Usage: $0 <VAULT_ROOT_TOKEN>"
+  exit 2
+fi
+export VAULT_ADDR
+vault login "$VAULT_TOKEN"
+vault secrets enable -path=secret kv-v2 || true
+vault policy write agent-read - <<EOF
+path "secret/data/agent/*" {
+  capabilities = ["read","list"]
+}
+EOF
+echo "Vault bootstrap complete. Create roles with scripts/vault_k8s_auth_setup.sh"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/migrate_secrets_to_vault.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Migrate secrets (JSON map) into Vault KV v2
+# Format: {"secret/path":{"key":"val", ...}, ...}
+FILE=${1:-secrets-to-migrate.json}
+if [ ! -f "$FILE" ]; then
+  echo "Please provide secrets JSON file: $FILE"
+  exit 2
+fi
+for path in $(jq -r 'keys[]' "$FILE"); do
+  data=$(jq -r ".\"$path\"" "$FILE")
+  echo "Writing to secret/data/$path"
+  python3 - <<PY
+import json, subprocess
+data=json.loads('''$data''')
+cmd=['vault','kv','put','secret/data/${path}']
+for k,v in data.items():
+    cmd.append(f"{k}={v}")
+subprocess.check_call(cmd)
+PY
+done
+echo "Secrets migrated"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/attestation/attestation-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: attestation
+  namespace: ops
+  labels:
+    app: attestation
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: attestation
+  template:
+    metadata:
+      labels:
+        app: attestation
+    spec:
+      containers:
+      - name: attestation
+        image: ghcr.io/yourorg/attestation-service:latest
+        ports:
+        - containerPort: 8443
+        readinessProbe:
+          httpGet:
+            path: /health
+            port: 8443
+          initialDelaySeconds: 10
+          periodSeconds: 5
+        livenessProbe:
+          httpGet:
+            path: /health
+            port: 8443
+          initialDelaySeconds: 30
+          periodSeconds: 20
+        env:
+          - name: REKOR_URL
+            value: "https://rekor.sigstore.dev"
+          - name: TRIVY_BIN
+            value: "/usr/local/bin/trivy"
+        volumeMounts:
+          - name: tls
+            mountPath: /tls
+            readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: attestation-tls
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: attestation
+  namespace: ops
+spec:
+  ports:
+    - port: 8443
+      targetPort: 8443
+  selector:
+    app: attestation
+
*** End Patch
*** Begin Patch
*** Add File: k8s/webhook/validatingwebhook.yaml
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  name: aegis-attestation-webhook
+webhooks:
+  - name: attestation.aegis.local
+    sideEffects: None
+    admissionReviewVersions: ["v1"]
+    clientConfig:
+      service:
+        name: attestation
+        namespace: ops
+        path: /verify
+      caBundle: "" # inject base64 CA from attestation-tls
+    rules:
+      - operations: ["CREATE","UPDATE"]
+        apiGroups: ["apps",""]
+        apiVersions: ["v1"]
+        resources: ["deployments","pods"]
+    failurePolicy: Fail
+    timeoutSeconds: 10
+    matchPolicy: Equivalent
+
*** End Patch
*** Begin Patch
*** Add File: scripts/inject_ca_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+WEBHOOK_NAME=${1:-aegis-attestation-webhook}
+CA_B64=${2:-}
+if [ -z "$CA_B64" ]; then
+  echo "Usage: $0 <webhook-name> <ca-bundle-base64>"
+  exit 2
+fi
+kubectl get validatingwebhookconfiguration "$WEBHOOK_NAME" -o yaml > /tmp/webhook.yaml
+python3 - <<PY
+import yaml
+f="/tmp/webhook.yaml"
+data=yaml.safe_load(open(f))
+for w in data.get("webhooks",[]):
+  w["clientConfig"]["caBundle"] = "$CA_B64"
+open(f,"w").write(yaml.safe_dump(data))
+print("Patched /tmp/webhook.yaml")
+PY
+kubectl apply -f /tmp/webhook.yaml
+echo "Injected CA bundle into $WEBHOOK_NAME"
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constrainttemplate_attestation_strict.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequireattestationstrict
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequireAttestationStrict
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8srequireattestationstrict
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Deployment"
+          not valid_attestation(input.review.object)
+          msg := "Deployment does not have valid attestation (cosign + rekor) or aegis.attested=true annotation"
+        }
+
+        valid_attestation(obj) {
+          ann := obj.metadata.annotations
+          ann["aegis.attested"] == "true"
+        }
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_attestation_strict.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequireAttestationStrict
+metadata:
+  name: require-attestation-strict
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["staging","ops"]
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/postgres-statefulset.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: postgres
+  namespace: ops
+spec:
+  serviceName: postgres
+  replicas: 2
+  selector:
+    matchLabels:
+      app: postgres
+  template:
+    metadata:
+      labels:
+        app: postgres
+    spec:
+      affinity:
+        podAntiAffinity:
+          requiredDuringSchedulingIgnoredDuringExecution:
+            - labelSelector:
+                matchLabels:
+                  app: postgres
+              topologyKey: "kubernetes.io/hostname"
+      containers:
+      - name: postgres
+        image: postgres:15
+        env:
+        - name: POSTGRES_USER
+          valueFrom:
+            secretKeyRef:
+              name: aegis-db
+              key: user
+        - name: POSTGRES_PASSWORD
+          valueFrom:
+            secretKeyRef:
+              name: aegis-db
+              key: password
+        ports:
+        - containerPort: 5432
+        readinessProbe:
+          exec:
+            command: ["sh","-c","pg_isready -U $POSTGRES_USER"]
+          initialDelaySeconds: 10
+          periodSeconds: 10
+        volumeMounts:
+        - name: data
+          mountPath: /var/lib/postgresql/data
+  volumeClaimTemplates:
+  - metadata:
+      name: data
+    spec:
+      accessModes: ["ReadWriteOnce"]
+      resources:
+        requests:
+          storage: 100Gi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/pg_backup_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: postgres-backup
+  namespace: ops
+spec:
+  schedule: "0 3 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: backup-sa
+          containers:
+          - name: pgdump
+            image: bitnami/postgresql:15
+            env:
+              - name: PGPASSWORD
+                valueFrom:
+                  secretKeyRef:
+                    name: aegis-db
+                    key: password
+            command: ["/bin/sh","-c"]
+            args:
+              - |
+                pg_dump -h postgres -U $POSTGRES_USER -d aegis > /tmp/backup.sql
+                if [ -n "${COMPLIANCE_BUCKET:-}" ]; then
+                  aws s3 cp /tmp/backup.sql ${COMPLIANCE_BUCKET}/backups/backup-$(date +%s).sql
+                fi
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: k8s/hardening/networkpolicy-agent-exec.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: agent-exec-egress-allowlist
+  namespace: agent-exec
+spec:
+  podSelector: {}
+  policyTypes:
+  - Egress
+  egress:
+    - to:
+      - namespaceSelector:
+          matchLabels:
+            name: ops
+      ports:
+      - protocol: TCP
+        port: 443
+    - to:
+      - ipBlock:
+          cidr: 10.0.0.0/8
+    - to:
+      - ipBlock:
+          cidr: 0.0.0.0/0
+      ports:
+      - protocol: TCP
+        port: 443
+
*** End Patch
*** Begin Patch
*** Add File: k8s/hardening/podsecurity_restrictive.yaml
+apiVersion: policy/v1beta1
+kind: PodSecurityPolicy
+metadata:
+  name: aegis-restrictive
+spec:
+  privileged: false
+  allowPrivilegeEscalation: false
+  requiredDropCapabilities:
+    - ALL
+  volumes:
+    - 'configMap'
+    - 'emptyDir'
+    - 'persistentVolumeClaim'
+  hostNetwork: false
+  hostIPC: false
+  hostPID: false
+  readOnlyRootFilesystem: true
+  runAsUser:
+    rule: 'MustRunAsNonRoot'
+  seLinux:
+    rule: RunAsAny
+  fsGroup:
+    rule: RunAsAny
+
*** End Patch
*** Begin Patch
*** Add File: services/loadtest/locustfile.py
+from locust import HttpUser, task, between
+
+class InferenceUser(HttpUser):
+    wait_time = between(0.2, 0.8)
+
+    @task(8)
+    def predict(self):
+        payload = {"inputs": ["test"]*8}
+        self.client.post("/predict", json=payload, timeout=30)
+
+    @task(1)
+    def health(self):
+        self.client.get("/health")
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_full_validation.sh
+#!/usr/bin/env bash
+set -euo pipefail
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+mkdir -p artifacts
+
+echo "1) Run attestation smoke test"
+./scripts/attestation_smoke_test.sh || { echo "Attestation test failed"; exit 1; }
+
+echo "2) Run pen test harness"
+./scripts/pen_test_harness.sh || echo "Pen tests reported issues - inspect artifacts"
+
+echo "3) Run Milvus scale test"
+python3 scripts/milvus_scale_test.py || echo "Milvus scale test failed"
+
+echo "4) Run Locust short load"
+python3 -m pip install locust >/dev/null 2>&1 || true
+locust -f services/loadtest/locustfile.py --headless -u 200 -r 20 --run-time 2m --host ${INFERENCE_HOST:-http://model-serving.staging.svc.cluster.local} || true
+
+echo "5) Collect evidence"
+./scripts/collect_evidence.sh artifacts/evidence_$(date +%F).tgz
+
+popd >/dev/null
+echo "Full validation run completed. Inspect artifacts/"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/pen_test_harness.sh
+#!/usr/bin/env bash
+set -euo pipefail
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+mkdir -p artifacts/pen_tests
+
+echo "Attempt metadata server access from agent-exec job"
+kubectl run pen-metadata --rm -i --restart=Never --image=busybox -n agent-exec -- sh -c "wget -qO- http://169.254.169.254/latest/meta-data || echo no-metadata" > artifacts/pen_tests/metadata_attempt.txt || true
+
+echo "Test hostPath mount attempt"
+cat > /tmp/hostpath-pod.yaml <<EOF
+apiVersion: v1
+kind: Pod
+metadata:
+  name: hostpath-test
+  namespace: agent-exec
+spec:
+  containers:
+  - name: test
+    image: busybox
+    command: ["sh","-c","sleep 10"]
+    volumeMounts:
+    - name: host
+      mountPath: /host
+  volumes:
+  - name: host
+    hostPath:
+      path: /
+EOF
+if kubectl apply -f /tmp/hostpath-pod.yaml; then
+  echo "HostPath pod creation succeeded - potential policy gap" > artifacts/pen_tests/hostpath_result.txt
+  kubectl delete -f /tmp/hostpath-pod.yaml --ignore-not-found
+else
+  echo "HostPath creation denied (expected)" > artifacts/pen_tests/hostpath_result.txt
+fi
+
+echo "Attempt privileged container"
+kubectl run priv-test --image=busybox -n agent-exec --overrides='{"apiVersion":"v1","spec":{"containers":[{"name":"priv","image":"busybox","securityContext":{"privileged":true},"command":["sleep","10"]}]}}' --restart=Never || echo "Privileged pod denied (expected)"
+
+echo "Unsigned deployment attempt (expect rejection)"
+if kubectl apply -f tests/unsigned_deploy.yaml; then
+  echo "Unsigned deployment accepted -> FAIL" > artifacts/pen_tests/unsigned_result.txt
+  kubectl delete -f tests/unsigned_deploy.yaml --ignore-not-found
+else
+  echo "Unsigned deployment rejected -> OK" > artifacts/pen_tests/unsigned_result.txt
+fi
+
+echo "Collecting logs..."
+for ns in agent-exec ops staging; do
+  mkdir -p artifacts/pen_tests/logs/$ns
+  kubectl get pods -n $ns -o name | while read -r p; do
+    name=$(echo $p | sed 's#pod/##')
+    kubectl logs -n $ns "$name" --all-containers > artifacts/pen_tests/logs/$ns/$name.log || true
+  done
+done
+
+popd >/dev/null
+echo "Pen tests complete. Artifacts: artifacts/pen_tests"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/production_signoff_checklist.md
+# Production Signoff Checklist (Aegis)
+
+1) KMS & CI Signing
+ - KMS key provisioned and key ARN recorded
+ - GitHub OIDC role/service account created and assigned minimal KMS Sign permission
+ - CI signing success (cosign with KMS) and Rekor entry URL recorded
+
+2) Vault & Secrets
+ - Vault deployed HA with auto-unseal configured
+ - All critical secrets migrated to Vault
+ - Applications configured to use Vault Agent/Injector or K8s auth
+
+3) Attestation & Gatekeeper
+ - cert-manager configured and attestation certificate issued
+ - ValidatingWebhookConfiguration caBundle injected and verified
+ - Gatekeeper constraints applied and attestation smoke test passed
+
+4) HA infra & Backups
+ - Postgres StatefulSet or managed RDS Multi-AZ deployed and backups validated (restore tested)
+ - Redis cluster / managed Redis validated
+ - Milvus HA and index backup/restore validated
+
+5) Scale & SLOs
+ - Distributed training run completed; checkpoints uploaded to compliance bucket
+ - Milvus insert/search at target cardinality validated
+ - Inference load tests meet p95/p99 SLOs, or mitigation documented
+
+6) Security & Pen-tests
+ - Agent sandbox pen tests executed and remediated
+ - Formal pen test scheduled/completed
+ - Evidence packaged
+
+7) Runbooks & DR
+ - DR playbooks exercised; incident runbooks validated
+ - On-call rotation and escalation configured
+
+Signatures:
+- Platform lead:
+- Security lead:
+- SRE lead:
+
*** End Patch
*** End Patch
