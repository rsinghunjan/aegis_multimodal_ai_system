  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aegis-inference
  labels:
    app: aegis-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: aegis-inference
  template:
    metadata:
      labels:
        app: aegis-inference
    spec:
      containers:
        - name: inference
          image: <your-registry>/aegis-inference:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9000   # inference HTTP
            - containerPort: 8000   # prometheus metrics
          env:
            - name: INFERENCE_MODEL_NAME
              value: "your-model-name"
            - name: INFERENCE_MODEL_VERSION
              value: "v1"
            - name: BATCH_MAX_SIZE
              value: "8"
            - name: BATCH_MAX_LATENCY_MS
              value: "50"
            # mount any credentials/secret via Secret or projected volumes in your manifest
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "2000m"
              memory: "8Gi"
          readinessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 20
k8s/deployment.yaml
