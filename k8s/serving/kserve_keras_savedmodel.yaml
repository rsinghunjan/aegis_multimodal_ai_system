apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: aegis-keras-model
  namespace: aegis
  annotations:
    autoscaling.knative.dev/target: "100"
    serving.kserve.io/enable-batcher: "true"
    serving.kserve.io/request-batch-size: "8"
    serving.kserve.io/request-batch-max-delay: "50ms"
spec:
  predictor:
    tensorflow:
      storageUri: "s3://MODEL_BUCKET/model-archives/aegis-keras-model/saved_model/"
      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
          nvidia.com/gpu: "1"
      runtimeVersion: "2.11.0" # pick TF version matching your model
