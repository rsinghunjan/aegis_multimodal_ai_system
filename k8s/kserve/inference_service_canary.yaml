  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
apiVersion: serving.kubeflow.org/v1beta1
kind: InferenceService
metadata:
  name: cifar-canary
  namespace: aegis-canary
  labels:
    app: aegis-model-canary
spec:
  predictor:
    triton:
      runtimeVersion: "23.08"
      storage:
        uri: "s3://REPLACE_WITH_BUCKET/model-archives/demo/cifar-deepspeed-0.1/"
      resources:
        limits:
          cpu: "2000m"
          memory: "8Gi"
          nvidia.com/gpu: "1"
      # Pod-level overrides: serviceAccount, initContainers, volumes
      podSpec:
        serviceAccountName: model-fetcher-sa
        initContainers:
          - name: verify-and-fetch
            image: ghcr.io/rsinghunjan/aegis-tools:latest
            command: ["/bin/sh", "-c"]
            args:
              - |
                set -euo pipefail
                # Parameters: first arg is model S3 URI, second is local dest dir
                /opt/aegis/verify_and_fetch.sh "s3://REPLACE_WITH_BUCKET/model-archives/demo/cifar-deepspeed-0.1/artifact.tar.gz" /models
            volumeMounts:
              - name: model-volume
                mountPath: /models
              - name: cosign-pub
                mountPath: /etc/cosign
                readOnly: true
        containers:
          - name: tritonserver
            image: nvcr.io/nvidia/tritonserver:23.08-py3
            args: ["tritonserver", "--model-repository=/models"]
            volumeMounts:
              - name: model-volume
                mountPath: /models
        volumes:
          - name: model-volume
            emptyDir: {}
          - name: cosign-pub
            secret:
              secretName: cosign-public-key
k8s/kserve/inference_service_canary.yaml
