*** Begin Patch
*** Add File:scheduler/telemetry/power_telemetry_collector.py
+#!/usr/bin/env python3
+"""
+Power Telemetry Collector
+ - Collects power telemetry from multiple sources:
+   * Node exporter / Prometheus (if node exporter exposes power metrics)
+   * Cloud provider telemetry endpoints (placeholders)
+   * DCIM CSV upload (file)
+ - Writes normalized CSV to /data/telemetry/YYYYMMDD_power.csv for calibration pipeline.
+
+Install/run:
+ - Run as a DaemonSet or CronJob on cluster with Prometheus access and cloud credentials.
+"""
+import time
+import csv
+import os
+import requests
+from datetime import datetime
+from prometheus_api_client import PrometheusConnect
+
+OUT_DIR = os.environ.get("TELEM_OUT_DIR", "/data/telemetry")
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.observability.svc:9090")
+PROM_QUERY = os.environ.get("PROM_POWER_QUERY", 'node_power_watts')  # customize to your exporter
+POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))
+
+os.makedirs(OUT_DIR, exist_ok=True)
+
+pc = PrometheusConnect(url=PROM_URL, disable_ssl=True)
+
+def collect_prometheus():
+    # Query for nodes' power metric
+    now = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
+    try:
+        res = pc.get_current_metric_value(PROM_QUERY)
+    except Exception as e:
+        print("Prometheus query failed:", e)
+        return []
+    rows = []
+    for r in res:
+        # metric labels may include instance/node name
+        labels = r.get("metric", {})
+        val = float(r.get("value", [None, 0])[1])
+        node = labels.get("instance") or labels.get("node") or labels.get("hostname") or "unknown"
+        rows.append({"timestamp": now, "node_type": labels.get("node_type", "default"), "node": node, "measured_power_w": val})
+    return rows
+
+def write_csv(rows):
+    if not rows:
+        return
+    fname = os.path.join(OUT_DIR, f"{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}_power.csv")
+    with open(fname, "w", newline="") as f:
+        w = csv.DictWriter(f, fieldnames=["timestamp", "node_type", "node", "measured_power_w"])
+        w.writeheader()
+        for r in rows:
+            w.writerow(r)
+    print("Wrote telemetry CSV:", fname)
+
+def main_loop():
+    while True:
+        rows = collect_prometheus()
+        write_csv(rows)
+        time.sleep(POLL_INTERVAL)
+
+if __name__ == "__main__":
+    main_loop()
+
*** End Patch
*** Begin Patch
*** Add File:scheduler/telemetry/telemetry_daemon_deploy.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: power-telemetry-collector
+  namespace: observability
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: power-telemetry-collector
+  template:
+    metadata:
+      labels:
+        app: power-telemetry-collector
+    spec:
+      containers:
+        - name: collector
+          image: ghcr.io/yourorg/aegis-telemetry-collector:latest
+          env:
+            - name: PROM_URL
+              value: "http://prometheus.observability.svc:9090"
+            - name: TELEM_OUT_DIR
+              value: "/data/telemetry"
+          volumeMounts:
+            - name: telemetry
+              mountPath: /data/telemetry
+      volumes:
+        - name: telemetry
+          persistentVolumeClaim:
+            claimName: telemetry-pvc
+
*** End Patch
*** Begin Patch
*** Add File:calibration/calibration_pipeline.py
+#!/usr/bin/env python3
+"""
+Calibration pipeline:
+ - Ingest telemetry CSVs from telemetry collector directory
+ - Aggregate by node_type and produce averaged power_profiles.yaml
+ - Optionally push profiles back to /app/power_profiles.yaml or upload to S3 / config store
+"""
+import glob
+import csv
+import yaml
+import os
+from collections import defaultdict
+
+TELEM_DIR = os.environ.get("TELEM_DIR", "/data/telemetry")
+OUT_PATH = os.environ.get("POWER_PROFILES_OUT", "/app/power_profiles.yaml")
+
+def aggregate():
+    sums = defaultdict(float)
+    counts = defaultdict(int)
+    files = glob.glob(os.path.join(TELEM_DIR, "*.csv"))
+    for f in files:
+        with open(f) as fh:
+            reader = csv.DictReader(fh)
+            for r in reader:
+                nt = r.get("node_type") or "default"
+                try:
+                    w = float(r.get("measured_power_w") or 0)
+                except Exception:
+                    w = 0
+                sums[nt] += w
+                counts[nt] += 1
+    profiles = {}
+    for k in sums:
+        avg = sums[k] / counts[k]
+        # assume gpu present if avg > threshold (heuristic) - operators should refine
+        gpu_guess = 1 if avg > 400 else 0
+        profiles[k] = {"base_power_w": max(50, round(avg * 0.7)), "gpu_power_w_per_card": 300 if gpu_guess else 0}
+    with open(OUT_PATH, "w") as f:
+        yaml.safe_dump(profiles, f)
+    print("Wrote calibrated power profiles to", OUT_PATH)
+
+if __name__ == "__main__":
+    aggregate()
+
*** End Patch
*** Begin Patch
*** Add File:ops/webhook/cert_manager.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple CA + certificate generator for Kubernetes MutatingWebhookConfiguration.
+# Generates a self-signed CA and signs a server cert for the webhook service.
+# Writes Kubernetes secret 'aegis-carbon-webhook-tls' in namespace 'aegis'. Operator must secure CA root separately.
+
+NS=${1:-aegis}
+SERVICE_NAME=${2:-aegis-carbon-webhook}
+
+echo "Generating CA..."
+openssl genrsa -out ca.key 2048
+openssl req -x509 -new -nodes -key ca.key -subj "/CN=aegis-carbon-ca" -days 3650 -out ca.crt
+
+echo "Generating server key & CSR..."
+openssl genrsa -out server.key 2048
+openssl req -new -key server.key -subj "/CN=${SERVICE_NAME}.${NS}.svc" -out server.csr -config <(cat /etc/ssl/openssl.cnf 2>/dev/null || true)
+
+echo "Signing server cert..."
+openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365
+
+echo "Creating kubernetes secret..."
+kubectl -n ${NS} create secret tls ${SERVICE_NAME}-tls --cert=server.crt --key=server.key --dry-run=client -o yaml | kubectl apply -f -
+
+echo "CA certificate (base64) for MutatingWebhookConfiguration:"
+cat ca.crt | base64 | tr -d '\n'
+echo
+echo "Save this base64 value into mutating webhook's caBundle field (mutating_webhook_k8s.yaml)."
+
*** End Patch
*** Begin Patch
*** Add File:ops/webhook/rotate_webhook_cert_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: webhook-cert-rotate
+  namespace: aegis
+spec:
+  schedule: "0 0 1 * *"   # monthly rotation
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: rotate
+              image: bitnami/kubectl:latest
+              command: ["/bin/sh","-c"]
+              args:
+                - /opt/scripts/cert_manager.sh aegis aegis-carbon-webhook
+              volumeMounts:
+                - name: scripts
+                  mountPath: /opt/scripts
+          restartPolicy: OnFailure
+          volumes:
+            - name: scripts
+              configMap:
+                name: aegis-cert-scripts
+
*** End Patch
*** Begin Patch
*** Add File:scheduler/carbon/mutable_policy_service.py
+#!/usr/bin/env python3
+"""
+Policy service for carbon enforcement.
+ - Stores per-team policies (low-carbon-only / best-effort) in memory or persistent store.
+ - Provides API for webhook/controller to check policy and reason about enforcement.
+"""
+from flask import Flask, request, jsonify
+import yaml, os
+
+POLICY_FILE = os.environ.get("POLICY_FILE", "/app/policy_store.yaml")
+
+app = Flask("aegis-policy")
+
+def load_policies():
+    try:
+        with open(POLICY_FILE) as f:
+            return yaml.safe_load(f)
+    except Exception:
+        return {"policies": {}}
+
+@app.route("/policy/<team>", methods=["GET"])
+def get_policy(team):
+    pol = load_policies().get("policies", {}).get(team, {"mode":"best-effort","carbon_threshold":400})
+    return jsonify(pol)
+
+@app.route("/policy/<team>", methods=["POST"])
+def set_policy(team):
+    p = request.json or {}
+    allp = load_policies()
+    allp.setdefault("policies", {})[team] = p
+    with open(POLICY_FILE,"w") as f:
+        yaml.safe_dump(allp, f)
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT",8088)))
+
*** End Patch
*** Begin Patch
*** Add File:scheduler/controller/carbon_scheduler_controller.py
+#!/usr/bin/env python3
+"""
+Kubernetes controller to enforce carbon scheduling hints:
+ - Watches Argo Workflows (or Jobs/Pods) and applies actions:
+   * If annotation aegis.carbon.hint => {"action":"defer","defer_until":...} then suspend or reschedule.
+   * If hint suggests prefer_spot, inject nodeSelector for spot pools or add label for autoscaler to act.
+ - Uses kubernetes python client. Run as Deployment with RBAC.
+"""
+import os
+import time
+import json
+from kubernetes import client, config, watch
+from datetime import datetime, timezone
+
+NAMESPACE = os.environ.get("WATCH_NAMESPACE", "")
+SLEEP = int(os.environ.get("CONTROLLER_SLEEP", "5"))
+
+def now_iso():
+    return datetime.now(timezone.utc)
+
+def suspend_workflow(api, name, ns):
+    # Argo Workflows use custom resource; we patch the Workflow to set spec.suspend = True or metadata.annotations
+    try:
+        crd_api = client.CustomObjectsApi()
+        body = {"spec": {"suspend": True}}
+        crd_api.patch_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=ns, plural="workflows", name=name, body=body)
+        print("Suspended workflow", name)
+    except Exception as e:
+        print("Failed to suspend workflow:", e)
+
+def add_node_selector_to_workflow(api, name, ns, selector):
+    try:
+        crd_api = client.CustomObjectsApi()
+        wf = crd_api.get_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=ns, plural="workflows", name=name)
+        # patch: add nodeSelector to metadata.annotations or templates (best-effort)
+        patch = {"metadata": {"annotations": {"aegis.nodeSelector": json.dumps(selector)}}}
+        crd_api.patch_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=ns, plural="workflows", name=name, body=patch)
+        print("Patched workflow nodeSelector annotation", name, selector)
+    except Exception as e:
+        print("Failed to patch workflow:", e)
+
+def run():
+    config.load_incluster_config()
+    v1 = client.CoreV1Api()
+    crd_api = client.CustomObjectsApi()
+    w = watch.Watch()
+    print("Starting carbon scheduler controller...")
+    for event in w.stream(crd_api.list_cluster_custom_object, group="argoproj.io", version="v1alpha1", plural="workflows", timeout_seconds=60):
+        try:
+            obj = event['object']
+            meta = obj.get("metadata", {})
+            ns = meta.get("namespace", "default")
+            name = meta.get("name")
+            ann = meta.get("annotations") or {}
+            hint_json = ann.get("aegis.carbon.hint")
+            if not hint_json:
+                continue
+            hint = json.loads(hint_json)
+            if hint.get("action") == "defer":
+                defer_until = hint.get("defer_until")
+                print("Deferring workflow", name, "until", defer_until)
+                suspend_workflow(crd_api, name, ns)
+                # controller could schedule a job to resume later (not implemented)
+            elif hint.get("action") == "run":
+                if hint.get("prefer_spot"):
+                    add_node_selector_to_workflow(crd_api, name, ns, {"lifecycle": "spot"})
+        except Exception as e:
+            print("Controller iteration error:", e)
+        time.sleep(SLEEP)
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:scheduler/spot/spot_provisioner.py
+#!/usr/bin/env python3
+"""
+Spot/provisioner placeholder:
+ - For cloud providers, this module would call provider APIs (AWS/GCP/Azure) or interact with Cluster API
+   or kube-cloud-controller-manager to create/scale spot node pools.
+ - Here we implement a safe placeholder that labels an existing nodepool (operator must integrate with infra).
+"""
+import os
+import requests
+
+def label_nodepool_for_spot(nodepool_label="lifecycle=spot"):
+    # Placeholder: operator should implement provider-specific provisioning or Cluster API call
+    print("Label nodepool for spot (placeholder):", nodepool_label)
+    return True
+
+def ensure_spot_pool_capacity(min_nodes=1):
+    # Call token-budget or autoscaler hooks (placeholder)
+    print("Ensuring spot pool capacity (placeholder)", min_nodes)
+    return True
+
+if __name__ == "__main__":
+    label_nodepool_for_spot()
+
*** End Patch
*** Begin Patch
*** Add File:reports/reconciliation/reconcile_job_postrun.py
+#!/usr/bin/env python3
+"""
+Post-run reconciliation:
+ - After a job/workflow completes, collect telemetry (if available) for nodes used
+ - Compare measured energy vs estimate from annotations
+ - Produce a signed reconciliation report and upload to evidence bucket
+ - Optionally call calibrate to adjust power profiles
+"""
+import os
+import json
+import boto3
+from datetime import datetime
+import subprocess
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN", "")
+S3_CLIENT = boto3.client("s3") if EVIDENCE_BUCKET else None
+
+def collect_measured_energy(job_id):
+    # Placeholder: gather telemetry CSVs for the job's timeframe and nodes
+    # Real implementation would query Prometheus for energy/time series and integrate per-node power
+    return {"measured_energy_kwh": 12.3, "measured_emissions_kg": 2.46}
+
+def get_estimate_from_manifest(job_id):
+    # Placeholder: fetch manifest from S3 or local file referenced by job annotations
+    # For demo return a synthetic value
+    return {"estimated_energy_kwh": 13.0, "estimated_emissions_kg": 2.6}
+
+def produce_report(job_id):
+    est = get_estimate_from_manifest(job_id)
+    meas = collect_measured_energy(job_id)
+    report = {"job_id": job_id, "estimate": est, "measured": meas, "delta_kg": meas["measured_emissions_kg"] - est["estimated_emissions_kg"], "ts": datetime.utcnow().isoformat()+"Z"}
+    out = f"/tmp/reconcile_{job_id}.json"
+    with open(out,"w") as f:
+        json.dump(report, f, indent=2)
+    # sign with cosign if available
+    if COSIGN_KMS:
+        subprocess.run(["cosign","sign","--key", f"awskms://{COSIGN_KMS}", out], check=False)
+    if EVIDENCE_BUCKET:
+        key = f"reconciliations/{job_id}_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
+        S3_CLIENT.upload_file(out, EVIDENCE_BUCKET, key)
+        print("Uploaded reconciliation to s3://{}/{}".format(EVIDENCE_BUCKET, key))
+    else:
+        print("Reconciliation report:", out)
+
+if __name__ == "__main__":
+    import sys
+    if len(sys.argv) < 2:
+        print("Usage: reconcile_job_postrun.py <job_id>")
+        sys.exit(2)
+    produce_report(sys.argv[1])
+
*** End Patch
*** Begin Patch
*** Add File:ui/carbon/dashboard_api.py
+#!/usr/bin/env python3
+"""
+Minimal Carbon Reporting API + UI skeleton
+ - Exposes endpoints to fetch per-team aggregates and scheduled jobs
+ - UI is a simple HTML page that queries these endpoints
+"""
+from flask import Flask, jsonify, render_template_string
+import boto3, os
+
+app = Flask("carbon-ui")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+s3 = boto3.client("s3") if EVIDENCE_BUCKET else None
+
+TEMPLATE = """
+<html><head><title>Aegis Carbon Dashboard</title></head><body>
+<h1>Aegis Carbon Dashboard</h1>
+<div id=content>Loading...</div>
+<script>
+fetch('/api/summary').then(r=>r.json()).then(j=>{document.getElementById('content').innerText = JSON.stringify(j,null,2)});
+</script>
+</body></html>
+"""
+
+@app.route("/")
+def index():
+    return render_template_string(TEMPLATE)
+
+@app.route("/api/summary")
+def summary():
+    # For demo: read latest reconciliations (if s3 configured)
+    if not s3:
+        return jsonify({"ok": False, "reason": "no_s3"})
+    res = s3.list_objects_v2(Bucket=EVIDENCE_BUCKET, Prefix="reconciliations/", MaxKeys=10)
+    items = [{"key": o["Key"], "last_modified": o.get("LastModified").isoformat()} for o in res.get("Contents", [])] if res.get("Contents") else []
+    return jsonify({"ok": True, "recent_reconciliations": items})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8089)
+
*** End Patch
*** Begin Patch
*** Add File:READMEs/carbon_enterprise_README.md
+# Carbon Enterprise Enhancements for Aegis
+
+This patch adds components to make Aegis truly carbon-aware for enterprise usage:
+
+- Telemetry collection: power_telemetry_collector.py (deploy as DaemonSet or CronJob), calibration pipeline to produce power_profiles.yaml.
+- Secure webhook CA tooling & rotation: cert_manager.sh + CronJob skeleton for rotation.
+- Policy service: mutable_policy_service.py for per-team policies.
+- Controller to enforce hints: carbon_scheduler_controller.py watches Argo Workflows and suspends/reschedules or assigns nodeSelectors.
+- Mutating webhook (webhook.py) already exists; this patch provides TLS helpers and controller to act on hints.
+- Spot/Autoscaler placeholders: spot_provisioner.py and spot heuristics integrated into estimator.
+- Reconciliation pipeline: reconcile_job_postrun.py to compare actual vs estimated and upload signed reconciliation reports.
+- UI & reporting: dashboard_api.py to surface recent reconciliations and provide team-level summaries.
+
+Operator action items:
+1. Build images and replace placeholder ghcr.io images in manifests.
+2. Deploy telemetry collector, run calibration, and review generated power_profiles.yaml.
+3. Generate CA and TLS certs via cert_manager.sh, set caBundle in MutatingWebhookConfiguration, and deploy webhook.
+4. Deploy policy service and populate policies (policy_store_example.yaml).
+5. Deploy controller with RBAC that can read/write Argo Workflow CRs.
+6. Integrate reconciliation into Argo post-run steps or CI to run reconcile_job_postrun.py for completed jobs.
+
+Security & production notes:
+- Use a proper PKI for webhook TLS (not self-signed in production).
+- Ensure controller and webhook have minimal RBAC privileges.
+- Protect S3 buckets and COSIGN_KMS keys; rotate keys per policy.
+
*** End Patch
*** End Patch
