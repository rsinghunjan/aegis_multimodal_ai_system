*** Begin Patch
*** Add File:terraform/irsa/irsa_roles.tf
+/*
+IRSA Terraform example for Aegis quantum provider service accounts.
+
+This file creates IAM roles that can be assumed by Kubernetes service accounts
+via IRSA (OIDC). It is a template — replace placeholder variables and review
+the attached policies before applying.
+
+Usage:
+  terraform init
+  terraform apply -var="cluster_name=your-cluster" -var="account_id=123456789012" \
+    -var="oidc_provider_arn=arn:aws:iam::123456789012:oidc-provider/oidc.eks.region.amazonaws.com/id/XXXX" \
+    -var="policy_arn_braket=arn:aws:iam::123456789012:policy/aegis-braket" ...
+*/
+
+variable "cluster_name" {}
+variable "account_id" {}
+variable "oidc_provider_arn" {}
+variable "policy_arn_braket" {}
+variable "policy_arn_ibm" {}
+variable "policy_arn_azure" {}
+variable "policy_arn_rigetti" {}
+
+resource "aws_iam_role" "irsa_braket" {
+  name = "${var.cluster_name}-braket-irsa-role"
+  assume_role_policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Effect = "Allow"
+        Principal = {
+          Federated = var.oidc_provider_arn
+        }
+        Action = "sts:AssumeRoleWithWebIdentity"
+        Condition = {
+          StringEquals = {
+            # Adjust audience/issuer claim if necessary for your cluster's OIDC provider
+            "${replace(split("/", var.oidc_provider_arn)[-1], "oidc-provider/","")}:sub" = "system:serviceaccount:aegis:braket-sa"
+          }
+        }
+      }
+    ]
+  })
+}
+
+resource "aws_iam_role_policy_attachment" "attach_braket" {
+  role       = aws_iam_role.irsa_braket.name
+  policy_arn = var.policy_arn_braket
+}
+
+resource "aws_iam_role" "irsa_ibm" {
+  name = "${var.cluster_name}-ibm-irsa-role"
+  assume_role_policy = replace(aws_iam_role.irsa_braket.assume_role_policy, "braket-sa", "ibm-quantum-sa")
+}
+resource "aws_iam_role_policy_attachment" "attach_ibm" {
+  role       = aws_iam_role.irsa_ibm.name
+  policy_arn = var.policy_arn_ibm
+}
+
+resource "aws_iam_role" "irsa_azure" {
+  name = "${var.cluster_name}-azure-irsa-role"
+  assume_role_policy = replace(aws_iam_role.irsa_braket.assume_role_policy, "braket-sa", "azure-quantum-sa")
+}
+resource "aws_iam_role_policy_attachment" "attach_azure" {
+  role       = aws_iam_role.irsa_azure.name
+  policy_arn = var.policy_arn_azure
+}
+
+resource "aws_iam_role" "irsa_rigetti" {
+  name = "${var.cluster_name}-rigetti-irsa-role"
+  assume_role_policy = replace(aws_iam_role.irsa_braket.assume_role_policy, "braket-sa", "rigetti-quantum-sa")
+}
+resource "aws_iam_role_policy_attachment" "attach_rigetti" {
+  role       = aws_iam_role.irsa_rigetti.name
+  policy_arn = var.policy_arn_rigetti
+}
+
+output "irsa_braket_role_arn" {
+  value = aws_iam_role.irsa_braket.arn
+}
+output "irsa_ibm_role_arn" {
+  value = aws_iam_role.irsa_ibm.arn
+}
+output "irsa_azure_role_arn" {
+  value = aws_iam_role.irsa_azure.arn
+}
+output "irsa_rigetti_role_arn" {
+  value = aws_iam_role.irsa_rigetti.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/create_irsa_eksctl.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Convenience helper: create iamserviceaccount via eksctl for each provider.
+# Review and run with appropriate privileges.
+#
+# Usage:
+#   ./ops/create_irsa_eksctl.sh --cluster my-cluster --region us-east-1 --namespace aegis \
+#     --braket-policy arn:aws:iam::123:policy/aegis-braket --ibm-policy arn:aws:iam::123:policy/aegis-ibm ...
+#
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --cluster) CLUSTER="$2"; shift 2;;
+    --region) REGION="$2"; shift 2;;
+    --namespace) NS="$2"; shift 2;;
+    --braket-policy) BRAKET_POLICY="$2"; shift 2;;
+    --ibm-policy) IBM_POLICY="$2"; shift 2;;
+    --azure-policy) AZURE_POLICY="$2"; shift 2;;
+    --rigetti-policy) RIGETTI_POLICY="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+: "${CLUSTER:?--cluster required}"
+: "${REGION:=us-east-1}"
+: "${NS:=aegis}"
+
+echo "Creating IRSA iamserviceaccounts via eksctl in cluster $CLUSTER ($REGION) namespace $NS"
+
+if [ -n "${BRAKET_POLICY:-}" ]; then
+  eksctl create iamserviceaccount --cluster "$CLUSTER" --region "$REGION" --namespace "$NS" --name braket-sa --attach-policy-arn "$BRAKET_POLICY" --approve
+fi
+if [ -n "${IBM_POLICY:-}" ]; then
+  eksctl create iamserviceaccount --cluster "$CLUSTER" --region "$REGION" --namespace "$NS" --name ibm-quantum-sa --attach-policy-arn "$IBM_POLICY" --approve
+fi
+if [ -n "${AZURE_POLICY:-}" ]; then
+  eksctl create iamserviceaccount --cluster "$CLUSTER" --region "$REGION" --namespace "$NS" --name azure-quantum-sa --attach-policy-arn "$AZURE_POLICY" --approve
+fi
+if [ -n "${RIGETTI_POLICY:-}" ]; then
+  eksctl create iamserviceaccount --cluster "$CLUSTER" --region "$REGION" --namespace "$NS" --name rigetti-quantum-sa --attach-policy-arn "$RIGETTI_POLICY" --approve
+fi
+
+echo "IRSA creation completed. Verify role ARNs with: kubectl -n $NS get sa -o yaml"
+
*** End Patch
*** Begin Patch
*** Add File:provider_costs/braket_pricing.py
+#!/usr/bin/env python3
+"""
+Provider pricing lookup / estimator for AWS Braket using AWS Pricing API as a source of truth where possible.
+This script queries AWS Pricing for AmazonBraket SKUs and falls back to heuristics if pricing data unavailable.
+
+Notes:
+ - AWS Pricing API may not expose per-shot prices easily for specialized QPUs. Validate with provider commercial team.
+ - Use this file as the canonical place to implement company-specific pricing mapping.
+"""
+import boto3, json, argparse
+from decimal import Decimal
+
+def get_pricing_skus(region="us-east-1"):
+    pricing = boto3.client("pricing", region_name="us-east-1")
+    # Example filter - may need adjustments for exact sku/service code
+    try:
+        resp = pricing.get_products(ServiceCode='AmazonBraket', MaxResults=100)
+        return resp.get('PriceList', [])
+    except Exception as e:
+        print("Pricing API error:", e)
+        return []
+
+def heuristic_estimate(shots, device, circuit_complexity=1.0):
+    # conservative baseline per-shot cost
+    base_simulator_per_shot = Decimal("0.0005")
+    base_qpu_per_shot = Decimal("0.005")
+    per_shot = base_simulator_per_shot if "simulator" in device else base_qpu_per_shot
+    est = float((per_shot * Decimal(shots) * Decimal(1 + circuit_complexity * 0.1)))
+    return {"provider":"braket","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def estimate(shots, device, complexity=1.0):
+    skus = get_pricing_skus()
+    if skus:
+        # parsing AWS pricing SKUs is non-trivial and implementation-specific.
+        # For now, fallback to heuristic but leave hooks to implement SKU parsing.
+        return heuristic_estimate(shots, device, complexity)
+    else:
+        return heuristic_estimate(shots, device, complexity)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="simulator")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(estimate(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:provider_costs/ibm_pricing.py
+#!/usr/bin/env python3
+"""
+IBM Quantum cost estimator.
+IBM billing APIs are account-specific; this script provides a heuristic and a stub to integrate with IBM billing.
+Replace heuristic with calls to IBM billing API or internal price catalog.
+"""
+import json, argparse
+
+def heuristic_estimate(shots, device, complexity=1.0):
+    base_shot = 0.001  # placeholder
+    est = base_shot * shots * (1.0 + complexity * 0.1)
+    return {"provider":"ibm","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="ibm_washington")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(heuristic_estimate(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:provider_costs/azure_pricing.py
+#!/usr/bin/env python3
+"""
+Azure Quantum cost estimator stub.
+Replace heuristics with calls to Azure RateCard / Cost APIs or provider price catalog.
+"""
+import json, argparse
+
+def heuristic_estimate(shots, device, complexity=1.0):
+    base_shot = 0.0009
+    est = base_shot * shots * (1.0 + complexity * 0.1)
+    return {"provider":"azure","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="ionq")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(heuristic_estimate(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/staged_quantum_runbook.md
+# Aegis — Staged Runbook for Quantum Provider Integration (Braket, IBM, Azure, Rigetti)
+
+Purpose
+- Consolidated operational runbook to create IRSA roles, wire policies, validate provider access, enforce budgets and run simulated/staged QPU experiments.
+
+Preconditions
+- Terraform & eksctl installed locally or in CI
+- AWS CLI/IBM/Azure creds for operator (least-privileged admin)
+- GitHub gh CLI and kubectl
+- ARGO CLI (for submission from an operator workstation)
+
+Variables to replace (set in your environment)
+- CLUSTER_NAME, AWS_ACCOUNT_ID, OIDC_PROVIDER_ARN
+- EVIDENCE_BUCKET, COSIGN_KMS_ARN, REKOR_URL
+- GHCR_ORG, IMAGE_TAG
+
+Step A — Create IRSA roles via Terraform (preferred)
+1. Edit terraform/irsa/irsa_roles.tf variables or create terraform.tfvars with:
+   cluster_name = "your-cluster"
+   account_id = "123456789012"
+   oidc_provider_arn = "arn:aws:iam::123456789012:oidc-provider/oidc.eks.region.amazonaws.com/id/XXXX"
+   policy_arn_braket = "arn:aws:iam::123456789012:policy/aegis-braket"
+   policy_arn_ibm = "arn:aws:iam::123456789012:policy/aegis-ibm"
+   policy_arn_azure = "arn:aws:iam::123456789012:policy/aegis-azure"
+   policy_arn_rigetti = "arn:aws:iam::123456789012:policy/aegis-rigetti"
+2. terraform init && terraform apply -auto-approve
+3. Note outputs irsa_*_role_arn and annotate ServiceAccounts if you prefer not to create ServiceAccounts in TF.
+
+Step B — Alternative: create via eksctl helper
+1. Run ops/create_irsa_eksctl.sh with cluster, region and policy ARNs:
+   ./ops/create_irsa_eksctl.sh --cluster my-cluster --region us-east-1 --namespace aegis --braket-policy arn:aws:iam::...:policy/aegis-braket ...
+2. Verify SAs:
+   kubectl -n aegis get sa braket-sa -o yaml
+
+Step C — Populate ExternalSecrets (if not IRSA)
+1. Create provider credentials in SecretsManager/KeyVault/Vault
+2. Apply k8s/externalsecrets/* manifests after configuring SecretStore
+
+Step D — Build & publish quantum image, sign & verify
+1. Trigger CI build or run:
+   ./ops/run_build_and_verify.sh latest
+2. Verify cosign signatures:
+   cosign verify --rekor-server $REKOR_URL ghcr.io/yourorg/aegis-quantum:latest
+
+Step E — Test simulate flow in staging
+1. Patch argo/quantum_*_workflow.yaml with real image & s3 prefixes
+2. Run simulated submission:
+   argo submit -n aegis argo/quantum_braket_workflow.yaml -p run-type=simulate -p image=ghcr.io/yourorg/aegis-quantum:latest -p circuit-file-s3=s3://.../example.py
+3. Confirm artifacts:
+   - MLflow run logged with quantum/metadata.json
+   - s3://.../results/.../job.tgz exists
+   - If COSIGN configured, job.tgz.sig exists and cosign verify works
+
+Step F — Enforce approval & budget gates
+1. Enable GitHub checks: require .github/workflows/qpu_pre_submit_check.yml and qpu_approval_check.yml as required status checks for PRs that touch quantum artifacts.
+2. Ensure PRs touching .qpu/ or argo workflows add qpu-approved label and at least one approver review.
+3. Use ops/argo_submit_guard.sh in operator workflows to block submissions that exceed budget (see examples in repo).
+
+Step G — Nightly sandbox & reports
+1. Configure secrets SANDBOX_QPU_S3_PREFIX, SANDBOX_QPU_BUDGET_USD and SANDBOX_MLFLOW_URI in GitHub repo.
+2. Nightly workflow runs will estimate cost and upload sandbox report to S3. Operator must run Argo submit in controlled environment or enable operator account to auto-submit for a tightly controlled sandbox.
+
+Safety & Legal
+- Do not send PII or regulated data to external QPUs. Use ops/legal_safe_sanitizer.py to redact or fail.
+- Obtain legal signoff recorded in evidence S3 before production QPU runs.
+
+Acceptance criteria
+- IRSA roles created and SAs annotated
+- Simulator flows complete end-to-end in staging
+- QPU runs are blocked without qpu-approved label and approval
+- Cost estimator prevents submissions beyond budget
+- MLflow runs store quantum/metadata.json and pass schema validation
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/mlflow_quantum_qa_and_issue.yml
+name: MLflow Quantum QA & Issue Reporter
+
+on:
+  schedule:
+    - cron: "0 2 * * *"   # daily at 02:00 UTC
+  workflow_dispatch:
+
+jobs:
+  validate-quantum-mlflow:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: 3.10
+      - name: Install deps
+        run: |
+          pip install mlflow jsonschema boto3
+      - name: Run MLflow Quantum QA
+        id: qa
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          python3 tests/mlflow_quantum_qa.py --tracking-uri "${MLFLOW_TRACKING_URI}" || echo "::set-output name=failed::true"
+      - name: Open issue on failure
+        if: steps.qa.outputs.failed == 'true' || steps.qa.outcome == 'failure'
+        uses: actions/github-script@v6
+        with:
+          script: |
+            const body = `Automated MLflow Quantum QA check detected failing MLflow quantum runs.\n\nPlease investigate recent runs that fail the schema validation. This job runs daily and reports issues when failures are detected.\n\nSteps to reproduce:\n1. Run tests/mlflow_quantum_qa.py --tracking-uri ${process.env.MLFLOW_TRACKING_URI}\n\nAttached info: workflow run: ${{ github.run_id }}`;
+            const issue = await github.rest.issues.create({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              title: "MLflow Quantum QA failures detected — investigate",
+              body: body,
+            });
+            console.log("Created issue:", issue.data.html_url);
+
*** End Patch
*** End Patch
