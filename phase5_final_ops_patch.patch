*** Begin Patch
*** Add File: provider/legal/gha_baa_enforce_and_archive.yml
+name: BAA Enforcement, Archive & Attestation
+on:
+  workflow_dispatch:
+
+jobs:
+  archive-and-attest-baa:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Validate inputs
+        run: |
+          if [ -z "${{ github.event.inputs.provider }}" ] || [ -z "${{ github.event.inputs.file }}" ]; then
+            echo "Requires inputs: provider, file" && exit 2
+          fi
+
+      - name: Upload and archive BAA
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          S3_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python provider/legal/upload_baa.py --provider "${{ github.event.inputs.provider }}" --file "${{ github.event.inputs.file }}" --bucket "${S3_BUCKET}" --uploader "${{ github.actor }}"
+
+      - name: Sign archived BAA with cosign (KMS/HSM)
+        env:
+          COSIGN_KMS_KEY: ${{ secrets.COSIGN_KMS_KEY }}
+        run: |
+          # Sign the PDF in-place before archiving evidence (local copy required)
+          cosign sign --key "${COSIGN_KMS_KEY}" "${{ github.event.inputs.file }}" || true
+          # Create a Rekor/log entry by re-signing instrumented artifact
+          rekor-cli upload --artifact "${{ github.event.inputs.file }}" || true
+
+      - name: Build compliance evidence artifact
+        run: |
+          mkdir -p /tmp/baa_evidence
+          cp "${{ github.event.inputs.file }}" /tmp/baa_evidence/
+          echo '{"provider":"'"${{ github.event.inputs.provider }}"'","uploader":"'"${{ github.actor }}"'"}' > /tmp/baa_evidence/meta.json
+          tar czf /tmp/baa_evidence.tgz -C /tmp baa_evidence
+          cosign sign --key "${COSIGN_KMS_KEY}" /tmp/baa_evidence.tgz || true
+
+      - name: Upload evidence artifact to Compliance bucket
+        env:
+          S3_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          aws s3 cp /tmp/baa_evidence.tgz "s3://${S3_BUCKET}/compliance/BAA/${{ github.event.inputs.provider }}/baa_evidence_$(date -u +%Y%m%dT%H%M%SZ).tgz"
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/verify_archive_and_attest.py
+#!/usr/bin/env python3
+"""
+Verify that an archived BAA evidence bundle exists in S3 and has a Rekor entry.
+Usage: python provider/legal/verify_archive_and_attest.py --s3-url s3://bucket/path/evidence.tgz
+"""
+import argparse
+import subprocess
+import tempfile
+import os
+import boto3
+from urllib.parse import urlparse
+
+def download_s3(s3_url, out_dir):
+    parsed=urlparse(s3_url)
+    bucket=parsed.netloc
+    key=parsed.path.lstrip('/')
+    s3=boto3.client('s3')
+    dest=os.path.join(out_dir, os.path.basename(key))
+    s3.download_file(bucket, key, dest)
+    return dest
+
+def check_rekor(manifest_path):
+    digest=subprocess.check_output(["sha256sum", manifest_path]).decode().split()[0]
+    try:
+        out=subprocess.check_output(["rekor-cli","search","hash",digest], timeout=20)
+        return True
+    except subprocess.CalledProcessError:
+        return False
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--s3-url", required=True)
+    args=p.parse_args()
+    with tempfile.TemporaryDirectory() as td:
+        fn=download_s3(args.s3_url, td)
+        print("Downloaded", fn)
+        ok=check_rekor(fn)
+        print("Rekor entry found:", ok)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/cloudhsm_init_and_claim.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrator scaffold for CloudHSM initialization steps that must be run once cluster is created.
+# This script helps operator run the sequence of AWS CLI / client commands and documents expected outputs.
+# NOTE: Some steps are manual / interactive per AWS docs. Use this script as a guided checklist.
+
+echo "1) Ensure cluster exists and HSMs are available"
+aws cloudhsmv2 describe-clusters --query "HsmList[].ClusterId" || true
+
+echo "2) Initialize cluster (operator MUST follow AWS console prompts)"
+echo "  - Claim HSMs and wait for INITIALIZED state."
+echo "3) On a management host with cloudhsm-client installed, configure the client and run 'configure' then 'key_mgmt_util' to create Crypto User (CU)."
+echo "4) Example: create a Crypto User (interactive)"
+echo "   key_mgmt_util -l -c createUser -u CUadmin -p <password> -r <roles>"
+
+echo "5) After CU exists, generate or import a key and label it. Use pkcs11-tool or vendor utilities. Example placeholder follows:"
+echo "   # pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so --keypairgen --key-type RSA:2048 --label cosign-key"
+
+echo "6) Validate PKCS11 key is visible:"
+echo "   pkcs11-tool --module /usr/lib/libcloudhsm_pkcs11.so -O"
+
+echo "7) Run cosign PKCS11 validation: hsm/pkcs11/cosign_pkcs11_validation.sh manifest.json"
+echo "8) After making changes, ensure CloudTrail is enabled and run hsm/cloudtrail/generate_audit_report.py with COSIGN_KEY_ID set."
+
+echo "CloudHSM init scaffold complete."
+
*** End Patch
*** Begin Patch
*** Add File: hsm/pkcs11/create_and_label_key_example.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example commands for generating an RSA key in a PKCS11 HSM and labeling it for cosign use.
+# These commands are vendor-dependent and are presented as examples only.
+
+PKCS11_MODULE="${PKCS11_MODULE:-/usr/lib/libcloudhsm_pkcs11.so}"
+LABEL="${1:-cosign-key}"
+
+echo "Listing tokens via pkcs11-tool"
+pkcs11-tool --module "${PKCS11_MODULE}" --list-slots
+
+echo "Generating RSA keypair in HSM (example)"
+pkcs11-tool --module "${PKCS11_MODULE}" --keypairgen --key-type rsa:2048 --label "${LABEL}" --id 01
+
+echo "Verify key is present:"
+pkcs11-tool --module "${PKCS11_MODULE}" -O
+
+echo "Now cosign can use PKCS11 label ${LABEL} if cosign built with pkcs11 support."
+
*** End Patch
*** Begin Patch
*** Add File: hsm/ops/rotate_and_audit_orchestrator.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate a rotation drill: rotate KMS alias or provision new PKCS11 key, update CI secret and validate usage audit.
+if [ "${USE_CLOUDHSM:-false}" = "true" ]; then
+  echo "CloudHSM rotation: create new key in HSM and update COSIGN_PKCS11_LABEL in CI manually."
+  echo "After update, run cosign_pkcs11_validation.sh against a sample manifest."
+else
+  echo "Creating new AWS KMS key and switching alias"
+  NEW_KEY=$(aws kms create-key --description "cosign rotation drill" --query KeyMetadata.KeyId --output text)
+  aws kms create-alias --alias-name alias/aegis-cosign --target-key-id "$NEW_KEY"
+  echo "New KMS key id: $NEW_KEY"
+  # optionally update GH secret
+  if command -v gh >/dev/null 2>&1; then
+    REPO="${1:-owner/repo}"
+    gh secret set COSIGN_KMS_KEY --repo "$REPO" --body "awskms:///$NEW_KEY"
+    echo "Updated GitHub secret COSIGN_KMS_KEY"
+  fi
+  echo "Validate signing using new key:"
+  cosign sign --key "awskms:///$NEW_KEY" manifest.json || true
+  cosign verify manifest.json || true
+fi
+
+echo "Query CloudTrail events for recent kms:Sign for audit review"
+aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign --max-results 50 > /tmp/cloudtrail_kms_events.json
+echo "Saved /tmp/cloudtrail_kms_events.json - review for unexpected principals"
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/bastion_and_site_hardening.yml
+- name: Provision bastion host & apply site-level hardening for runner site
+  hosts: bastion
+  become: true
+  vars:
+    admin_allowed_cidrs: ["203.0.113.0/24"]
+  tasks:
+    - name: Ensure essential packages installed
+      apt:
+        name:
+          - ufw
+          - fail2ban
+          - auditd
+          - awscli
+        state: present
+        update_cache: yes
+
+    - name: Configure UFW - allow SSH from admin CIDRs only
+      ufw:
+        state: enabled
+        rule: allow
+        proto: tcp
+        port: 22
+        from_ip: "{{ admin_allowed_cidrs | join(',') }}"
+
+    - name: Harden sshd (disable password auth)
+      lineinfile:
+        path: /etc/ssh/sshd_config
+        regexp: '^#?PasswordAuthentication'
+        line: 'PasswordAuthentication no'
+      notify: restart ssh
+
+    - name: Place bastion jump script
+      copy:
+        dest: /usr/local/bin/jump-to-runner
+        content: |
+          #!/usr/bin/env bash
+          # Use: jump-to-runner runner-host
+          if [ -z "$1" ]; then echo "usage"; exit 1; fi
+          ssh -A admin@"$1"
+        mode: '0755'
+
+  handlers:
+    - name: restart ssh
+      service:
+        name: ssh
+        state: restarted
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/runner_site_onboard.yml
+- name: Onboard runner site devices (assumes cloud-init completed)
+  hosts: runners
+  become: true
+  vars:
+    runner_user: aegis-runner
+  tasks:
+    - name: Ensure runner user exists
+      user:
+        name: "{{ runner_user }}"
+        create_home: yes
+
+    - name: Deploy MDM agent & config
+      copy:
+        src: runner/mdm/mdm_agent.py
+        dest: /usr/local/bin/mdm_agent.py
+        mode: '0755'
+
+    - name: Deploy mdm systemd unit
+      copy:
+        src: runner/mdm/mdm_agent.service
+        dest: /etc/systemd/system/mdm_agent.service
+        mode: '0644'
+
+    - name: Enable and start mdm agent
+      systemd:
+        name: mdm_agent
+        state: started
+        enabled: yes
+
+    - name: Install node_exporter (ARM) for monitoring
+      include_tasks: monitoring/node_exporter_install.yml
+
+    - name: Apply site hardening role
+      include_role:
+        name: aegis.site_hardening
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/runner_health_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-runner-health
+  namespace: monitoring
+spec:
+  groups:
+    - name: runner.health.rules
+      rules:
+        - alert: RunnerDownHigh
+          expr: sum(up{job="node_exporter", instance=~"runner-.*"} == 0) > 10
+          for: 10m
+          labels:
+            severity: critical
+          annotations:
+            summary: "More than 10 runners are down"
+
*** End Patch
*** Begin Patch
*** Add File: runners/ingest/kafka_telemetry_producer.py
+#!/usr/bin/env python3
+"""
+Lightweight telemetry producer to publish device telemetry to Kafka for ingestion offload.
+Devices forward telemetry to controller which publishes to Kafka; this helps buffer spikes.
+"""
+import json, time, os
+from kafka import KafkaProducer
+
+KAFKA_BOOTSTRAP = os.environ.get("KAFKA_BOOTSTRAP", "kafka:9092")
+TOPIC = os.environ.get("KAFIS_TOPIC", "aegis-telemetry")
+
+producer = KafkaProducer(bootstrap_servers=[KAFKA_BOOTSTRAP], value_serializer=lambda v: json.dumps(v).encode('utf-8'))
+
+def publish(device_id, payload):
+    msg = {"device_id": device_id, "ts": int(time.time()), "payload": payload}
+    producer.send(TOPIC, msg)
+    producer.flush()
+
+if __name__=="__main__":
+    # smoke publish loop for local test
+    for i in range(10):
+        publish(f"device-{i}", {"temp": 42+i})
+        time.sleep(0.1)
+
*** End Patch
*** Begin Patch
*** Add File: runners/ingest/kafka_ingest_consumer.py
+#!/usr/bin/env python3
+"""
+Consumes telemetry from Kafka and writes in batches to partitioned Postgres telemetry table.
+This worker is intended to be run as a scalable k8s Deployment and will commit batches to DB.
+"""
+import os, json, time
+from kafka import KafkaConsumer
+import psycopg2
+
+KAFKA_BOOTSTRAP = os.environ.get("KAFKA_BOOTSTRAP", "kafka:9092")
+TOPIC = os.environ.get("KAFKA_TOPIC", "aegis-telemetry")
+BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "100"))
+DB_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@postgres:5432/aegis")
+
+def get_conn():
+    return psycopg2.connect(DB_URL)
+
+def write_batch(conn, rows):
+    with conn.cursor() as cur:
+        args_str = ",".join(cur.mogrify("(%s,%s,to_timestamp(%s),%s)", (r["device_id"], r["device_id"], r["ts"], json.dumps(r["payload"]))).decode('utf-8') for r in rows)
+        cur.execute("INSERT INTO device_telemetry (device_id, device_id_hash, ts, payload) VALUES " + args_str)
+    conn.commit()
+
+def main():
+    consumer = KafkaConsumer(TOPIC, bootstrap_servers=[KAFKA_BOOTSTRAP], auto_offset_reset='earliest', enable_auto_commit=True, value_deserializer=lambda m: json.loads(m.decode('utf-8')))
+    batch=[]
+    conn = get_conn()
+    for msg in consumer:
+        batch.append(msg.value)
+        if len(batch)>=BATCH_SIZE:
+            write_batch(conn, batch)
+            batch=[]
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/measure_and_tune.py
+#!/usr/bin/env python3
+"""
+Run the SLA harness, aggregate metrics and produce adapter tuning recommendations.
+This script will:
+ - run distributed SLA harness (assumes harness writes JSON reports to /tmp/qpu_reports)
+ - aggregate reports and produce /tmp/adapter_recommendations.json consumed by adapter config updater
+ - optionally apply configmap and restart adapter pods
+"""
+import os, subprocess, json
+from statistics import median
+
+REPORT_DIR="/tmp/qpu_reports"
+AGG_OUT="/tmp/qpu_sla_summary.json"
+RECS_OUT="/tmp/adapter_recommendations.json"
+
+def aggregate():
+    reports=[json.load(open(p)) for p in sorted(os.listdir(REPORT_DIR)) if p.endswith('.json')]
+    latencies=[r.get("elapsed",0) for r in reports]
+    errors=sum(1 for r in reports if r.get("error",False))
+    summary={"count":len(reports),"p95_latency":max(latencies) if latencies else None,"errors":errors}
+    open(AGG_OUT,"w").write(json.dumps(summary,indent=2))
+    # naive recommendations
+    recs=[]
+    for provider in ["braket","ibm"]:
+        recs.append({"provider":provider,"retries":8 if summary["errors"]>5 else 4,"backoff":2.0 if summary["p95_latency"] and summary["p95_latency"]>60 else 1.0,"throttle_rate":0.5 if summary["errors"]>5 else 2.0})
+    open(RECS_OUT,"w").write(json.dumps(recs,indent=2))
+    return RECS_OUT
+
+def apply(recs_file):
+    subprocess.run(["python","quantum/tuning/apply_adapter_config.py"], check=False)
+
+if __name__=="__main__":
+    recs=aggregate()
+    apply(recs)
+    print("Tuning complete; recommendations written to", recs)
+
*** End Patch
*** Begin Patch
*** Add File: mitigation/run_mitigation_batch.py
+#!/usr/bin/env python3
+"""
+Batch-run mitigation experiments (ZNE + readout) for a set of devices using collected calibration snapshots.
+Stores mitigation artifacts into MLflow and writes suggested plugin config to /tmp/mitigation_suggestions.json
+"""
+import json, time
+from mitigation.collect_calibration_mlflow import collect
+from mitigation.znereadout_integration import run_for_device
+
+DEVICES=["ibm_santiago","ibm_perth"]
+OUT="/tmp/mitigation_suggestions.json"
+suggestions=[]
+for d in DEVICES:
+    print("Collecting calibration for", d)
+    props=collect(d)
+    print("Running mitigation experiments for", d)
+    res=run_for_device(d, "OPENQASM 2.0;")  # placeholder circuit
+    suggestions.append({"device":d,"suggestion":res})
+open(OUT,"w").write(json.dumps(suggestions,indent=2))
+print("Wrote mitigation suggestions to", OUT)
+
*** End Patch
*** Begin Patch
*** Add File: billing/cron/reconcile_daily.yml
+name: Daily Billing Reconciliation
+on:
+  schedule:
+    - cron: "0 6 * * *"  # daily at 06:00 UTC
+
+jobs:
+  reconcile:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+      - name: Install deps
+        run: pip install boto3 sqlalchemy psycopg2-binary
+      - name: Run reconciliation
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          DATABASE_URL: ${{ secrets.DATABASE_URL }}
+          BILLING_SNS_TOPIC: ${{ secrets.BILLING_SNS_TOPIC }}
+        run: |
+          python billing/reconcile_automated.py
+
*** End Patch
*** Begin Patch
*** Add File: edge/loadtest/run_full_scale_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate a full-scale locust run: deploy master, create N worker k8s jobs, collect metrics and generate DB shard recommendation
+N="${1:-500}"
+MASTER_YAML="edge/loadtest/locust_master_deployment.yaml"
+WORKER_JOB_YAML="edge/loadtest/locust_distributed_jobs.yaml"
+echo "Deploying locust master"
+kubectl apply -f "${MASTER_YAML}"
+echo "Launching ${N} worker jobs"
+for i in $(seq 1 ${N}); do
+  kubectl create -f ${WORKER_JOB_YAML} || true
+done
+echo "Run for 30 minutes then collect metrics"
+sleep 1800
+echo "Dump Prometheus metrics (requires promsvc)"
+kubectl port-forward svc/prometheus-operated -n monitoring 9090:9090 &
+PROMPORT=$!
+python3 - <<PY
+import requests, json
+r=requests.get("http://127.0.0.1:9090/api/v1/query", params={"query":"sum(rate(http_requests_total[1m]))"})
+print(r.json())
+PY
+kill ${PROMPORT} || true
+echo "Run shard recommendation helper"
+python edge/db/shard_recommend.py --shards 8
+
*** End Patch
*** Begin Patch
*** Add File: db/partitioning/cron_create_partitions.yml
+name: Daily DB Partition Create & Maintenance
+on:
+  schedule:
+    - cron: "5 0 * * *"
+
+jobs:
+  create-partitions:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Run partition maintenance
+        env:
+          DATABASE_URL: ${{ secrets.DATABASE_URL }}
+        run: |
+          psql "${DATABASE_URL}" -f db/partitioning/maintenance.sql
+
*** End Patch
*** End Patch
