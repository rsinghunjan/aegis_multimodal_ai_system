*** Begin Patch
*** Add File:quantum/cosign_hybrid/README.md
+cosign-hybrid — Hybrid classical + post‑quantum signing helper
+=============================================================
+
+Purpose
+- Produce a hybrid signature for artifacts: a classical signature (via cosign) and a post‑quantum (PQ) signature (via python-oqs / liboqs bindings).
+- Store both signatures and metadata together (JSON) so Rekor/your provenance store can record hybrid entries.
+
+Notes & safety
+- PQ primitives / HSMs are still emerging. This tool implements a hybrid approach today (classical + PQ) so artifacts remain verifiable in the short and long term.
+- If python-oqs / liboqs is not available, cosign-hybrid will still create classical signature and a placeholder PQ record. Replace placeholder with your HSM call when available.
+
+Quick example
+- Sign an artifact:
+  python3 cosign_hybrid.py sign --artifact path/to/art.bin --output-signatures ./sigs --cosign-key cosign.key
+
+- Verify signatures:
+  python3 cosign_hybrid.py verify --artifact path/to/art.bin --signatures ./sigs
+
+Files
+- cosign_hybrid.py — main signing / verification CLI
+- requirements.txt — optional Python reqs (python-oqs if available)
+
*** End Patch
*** Begin Patch
*** Add File:quantum/cosign_hybrid/cosign_hybrid.py
+#!/usr/bin/env python3
+"""
+cosign_hybrid.py
+
+Create hybrid signatures: classical (cosign) + post-quantum (python-oqs if available).
+Writes out a JSON metadata file with both signatures and signer info.
+
+Usage:
+  python3 cosign_hybrid.py sign --artifact <path> --output-signatures <dir> [--cosign-key <key>]
+  python3 cosign_hybrid.py verify --artifact <path> --signatures <dir>
+
+Notes:
+- Requires cosign in PATH for classical signature.
+- Optional: python-oqs (oqs) for PQ signatures. If not available, writes a placeholder PQ entry.
+"""
+import os, sys, json, argparse, subprocess, base64, hashlib, time
+
+try:
+    import oqs
+    OQS_AVAILABLE = True
+except Exception:
+    OQS_AVAILABLE = False
+
+def sha256(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as f:
+        while True:
+            b = f.read(8192)
+            if not b: break
+            h.update(b)
+    return h.hexdigest()
+
+def cosign_sign(artifact, cosign_key=None, out_sig_path=None):
+    # cosign sign-blob --key <key> --output-signature <sigfile> <artifact>
+    cmd = ["cosign","sign-blob"]
+    if cosign_key:
+        cmd += ["--key", cosign_key]
+    if out_sig_path:
+        cmd += ["--output-signature", out_sig_path]
+    cmd += [artifact]
+    subprocess.check_call(cmd)
+    return out_sig_path
+
+def pq_sign(artifact):
+    if not OQS_AVAILABLE:
+        return {"algorithm":"PQ_PLACEHOLDER","signature":None,"note":"python-oqs not installed; replace with HSM/PQ signer"}
+    # Use Dilithium2 as example PQ signature algorithm (update per policy)
+    algo = "Dilithium2"
+    with oqs.Signature(algo) as sig:
+        with open(artifact,"rb") as f:
+            data = f.read()
+        signature = sig.sign(data)
+        pubkey = sig.generate_keypair()[1] if False else None
+    return {"algorithm": algo, "signature": base64.b64encode(signature).decode("ascii"), "public_key": None}
+
+def sign_artifact(args):
+    artifact = args.artifact
+    outdir = os.path.abspath(args.output_signatures)
+    os.makedirs(outdir, exist_ok=True)
+    meta = {
+        "artifact": os.path.abspath(artifact),
+        "sha256": sha256(artifact),
+        "timestamp": int(time.time()),
+        "classical": None,
+        "pq": None
+    }
+    # classical via cosign
+    classical_sig = os.path.join(outdir, "classical.sig")
+    try:
+        cosign_sign(artifact, cosign_key=args.cosign_key, out_sig_path=classical_sig)
+        with open(classical_sig,"rb") as f:
+            classical_b64 = base64.b64encode(f.read()).decode("ascii")
+        meta["classical"] = {"tool":"cosign","signature_b64":classical_b64}
+    except Exception as e:
+        meta["classical"] = {"error": str(e)}
+    # PQ signature
+    pq = pq_sign(artifact)
+    meta["pq"] = pq
+    # write metadata
+    meta_path = os.path.join(outdir, "hybrid-signature.json")
+    with open(meta_path,"w") as f:
+        json.dump(meta, f, indent=2)
+    print("Wrote hybrid signature metadata to", meta_path)
+
+def verify_artifact(args):
+    artifact = args.artifact
+    sigdir = args.signatures
+    meta_path = os.path.join(sigdir, "hybrid-signature.json")
+    if not os.path.exists(meta_path):
+        print("Missing hybrid signature metadata:", meta_path)
+        sys.exit(2)
+    meta = json.load(open(meta_path))
+    # verify classical signature using cosign verify-blob --signature <sig> <artifact>
+    classical_sig_file = os.path.join(sigdir, "classical.sig")
+    if os.path.exists(classical_sig_file):
+        try:
+            subprocess.check_call(["cosign","verify-blob","--signature", classical_sig_file, artifact])
+            print("Classical signature verified (cosign).")
+        except subprocess.CalledProcessError as e:
+            print("Classical verification failed:", e)
+    else:
+        print("No classical signature file found.")
+    # PQ verification: depends on PQ tooling - placeholder check
+    pq = meta.get("pq", {})
+    if pq and pq.get("algorithm") and pq.get("signature"):
+        if not OQS_AVAILABLE:
+            print("PQ signature present in metadata but local PQ verification not available (python-oqs missing).")
+        else:
+            # perform verification with oqs (example - depends on how public key is stored)
+            print("PQ verification is not fully implemented in this helper; implement based on chosen PQ stack.")
+    else:
+        print("No PQ signature found or signature field empty.")
+    print("Artifact SHA256:", meta.get("sha256"))
+
+def main():
+    p = argparse.ArgumentParser()
+    sub = p.add_subparsers(dest="cmd")
+    s = sub.add_parser("sign")
+    s.add_argument("--artifact", required=True)
+    s.add_argument("--output-signatures", required=True)
+    s.add_argument("--cosign-key", default=None)
+    v = sub.add_parser("verify")
+    v.add_argument("--artifact", required=True)
+    v.add_argument("--signatures", required=True)
+    args = p.parse_args()
+    if args.cmd == "sign":
+        sign_artifact(args)
+    elif args.cmd == "verify":
+        verify_artifact(args)
+    else:
+        p.print_help()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/cosign_hybrid/requirements.txt
+oqs>=1.0.0 ; platform_system != "Windows"
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/README.md
+Quantum backend adapters for Aegis
+=================================
+
+This directory provides a pluggable interface for executing quantum circuits either on local simulators or cloud QPU providers.
+
+Components
+- adapter.py: interface definition
+- qiskit_adapter.py: example adapter implemented with Qiskit (Aer simulator + IBMQ skeleton)
+
+Usage
+- Import adapter and call submit_job() / get_result() from your job broker or MLflow logger.
+
+Security & provenance
+- All job submissions should include metadata (seed, noise model snapshot, requested backend) and be logged to MLflow and Rekor via the quantum_logger.
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/adapter.py
+"""
+Quantum backend adapter interface
+
+Interface:
+  submit_job(job_spec) -> job_id
+  get_result(job_id) -> dict with 'counts', 'raw_shots', 'metadata'
+  cancel_job(job_id)
+  list_backends() -> list of backend descriptors
+"""
+from abc import ABC, abstractmethod
+
+class QuantumBackendAdapter(ABC):
+    @abstractmethod
+    def submit_job(self, job_spec: dict) -> str:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def get_result(self, job_id: str) -> dict:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def cancel_job(self, job_id: str) -> bool:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def list_backends(self) -> list:
+        raise NotImplementedError()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/qiskit_adapter.py
+"""
+Qiskit adapter example: supports Aer simulator and skeleton for IBMQ provider.
+
+Requires: qiskit, qiskit-aer (for simulator), and optional IBMQ account for real hardware.
+"""
+import uuid, json, os
+from qiskit import QuantumCircuit, transpile
+from qiskit.providers.aer import AerSimulator
+try:
+    from qiskit_ibm_runtime import IBMQRuntimeService, Sampler  # optional provider SDK
+    IBM_AVAILABLE = True
+except Exception:
+    IBM_AVAILABLE = False
+
+from .adapter import QuantumBackendAdapter
+
+class QiskitAdapter(QuantumBackendAdapter):
+    def __init__(self, default_backend="aer_simulator"):
+        self.default_backend = default_backend
+        self.jobs = {}  # in-memory job store for demo; production: durable queue/db
+
+    def submit_job(self, job_spec: dict) -> str:
+        # job_spec: { "circuit_qasm": <qasm str> or "circuit_json": ... , "shots": int, "backend": "aer"|"ibm" }
+        job_id = str(uuid.uuid4())
+        backend = job_spec.get("backend", self.default_backend)
+        shots = int(job_spec.get("shots", 1024))
+        # For demo run synchronously on AerSimulator and store result in jobs
+        if backend in ("aer", "aer_simulator", "aer_sim"):
+            sim = AerSimulator()
+            qc = QuantumCircuit.from_qasm_str(job_spec["circuit_qasm"])
+            transpiled = transpile(qc, sim)
+            result = sim.run(transpiled, shots=shots).result()
+            counts = result.get_counts()
+            self.jobs[job_id] = {"status":"DONE","counts":counts,"raw_shots":None,"metadata":{"backend":"aer_simulator"}}
+        elif backend.startswith("ibm") and IBM_AVAILABLE:
+            # Placeholder: submit to IBMQ using IBMQRuntimeService or provider SDK
+            service = IBMQRuntimeService()
+            # Implement provider-specific submission here
+            self.jobs[job_id] = {"status":"QUEUED","metadata":{"backend":"ibm"}}
+        else:
+            self.jobs[job_id] = {"status":"ERROR","error":"Unsupported backend or provider not configured"}
+        return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        return self.jobs.get(job_id, {"status":"UNKNOWN"})
+
+    def cancel_job(self, job_id: str) -> bool:
+        # Aer simulator runs are instant; for queued jobs mark cancelled
+        j = self.jobs.get(job_id)
+        if not j:
+            return False
+        j["status"] = "CANCELLED"
+        return True
+
+    def list_backends(self) -> list:
+        backends = [{"id":"aer_simulator","type":"simulator","capabilities":["shots","statevector"]}]
+        if IBM_AVAILABLE:
+            backends.append({"id":"ibm_qpu","type":"hardware","capabilities":["shots"]})
+        return backends
+
*** End Patch
*** Begin Patch
*** Add File:quantum/mlflow_logger/quantum_logger.py
+"""
+Quantum experiment logger
+
+Logs circuit definitions, backend metadata, noise model snapshots, and raw shots to MLflow + lakeFS (S3).
+Creates a Rekor/co-sign hybrid signature entry for the circuit+result metadata using cosign_hybrid.
+"""
+import os, json, time, mlflow, subprocess
+from typing import Dict
+
+def store_raw_shots_to_s3(shots_data_path: str, s3_bucket: str, s3_key_prefix: str):
+    import subprocess
+    key = f"{s3_key_prefix}/{os.path.basename(shots_data_path)}"
+    dest = f"s3://{s3_bucket}/{key}"
+    subprocess.check_call(["aws","s3","cp", shots_data_path, dest])
+    return dest
+
+def log_quantum_experiment(run_name: str, circuit_qasm: str, backend_info: Dict, shots_counts: Dict, raw_shots_file: str=None, s3_bucket: str=None, s3_prefix: str=None, sign_with=None):
+    mlflow.set_experiment("quantum-experiments")
+    with mlflow.start_run(run_name=run_name) as run:
+        mlflow.log_param("backend", backend_info.get("id"))
+        mlflow.log_param("backend_type", backend_info.get("type"))
+        mlflow.log_param("circuit_sha256", hashlib_sha(circuit_qasm))
+        # store circuit as artifact
+        circuit_file = f"/tmp/{run.info.run_id}_circuit.qasm"
+        with open(circuit_file,"w") as f:
+            f.write(circuit_qasm)
+        mlflow.log_artifact(circuit_file, artifact_path="circuit")
+        # store counts as metric/artifact
+        counts_file = f"/tmp/{run.info.run_id}_counts.json"
+        with open(counts_file,"w") as f:
+            json.dump(shots_counts, f)
+        mlflow.log_artifact(counts_file, artifact_path="results")
+        # optionally upload raw shots to S3/lakeFS
+        s3_path = None
+        if raw_shots_file and s3_bucket and s3_prefix:
+            s3_path = store_raw_shots_to_s3(raw_shots_file, s3_bucket, s3_prefix)
+            mlflow.log_param("raw_shots_s3", s3_path)
+        # produce combined metadata for signing
+        meta = {
+            "run_id": run.info.run_id,
+            "timestamp": int(time.time()),
+            "backend": backend_info,
+            "counts": shots_counts,
+            "raw_shots": s3_path
+        }
+        meta_file = f"/tmp/{run.info.run_id}_meta.json"
+        with open(meta_file,"w") as f:
+            json.dump(meta, f, indent=2)
+        mlflow.log_artifact(meta_file, artifact_path="meta")
+        # sign metadata using cosign_hybrid if requested
+        if sign_with:
+            sig_out = os.path.join("/tmp", f"{run.info.run_id}_sigs")
+            os.makedirs(sig_out, exist_ok=True)
+            subprocess.check_call(["python3","../cosign_hybrid/cosign_hybrid.py","sign","--artifact",meta_file,"--output-signatures",sig_out,"--cosign-key",sign_with])
+            # upload signature metadata as artifact
+            mlflow.log_artifact(os.path.join(sig_out,"hybrid-signature.json"), artifact_path="meta/sigs")
+        return run.info.run_id
+
+def hashlib_sha(s: str):
+    import hashlib
+    h = hashlib.sha256()
+    h.update(s.encode("utf-8"))
+    return h.hexdigest()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/notebooks/demo_quantum_pipeline.py
+"""
+Demo script: build a small quantum circuit via Qiskit, run on Aer simulator via QiskitAdapter,
+and log the experiment via quantum_logger to MLflow.
+
+Run:
+  python3 demo_quantum_pipeline.py
+"""
+from qiskit import QuantumCircuit
+from qiskit.quantum_info import random_statevector
+from quantum.qbackend.qiskit_adapter import QiskitAdapter
+from quantum.mlflow_logger.quantum_logger import log_quantum_experiment
+import os, json, tempfile
+
+def build_simple_bell():
+    qc = QuantumCircuit(2,2)
+    qc.h(0)
+    qc.cx(0,1)
+    qc.measure([0,1],[0,1])
+    return qc
+
+def main():
+    qc = build_simple_bell()
+    qasm = qc.qasm()
+    adapter = QiskitAdapter()
+    job_id = adapter.submit_job({"circuit_qasm": qasm, "shots": 1024, "backend": "aer_simulator"})
+    result = adapter.get_result(job_id)
+    counts = result.get("counts", {})
+    # write raw shots placeholder
+    raw_file = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
+    json.dump({"counts": counts}, raw_file)
+    raw_file.close()
+    run_id = log_quantum_experiment(run_name="demo-bell", circuit_qasm=qasm, backend_info={"id":"aer_simulator","type":"simulator"}, shots_counts=counts, raw_shots_file=raw_file.name, s3_bucket=os.environ.get("QUANTUM_RAW_BUCKET",""), s3_prefix="quantum/raw", sign_with=os.environ.get("COSIGN_KEY",""))
+    print("Logged run:", run_id)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/README.md
+Quantum job broker
+==================
+
+Lightweight FastAPI service to accept quantum jobs, persist them into a SQLite queue and dispatch to configured backend adapters.
+Includes a worker loop that processes queued jobs using the QiskitAdapter (or pluggable adapters).
+
+Files
+- app.py — FastAPI server
+- worker.py — background worker process
+- Dockerfile — container image
+- k8s/job-broker-deployment.yaml — example Kubernetes deployment & service
+
+Usage (local)
+- Start broker: python3 app.py
+- In another terminal, start worker: python3 worker.py
+- Submit job: curl -X POST http://localhost:8080/submit -H "Content-Type: application/json" -d '{"circuit_qasm":"...", "shots":1024}'
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/app.py
+#!/usr/bin/env python3
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+import sqlite3, os, json, uuid, time
+from threading import Thread
+
+DB = os.environ.get("JOB_DB", "quantum_jobs.db")
+os.makedirs(os.path.dirname(DB) or ".", exist_ok=True)
+
+def init_db():
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    c.execute("CREATE TABLE IF NOT EXISTS jobs (id TEXT PRIMARY KEY, spec TEXT, status TEXT, created_at TEXT, result TEXT)")
+    conn.commit()
+    conn.close()
+
+init_db()
+
+app = FastAPI()
+
+class JobSpec(BaseModel):
+    circuit_qasm: str
+    shots: int = 1024
+    backend: str = "aer_simulator"
+
+@app.post("/submit")
+def submit(spec: JobSpec):
+    job_id = str(uuid.uuid4())
+    created = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    c.execute("INSERT INTO jobs (id,spec,status,created_at) VALUES (?,?,?,?)", (job_id, json.dumps(spec.dict()), "PENDING", created))
+    conn.commit()
+    conn.close()
+    return {"job_id": job_id}
+
+@app.get("/status/{job_id}")
+def status(job_id: str):
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    r = c.execute("SELECT status,result FROM jobs WHERE id=?", (job_id,)).fetchone()
+    conn.close()
+    if not r:
+        raise HTTPException(status_code=404, detail="job not found")
+    status, result = r
+    return {"job_id": job_id, "status": status, "result": json.loads(result) if result else None}
+
+@app.get("/health")
+def health():
+    return {"ok": True}
+
+if __name__ == "__main__":
+    import uvicorn
+    uvicorn.run(app, host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/worker.py
+#!/usr/bin/env python3
+"""
+Simple worker: polls SQLite DB for PENDING jobs, executes via QiskitAdapter, persists result.
+"""
+import time, sqlite3, json, os
+from quantum.qbackend.qiskit_adapter import QiskitAdapter
+
+DB = os.environ.get("JOB_DB", "quantum_jobs.db")
+adapter = QiskitAdapter()
+
+def poll_loop():
+    while True:
+        conn = sqlite3.connect(DB)
+        c = conn.cursor()
+        rows = c.execute("SELECT id, spec FROM jobs WHERE status='PENDING' LIMIT 5").fetchall()
+        for r in rows:
+            job_id, spec_json = r
+            spec = json.loads(spec_json)
+            print("Processing job", job_id)
+            try:
+                jid = adapter.submit_job(spec)
+                res = adapter.get_result(jid)
+                c.execute("UPDATE jobs SET status=?, result=? WHERE id=?", ("DONE", json.dumps(res), job_id))
+            except Exception as e:
+                c.execute("UPDATE jobs SET status=?, result=? WHERE id=?", ("ERROR", json.dumps({"error": str(e)}), job_id))
+            conn.commit()
+        conn.close()
+        time.sleep(2)
+
+if __name__ == "__main__":
+    poll_loop()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+RUN apt-get update && apt-get install -y gcc libffi-dev build-essential && rm -rf /var/lib/apt/lists/*
+COPY quantum/job_broker /app
+RUN pip install fastapi uvicorn pydantic qiskit qiskit-aer
+EXPOSE 8080
+CMD ["python3","app.py"]
+
*** End Patch
*** Begin Patch
*** Add File:quantum/job_broker/k8s/job-broker-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-quantum-job-broker
+  labels:
+    app: aegis-quantum
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-quantum
+  template:
+    metadata:
+      labels:
+        app: aegis-quantum
+    spec:
+      containers:
+        - name: job-broker
+          image: aegis/quantum-job-broker:latest
+          ports:
+            - containerPort: 8080
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-quantum-broker
+spec:
+  selector:
+    app: aegis-quantum
+  ports:
+    - protocol: TCP
+      port: 80
+      targetPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File:quantum/vault_pq/pq_key_runbook.md
+Post-Quantum Key Management Runbook (Aegis)
+==========================================
+
+Purpose
+- Describe how to prepare key management for post‑quantum (PQ) signing and hybrid signatures.
+
+Assumptions
+- Vault is used for classical key management today (transit, kv).
+- PQ primitives may be provided by an HSM vendor or via liboqs when available. For now we implement hybrid signing helper that uses cosign + PQ signer.
+
+Steps — short
+1) Inventory current signing keys
+   - List Vault transit keys used for cosign/vault signing: vault list transit/keys
+2) Create PQ key placeholders
+   - If HSM vendor provides PQ key type, provision key in HSM and create an alias in Vault (via HSM integration).
+   - If not available, create a KV secret that will hold PQ public verification keys (rotate when HSM available).
+3) Update signing workflows to use hybrid signature helper (cosign_hybrid).
+   - Ensure cosign signs classical and cosign_hybrid appends PQ signature metadata to Rekor entry.
+4) Rotate & retire keys
+   - Maintain key version metadata with timestamps and "valid from" fields.
+   - Implement verification that accepts either classical-only (for legacy) or hybrid depending on policy.
+5) Audit & backup
+   - Store verification public keys and rotation history in compliance/evidence storage (encrypted).
+
+Detailed example (Vault + KV placeholder)
+- Create a kv path for PQ verification public keys:
+  vault kv put secret/pq-keys/customer1 public_key=@pubkey.pem algorithm="Dilithium2" created_at=$(date -u +"%FT%TZ")
+- Add an access policy limiting who can read/rotate these keys.
+
+Notes & risk
+- PQ HSM support is vendor-dependent and may introduce lead time. Use hybrid signatures now for maximum future resilience.
+- Maintain evidence of all rotations for auditors.
+
*** End Patch
*** Begin Patch
*** Add File:quantum/vault_pq/terraform_pq_key_example.tf
+# Terraform snippet: store PQ public key in Vault KV as a placeholder until HSM PQ support is available
+provider "vault" {
+  address = var.vault_addr
+  token   = var.vault_token
+}
+
+resource "vault_kv_secret_v2" "pq_public_key" {
+  path = "pq-keys/${var.customer}"
+  data_json = jsonencode({
+    public_key = file(var.pq_public_key_file)
+    algorithm  = var.pq_algorithm
+    created_at = timestamp()
+  })
+}
+
+variable "vault_addr" { type = string }
+variable "vault_token" { type = string sensitive = true }
+variable "customer" { type = string }
+variable "pq_public_key_file" { type = string }
+variable "pq_algorithm" { type = string default = "Dilithium2" }
+
*** End Patch
*** Begin Patch
*** Add File:quantum/tests/smoke_run.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Running quantum demo smoke test..."
+python3 -c "import sys; from quantum.notebooks.demo_quantum_pipeline import main; main()"
+echo "Demo run complete. Check MLflow experiment 'quantum-experiments' artifacts and logs."
+
*** End Patch
*** Begin Patch
*** Add File:quantum/requirements.txt
+qiskit>=0.45.0
+qiskit-aer>=0.10.0
+mlflow>=2.0.0
+fastapi>=0.85.0
+uvicorn>=0.18.0
+oqs>=1.0.0 ; platform_system != "Windows"
+
*** End Patch
*** Begin Patch
*** Add File:quantum/README.md
+Quantum readiness artifacts for Aegis
+====================================
+
+What this patch adds
+- Hybrid signing helper (classical + PQ): quantum/cosign_hybrid
+- Quantum backend adapter & Qiskit example: quantum/qbackend
+- MLflow quantum logger and demo: quantum/mlflow_logger and quantum/notebooks
+- Job broker + worker (FastAPI) and Kubernetes manifest: quantum/job_broker
+- Vault PQ runbook & Terraform placeholder for PQ public keys: quantum/vault_pq
+- Tests and requirements
+
+How to use (quick start)
+1. Install dependencies (prefer a venv):
+   pip install -r quantum/requirements.txt
+2. Run the demo pipeline locally:
+   ./quantum/tests/smoke_run.sh
+3. Start the job broker (optional):
+   python3 quantum/job_broker/app.py
+   python3 quantum/job_broker/worker.py
+4. Use cosign_hybrid to sign experiment metadata before recording provenance.
+
+Security & ops notes
+- Replace placeholder PQ signing with HSM-based PQ signatures when vendor support is available.
+- Review cost / quota when connecting to cloud QPU providers.
+- Capture noise model and calibration metadata for reproducibility of hardware runs.
+
*** End Patch
*** End Patch
