*** Begin Patch
*** Add File:edge/device_registry/requirements.txt
fastapi
uvicorn[standard]
SQLAlchemy
psycopg2-binary
alembic
pydantic
boto3
python-multipart
prometheus-client
cryptography
*** End Patch
*** Begin Patch
*** Add File:edge/device_registry/models.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime, JSON, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class Device(Base):
    __tablename__ = "devices"
    id = Column(Integer, primary_key=True, index=True)
    device_id = Column(String(128), unique=True, index=True, nullable=False)
    public_key = Column(Text, nullable=True)
    status = Column(String(32), default="inactive")  # active, revoked, inactive
    metadata = Column(JSON, default={})
    last_seen = Column(DateTime, server_default=func.now(), onupdate=func.now())
    created_at = Column(DateTime, server_default=func.now())

class Telemetry(Base):
    __tablename__ = "telemetry"
    id = Column(Integer, primary_key=True, index=True)
    device_id = Column(String(128), index=True, nullable=False)
    ts = Column(DateTime, server_default=func.now())
    metrics = Column(JSON)

*** End Patch
*** Begin Patch
*** Add File:edge/device_registry/app.py
#!/usr/bin/env python3
"""
Device Registry & Telemetry API (FastAPI)
 - Enroll device (provides device_id and device public key)
 - Revoke device
 - Heartbeat / telemetry ingestion
 - List devices, get device
 - Simple auth is intentionally left for operator implementation (OIDC/SSO)
"""
import os
from typing import Optional, List
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from models import Base, Device, Telemetry
from prometheus_client import Counter, start_http_server

DATABASE_URL = os.environ.get("DEVICE_DB_URL", "sqlite:///./devices.db")
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base.metadata.create_all(bind=engine)

app = FastAPI(title="Aegis Device Registry")

# Metrics
start_http_server(int(os.environ.get("DEVICE_METRICS_PORT", "9205")))
API_ENROLL = Counter("aegis_device_enroll_total", "Total device enrollments")
API_HEARTBEAT = Counter("aegis_device_heartbeat_total", "Total device heartbeats")

class EnrollReq(BaseModel):
    device_id: str
    public_key_pem: Optional[str] = None
    metadata: Optional[dict] = {}

class HeartbeatReq(BaseModel):
    device_id: str
    metrics: Optional[dict] = {}

@app.post("/devices/enroll")
def enroll(req: EnrollReq):
    API_ENROLL.inc()
    db = SessionLocal()
    existing = db.query(Device).filter(Device.device_id == req.device_id).first()
    if existing:
        existing.public_key = req.public_key_pem or existing.public_key
        existing.metadata = req.metadata or existing.metadata
        existing.status = "active"
        db.add(existing)
        db.commit()
        db.close()
        return {"device_id": req.device_id, "status": "re-enrolled"}
    d = Device(device_id=req.device_id, public_key=req.public_key_pem, metadata=req.metadata, status="active")
    db.add(d)
    db.commit()
    db.close()
    return {"device_id": req.device_id, "status": "enrolled"}

@app.post("/devices/revoke/{device_id}")
def revoke(device_id: str):
    db = SessionLocal()
    d = db.query(Device).filter(Device.device_id == device_id).first()
    if not d:
        db.close()
        raise HTTPException(status_code=404, detail="device not found")
    d.status = "revoked"
    db.add(d)
    db.commit()
    db.close()
    return {"device_id": device_id, "status": "revoked"}

@app.post("/devices/heartbeat")
def heartbeat(req: HeartbeatReq):
    API_HEARTBEAT.inc()
    db = SessionLocal()
    d = db.query(Device).filter(Device.device_id == req.device_id).first()
    if not d:
        db.close()
        raise HTTPException(status_code=404, detail="device not enrolled")
    t = Telemetry(device_id=req.device_id, metrics=req.metrics or {})
    db.add(t)
    d.last_seen = None  # SQLAlchemy will update timestamp via DB default/update
    db.add(d)
    db.commit()
    db.close()
    return {"device_id": req.device_id, "status": "ok"}

@app.get("/devices")
def list_devices():
    db = SessionLocal()
    ds = db.query(Device).all()
    out = [{"device_id": d.device_id, "status": d.status, "last_seen": d.last_seen, "metadata": d.metadata} for d in ds]
    db.close()
    return out

@app.get("/devices/{device_id}")
def get_device(device_id: str):
    db = SessionLocal()
    d = db.query(Device).filter(Device.device_id == device_id).first()
    db.close()
    if not d:
        raise HTTPException(status_code=404, detail="device not found")
    return {"device_id": d.device_id, "status": d.status, "metadata": d.metadata, "last_seen": d.last_seen}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("DEVICE_REGISTRY_PORT", "8081")))

*** End Patch
*** Begin Patch
*** Add File:edge/device_client/client.py
#!/usr/bin/env python3
"""
Edge device client skeleton:
 - enroll with device registry
 - heartbeat loop sending telemetry
 - fetch OTA manifest, decide whether to download artifact (supports staged rollout)
 - download with resume, verify signature via cosign (invokes cosign CLI)
 - apply delta/bundle updates (placeholder)
"""
import os
import time
import requests
import subprocess
import hashlib

REGISTRY_URL = os.environ.get("DEVICE_REGISTRY_URL", "http://device-registry.aegis.svc.cluster.local:8081")
DEVICE_ID = os.environ.get("DEVICE_ID", "device-0001")
PUBLIC_KEY_PEM = os.environ.get("DEVICE_PUBLIC_KEY_PEM", "")

def enroll():
    r = requests.post(f"{REGISTRY_URL}/devices/enroll", json={"device_id": DEVICE_ID, "public_key_pem": PUBLIC_KEY_PEM})
    r.raise_for_status()
    print("Enroll:", r.json())

def heartbeat_loop():
    while True:
        metrics = {"cpu": 0.1, "mem": 0.2}  # placeholder
        try:
            r = requests.post(f"{REGISTRY_URL}/devices/heartbeat", json={"device_id": DEVICE_ID, "metrics": metrics}, timeout=5)
            r.raise_for_status()
        except Exception as e:
            print("Heartbeat failed:", e)
        time.sleep(30)

def fetch_manifest():
    r = requests.get(f"{REGISTRY_URL}/ota/manifest/{DEVICE_ID.replace('-', '')}", timeout=5)  # optional per-device manifest
    if r.status_code == 404:
        return None
    r.raise_for_status()
    return r.json()

def download_with_resume(url, out_path):
    # Basic resume using HTTP Range. This assumes server supports Range.
    headers = {}
    if os.path.exists(out_path):
        curr = os.path.getsize(out_path)
        headers["Range"] = f"bytes={curr}-"
    else:
        curr = 0
    with requests.get(url, stream=True, headers=headers, timeout=60) as r:
        if r.status_code not in (200, 206):
            r.raise_for_status()
        mode = "ab" if curr else "wb"
        with open(out_path, mode) as f:
            for chunk in r.iter_content(chunk_size=1024*1024):
                if chunk:
                    f.write(chunk)
    return out_path

def verify_cosign_signature(artifact_path):
    # Requires cosign in PATH and appropriate key/trust configured on device.
    try:
        proc = subprocess.run(["cosign", "verify", artifact_path], check=False, capture_output=True, text=True)
        print("cosign verify stdout:", proc.stdout)
        print("cosign verify stderr:", proc.stderr)
        return proc.returncode == 0
    except Exception as e:
        print("cosign verification error:", e)
        return False

def apply_update(artifact_path):
    # Placeholder: extract bundle and atomically swap model or container
    print("Applying update from", artifact_path)
    # e.g., stop local inference service, replace files, restart
    return True

if __name__ == "__main__":
    enroll()
    # In production client runs concurrent threads: heartbeat + ota poll + inference
    # For demo, just run a single poll
    manifest = fetch_manifest()
    if manifest:
        artifact = manifest.get("artifact_url")
        if artifact:
            out = download_with_resume(artifact, "/tmp/update.bundle")
            ok = verify_cosign_signature(out)
            if ok:
                apply_update(out)
            else:
                print("Signature verification failed")
    # start heartbeat loop (blocking)
    heartbeat_loop()

*** End Patch
*** Begin Patch
*** Add File:edge/ota/ota_server.py
#!/usr/bin/env python3
"""
Simple OTA manifest server with staged rollout semantics and cosign-signed manifests.
 - Devices poll /ota/manifest/<device_group>
 - Manifest contains artifacts with versions, rollout_percent, and s3/http url
 - Operators sign manifest with cosign (off-line); server verifies signature at publish time and can return manifest to devices
 - This server provides simple staged-rollout logic by storing a rollout percentage and selecting which devices should get the update (hash of device_id)
"""
import os
import json
import subprocess
from typing import Dict
from fastapi import FastAPI, HTTPException

MANIFEST_STORE = os.environ.get("OTA_MANIFEST_STORE", "./manifests")  # directory with JSON manifests
SIGNED_MANIFEST_ANNOTATION = "aegis.cosign.signed"

app = FastAPI(title="Aegis OTA Manifest Server")

def load_manifest(group: str) -> Dict:
    path = os.path.join(MANIFEST_STORE, f"{group}.json")
    if not os.path.exists(path):
        raise FileNotFoundError
    with open(path, "r") as f:
        return json.load(f)

def is_in_rollout(device_id: str, rollout_pct: int) -> bool:
    # deterministic hash -> percentage
    h = int.from_bytes(device_id.encode("utf-8"), "little") % 100
    return h < rollout_pct

@app.get("/ota/manifest/{group}/{device_id}")
def manifest_for_device(group: str, device_id: str):
    try:
        m = load_manifest(group)
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="manifest not found")
    # each artifact entry has rollout_pct
    artifacts = []
    for a in m.get("artifacts", []):
        pct = int(a.get("rollout_pct", 100))
        if is_in_rollout(device_id, pct):
            artifacts.append(a)
    if not artifacts:
        return {"status": "no_update"}
    # return filtered manifest
    out = {"group": group, "artifacts": artifacts, "version": m.get("version")}
    return out

@app.post("/ota/publish/{group}")
def publish_manifest(group: str):
    """
    Accepts a server-side manifest placed into manifest store (operator step).
    Optionally verify cosign signature of artifact files here (best-effort).
    """
    path = os.path.join(MANIFEST_STORE, f"{group}.json")
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="manifest not found")
    # Operators should sign the manifest file with cosign externally; server can optionally verify
    return {"status": "published", "group": group}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("OTA_PORT", "8092")))

*** End Patch
*** Begin Patch
*** Add File:edge/ota/TUF_README.md
# TUF / Uptane guidance for Aegis OTA

This file describes recommended integration points to make OTA robust and secure.

- TUF (The Update Framework) provides metadata signing, versioning and rollback/rollback protection.
- Uptane extends TUF for vehicle/embedded contexts with separate metadata for director/targets.

Quick options:
1. Lightweight: use cosign-signed manifests + artifact signatures (fast to adopt).
   - Operator signs artifact (container or bundle) with cosign (KMS).
   - Server publishes signed manifest (JSON) indicating rollout policies.
   - Device verifies signature with cosign before installing.

2. Full TUF adoption:
   - Use python-tuf or in-toto to generate timestamp/root/release metadata.
   - Devices embed root keys and use TUF client to fetch consistent metadata and target files.
   - Recommended for high-assurance deployments.

Operator checklist:
- Choose root signing keys and store in KMS/HSM.
- Implement staged rollouts and canary groups.
- Devices must verify signatures before applying updates.
- Keep manifest metadata and snapshot in evidence S3 and include in audit packages.

*** End Patch
*** Begin Patch
*** Add File:edge/packaging/build_edge_bundle.sh
#!/usr/bin/env bash
set -euo pipefail
#
# Build an edge update bundle:
#  - inputs: quantized model file, extra files
#  - output: tar.gz artifact + manifest.json
#  - sign artifact with cosign (operator must have COSIGN_KMS_ARN in env)
#
MODEL_FILE=${1:-"./model/slm.quant.onnx"}
OUT_DIR=${2:-"./out"}
VERSION=${3:-"v0.0.1"}
GROUP=${4:-"default"}
mkdir -p "$OUT_DIR"
artifact="${OUT_DIR}/bundle-${VERSION}.tar.gz"
tar -czf "$artifact" -C "$(dirname "$MODEL_FILE")" "$(basename "$MODEL_FILE")"
MANIFEST="${OUT_DIR}/manifest-${VERSION}.json"
cat > "$MANIFEST" <<EOF
{
  "version": "${VERSION}",
  "group": "${GROUP}",
  "artifacts": [
    {
      "name": "slm",
      "artifact_url": "https://storage.example/${GROUP}/bundle-${VERSION}.tar.gz",
      "rollout_pct": 100
    }
  ]
}
EOF
echo "Bundle created: $artifact"
if [ -n "${COSIGN_KMS_ARN:-}" ]; then
  echo "Signing artifact with cosign via KMS..."
  cosign sign --key "awskms://${COSIGN_KMS_ARN}" "$artifact" || echo "cosign sign failed"
fi
echo "Manifest: $MANIFEST (operator should upload & publish)"
*** End Patch
*** Begin Patch
*** Add File:edge/packaging/convert_to_ggml.py
#!/usr/bin/env python3
"""
Placeholder conversion script that outlines a flow to produce compact edge artifacts.
- Input: HF checkpoint or ONNX model
- Output: GGML or other CPU-friendly artifact
This script wraps existing community tools (llama.cpp, ggml converters or optimum/transformers exports).
Operators should plug in their chosen conversion toolchain.
"""
import argparse
import os
import subprocess

def convert_onnx_to_ggml(onnx_path, out_path):
    # Example placeholder: call external converter (user must install)
    # e.g., convert-onnx-to-ggml <onnx> <out>
    cmd = ["convert-onnx-to-ggml", onnx_path, out_path]
    subprocess.run(cmd, check=True)
    print("Converted to ggml:", out_path)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--onnx", required=True)
    p.add_argument("--out", required=True)
    args = p.parse_args()
    convert_onnx_to_ggml(args.onnx, args.out)

*** End Patch
*** Begin Patch
*** Add File:edge/privacy/pii_scrubber.py
#!/usr/bin/env python3
"""
Simple PII scrubber for edge devices.
- Uses regex heuristics to remove email, phone, ssn patterns before upload.
- In production, replace with a more robust library (PII detection models).
"""
import re

EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{7,}\d")
SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")

def scrub_text(s: str) -> str:
    s = EMAIL_RE.sub("[REDACTED_EMAIL]", s)
    s = PHONE_RE.sub("[REDACTED_PHONE]", s)
    s = SSN_RE.sub("[REDACTED_SSN]", s)
    return s

def scrub_payload(payload: dict) -> dict:
    # Recursively scrub strings
    def _scrub(x):
        if isinstance(x, str):
            return scrub_text(x)
        if isinstance(x, dict):
            return {k: _scrub(v) for k,v in x.items()}
        if isinstance(x, list):
            return [_scrub(i) for i in x]
        return x
    return _scrub(payload)

if __name__ == "__main__":
    sample = {"note":"contact john.doe@example.com or +1 555-123-4567", "nested":{"ssn":"123-45-6789"}}
    print(scrub_payload(sample))

*** End Patch
*** Begin Patch
*** Add File:edge/monitoring/local_metrics.py
#!/usr/bin/env python3
"""
Local edge metrics exporter (Prometheus)
 - Exposes resource usage (simple) and device-level metrics for scraping by gateway monitoring
"""
import time
from prometheus_client import Gauge, start_http_server
import psutil
import os

DEVICE_ID = os.environ.get("DEVICE_ID", "device-001")
PORT = int(os.environ.get("EDGE_METRICS_PORT", "9210"))

cpu_g = Gauge("edge_cpu_percent", "Edge CPU percent", ["device"])
mem_g = Gauge("edge_mem_percent", "Edge memory percent", ["device"])
uptime_g = Gauge("edge_uptime_seconds", "Edge uptime seconds", ["device"])

def collect():
    cpu_g.labels(device=DEVICE_ID).set(psutil.cpu_percent(interval=1))
    mem_g.labels(device=DEVICE_ID).set(psutil.virtual_memory().percent)
    uptime_g.labels(device=DEVICE_ID).set(psutil.boot_time())

if __name__ == "__main__":
    start_http_server(PORT)
    while True:
        collect()
        time.sleep(10)

*** End Patch
*** Begin Patch
*** Add File:hpc/slurm/job_template.sbatch
#!/bin/bash
#
#SBATCH --job-name=aegis-ds-train
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=8
#SBATCH --time=08:00:00
#SBATCH --output=slurm-%j.out
#SBATCH --error=slurm-%j.err
#SBATCH --partition=gpu
#SBATCH --gres=gpu:8
#SBATCH --hint=nomultithread
#
# Example Deepspeed ZeRO run with checkpointing to local disk then staged upload
module load cuda/11.7
module load anaconda
source activate aegis-ml

export LOCAL_CHECKPOINT_DIR=/local/scratch/${SLURM_JOB_ID}/checkpoints
mkdir -p ${LOCAL_CHECKPOINT_DIR}
# Run training (train.py should write checkpoints to LOCAL_CHECKPOINT_DIR)
srun python train.py --deepspeed --deepspeed_config ds_config.json --local_ckpt ${LOCAL_CHECKPOINT_DIR}

# After training finishes (or periodically via cron), shard & upload checkpoints
python upload_checkpoint_shards.py --src ${LOCAL_CHECKPOINT_DIR} --s3-prefix s3://REPLACE_EVIDENCE_BUCKET/hpc/checkpoints/${SLURM_JOB_ID}/

*** End Patch
*** Begin Patch
*** Add File:hpc/slurm/submit_and_watch.py
#!/usr/bin/env python3
"""
Submit a Slurm job (sbatch) and watch for preemption/requeue. On preemption, resubmit with resume flag.
Note: Requires Slurm CLI tools available (sbatch, scontrol, sacct).
"""
import argparse
import subprocess
import time
import shlex

def submit(script_path):
    res = subprocess.check_output(["sbatch", script_path]).decode()
    # expected: "Submitted batch job <id>"
    jid = res.strip().split()[-1]
    return jid

def job_state(jobid):
    try:
        out = subprocess.check_output(["sacct", "-j", jobid, "--format=JobID,State%20", "--parsable2", "--noheader"])
        lines = out.decode().strip().splitlines()
        if not lines:
            return None
        # take first line
        parts = lines[0].split("|")
        return parts[1]
    except subprocess.CalledProcessError:
        return None

def watch(jobid, script):
    print("Watching job", jobid)
    while True:
        st = job_state(jobid)
        print("State:", st)
        if st is None:
            print("No state yet; sleeping")
        elif "COMPLETED" in st:
            print("Job completed")
            return 0
        elif "FAILED" in st:
            print("Job failed; inspect logs")
            return 2
        elif "PREEMPTED" in st or "CANCELLED" in st:
            print("Job preempted; resubmitting with resume")
            # operator must ensure resume flags exist in job script or create a resume wrapper
            jid = submit(script)
            jobid = jid
            print("Resubmitted new job", jobid)
        time.sleep(60)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--script", required=True)
    args = p.parse_args()
    jid = submit(args.script)
    print("Submitted job", jid)
    watch(jid, args.script)

*** End Patch
*** Begin Patch
*** Add File:hpc/checkpoint/shard_and_upload.py
#!/usr/bin/env python3
"""
Shard a directory of checkpoint files into multiple tar shards and upload to S3 in parallel.
Creates a manifest.json describing shards for later reconstruction.
"""
import os
import tarfile
import tempfile
import uuid
import json
import boto3
from concurrent.futures import ThreadPoolExecutor

S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))

def make_shards(src_dir, out_dir, files_per_shard=50):
    files = []
    for root,_,fnames in os.walk(src_dir):
        for f in fnames:
            files.append(os.path.join(root,f))
    os.makedirs(out_dir, exist_ok=True)
    shards = []
    for i in range(0, len(files), files_per_shard):
        shard_files = files[i:i+files_per_shard]
        shard_name = f"shard-{i//files_per_shard}-{uuid.uuid4().hex}.tar.gz"
        shard_path = os.path.join(out_dir, shard_name)
        with tarfile.open(shard_path, "w:gz") as tar:
            for f in shard_files:
                arcname = os.path.relpath(f, src_dir)
                tar.add(f, arcname=arcname)
        shards.append(shard_path)
    return shards

def upload_shard(shard_path, s3_prefix):
    key = os.path.join(s3_prefix, os.path.basename(shard_path))
    s3.upload_file(shard_path, S3_BUCKET, key)
    return key

def create_manifest(shard_keys, s3_prefix):
    manifest = {"shards": shard_keys, "s3_prefix": s3_prefix}
    key = os.path.join(s3_prefix, "manifest.json")
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=json.dumps(manifest).encode())
    return key

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--src", required=True)
    p.add_argument("--s3-prefix", required=True)
    p.add_argument("--tmp", default="/tmp/shards")
    args = p.parse_args()
    shards = make_shards(args.src, args.tmp)
    s3_prefix = args.s3_prefix.strip("/")

    keys = []
    with ThreadPoolExecutor(max_workers=4) as ex:
        for key in ex.map(lambda p: upload_shard(p, s3_prefix), shards):
            keys.append(key)
    manifest_key = create_manifest(keys, s3_prefix)
    print("Uploaded shards. manifest:", manifest_key)

*** End Patch
*** Begin Patch
*** Add File:hpc/stage/dataset_stager.py
#!/usr/bin/env python3
"""
Simple dataset stager: copy dataset from S3 to local SSD on compute nodes using parallel download (s5cmd recommended)
Fallback to aws s3 sync if s5cmd not available.
"""
import os
import shutil
import subprocess

S3_PREFIX = os.environ.get("DATASET_S3_PREFIX", "s3://REPLACE_BUCKET/datasets/sample")
LOCAL_STAGING = os.environ.get("LOCAL_STAGE", "/local/scratch/dataset")

def stage_with_s5cmd(s3_prefix, out_dir):
    cmd = ["s5cmd", "cp", f"{s3_prefix}/*", out_dir]
    subprocess.run(cmd, check=True)

def stage_with_awscli(s3_prefix, out_dir):
    cmd = ["aws", "s3", "sync", s3_prefix, out_dir]
    subprocess.run(cmd, check=True)

def stage():
    os.makedirs(LOCAL_STAGING, exist_ok=True)
    try:
        stage_with_s5cmd(S3_PREFIX, LOCAL_STAGING)
    except Exception:
        stage_with_awscli(S3_PREFIX, LOCAL_STAGING)
    print("Staged dataset to", LOCAL_STAGING)

if __name__ == "__main__":
    stage()

*** End Patch
*** Begin Patch
*** Add File:hpc/resume/resume_helper.py
#!/usr/bin/env python3
"""
Resume helper downloads checkpoint manifest shards, unpacks into local checkpoint dir, and prepares to restart training.
"""
import os
import json
import tarfile
import tempfile
import boto3
import requests

S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))

def download_key(key, out_dir):
    os.makedirs(out_dir, exist_ok=True)
    local = os.path.join(out_dir, os.path.basename(key))
    s3.download_file(S3_BUCKET, key, local)
    return local

def reconstruct(manifest_key, out_dir):
    # manifest_key is s3 key for manifest.json
    resp = s3.get_object(Bucket=S3_BUCKET, Key=manifest_key)
    manifest = json.loads(resp["Body"].read())
    shards = manifest.get("shards", [])
    for s in shards:
        local = download_key(s, out_dir)
        with tarfile.open(local, "r:gz") as tar:
            tar.extractall(path=out_dir)
    print("Reconstructed checkpoint in", out_dir)

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--manifest-key", required=True)
    p.add_argument("--out", default="/local/scratch/checkpoint")
    args = p.parse_args()
    reconstruct(args.manifest_key, args.out)

*** End Patch
*** Begin Patch
*** Add File:hpc/cost/accounting.py
#!/usr/bin/env python3
"""
HPC cost accounting stub: record GPU hours to Redis (or local CSV) for later aggregation.
"""
import os
import time
try:
    import redis
except Exception:
    redis = None

REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/1")
r = redis.from_url(REDIS_URL) if redis else None

def record_usage(job_id, gpu_hours, tenant="default"):
    key = f"hpc:usage:{tenant}"
    if r:
        r.hincrbyfloat(key, job_id, gpu_hours)
    else:
        with open("/tmp/hpc_usage.csv", "a") as f:
            f.write(f"{time.time()},{tenant},{job_id},{gpu_hours}\n")

if __name__ == "__main__":
    record_usage("job-123", 16.0, "team-a")

*** End Patch
*** Begin Patch
*** Add File:quantum/adapter/requirements.txt
fastapi
uvicorn[standard]
boto3
qiskit
amazon-braket-sdk
requests
pyyaml
prometheus-client
*** End Patch
*** Begin Patch
*** Add File:quantum/adapter/queue_adapter.py
#!/usr/bin/env python3
"""
Quantum job queue adapter (FastAPI)
 - Accept job descriptors, sign/verify (operators should sign descriptors)
 - Place in local DB queue (sqlite for demo) or SQS/RabbitMQ in production
 - Worker polls queue, submits to backend with retry/backoff
 - Stores results to S3 and emits an evidence manifest entry for audit packaging
"""
import os
import time
import json
import subprocess
import sqlite3
from typing import Dict
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import boto3

DB_PATH = os.environ.get("QUANTUM_DB", "/tmp/quantum_jobs.db")
S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))

conn = sqlite3.connect(DB_PATH, check_same_thread=False)
conn.execute("""CREATE TABLE IF NOT EXISTS jobs(
    id TEXT PRIMARY KEY, descriptor TEXT, status TEXT, attempts INTEGER DEFAULT 0, created_at INTEGER
)""")
conn.commit()

app = FastAPI(title="Aegis Quantum Queue Adapter")

class JobReq(BaseModel):
    job_id: str
    descriptor_s3_key: str  # operator uploads signed JSON descriptor to S3, then posts key
    signer: str = None

def enqueue_job(job: JobReq):
    now = int(time.time())
    conn.execute("INSERT OR REPLACE INTO jobs(id, descriptor, status, attempts, created_at) VALUES(?,?,?,?,?)",
                 (job.job_id, job.descriptor_s3_key, "PENDING", 0, now))
    conn.commit()

@app.post("/quantum/enqueue")
def enqueue(req: JobReq, background: BackgroundTasks):
    # Validate descriptor exists
    try:
        s3.head_object(Bucket=S3_BUCKET, Key=req.descriptor_s3_key)
    except Exception:
        raise HTTPException(status_code=400, detail="descriptor not found in S3")
    enqueue_job(req)
    background.add_task(worker_poll_once)
    return {"status":"queued", "job_id": req.job_id}

def worker_poll_once():
    row = conn.execute("SELECT id,descriptor,attempts FROM jobs WHERE status='PENDING' ORDER BY created_at LIMIT 1").fetchone()
    if not row:
        return
    job_id, descriptor_key, attempts = row
    # download descriptor
    tmp = f"/tmp/{job_id}_desc.json"
    s3.download_file(S3_BUCKET, descriptor_key, tmp)
    with open(tmp, "r") as f:
        desc = json.load(f)
    # verify signature (operator: sign descriptor with cosign and upload signature file next to descriptor)
    # Here we assume operator placed descriptor and signature; verification delegated to cosign CLI (best-effort)
    # Submit to backend
    provider = desc.get("provider", "qiskit")
    if provider == "qiskit":
        submit_qiskit_job(job_id, desc, descriptor_key)
    elif provider == "braket":
        submit_braket_job(job_id, desc, descriptor_key)
    else:
        conn.execute("UPDATE jobs SET status='FAILED' WHERE id=?", (job_id,))
        conn.commit()

def submit_qiskit_job(job_id, desc, descriptor_key):
    from qiskit import QuantumCircuit, Aer, execute
    # For demo: run a local simulator job
    shots = desc.get("shots", 1024)
    qc = QuantumCircuit(1,1)
    qc.h(0); qc.measure(0,0)
    backend = Aer.get_backend('qasm_simulator')
    job = execute(qc, backend=backend, shots=shots)
    result = job.result().get_counts()
    # persist result to S3
    key = f"quantum/results/{job_id}.json"
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=json.dumps(result).encode())
    # produce evidence manifest entry (simple)
    manifest_key = f"quantum/evidence/{job_id}_manifest.json"
    manifest = {"job_id": job_id, "descriptor_s3": descriptor_key, "result_s3": key, "ts": int(time.time())}
    s3.put_object(Bucket=S3_BUCKET, Key=manifest_key, Body=json.dumps(manifest).encode())
    conn.execute("UPDATE jobs SET status='COMPLETED' WHERE id=?", (job_id,))
    conn.commit()

def submit_braket_job(job_id, desc, descriptor_key):
    # Placeholder: in production call braket SDK, submit job, wait for completion or poll, collect results
    # For now mark as queued->completed immediate (simulator fallback)
    submit_qiskit_job(job_id, desc, descriptor_key)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("QUANTUM_QUEUE_PORT", "8083")))

*** End Patch
*** Begin Patch
*** Add File:quantum/adapter/job_runner.py
#!/usr/bin/env python3
"""
Job runner that:
 - Verifies signed descriptor (best-effort: cosign verify)
 - Submits to provider with retry/backoff
 - Writes results to S3 and creates an evidence manifest entry
 - Posts to Rekor (optional) for auditable record of descriptor usage
"""
import os
import subprocess
import json
import time
import boto3
import requests

S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
REKOR_URL = os.environ.get("REKOR_URL", "")
s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION", "us-west-2"))

def verify_descriptor(descriptor_path):
    # Best-effort: use cosign verify
    try:
        res = subprocess.run(["cosign", "verify", descriptor_path], capture_output=True, text=True)
        return res.returncode == 0
    except Exception:
        return False

def submit_with_retry(submit_fn, max_attempts=5, base_sleep=5):
    attempt = 0
    while attempt < max_attempts:
        try:
            return submit_fn()
        except Exception as e:
            attempt += 1
            sleep = base_sleep * (2 ** (attempt-1))
            time.sleep(sleep)
    raise RuntimeError("Max attempts exceeded")

def example_submit_qiskit(desc):
    # local simulator placeholder
    from qiskit import QuantumCircuit, Aer, execute
    shots = desc.get("shots", 1024)
    qc = QuantumCircuit(1,1)
    qc.h(0); qc.measure(0,0)
    backend = Aer.get_backend('qasm_simulator')
    job = execute(qc, backend=backend, shots=shots)
    result = job.result().get_counts()
    return result

def run(job_id, descriptor_s3_key):
    tmp = f"/tmp/{job_id}_desc.json"
    s3.download_file(S3_BUCKET, descriptor_s3_key, tmp)
    ok = verify_descriptor(tmp)
    if not ok:
        raise RuntimeError("Descriptor signature verification failed")
    with open(tmp, "r") as f:
        desc = json.load(f)
    provider = desc.get("provider","qiskit")
    # closure
    def do_submit():
        if provider == "qiskit":
            return example_submit_qiskit(desc)
        elif provider == "braket":
            # integrate Amazon Braket SDK here
            return example_submit_qiskit(desc)
        else:
            raise RuntimeError("unknown provider")
    result = submit_with_retry(do_submit)
    key = f"quantum/results/{job_id}.json"
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=json.dumps(result).encode())
    manifest = {"job_id": job_id, "descriptor_s3": descriptor_s3_key, "result_s3": key, "ts": int(time.time())}
    manifest_key = f"quantum/evidence/{job_id}_manifest.json"
    s3.put_object(Bucket=S3_BUCKET, Key=manifest_key, Body=json.dumps(manifest).encode())
    # Rekor post (best-effort)
    if REKOR_URL:
        try:
            requests.post(f"{REKOR_URL}/api/v1/log/entries", json={"job_id": job_id, "descriptor": f"s3://{S3_BUCKET}/{descriptor_s3_key}", "result": f"s3://{S3_BUCKET}/{key}"}, timeout=5)
        except Exception:
            pass
    return manifest_key

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--job-id", required=True)
    p.add_argument("--descriptor-key", required=True)
    args = p.parse_args()
    print(run(args.job_id, args.descriptor_key))

*** End Patch
*** Begin Patch
*** Add File:quantum/simulator/sim_ci.yml
name: Quantum simulator CI
on:
  workflow_dispatch:
jobs:
  sim-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install Qiskit
        run: |
          pip install qiskit boto3
      - name: Run simulator smoke
        run: |
          python - <<'PY'
          from qiskit import QuantumCircuit, Aer, execute
          qc = QuantumCircuit(1,1)
          qc.h(0); qc.measure(0,0)
          backend = Aer.get_backend('qasm_simulator')
          job = execute(qc, backend=backend, shots=64)
          res = job.result().get_counts()
          print("Sim result:", res)
          PY
*** End Patch
*** Begin Patch
*** Add File:edge/runbooks/edge_hpc_quantum_runbook.md
# Edge / HPC / Quantum Operator Runbook (Aegis)

This runbook summarizes the operator commands and expected evidence keys produced by Edge, HPC and Quantum flows.

Edge (device enrollment & OTA)
- Enroll device:
  curl -X POST http://device-registry.aegis.svc.cluster.local:8081/devices/enroll -d '{"device_id":"device-001","public_key_pem":"..."}'
  Evidence: devices table and telemetry rows in device DB; device enroll events captured in audit package under agents/device-events/

- Publish OTA manifest:
  1) Build bundle: edge/packaging/build_edge_bundle.sh ./model/slm.quant.onnx ./out v0.1 groupA
  2) Upload bundle to storage (S3) and upload manifest.json
  3) Sign artifact with cosign (operator)
  4) Publish manifest to OTA server (or upload file to MANIFEST_STORE)
  Evidence keys: s3://<EVIDENCE_BUCKET>/ota/groupA/manifest-v0.1.json and s3://<EVIDENCE_BUCKET>/ota/groupA/bundle-v0.1.tar.gz.sig

- Device applies update:
  Devices verify signature via cosign and write audit evidence to S3 under agents/<device_id>/updates/

HPC (Slurm Deepspeed runs)
- Submit job:
  sbatch hpc/slurm/job_template.sbatch
  Evidence: /local/scratch/<JOBID>/checkpoints -> shards uploaded to s3://<EVIDENCE_BUCKET>/hpc/checkpoints/<JOBID>/manifest.json

- Resume job:
  hpc/resume/resume_helper.py --manifest-key hpc/checkpoints/<JOBID>/manifest.json --out /local/scratch/checkpoint
  Evidence: s3 shard keys, manifest.json

Quantum
- Submit quantum job:
  1) Operator uploads signed job descriptor to S3: s3://<EVIDENCE_BUCKET>/quantum/descriptors/<jobid>.json
  2) Enqueue via API: POST /quantum/enqueue {job_id, descriptor_s3_key}
  Evidence keys: s3://<EVIDENCE_BUCKET>/quantum/evidence/<jobid>_manifest.json and results at s3://<EVIDENCE_BUCKET>/quantum/results/<jobid>.json

Audit packaging
- Use compliance/audit/audit_package_collector.py to collect evidence prefixes and create tarball:
  python compliance/audit/audit_package_collector.py <run_id>
- Expected audit package location:
  s3://<EVIDENCE_BUCKET>/audit-packages/<run_id>_package_<ts>.tar.gz
  Signature: .sig and optional Rekor entry

*** End Patch
*** End Patch
