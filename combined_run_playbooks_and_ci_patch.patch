*** Begin Patch
*** Add File: configs/accelerate_multi_node.yaml
+compute_environment: "MULTI_NODE"
+distributed_type: MULTI_GPU
+downcast_bfloat16: "no"
+mixed_precision: "fp16"
+machine_rank: 0
+main_process_ip: "MAIN_PROCESS_IP_PLACEHOLDER"  # replace at job launch with actual launcher IP / service DNS
+main_process_port: 29500
+num_machines: 2               # default for staging multi-node pilot; override at launch
+num_processes: 2              # processes per machine (set to GPUs per node)
+use_cpu: false
+fsdp: "no"
+# Performance tuning hints (tweak for your hardware):
+# - mixed_precision: fp16 for NVIDIA Ampere/A100; set to bf16 for supported hardware if desired.
+# - gradient_accumulation: handled in training args; for small GPUs increase accumulation instead of batch size.
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/rlhf_full_staging.yml
+name: RLHF Full Staging Run (multi-node)
+on:
+  workflow_dispatch:
+    inputs:
+      mode:
+        description: "run-mode: 'k8s' to submit k8s Job, 'local' to run accelerate in runner (small)"
+        required: true
+        default: "local"
+      num_machines:
+        description: "Number of machines for multi-node (k8s mode)"
+        required: false
+        default: "2"
+      num_processes:
+        description: "Processes per machine (GPUs per node)"
+        required: false
+        default: "2"
+      model_name:
+        required: false
+        default: "distilgpt2"
+
+jobs:
+  run:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python and tools
+        run: |
+          python -m pip install --upgrade pip
+          pip install accelerate transformers datasets mlflow boto3
+      - name: Run staging RLHF
+        env:
+          MODE: ${{ github.event.inputs.mode }}
+          NUM_MACHINES: ${{ github.event.inputs.num_machines }}
+          NUM_PROCESSES: ${{ github.event.inputs.num_processes }}
+          MODEL_NAME: ${{ github.event.inputs.model_name }}
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+        run: |
+          if [ "${MODE}" = "k8s" ]; then
+            echo "Submitting k8s job..."
+            # Patch job with provided machine/process counts and model
+            kubectl apply -f k8s/rlhf/accelerate_distributed_job.yaml
+            # Wait for job completion (basic)
+            kubectl wait --for=condition=complete job/rlhf-accelerate-distributed -n aegis-ml --timeout=2h
+            echo "K8s job completed"
+          else
+            echo "Running local accelerate small distributed run (2 processes) as a smoke test..."
+            python -m pip install --upgrade accelerate
+            accelerate launch --num_processes 2 rl/distributed_train.py --model-name "${MODEL_NAME}" --output-dir "/tmp/rlhf_smoke" --epochs 1 --per-device-batch-size 1 --save-steps 10
+            echo "Local accelerate run finished"
+          fi
+      - name: Collect artifacts and validate checkpoint
+        if: success()
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          # Try to find produced checkpoint (either local artifact or in mlflow)
+          CKPT=$(ls /tmp/rlhf_smoke/*.tar.gz 2>/dev/null || true)
+          if [ -n "$CKPT" ]; then
+            echo "Found local checkpoint: $CKPT"
+            python rl/checkpoint_validate.py --ckpt "$CKPT"
+          else
+            echo "No local tarball found; try to inspect MLflow latest run"
+            python - <<PY
+import mlflow, os, json
+mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI"))
+client = mlflow.tracking.MlflowClient()
+runs = client.search_runs(experiment_ids=None, filter_string="", max_results=10)
+print("Recent runs:", [r.info.run_id for r in runs])
+PY
+
*** End Patch
*** Begin Patch
*** Add File: docs/rlhf_operator_playbook.md
+# RLHF Operator Playbook (Staging multi-node)
+
+Purpose
+- Guide for operators to run, monitor, resume, and validate distributed RLHF runs in staging.
+
+Prereqs
+- K8s staging cluster with GPU nodepool and network connectivity for multi-node (Rendezvous).
+- MLflow tracking server reachable from training pods.
+- Image aegis/rlhf:latest built with CUDA, torch, accelerate and required Python deps.
+- Secrets: mlflow-secrets, aegis-secrets (artifact bucket) present in aegis-ml namespace.
+
+Run (k8s mode)
+1. Update k8s/rlhf/accelerate_distributed_job.yaml env:
+   - set NUM_MACHINES to desired node count
+   - set NUM_PROCESSES to GPUs per node (or processes per node)
+   - set MAIN_PROCESS_IP to the launcher's pod IP or use a headless service; for single-node leave blank.
+   - set MODEL_NAME, OUTPUT_DIR as needed.
+2. Apply job:
+   kubectl apply -f k8s/rlhf/accelerate_distributed_job.yaml -n aegis-ml
+3. Monitor logs:
+   kubectl logs job/rlhf-accelerate-distributed -n aegis-ml --follow
+4. On completion, retrieve MLflow run artifacts:
+   - Inspect MLflow UI for run metadata, params, and artifacts (checkpoint tar).
+5. Validate checkpoint locally or via rl/checkpoint_validate.py and run adversarial harness before promoting.
+
+Run (local/CI smoke)
+1. Use GitHub Action RLHF Full Staging Run (mode=local) or run accelerate locally:
+   accelerate launch --num_processes 2 rl/distributed_train.py --model-name <model> --output-dir /tmp/rlhf_smoke --epochs 1
+2. Validate checkpoint: python rl/checkpoint_validate.py --ckpt /tmp/rlhf_smoke/<checkpoint>.tar.gz
+3. Run adversarial harness: python safety/adversarial_harness.py
+
+Resuming from checkpoint
+- Trainer.resume_from_checkpoint is the recommended path. Provide the model directory or tar extracted path in future training args.
+- For now, extract checkpoint tar into an accessible S3 path and update training script to load from that path (implementation task).
+
+Troubleshooting
+- If workers cannot reach the launcher: check network policies and pod DNS; set MAIN_PROCESS_IP to a reachable IP/Service.
+- OOMs: reduce per-device batch size and enable gradient accumulation.
+- If MLflow fails to receive artifacts: verify credentials and bucket policy.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/billing/alert_reconcile_cronjob.yaml
+apiVersion: batch/v1beta1
+kind: CronJob
+metadata:
+  name: billing-reconcile
+  namespace: aegis
+spec:
+  schedule: "*/15 * * * *"
+  concurrencyPolicy: Forbid
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: reconcile
+              image: aegis/billing-tools:latest
+              command: ["python","/opt/billing/alert_reconcile_worker.py"]
+              env:
+                - name: PROM_URL
+                  value: "http://prometheus.monitoring.svc:9090"
+                - name: COMPLIANCE_BUCKET
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-secrets
+                      key: compliance-bucket
+                - name: OPERATOR_NOTIFY_WEBHOOK
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-secrets
+                      key: operator-webhook
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: docs/billing_mitigation_test_plan.md
+# Billing & Throttle Mitigation Test Plan
+
+Objectives
+- Validate gateway quota enforcement under high load.
+- Verify Prometheus metrics and billing_reconcile alerts trigger.
+- Verify auto_throttle_manager marks models throttled and that gateway rejects requests when throttled.
+
+Test components
+- Gateway + quota middleware (llm/gateway_quota_middleware.py)
+- Redis for token buckets
+- Prometheus metrics scrapped (aegis_gateway_tokens_consumed_total, aegis_gateway_tokens_rejected_total)
+- auto_throttle_manager service
+- Billing reconcile CronJob
+
+Steps
+1. Baseline: ensure gateway healthy and quotas exist for test model "test-model".
+2. Run request storm:
+   - Use GitHub Action Cost Control Simulation (workflow) or run tests/simulate_gateway_requests.py targeting gateway endpoint.
+   - Observe Prometheus counters for tokens consumed.
+3. Observe billing_reconcile CronJob logs and verify no anomalies flagged initially.
+4. Force high sustained token rate (>THRESH_TOKENS_PER_SEC) to trigger auto_throttle_manager:
+   - Run simulate_gateway_requests with high concurrency and volume.
+   - Verify auto_throttle_manager writes model:throttle:{model} and sends compliance record.
+5. Confirm gateway now rejects requests for throttled model (HTTP 429 from quota_dependency).
+6. Cooldown & recovery:
+   - Stop request storm, wait COOLDOWN_SEC.
+   - auto_throttle_manager should clear throttle and gateway accepts requests again.
+
+Validation & Acceptance
+- Gateway rejects requests (429) while model throttled.
+- Billing reconcile produced alerts when usage spiked.
+- auto_throttle_manager recorded throttle/unthrottle events to COMPLIANCE_BUCKET and notified operator webhook.
+- Document results in compliance artifacts and remediate any false positives.
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/billing_mitigation_simulation.yml
+name: Billing Mitigation Simulation
+on:
+  workflow_dispatch:
+    inputs:
+      gateway_url:
+        required: true
+      model:
+        required: false
+        default: "test-model"
+      tenant:
+        required: false
+        default: "tenantA"
+
+jobs:
+  simulate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests
+      - name: Run request storm against gateway
+        run: |
+          python tests/simulate_gateway_requests.py --url "${{ github.event.inputs.gateway_url }}" --model "${{ github.event.inputs.model }}" --tenant "${{ github.event.inputs.tenant }}" --concurrency 100 --requests 5000
+      - name: Sleep to allow throttle manager to act
+        run: sleep 120
+      - name: Query throttle state in Redis (optional)
+        env:
+          REDIS_URL: ${{ secrets.REDIS_URL }}
+        run: |
+          python - <<PY
+import redis,os
+r=redis.from_url(os.environ["REDIS_URL"])
+print("Throttled keys:", r.keys("model:throttle:*"))
+PY
+
*** End Patch
*** Begin Patch
*** Add File: docs/quantum_operator_playbook.md
+# Quantum Staging Operator Playbook
+
+Purpose
+- Steps for an operator to provision provider credentials in Vault, run an approved staging QPU job, and reconcile provider invoices.
+
+Prereqs
+- Vault admin token to store provider creds.
+- Approval Orchestrator and Approval UI available.
+- quantum/submit_service and quantum/job_queue deployed in staging.
+- COMPLIANCE_BUCKET configured for evidence.
+
+Provision provider credentials
+1. Run vault/scripts/store_real_provider_creds.sh on an operator host:
+   VAULT_ADDR=https://vault.example VAULT_TOKEN=... ./vault/scripts/store_real_provider_creds.sh
+2. Confirm secrets stored at secret/aegis/quantum/ibm and secret/aegis/quantum/braket.
+
+Submit & approve staging job
+1. Create an approval request via Approval UI or CLI:
+   curl -X POST $APPROVAL_API/submit -d '{"plan":{"type":"qpu_run","backend":"ibm","tenant":"staging","qasm":"<...>"} }'
+2. Operator approves via Approval Orchestrator UI (or use approval_orchestrator API: /approve/{id}).
+3. After approval, run submit_and_wait.py (quantum/staging/submit_and_wait.py) to submit job and monitor until dispatched.
+
+Reconcile invoices
+1. After job completion, run:
+   python quantum/reconcile_provider_billing.py
+2. The script writes reconciliation reports to COMPLIANCE_BUCKET and audit_indexer.
+3. If discrepancies found, open a remediation issue and attach evidence.
+
+Evidence & Audit
+- Ensure each job has audit entries in OpenSearch (aegis-audit).
+- Store provider billing receipts in COMPLIANCE_BUCKET and link them to reconciliation report.
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/quantum_reconcile_and_report.yml
+name: Quantum Reconcile & Report (post-run)
+on:
+  workflow_dispatch:
+    inputs:
+      run_id:
+        required: true
+
+jobs:
+  reconcile:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install boto3 elasticsearch
+      - name: Run reconciliation
+        env:
+          ES_HOST: ${{ secrets.ES_HOST }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python quantum/reconcile_provider_billing.py
+      - name: Upload reconciliation artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: quantum-reconcile-report-${{ github.event.inputs.run_id }}
+          path: /tmp  # reconcile script writes reports to /tmp by default
+
*** End Patch
*** End Patch
