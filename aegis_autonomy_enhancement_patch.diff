diff --git a/policy/opa/agents_full_runtime.rego b/policy/opa/agents_full_runtime.rego
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/policy/opa/agents_full_runtime.rego
@@ -0,0 +1,300 @@
+package aegis.policies.agents_full_runtime
+
+import data.models
+import future.keywords.ifelse
+
+default decision = {"allow": false, "defer": false, "reason": "no-match"}
+
+# Data-driven runtime thresholds (set via OPA data)
+# data.runtime_thresholds = {
+#   "canary_min_pass_rate": 0.95,
+#   "p95_default": 1.0,
+#   "error_rate_default": 0.01,
+#   "cost_fraction_for_auto_approve": 0.10,
+#   "budget_burn_fraction_for_reject": 0.5
+# }
+
+budget_for_model[m] = b {
+  bm := data.models.models[m]
+  b := bm.budget_usd_per_month
+}
+budget_for_model[m] = 1000 {
+  not data.models.models[m]
+}
+
+canary_min_pass_rate := data.runtime_thresholds.canary_min_pass_rate
+p95_default(model) = t {
+  t = data.runtime_thresholds.p95_default
+}
+error_rate_default := data.runtime_thresholds.error_rate_default
+cost_fraction := data.runtime_thresholds.cost_fraction_for_auto_approve
+budget_burn_fraction := data.runtime_thresholds.budget_burn_fraction_for_reject
+
+eligible_auto_actions = {"scale_deployment", "restart_deployment", "retrain_non_critical", "generate_low_risk"}
+high_risk_actions = {"promote", "snapshot", "restore", "delete_data"}
+
+input_has_metrics {
+  input.metrics != null
+}
+
+canary_ok {
+  input.metrics.canary_pass_rate != null
+  input.metrics.canary_pass_rate >= canary_min_pass_rate
+}
+
+slo_ok {
+  input.metrics.p95_latency != null
+  input.metrics.error_rate != null
+  input.metrics.p95_latency <= p95_default(input.model)
+  input.metrics.error_rate <= error_rate_default
+}
+
+cost_within_fraction {
+  input.params.estimated_cost_usd != null
+  b := budget_for_model[input.model]
+  input.params.estimated_cost_usd <= (b * cost_fraction)
+}
+
+budget_burn_ok {
+  # If month-to-date burn is provided, do not allow if burn fraction exceeded
+  not input.metrics.budget_month_to_date_usd
+}
+budget_burn_ok {
+  b := budget_for_model[input.model]
+  burn := input.metrics.budget_month_to_date_usd
+  burn <= (b * budget_burn_fraction)
+}
+
+is_eligible_auto {
+  input.action == a
+  a := eligible_auto_actions[_]
+}
+
+is_high_risk {
+  input.action == a
+  a := high_risk_actions[_]
+}
+
+# If action eligible and all runtime checks pass -> auto-approve
+auto_approve {
+  is_eligible_auto
+  input_has_metrics
+  canary_ok
+  slo_ok
+  cost_within_fraction
+  budget_burn_ok
+}
+
+# If eligible but missing metrics or marginal metrics -> defer for human signoff (time-box)
+defer_for_signoff {
+  is_eligible_auto
+  not auto_approve
+}
+
+# If high risk -> deny unless explicit approver present in input.params.approved_by
+explicit_approve_required {
+  is_high_risk
+  input.params.approved_by == null
+}
+
+decision = {"allow": true, "defer": false, "reason": "auto-approved by runtime policy", "meta": {"auto": true}} {
+  auto_approve
+}
+
+decision = {"allow": false, "defer": true, "reason": "deferred: runtime metrics missing or marginal", "timeout_seconds": 3600} {
+  defer_for_signoff
+}
+
+decision = {"allow": false, "defer": false, "reason": "explicit approval required (high risk)"} {
+  explicit_approve_required
+}
+
+decision = {"allow": false, "defer": false, "reason": "denied: action not eligible for auto-approval"} {
+  not auto_approve
+  not defer_for_signoff
+  not explicit_approve_required
+}
+
+# Helper: produce a structured audit hint for orchestration
+audit_hint[h] {
+  h := {
+    "policy": "agents_full_runtime",
+    "auto_approve": auto_approve,
+    "defer": defer_for_signoff,
+    "explicit_required": explicit_approve_required
+  }
+}
+
diff --git a/services/executor_service/token_rotator.py b/services/executor_service/token_rotator.py
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/services/executor_service/token_rotator.py
@@ -0,0 +1,220 @@
+#!/usr/bin/env python3
+"""
+Token rotator:
+ - Generates short-lived JWTs for orchestrators to call the executor service
+ - Signs tokens with a private key (stored in K8s secret or Vault)
+ - Writes tokens into a Kubernetes secret or outputs them to stdout for CI
+ - Intended to run as a CronJob or rotation controller with low privileges
+"""
+import os
+import time
+import jwt
+from datetime import datetime, timedelta
+
+PRIV_KEY_PATH = os.environ.get("PRIV_KEY_PATH", "/etc/executor-keys/priv.pem")
+K8S_SECRET_NAME = os.environ.get("ROTATED_SECRET_NAME", "aegis-executor-rotated-token")
+K8S_NAMESPACE = os.environ.get("ROTATED_SECRET_NAMESPACE", "aegis-ml")
+TOKEN_LIFETIME_SECONDS = int(os.environ.get("TOKEN_LIFETIME_SECONDS", "300"))  # short lived (5m)
+
+def load_privkey():
+    with open(PRIV_KEY_PATH, "rb") as fh:
+        return fh.read()
+
+def generate_token(privkey_pem, subject="orchestrator", aud="aegis-executor", lifetime=TOKEN_LIFETIME_SECONDS):
+    now = int(time.time())
+    payload = {
+        "sub": subject,
+        "iat": now,
+        "exp": now + lifetime,
+        "aud": aud,
+        "iss": "aegis-token-rotator"
+    }
+    token = jwt.encode(payload, privkey_pem, algorithm="RS256")
+    return token
+
+def write_to_k8s_secret(token):
+    try:
+        from kubernetes import client, config
+        try:
+            config.load_incluster_config()
+        except Exception:
+            config.load_kube_config()
+        v1 = client.CoreV1Api()
+        secret_body = client.V1Secret(metadata=client.V1ObjectMeta(name=K8S_SECRET_NAME), string_data={"token": token.decode() if isinstance(token, bytes) else token})
+        try:
+            v1.replace_namespaced_secret(K8S_SECRET_NAME, K8S_NAMESPACE, secret_body)
+        except Exception:
+            v1.create_namespaced_secret(K8S_NAMESPACE, secret_body)
+        print("Wrote token to secret", f"{K8S_NAMESPACE}/{K8S_SECRET_NAME}")
+    except Exception as e:
+        print("k8s write failed:", e)
+
+def main():
+    priv = load_privkey()
+    token = generate_token(priv)
+    if os.environ.get("WRITE_TO_K8S", "true").lower() in ("1","true","yes"):
+        write_to_k8s_secret(token)
+    else:
+        print(token)
+
+if __name__ == "__main__":
+    main()
+
diff --git a/k8s/manifests/executor_mtls_and_auth.yaml b/k8s/manifests/executor_mtls_and_auth.yaml
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/k8s/manifests/executor_mtls_and_auth.yaml
@@ -0,0 +1,220 @@
+apiVersion: v1
+kind: Secret
+metadata:
+  name: aegis-executor-keys
+  namespace: aegis-ml
+type: Opaque
+data:
+  priv.pem: "<base64-encoded-privkey>"    # rotor will mount this
+  pub.pem: "<base64-encoded-pubkey>"
+
+---
+# Example Istio ServiceMesh annotations to enforce mTLS for executor service (if Istio is present)
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-executor
+  namespace: aegis-ml
+spec:
+  template:
+    metadata:
+      annotations:
+        sidecar.istio.io/inject: "true"
+        traffic.sidecar.istio.io/excludeInboundPorts: "15000"
+        proxy.istio.io/config: '{"holdApplicationUntilProxyStarts": true}'
+    spec:
+      containers:
+        - name: executor
+          image: <REGISTRY>/aegis-executor:latest
+          volumeMounts:
+            - name: keys
+              mountPath: /etc/executor-keys
+              readOnly: true
+      volumes:
+        - name: keys
+          secret:
+            secretName: aegis-executor-keys
+
+---
+# Example PeerAuthentication (Istio) to enforce mTLS for executor
+apiVersion: "security.istio.io/v1beta1"
+kind: "PeerAuthentication"
+metadata:
+  name: "aegis-executor-mtls"
+  namespace: aegis-ml
+spec:
+  selector:
+    matchLabels:
+      app: aegis-executor
+  mtls:
+    mode: STRICT
+
diff --git a/tools/chaos_matrix_runner.sh b/tools/chaos_matrix_runner.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/tools/chaos_matrix_runner.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Run chaos/rollback tests across multiple topologies and failure modes
+# Collects reports per run and summarizes PASS/FAIL
+
+set -euo pipefail
+
+TOPOLOGIES="${TOPOLOGIES:-4,8}"
+FAILURE_MODES="${FAILURE_MODES:-pod_kill,node_drain,inject_latency}"
+ROUNDS="${ROUNDS:-1}"
+ARTIFACT_DIR="${ARTIFACT_DIR:-./chaos_reports}"
+
+mkdir -p "${ARTIFACT_DIR}"
+
+IFS=',' read -r -a TOPS <<< "$TOPOLOGIES"
+IFS=',' read -r -a MODES <<< "$FAILURE_MODES"
+
+SUMMARY="${ARTIFACT_DIR}/summary_$(date +%Y%m%d%H%M%S).json"
+echo "[]" > "$SUMMARY"
+
+for topo in "${TOPS[@]}"; do
+  for mode in "${MODES[@]}"; do
+    for r in $(seq 1 "$ROUNDS"); do
+      run_dir="${ARTIFACT_DIR}/topo_${topo}/${mode}/round_${r}"
+      mkdir -p "$run_dir"
+      echo "=== Run topo=${topo} mode=${mode} round=${r} -> ${run_dir} ==="
+      # Optionally scale cluster/workloads here (operator responsibility)
+      # Invoke extended chaos test script with parameters
+      if python3 tools/chaos_rollback_test.py --mode "$mode" --topo "$topo" > "${run_dir}/raw_output.log" 2>&1; then
+        status="PASS"
+      else
+        status="FAIL"
+      fi
+      # capture decision_log snippet
+      if command -v kubectl >/dev/null 2>&1; then
+        kubectl -n aegis-ml get events --sort-by='.lastTimestamp' -o json > "${run_dir}/k8s_events.json" || true
+      fi
+      # append to summary
+      python3 - <<PY
+import json,sys,os
+summary_file = "$SUMMARY"
+entry = {"topo": "$topo", "mode":"$mode", "round": $r, "run_dir": "$run_dir", "status":"$status"}
+arr = json.load(open(summary_file))
+arr.append(entry)
+open(summary_file,"w").write(json.dumps(arr,indent=2))
+print("Appended", entry)
+PY
+    done
+  done
+done
+
+echo "Chaos matrix run complete; summary at $SUMMARY"
+cat "$SUMMARY"
+exit 0
+
diff --git a/tools/chaos_rollback_test.py b/tools/chaos_rollback_test.py
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/tools/chaos_rollback_test.py
@@ -0,0 +1,340 @@
+#!/usr/bin/env python3
+"""
+Extended chaos & rollback test harness.
+Supports multiple failure modes:
+ - pod_kill: deletes a pod from the target deployment
+ - node_drain: cordons/drains a node hosting the target pods
+ - inject_latency: annotates deployment to enable ingress latency (requires app opt-in)
+
+The script then waits for compensation_controller/compensation actions and validates decision_log entries.
+"""
+import argparse
+import time
+import subprocess
+import json
+import os
+
+try:
+    from tools.decisionlog_client import query_decision_log_last
+except Exception:
+    def query_decision_log_last(limit=20):
+        print("decision_log_client missing; returning empty list")
+        return []
+
+NAMESPACE = os.environ.get("NAMESPACE", "aegis-ml")
+TARGET_DEPLOYMENT = os.environ.get("TARGET_DEPLOYMENT", "aegis-vllm")
+SLEEP_AFTER_INJECTION = int(os.environ.get("SLEEP_AFTER_INJECTION", "120"))
+
+def run_cmd(cmd):
+    print("RUN:", cmd)
+    return subprocess.call(cmd, shell=True)
+
+def pod_kill():
+    out = subprocess.check_output(["kubectl","-n",NAMESPACE,"get","pods","-l","app=aegis-vllm","-o","jsonpath={.items[*].metadata.name}"])
+    pods = out.decode().strip().split()
+    if not pods:
+        print("No pods found")
+        return False
+    victim = pods[0]
+    print("Deleting pod", victim)
+    run_cmd(f"kubectl -n {NAMESPACE} delete pod {victim} --grace-period=0 --force")
+    return True
+
+def node_drain():
+    # pick a node with one of the target pods
+    out = subprocess.check_output(["kubectl","-n",NAMESPACE,"get","pods","-l","app=aegis-vllm","-o","jsonpath={.items[0].spec.nodeName}"])
+    node = out.decode().strip()
+    if not node:
+        print("No node found")
+        return False
+    print("Cordoning node", node)
+    run_cmd(f"kubectl cordon {node}")
+    print("Draining node", node)
+    run_cmd(f"kubectl drain {node} --ignore-daemonsets --delete-local-data --force --timeout=120s")
+    # uncordon at end
+    return True
+
+def inject_latency():
+    # This is application-specific: we annotate the deployment and the app must implement fault injection
+    print("Annotating deployment to inject latency")
+    run_cmd(f"kubectl -n {NAMESPACE} patch deployment {TARGET_DEPLOYMENT} -p '{{\"spec\":{{\"template\":{{\"metadata\":{{\"annotations\":{{\"aegis-inject-latency\":\"true\"}}}}}}}}}}'")
+    return True
+
+def revert_latency():
+    run_cmd(f"kubectl -n {NAMESPACE} patch deployment {TARGET_DEPLOYMENT} -p '{{\"spec\":{{\"template\":{{\"metadata\":{{\"annotations\":{{\"aegis-inject-latency\":null}}}}}}}}}}'")
+
+def wait_for_compensation_event(timeout=300):
+    start = time.time()
+    while time.time() - start < timeout:
+        entries = query_decision_log_last(50)
+        for e in entries:
+            s = json.dumps(e).lower()
+            if "compensation_controller" in s or "executor" in s:
+                print("Found compensation-related decision_log entry:", e)
+                return True
+        time.sleep(10)
+    return False
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--mode", choices=["pod_kill","node_drain","inject_latency"], default="pod_kill")
+    p.add_argument("--topo", type=int, default=4)
+    p.add_argument("--timeout", type=int, default=300)
+    args = p.parse_args()
+
+    print("Chaos test mode:", args.mode)
+    success = False
+    try:
+        if args.mode == "pod_kill":
+            success = pod_kill()
+        elif args.mode == "node_drain":
+            success = node_drain()
+        elif args.mode == "inject_latency":
+            success = inject_latency()
+        else:
+            print("unknown mode")
+            return 2
+
+        if not success:
+            print("Injection failed to start")
+            return 2
+
+        print("Waiting after injection...")
+        time.sleep(SLEEP_AFTER_INJECTION)
+
+        print("Waiting for compensation/rollback event in decision_log")
+        if wait_for_compensation_event(timeout=args.timeout):
+            print("Compensation detected -> PASS")
+            return 0
+        else:
+            print("No compensation detected -> FAIL")
+            return 3
+    finally:
+        if args.mode == "inject_latency":
+            revert_latency()
+        if args.mode == "node_drain":
+            # attempt to uncordon nodes (best-effort)
+            try:
+                out = subprocess.check_output(["kubectl","-n",NAMESPACE,"get","pods","-l","app=aegis-vllm","-o","jsonpath={.items[*].spec.nodeName}"])
+                nodes = set(out.decode().strip().split())
+                for n in nodes:
+                    run_cmd(f"kubectl uncordon {n}")
+            except Exception:
+                pass
+
+if __name__ == "__main__":
+    import sys
+    sys.exit(main())
+
diff --git a/services/compensation_controller.py b/services/compensation_controller.py
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/services/compensation_controller.py
@@ -0,0 +1,360 @@
+#!/usr/bin/env python3
+"""
+Compensation controller with richer observability and correlation IDs.
+ - Monitors Prometheus for SLO breaches
+ - When triggered, emits correlation_id and calls executor to perform compensation
+ - Exposes a /metrics endpoint for Prometheus with counters and latencies
+ - Records decision_log entries with correlation_id
+"""
+import os
+import time
+import requests
+import uuid
+import json
+from flask import Flask, jsonify
+from time import monotonic
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus-operated.monitoring.svc.cluster.local:9090")
+EXECUTOR_URL = os.environ.get("EXECUTOR_URL", "http://aegis-executor.aegis-ml.svc.cluster.local:8080")
+CHECK_INTERVAL = int(os.environ.get("CHECK_INTERVAL", "30"))
+
+app = Flask(__name__)
+
+METRICS = {"compensation_invocations_total": 0, "last_invocation_latency_seconds": 0.0}
+
+def inc(name):
+    METRICS[name] = METRICS.get(name, 0) + 1
+
+def set_metric(name, val):
+    METRICS[name] = val
+
+SLO_QUERIES = {
+    "p95_latency_breach": "histogram_quantile(0.95, sum(rate(aegis_inference_request_latency_seconds_bucket[5m])) by (le)) > 1.0",
+}
+
+try:
+    from tools.decisionlog_client import insert_decision
+except Exception:
+    def insert_decision(agent, action, payload, evidence):
+        print("[decision_log stub]", agent, action, payload, evidence)
+        return None
+
+def prom_query(expr):
+    try:
+        r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr}, timeout=10)
+        r.raise_for_status()
+        data = r.json()
+        if data.get("status") == "success":
+            return data.get("data", {}).get("result", [])
+    except Exception as e:
+        print("Prometheus query failed:", e)
+    return []
+
+def call_executor(payload, correlation_id):
+    headers = {"Content-Type": "application/json", "X-Correlation-ID": correlation_id}
+    try:
+        start = monotonic()
+        r = requests.post(f"{EXECUTOR_URL}/execute", json=payload, headers=headers, timeout=20)
+        latency = monotonic() - start
+        set_metric("last_invocation_latency_seconds", latency)
+        inc("compensation_invocations_total")
+        return r.json()
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+@app.route("/metrics")
+def metrics():
+    lines = []
+    lines.append(f'aegis_compensation_invocations_total {METRICS.get("compensation_invocations_total",0)}')
+    lines.append(f'aegis_compensation_last_latency_seconds {METRICS.get("last_invocation_latency_seconds",0.0)}')
+    return "\n".join(lines), 200, {"Content-Type": "text/plain; version=0.0.4"}
+
+def main_loop():
+    while True:
+        for name, q in SLO_QUERIES.items():
+            res = prom_query(q)
+            if res:
+                correlation_id = str(uuid.uuid4())
+                print(f"SLO breach {name}; invoking compensation with correlation_id={correlation_id}")
+                payload = {
+                    "action": "restart_deployment",
+                    "params": {"namespace": "aegis-ml", "name": "aegis-vllm"},
+                    "metadata": {"reason": f"auto_compensation_{name}", "correlation_id": correlation_id},
+                }
+                insert_decision("compensation_controller", "invoke_executor", {"slo": name, "correlation_id": correlation_id}, {"payload": payload})
+                resp = call_executor(payload, correlation_id)
+                insert_decision("compensation_controller", "executor_result", {"slo": name, "correlation_id": correlation_id}, resp)
+        time.sleep(CHECK_INTERVAL)
+
+if __name__ == "__main__":
+    # run as a long-running process (or container)
+    main_loop()
+
diff --git a/monitoring/prometheus/auto_approval_metrics_rules.yaml b/monitoring/prometheus/auto_approval_metrics_rules.yaml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/monitoring/prometheus/auto_approval_metrics_rules.yaml
@@ -0,0 +1,200 @@
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-auto-approval-metrics
+  namespace: aegis-ml
+spec:
+  groups:
+    - name: aegis.auto_approval.metrics
+      rules:
+        - record: aegis:executor:executions_total:rate5m
+          expr: increase(aegis_executor_executions_total[5m])
+        - record: aegis:executor:denials_total:rate5m
+          expr: increase(aegis_executor_denials_total[5m])
+        - alert: AutoApprovalsErrorSpike
+          expr: increase(aegis_executor_denials_total[15m]) > 10
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Spike in auto-approval denials"
+
diff --git a/ui/auditor_dashboard_enhanced.py b/ui/auditor_dashboard_enhanced.py
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/ui/auditor_dashboard_enhanced.py
@@ -0,0 +1,420 @@
+#!/usr/bin/env python3
+"""
+Auditor UI (production-ready prototype)
+ - JWT OIDC authentication (JWKS)
+ - Pagination and search (by model/action/status)
+ - RBAC groups (auditor, admin) via token claims 'groups'
+ - Bulk approve endpoint
+
+Env:
+ - DECISIONLOG_DB_DSN
+ - AUTH_JWKS_URL
+ - APPROVER_GROUP (defaults to 'auditors')
+"""
+import os
+import json
+import time
+from flask import Flask, jsonify, request, abort
+import psycopg2
+import psycopg2.extras
+import requests
+import jwt
+
+DSN = os.environ.get("DECISIONLOG_DB_DSN")
+AUTH_JWKS_URL = os.environ.get("AUTH_JWKS_URL")
+APPROVER_GROUP = os.environ.get("APPROVER_GROUP", "auditors")
+PAGE_SIZE_DEFAULT = int(os.environ.get("PAGE_SIZE", "50"))
+
+app = Flask(__name__)
+
+def get_conn():
+    if not DSN:
+        raise RuntimeError("DECISIONLOG_DB_DSN not set")
+    return psycopg2.connect(DSN, cursor_factory=psycopg2.extras.RealDictCursor)
+
+JWKS_CACHE = {"jwks": None, "fetched": 0}
+
+def get_jwks():
+    if JWKS_CACHE["jwks"] and (time.time() - JWKS_CACHE["fetched"] < 300):
+        return JWKS_CACHE["jwks"]
+    if not AUTH_JWKS_URL:
+        raise RuntimeError("AUTH_JWKS_URL not configured")
+    r = requests.get(AUTH_JWKS_URL, timeout=5)
+    r.raise_for_status()
+    JWKS_CACHE["jwks"] = r.json()
+    JWKS_CACHE["fetched"] = time.time()
+    return JWKS_CACHE["jwks"]
+
+def verify_jwt_and_get_claims(auth_header):
+    if not auth_header or not auth_header.startswith("Bearer "):
+        raise RuntimeError("missing bearer token")
+    token = auth_header.split(" ",1)[1]
+    jwks = get_jwks()
+    for key in jwks.get("keys", []):
+        try:
+            pub = jwt.algorithms.RSAAlgorithm.from_jwk(json.dumps(key))
+            claims = jwt.decode(token, pub, algorithms=[key.get("alg","RS256")], options={"verify_aud": False})
+            return claims
+        except Exception:
+            continue
+    raise RuntimeError("jwt verification failed")
+
+def require_approver(f):
+    def wrapper(*args, **kwargs):
+        auth = request.headers.get("Authorization", "")
+        try:
+            claims = verify_jwt_and_get_claims(auth)
+        except Exception:
+            abort(401)
+        groups = claims.get("groups") or claims.get("roles") or []
+        if isinstance(groups, str):
+            groups = [groups]
+        if APPROVER_GROUP not in groups and "admin" not in groups:
+            abort(403)
+        request.environ["aegis_user"] = claims.get("sub") or claims.get("email") or "unknown"
+        return f(*args, **kwargs)
+    wrapper.__name__ = f.__name__
+    return wrapper
+
+@app.route("/healthz")
+def healthz():
+    return "ok", 200
+
+@app.route("/pending")
+@require_approver
+def pending():
+    # pagination/search params
+    page = int(request.args.get("page", "1"))
+    size = int(request.args.get("size", str(PAGE_SIZE_DEFAULT)))
+    offset = (page-1)*size
+    model = request.args.get("model")
+    action = request.args.get("action")
+    status = request.args.get("status")  # deferred/pending/approved
+    qparts = []
+    params = []
+    base = "SELECT id, created_at, agent, payload, evidence FROM decision_log WHERE payload->>'action' IS NOT NULL"
+    if model:
+        qparts.append(" AND payload->>'model' = %s"); params.append(model)
+    if action:
+        qparts.append(" AND payload->>'action' = %s"); params.append(action)
+    if status:
+        if status == "deferred":
+            qparts.append(" AND (evidence->>'status' = 'deferred' OR evidence->>'status' IS NULL)")
+        elif status == "approved":
+            qparts.append(" AND evidence->>'status' = 'approved'")
+    query = base + " " + " ".join(qparts) + " ORDER BY created_at DESC LIMIT %s OFFSET %s"
+    params.extend([size, offset])
+    conn = get_conn()
+    with conn.cursor() as cur:
+        cur.execute(query, tuple(params))
+        rows = cur.fetchall()
+        # total count (simplified)
+        cur.execute("SELECT count(1) FROM decision_log WHERE payload->>'action' IS NOT NULL")
+        total = cur.fetchone()[0]
+    conn.close()
+    return jsonify({"items": rows, "page": page, "size": size, "total": total})
+
+@app.route("/bulk_approve", methods=["POST"])
+@require_approver
+def bulk_approve():
+    data = request.get_json(force=True)
+    ids = data.get("ids", [])
+    approver = request.environ.get("aegis_user", "auditor")
+    if not ids:
+        return jsonify({"ok": False, "error": "no ids"}), 400
+    conn = get_conn()
+    created = []
+    with conn.cursor() as cur:
+        for dec_id in ids:
+            cur.execute("INSERT INTO decision_log (agent, payload, evidence) VALUES (%s, %s, %s) RETURNING id", ("auditor-ui-bulk", json.dumps({"approved_id": dec_id, "approver": approver}), json.dumps({"status": "approved", "approver": approver})))
+            new_id = cur.fetchone()[0]
+            created.append(new_id)
+        conn.commit()
+    conn.close()
+    return jsonify({"ok": True, "created": created})
+
+@app.route("/approve", methods=["POST"])
+@require_approver
+def approve():
+    data = request.get_json(force=True)
+    dec_id = data.get("id")
+    approver = request.environ.get("aegis_user", "auditor")
+    if not dec_id:
+        return jsonify({"ok": False, "error": "id required"}), 400
+    conn = get_conn()
+    with conn.cursor() as cur:
+        cur.execute("INSERT INTO decision_log (agent, payload, evidence) VALUES (%s, %s, %s) RETURNING id", ("auditor-ui", json.dumps({"approved_id": dec_id, "approver": approver}), json.dumps({"status": "approved", "approver": approver})))
+        new_id = cur.fetchone()[0]
+        conn.commit()
+    conn.close()
+    return jsonify({"ok": True, "id": new_id})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8081")))
+
diff --git a/docs/PROD_AUTONOMY_ENHANCEMENTS.md b/docs/PROD_AUTONOMY_ENHANCEMENTS.md
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/docs/PROD_AUTONOMY_ENHANCEMENTS.md
@@ -0,0 +1,220 @@
+# Production Autonomy Enhancements â€” Summary & Runbook
+
+This patch contains the following focused enhancements:
+
+- Extended Rego runtime policy: policy/opa/agents_full_runtime.rego to evaluate canary pass rates, p95 latency, error_rate, cost burn and budget signals for auto-approval decisions.
+- Short-lived token rotation utility: services/executor_service/token_rotator.py to produce JWTs and write to Kubernetes secret for orchestrators to use.
+- Example mTLS/mesh annotations and key secret manifest: k8s/manifests/executor_mtls_and_auth.yaml for stricter in-cluster auth.
+- Chaos matrix runner and extended chaos test: tools/chaos_matrix_runner.sh + tools/chaos_rollback_test.py to exercise pod_kill, node_drain, inject_latency modes across topologies.
+- Compensation controller with correlation IDs + metrics (services/compensation_controller.py) to call executor and record invocations/latencies.
+- Observability rules: monitoring/prometheus/auto_approval_metrics_rules.yaml for recording rules and alerts.
+- Enhanced Auditor UI (ui/auditor_dashboard_enhanced.py) with JWT OIDC auth, pagination, search, RBAC groups and bulk approve API.
+
+Deployment checklist (recommended):
+1. Load OPA data.runtime_thresholds and data.models before enabling auto-approval policy.
+2. Deploy executor with mesh mTLS or short-lived token verification enabled. Ensure token rotator writes rotated tokens to secret used by orchestrator.
+3. Deploy compensation_controller and ensure Prometheus is reachable. Tune SLO queries and thresholds using compensation_tuner.py (not included in this patch).
+4. Deploy chaos runner in staging and run matrix across topologies (tools/chaos_matrix_runner.sh).
+5. Deploy enhanced Auditor UI behind OIDC; set APPROVER_GROUP to match your auditors group claim.
+
+Security notes:
+- Use JWKS (IdP) issued JWTs with audience=sub=aegis-executor and short TTL for orchestrator calls.
+- Prefer service mesh mTLS + strict network policy in addition to JWT/HMAC verification.
+- Limit ALLOWED_RESOURCES and RBAC to the minimal set of deployments the executor may act upon.
+
+End of runbook.
+
