*** Begin Patch
*** Add File: services/ingest/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN python -m pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt
+COPY etl_runner.py .
+EXPOSE 8081
+CMD ["python", "etl_runner.py"]
+
*** End Patch
*** Add File: services/ingest/requirements.txt
+boto3==1.26.XXX
+requests==2.31.0
+prometheus_client==0.16.0
+pillow==9.4.0
+pydub==0.25.1
+torch==2.0.1  # optional; used by conversion stubs
+
*** End Patch
*** Add File: services/ingest/etl_runner.py
+"""
+ETL runner for multimodal ingest (images/audio/text/video).
+- Reads a source manifest or connector configuration
+- Normalizes artifacts and uploads to object store (S3/GCS)
+- Emits dataset manifest (JSONL) and registers dataset in dataset_catalog.json
+- Emits Prometheus metrics for throughput/errors
+
+This is a small, pluggable process intended to be run as an Argo Workflow step or as a long-running service.
+"""
+import os
+import json
+import time
+import uuid
+import logging
+from prometheus_client import Counter, Gauge, start_http_server
+import boto3
+from PIL import Image
+from io import BytesIO
+import requests
+
+LOG = logging.getLogger("etl_runner")
+LOG.setLevel(logging.INFO)
+
+# Metrics
+INGESTED = Counter("aegis_ingest_items_total", "Total items ingested")
+INGEST_ERRORS = Counter("aegis_ingest_errors_total", "Total ingest errors")
+LAST_INGEST_TIME = Gauge("aegis_ingest_last_ts", "Last ingest timestamp")
+
+# Config via env
+MANIFEST_PATH = os.environ.get("MANIFEST_PATH", "data/ingest_sources.json")
+OUTPUT_BUCKET = os.environ.get("COMPLIANCE_BUCKET")  # expect s3://bucket/path or gs://...
+REGION = os.environ.get("AWS_REGION", "us-east-1")
+
+def s3_client():
+    return boto3.client("s3", region_name=REGION)
+
+def upload_to_s3(bucket, key, data, content_type="application/octet-stream"):
+    s3 = s3_client()
+    s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)
+    return f"s3://{bucket}/{key}"
+
+def normalize_image_bytes(url_or_path):
+    # supports http(s) URLs or local paths
+    try:
+        if url_or_path.startswith("http"):
+            r = requests.get(url_or_path, timeout=10)
+            r.raise_for_status()
+            img = Image.open(BytesIO(r.content)).convert("RGB")
+        else:
+            img = Image.open(url_or_path).convert("RGB")
+        buf = BytesIO()
+        img.save(buf, format="JPEG", quality=85)
+        return buf.getvalue()
+    except Exception as e:
+        LOG.exception("image normalization failed")
+        raise
+
+def ingest_text(text):
+    # minimal normalization (placeholder)
+    return text.strip()
+
+def run_once():
+    if not os.path.exists(MANIFEST_PATH):
+        LOG.info("No manifest found at %s", MANIFEST_PATH)
+        return
+    with open(MANIFEST_PATH) as f:
+        sources = json.load(f)
+    dataset_manifest = []
+    for item in sources:
+        try:
+            item_id = item.get("id", str(uuid.uuid4()))
+            modality = item.get("modality", "text")
+            if modality == "image":
+                data = normalize_image_bytes(item["uri"])
+                key = f"datasets/{item.get('dataset','default')}/{item_id}.jpg"
+                # parse s3://bucket/prefix
+                if OUTPUT_BUCKET and OUTPUT_BUCKET.startswith("s3://"):
+                    bucket = OUTPUT_BUCKET.split("s3://",1)[1].split("/",1)[0]
+                    prefix = "/".join(OUTPUT_BUCKET.split("s3://",1)[1].split("/")[1:])
+                    upload_to_s3(bucket, f"{prefix}/{key}", data, content_type="image/jpeg")
+                    uri = f"{OUTPUT_BUCKET.rstrip('/')}/{key}"
+                else:
+                    uri = item["uri"]
+                dataset_manifest.append({"id": item_id, "modality":"image", "uri": uri, "metadata": item.get("metadata",{})})
+            elif modality == "text":
+                txt = ingest_text(item.get("text","") or requests.get(item["uri"]).text)
+                # store manifest entry referencing external or S3 location
+                dataset_manifest.append({"id": item_id, "modality":"text", "text": txt, "uri": item.get("uri","")})
+            else:
+                LOG.info("unsupported modality %s", modality)
+                continue
+            INGESTED.inc()
+            LAST_INGEST_TIME.set(time.time())
+        except Exception:
+            INGEST_ERRORS.inc()
+            LOG.exception("ingest failed for item: %s", item)
+    # write manifest
+    ds_name = sources[0].get("dataset", "default") if sources else "default"
+    out_path = f"/tmp/{ds_name}_manifest.jsonl"
+    with open(out_path, "w") as out:
+        for d in dataset_manifest:
+            out.write(json.dumps(d))
+            out.write("\n")
+    # register dataset in simple catalog
+    catalog_path = os.environ.get("DATASET_CATALOG", "/data/dataset_catalog.json")
+    catalog = {}
+    if os.path.exists(catalog_path):
+        with open(catalog_path) as c:
+            catalog = json.load(c)
+    catalog[ds_name] = {"manifest": out_path, "count": len(dataset_manifest), "ts": int(time.time())}
+    os.makedirs(os.path.dirname(catalog_path) or ".", exist_ok=True)
+    with open(catalog_path, "w") as c:
+        json.dump(catalog, c, indent=2)
+    LOG.info("Ingest complete: wrote manifest %s entries=%d", out_path, len(dataset_manifest))
+
+if __name__ == "__main__":
+    start_http_server(9001)
+    run_once()
+    LOG.info("ETL runner finished")
+
*** End Patch
*** Add File: argo/workflows/ingest_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: ingest-multimodal
+  namespace: staging
+spec:
+  entrypoint: ingest
+  templates:
+  - name: ingest
+    steps:
+    - - name: etl
+        template: etl-runner
+  - name: etl-runner
+    container:
+      image: ghcr.io/yourorg/ingest:latest
+      command: ["/bin/sh","-c"]
+      args: ["python /app/etl_runner.py"]
+      env:
+      - name: MANIFEST_PATH
+        value: "/workspace/manifest.json"
+    volumeMounts:
+      - name: workspace
+        mountPath: /workspace
+  volumes:
+  - name: workspace
+    emptyDir: {}
+
*** End Patch
*** Add File: services/labeling/requirements.txt
+Flask==2.2.5
+boto3==1.26.XXX
+requests==2.31.0
+
*** End Patch
*** Add File: services/labeling/labeling_service.py
+"""
+Minimal labeling service backend:
+- Provides labeling tasks from dataset manifest
+- Stores labels in SQLite and pushes finalized dataset to COMPLIANCE_BUCKET
+This is a simple stub to connect to a labeling UI (not included here).
+"""
+import os, sqlite3, json, time
+from flask import Flask, request, jsonify
+import boto3
+
+DB = os.environ.get("LABEL_DB", "/data/labels.db")
+CATALOG = os.environ.get("DATASET_CATALOG", "/data/dataset_catalog.json")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+app = Flask("labeling")
+
+def init_db():
+    os.makedirs(os.path.dirname(DB) or ".", exist_ok=True)
+    conn = sqlite3.connect(DB, check_same_thread=False)
+    c = conn.cursor()
+    c.execute("CREATE TABLE IF NOT EXISTS labels (id TEXT, dataset TEXT, annotator TEXT, label JSON, ts INTEGER)")
+    conn.commit()
+    return conn
+
+conn = init_db()
+
+@app.route("/tasks", methods=["GET"])
+def get_tasks():
+    # Simply read first dataset manifest and return items without label (very naive)
+    ds = request.args.get("dataset")
+    if not ds:
+        return jsonify({"error":"dataset required"}), 400
+    if not os.path.exists(CATALOG):
+        return jsonify([])
+    catalog = json.load(open(CATALOG))
+    manifest = catalog.get(ds, {}).get("manifest")
+    if not manifest:
+        return jsonify([])
+    tasks = []
+    with open(manifest) as f:
+        for line in f:
+            tasks.append(json.loads(line))
+    return jsonify(tasks[:50])
+
+@app.route("/submit", methods=["POST"])
+def submit_label():
+    body = request.get_json()
+    ds = body.get("dataset")
+    item_id = body.get("id")
+    label = body.get("label")
+    annotator = body.get("annotator")
+
î€€
