*** Begin Patch
*** Add File:hf_integration/README.md
+Hugging Face -> MLflow integration
+==================================
+
+This folder contains a small utility to import a Hugging Face Hub model (model files + model card)
+into MLflow as an artifact and register a model entry with metadata. The script is intended for
+operator usage in staging: it downloads model artifacts (tokenizer + model files + model card)
+and logs them to MLflow so the Aegis pipelines can sign and promote them.
+
+Prereqs
+- Python 3.8+
+- pip install -r hf_integration/requirements.txt
+- Environment variables:
+  - MLFLOW_TRACKING_URI (or pass via CLI)
+  - HF_TOKEN (if accessing private HF repos)
+
+Usage example:
+  python hf_integration/import_hf_to_mlflow.py --model-id distilbert-base-uncased-finetuned-sst-2-english --name demo-sst --register
+
+Notes:
+- The script logs model files into an MLflow run under `model/` artifact path and then attempts to register
+  a model named <name> in the MLflow Model Registry. If you prefer to inspect artifacts before registering,
+  omit `--register`.
+
*** End Patch
*** Begin Patch
*** Add File:hf_integration/requirements.txt
+transformers>=4.30.0
+huggingface-hub>=0.14.1
+mlflow>=2.0.0
+boto3
+pyyaml
+
*** End Patch
*** Begin Patch
*** Add File:hf_integration/import_hf_to_mlflow.py
+#!/usr/bin/env python3
+"""
+Download a Hugging Face model (weights + tokenizer + model card) and log into MLflow as artifacts.
+Optionally register a model in MLflow Model Registry (artifact is "model").
+
+This is a pragmatic integration: it ensures that HF-hosted artifacts are captured in MLflow so
+they can be further processed (validation, signing, promotion) by Aegis pipelines.
+"""
+import argparse
+import os
+import tempfile
+import shutil
+import json
+from huggingface_hub import snapshot_download, hf_hub_download, HfApi
+from transformers import AutoTokenizer, AutoConfig, AutoModel
+import mlflow
+
+def download_model_files(model_id, cache_dir=None, token=None):
+    # Download the repo snapshot (weights/config/tokenizer/etc.)
+    print(f"Downloading model {model_id} from Hugging Face Hub...")
+    path = snapshot_download(repo_id=model_id, cache_dir=cache_dir, token=token)
+    print("Downloaded to:", path)
+    return path
+
+def package_and_log(model_path, name, register=False, mlflow_tracking_uri=None):
+    if mlflow_tracking_uri:
+        mlflow.set_tracking_uri(mlflow_tracking_uri)
+    # start a run and log the `model` folder as the artifact root for this model
+    with mlflow.start_run() as run:
+        run_id = run.info.run_id
+        artifact_src = os.path.join(model_path)
+        # copy model files into a staging dir to ensure consistent artifact path
+        staging = tempfile.mkdtemp(prefix="aegis-hf-")
+        dst = os.path.join(staging, "model")
+        shutil.copytree(artifact_src, dst)
+        print("Logging artifacts to MLflow run:", run_id)
+        mlflow.log_artifacts(dst, artifact_path="model")
+        # record metadata
+        mlflow.set_tag("hf_model_id", args.model_id)
+        mlflow.set_tag("source", "huggingface")
+        mlflow.log_param("model_name", name)
+        # If model card exists, log it explicitly
+        card_path = os.path.join(artifact_src, "README.md")
+        if os.path.exists(card_path):
+            mlflow.log_artifact(card_path, artifact_path="model_card")
+        print("Artifacts logged under runs:/{}/model".format(run_id))
+
+        if register:
+            model_uri = f"runs:/{run_id}/model"
+            print("Registering model '{}' as MLflow model with uri {}".format(name, model_uri))
+            # Note: registering a raw artifact as MLflow Model is OK for bookkeeping;
+            # production use should adapt a proper mlflow.pyfunc flavor wrapper if serving via mlflow.
+            mlflow.register_model(model_uri, name)
+            print("Registered model in MLflow Model Registry.")
+        else:
+            print("Run ID:", run_id, " â€” use this in the Aegis pipelines to sign/register manually.")
+        shutil.rmtree(staging)
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--model-id", required=True, help="Hugging Face model id (e.g. distilbert-base-uncased)")
+    parser.add_argument("--name", required=True, help="Name to register in MLflow")
+    parser.add_argument("--cache-dir", default=None)
+    parser.add_argument("--hf-token", default=None, help="Hugging Face token (optional)")
+    parser.add_argument("--register", action="store_true", help="Register the artifact in MLflow Model Registry")
+    parser.add_argument("--mlflow-tracking-uri", default=None, help="Optional MLflow tracking URI")
+    args = parser.parse_args()
+    # Download
+    model_path = download_model_files(args.model_id, cache_dir=args.cache_dir, token=args.hf_token)
+    package_and_log(model_path, args.name, register=args.register, mlflow_tracking_uri=args.mlflow_tracking_uri)
+
*** End Patch
*** Begin Patch
*** Add File:langchain_adapter/README.md
+LangChain adapter for Aegis Agent Manager
+=======================================
+
+This adapter allows running LangChain agents inside Aegis Agent workloads and integrates:
+- Agent memory via Aegis agent-memory HTTP service.
+- Tools via the Aegis tool-runner sidecar (HTTP /run endpoint).
+- Safety gating: before launching, the controller consults OPA; the adapter consults an optional policy endpoint for runtime tool calls.
+
+It is a minimal example: the adapter launches a simple LangChain agent loop (a single-step agent),
+uses the memory HTTP API to store/retrieve short-term memories, and calls tools by invoking the tool-runner
+HTTP endpoint in the pod (localhost:8081).
+
+Prereqs
+- Install dependencies: pip install -r langchain_adapter/requirements.txt
+- Ensure this runs inside an Agent pod created by the Agent Controller (or run locally and point to services).
+
*** End Patch
*** Begin Patch
*** Add File:langchain_adapter/requirements.txt
+langchain>=0.0.250
+requests
+boto3
+python-dotenv
+
*** End Patch
*** Begin Patch
*** Add File:langchain_adapter/adapter.py
+#!/usr/bin/env python3
+"""
+A small runtime adapter that runs a LangChain agent and integrates with Aegis platform services:
+- agent-memory service (HTTP)
+- tool-runner sidecar (HTTP /run)
+
+This demo uses a very small conversational agent pattern and maps LangChain tools to tool-runner calls.
+Run inside an Agent pod (sidecar tool-runner should be reachable at http://localhost:8081).
+"""
+import os
+import time
+import requests
+from langchain.llms import OpenAI  # fallback; operators may replace with a HF wrapper
+from langchain.agents import initialize_agent, Tool, AgentType
+from langchain.memory import ConversationBufferMemory
+from langchain import LLMChain, PromptTemplate
+
+AGENT_ID = os.environ.get("AGENT_ID", "langchain-agent")
+MEMORY_URL = os.environ.get("AGENT_MEMORY_URL", "http://agent-memory.ops.svc.cluster.local:8080")
+TOOL_RUNNER = os.environ.get("TOOL_RUNNER_URL", "http://localhost:8081")
+
+def memory_get(agent_id, key):
+    url = f"{MEMORY_URL}/memory/{agent_id}/{key}"
+    try:
+        r = requests.get(url, timeout=5)
+        if r.status_code == 200:
+            return r.json()
+    except Exception:
+        pass
+    return None
+
+def memory_put(agent_id, key, value):
+    url = f"{MEMORY_URL}/memory/{agent_id}/{key}"
+    try:
+        r = requests.put(url, json=value, timeout=5)
+        return r.status_code == 200
+    except Exception:
+        return False
+
+def tool_runner_call(tool_name, payload):
+    url = f"{TOOL_RUNNER}/run"
+    body = {"tool": tool_name, "input": payload}
+    r = requests.post(url, json=body, timeout=10)
+    try:
+        return r.json()
+    except Exception:
+        return {"error":"tool-runner error", "status_code": r.status_code}
+
+def build_tools():
+    # Example: define a question-answering tool that calls the tool-runner
+    def qna_tool(input_text: str):
+        res = tool_runner_call("qna", {"question": input_text})
+        return res.get("stdout") or res
+
+    tools = [
+        Tool(
+            name="qna",
+            func=qna_tool,
+            description="Answer simple factual questions using an onboard QnA tool"
+        )
+    ]
+    return tools
+
+def main_loop():
+    # NOTE: For the demo we use OpenAI wrapper if API key provided; otherwise a dummy echo LLM.
+    openai_key = os.environ.get("OPENAI_API_KEY")
+    if openai_key:
+        llm = OpenAI(openai_api_key=openai_key, temperature=0.2)
+    else:
+        # Fallback: tiny prompt-based chain that echoes input (replace with HF wrapper in prod)
+        class DummyLLM:
+            def __call__(self, prompt, **kwargs):
+                return "ECHO: " + prompt.splitlines()[-1]
+        llm = DummyLLM()
+
+    tools = build_tools()
+    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
+
+    # A minimal agent using the Zero-shot-react-description pattern
+    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, memory=memory, verbose=True)
+
+    print("Agent ready. Enter queries (ctrl-c to exit).")
+    while True:
+        try:
+            q = input("User> ")
+            # store to memory
+            memory_put(AGENT_ID, "last_user_input", {"text": q, "ts": time.time()})
+            res = agent.run(q)
+            print("Agent>", res)
+            # persist result to memory
+            memory_put(AGENT_ID, "last_agent_output", {"text": res, "ts": time.time()})
+        except KeyboardInterrupt:
+            print("Exiting.")
+            break
+        except Exception as e:
+            print("Error:", e)
+            time.sleep(1)
+
+if __name__ == "__main__":
+    main_loop()
+
*** End Patch
*** Begin Patch
*** Add File:examples/langchain_hf_demo/README.md
+LangChain + Hugging Face + Aegis end-to-end demo
+===============================================
+
+This example shows how to:
+1) Import a model from Hugging Face to MLflow (hf_integration/import_hf_to_mlflow.py)
+2) Register model and sign it via Aegis promotion pipeline (manual CI step or runbook)
+3) Deploy a LangChain agent (langchain_adapter) as an Agent CR using Agent Manager
+4) Agent uses the Aegis tool-runner and agent-memory services and will call the HF-backed model via the tool-runner
+
+Quick demo (operator)
+- Step 1: Import HF model to MLflow
+  python ../hf_integration/import_hf_to_mlflow.py --model-id distilbert-base-uncased-finetuned-sst-2-english --name demo-sst --register
+
+- Step 2: Ensure MLflow model is signed by Aegis promote+sign pipeline (run your CI or use mlflow/scripts/mlflow_promote_and_sign.sh)
+
+- Step 3: Deploy Agent Manager controller and OPA policy (see agent-manager/runbook)
+
+- Step 4: Build and push langchain_adapter image, tool-runner and agent-memory images and update Agent sample to reference the adapter
+
+- Step 5: Apply Agent CR for a LangChain agent and attach sample tools (agent-manager/controller/agent-sample.yaml is a starting point)
+
+Notes:
+- This demo assumes you have KServe inference endpoints or a tool-runner that can call a hosted HF API. Adapt tool-runner tool manifests to point to the correct model inference URL (KServe or HF Inference API).
+
*** End Patch
*** Begin Patch
*** Add File:docs/compare_aegis_vs_hf_langchain.md
+# Comparative report: Aegis vs Hugging Face vs LangChain
+
+Summary
+- Hugging Face: Best-in-class model hub, datasets and managed model hosting. Great UX and community models.
+- LangChain: Best-in-class agent & LLM application framework, rapid prototyping of chains and tools.
+- Aegis: Enterprise-grade operations, governance, security/supply-chain, multi-cloud/hardware ops, SRE/runbooks and auditing.
+
+Comparison Table (high level)
+| Capability | Hugging Face | LangChain | Aegis (this repo) |
+|---|---:|---:|---:|
+| Model hub & discovery | Excellent (HF Hub) | n/a | Integrates HF Hub into MLflow (via import) |
+| LLM/agent framework | Limited (inference) | Excellent (chains, agents, tools) | Provides agent runtime scaffolds + Agent Manager CRD (runtime & governance) |
+| Production serving (autoscale, GPU) | Managed endpoints (HF Infinity) | Not provided (framework) | KServe + Triton autoscale scaffolds + Volcano for gang scheduling |
+| Feature store & dataset lineage | Integrations | n/a | Feast + lakeFS + TF modules |
+| Supply-chain signing & provenance | Partial (cosign integrations) | n/a | Full pipeline: Vault transit + cosign + Rekor + admission enforcement |
+| Observability & SLOs | Integrations possible | Integrations possible | Prometheus/Grafana rules + SRE playbooks + drift detection |
+| Edge/IoT device attestation | Integrations | Not focused | Offline Rekor bundles + attestation demo + OTA scaffolds |
+| Agent safety & policy enforcement | Limited | Tooling for safety patterns | OPA-based gating + action audit + agent manager scaffolds |
+| Community & vendor support | Large & commercial | Large OSS ecosystem | Repo-based, operator-driven; can be productized |
+
+Recommended integration approach
+- Use HF Hub for model discovery/prototyping and import the final artifacts to MLflow via hf_integration/ to capture provenance.
+- Use LangChain for application/agent development; run LangChain agents inside Aegis Agent workloads (langchain_adapter) to gain governance and audit.
+- Enforce promotion/signing in Aegis before exposing models to production agents or serving endpoints.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hf_import_ci.yml
+name: HF Import CI (example)
+on:
+  workflow_dispatch:
+jobs:
+  import:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install -r hf_integration/requirements.txt
+      - name: Run HF import (example)
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        run: |
+          python hf_integration/import_hf_to_mlflow.py --model-id distilbert-base-uncased-finetuned-sst-2-english --name demo-sst --register
+
*** End Patch
*** Begin Patch
*** Add File:agent-manager/controller/agent-sample-langchain.yaml
+apiVersion: aegis.example.com/v1
+kind: Agent
+metadata:
+  name: langchain-demo-agent
+  namespace: default
+spec:
+  displayName: "LangChain Demo Agent"
+  image: "ghcr.io/yourorg/langchain-adapter:latest"  # built from langchain_adapter/adapter.py
+  replicas: 1
+  tools:
+    - name: qna
+      kind: wasm
+      image: "aegis/tools/qna:latest"
+      allowNetwork: false
+  memory:
+    backend: s3
+    bucket: "aegis-agent-memory"
+  policyRef: "default-agent-policy"
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/hf_langchain_aegis_integration_runbook.md
+# Runbook: integrate Hugging Face + LangChain into Aegis
+
+Goal
+- Use HF models for experimentation, import selected artifacts into MLflow, and run LangChain agents inside Aegis Agent Manager with full audit/signing and memory.
+
+High level steps
+1. Import model to MLflow:
+   - Run: python hf_integration/import_hf_to_mlflow.py --model-id <hf-model> --name <mlflow-name> --register
+2. Promote & sign:
+   - Use existing Aegis promote pipeline (mlflow/scripts/mlflow_promote_and_sign.sh or GitHub Action) to sign artifact and create Rekor entry.
+3. Build/push LangChain adapter image:
+   - docker build -t ghcr.io/yourorg/langchain-adapter:latest langchain_adapter/
+   - docker push ghcr.io/yourorg/langchain-adapter:latest
+4. Ensure Agent Manager and OPA are deployed:
+   - kubectl apply -f agent-manager/crds/agent.crd.yaml
+   - kubectl apply -f safety/opa/deployment.yaml
+   - deploy the controller and set image
+5. Deploy tool-runner and agent-memory services (images provided in repo).
+6. Create Agent CR referencing langchain-demo-agent (agent-manager/controller/agent-sample-langchain.yaml).
+7. Observe agent logs and audit: agent actions should be signed and Rekor entries created via agent-action-audit/submit_action.py in your orchestration.
+
+Verification
+- MLflow shows the imported model and the registered entry.
+- Rekor has a signature entry for the promoted model.
+- Agent pod runs and can call tool-runner at localhost:8081.
+- Agent-memory service stores/retrieves memory items.
+- Agent action audit writes signed entries to Rekor.
+
*** End Patch
*** End Patch
