*** Begin Patch
*** Add File:iam/ibm_policy.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowIBMQuantumAPIAccess",
+      "Effect": "Allow",
+      "Action": [
+        "iam:PassRole"
+      ],
+      "Resource": "*"
+    },
+    {
+      "Sid": "AllowS3ForQuantum",
+      "Effect": "Allow",
+      "Action": [
+        "s3:PutObject",
+        "s3:GetObject",
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET",
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:iam/azure_quantum_policy.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowAzureQuantumAccessPlaceholder",
+      "Effect": "Allow",
+      "Action": [
+        "sts:AssumeRole"
+      ],
+      "Resource": "*"
+    },
+    {
+      "Sid": "AllowS3ForQuantum",
+      "Effect": "Allow",
+      "Action": [
+        "s3:PutObject",
+        "s3:GetObject",
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET",
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/externalsecrets/ibm_secret.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: ibm-quantum-credentials
+  namespace: security
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: vault-secret-store
+    kind: SecretStore
+  target:
+    name: ibm-quantum-creds
+    creationPolicy: Owner
+  data:
+    - secretKey: IBM_API_KEY
+      remoteRef:
+        key: "aegis/ibm/credentials"
+        property: api_key
+
*** End Patch
*** Begin Patch
*** Add File:k8s/externalsecrets/azure_quantum_secret.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: azure-quantum-credentials
+  namespace: security
+spec:
+  refreshInterval: "1h"
+  secretStoreRef:
+    name: azure-keyvault-store
+    kind: SecretStore
+  target:
+    name: azure-quantum-creds
+    creationPolicy: Owner
+  data:
+    - secretKey: AZURE_CLIENT_ID
+      remoteRef:
+        key: "aegis/azure/credentials"
+        property: client_id
+    - secretKey: AZURE_TENANT_ID
+      remoteRef:
+        key: "aegis/azure/credentials"
+        property: tenant_id
+    - secretKey: AZURE_CLIENT_SECRET
+      remoteRef:
+        key: "aegis/azure/credentials"
+        property: client_secret
+
*** End Patch
*** Begin Patch
*** Add File:argo/quantum_ibm_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: quantum-ibm-
+  namespace: aegis
+spec:
+  entrypoint: ibm-job
+  arguments:
+    parameters:
+      - name: circuit-file-s3
+        value: "s3://REPLACE_BUCKET/quantum/circuits/example_ibm_circuit.py"
+      - name: run-type
+        value: "ibm" # or simulate
+      - name: device
+        value: "ibm_washington" # replace with IBM backend shortname
+      - name: s3-output
+        value: "s3://REPLACE_BUCKET/quantum/results/{{workflow.uid}}"
+      - name: image
+        value: "ghcr.io/yourorg/aegis-quantum:latest"
+  templates:
+    - name: ibm-job
+      inputs:
+        parameters:
+          - name: circuit-file-s3
+          - name: run-type
+          - name: device
+          - name: s3-output
+          - name: image
+      container:
+        image: "{{inputs.parameters.image}}"
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            # download circuit file
+            aws s3 cp "{{inputs.parameters.circuit-file-s3}}" /tmp/circuit.py
+            # Use a small wrapper that uses qiskit-ibm-provider to submit if run-type=ibm
+            python - <<'PY'
+import os, sys, json, subprocess
+from importlib import util
+run_type = os.environ.get("RUN_TYPE","{{inputs.parameters.run-type}}")
+device = "{{inputs.parameters.device}}"
+spec = util.spec_from_file_location("cmod", "/tmp/circuit.py")
+mod = util.module_from_spec(spec)
+spec.loader.exec_module(mod)
+if run_type == "ibm" and hasattr(mod, "submit_ibm_task"):
+    mod.submit_ibm_task(device, "{{inputs.parameters.s3-output}}")
+else:
+    # fallback to local simulation using Qiskit Aer
+    if hasattr(mod,"build_circuit"):
+        qc = mod.build_circuit()
+        from qiskit import Aer, execute
+        job = execute(qc, backend=Aer.get_backend('aer_simulator'), shots=256)
+        res = job.result().get_counts()
+        open('/tmp/result.json','w').write(json.dumps(res))
+        subprocess.check_call(['aws','s3','cp','/tmp/result.json','{{inputs.parameters.s3-output}}/result.json'])
+PY
+      resources:
+        requests:
+          cpu: "1000m"
+          memory: "2Gi"
+        limits:
+          cpu: "2000m"
+          memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:argo/quantum_azure_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: quantum-azure-
+  namespace: aegis
+spec:
+  entrypoint: azure-job
+  arguments:
+    parameters:
+      - name: circuit-file-s3
+        value: "s3://REPLACE_BUCKET/quantum/circuits/example_azure_circuit.py"
+      - name: run-type
+        value: "azure" # or simulate
+      - name: device
+        value: "ionq" # logical device name in Azure Quantum
+      - name: s3-output
+        value: "s3://REPLACE_BUCKET/quantum/results/{{workflow.uid}}"
+      - name: image
+        value: "ghcr.io/yourorg/aegis-quantum:latest"
+  templates:
+    - name: azure-job
+      inputs:
+        parameters:
+          - name: circuit-file-s3
+          - name: run-type
+          - name: device
+          - name: s3-output
+          - name: image
+      container:
+        image: "{{inputs.parameters.image}}"
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            aws s3 cp "{{inputs.parameters.circuit-file-s3}}" /tmp/circuit.py
+            python - <<'PY'
+import os, json, subprocess, importlib.util
+spec = importlib.util.spec_from_file_location("cmod","/tmp/circuit.py")
+mod = importlib.util.module_from_spec(spec)
+spec.loader.exec_module(mod)
+if hasattr(mod,'submit_azure_task') and "{{inputs.parameters.run-type}}" == "azure":
+    mod.submit_azure_task("{{inputs.parameters.device}}","{{inputs.parameters.s3-output}}")
+else:
+    # fallback to simulation
+    if hasattr(mod,'build_circuit'):
+        from qiskit import Aer, execute
+        qc = mod.build_circuit()
+        job = execute(qc, backend=Aer.get_backend('aer_simulator'), shots=256)
+        res = job.result().get_counts()
+        open('/tmp/result.json','w').write(json.dumps(res))
+        subprocess.check_call(['aws','s3','cp','/tmp/result.json','{{inputs.parameters.s3-output}}/result.json'])
+PY
+      resources:
+        requests:
+          cpu: "1000m"
+          memory: "2Gi"
+        limits:
+          cpu: "2000m"
+          memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_approval_check.yml
+name: QPU Approval Check
+
+on:
+  pull_request:
+    types: [opened, reopened, labeled, unlabeled, synchronize]
+
+jobs:
+  qpu-approval:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Check for QPU usage markers in PR
+        id: scan
+        run: |
+          set -e
+          # Search for files that reference quantum jobs / argo workflows
+          Q=$(git diff --name-only origin/main...HEAD | xargs grep -IEl "quantum|braket|ibm|azure-quantum|quantum_braket_workflow|quantum_ibm_workflow|quantum_azure_workflow" || true)
+          echo "changed_files<<EOF" >> $GITHUB_OUTPUT
+          echo "$Q" >> $GITHUB_OUTPUT
+          echo "EOF" >> $GITHUB_OUTPUT
+      - name: Require approval label and code-owner review for QPU changes
+        if: steps.scan.outputs.changed_files != ''
+        uses: actions/github-script@v6
+        with:
+          script: |
+            const labels = await github.rest.issues.listLabelsOnIssue({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              issue_number: context.issue.number
+            });
+            const labelNames = labels.data.map(l=>l.name);
+            if (!labelNames.includes('qpu-approved')) {
+              core.setFailed("This PR touches quantum workflows or circuits. It must have the label 'qpu-approved' applied by an authorized approver before merge.");
+            } else {
+              console.log("qpu-approved label found.");
+            }
+            // enforce at least one approving review
+            const reviews = await github.rest.pulls.listReviews({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              pull_number: context.issue.number
+            });
+            const approvals = reviews.data.filter(r => r.state.toLowerCase() === 'approved');
+            if (approvals.length < 1) {
+              core.setFailed("This PR must have at least one approving review from code owners for QPU changes.");
+            } else {
+              console.log("Approving review found.");
+            }
+
+  note:
+    runs-on: ubuntu-latest
+    steps:
+      - run: echo "If the PR does not touch quantum artifacts, this check is a no-op."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_nightly_sandbox_braket.yml
+name: Nightly Sandbox QPU (Braket)
+
+on:
+  schedule:
+    - cron: "0 4 * * *" # daily at 04:00 UTC
+  workflow_dispatch:
+
+jobs:
+  sandbox-braket:
+    runs-on: ubuntu-latest
+    env:
+      S3_OUTPUT_PREFIX: ${{ secrets.SANDBOX_QPU_S3_PREFIX }}
+      MLFLOW_URI: ${{ secrets.SANDBOX_MLFLOW_URI }}
+      BRACKET_DEVICE: ${{ secrets.SANDBOX_BRAKET_DEVICE }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install -r docker/quantum/requirements.txt || pip install amazon-braket-sdk qiskit qiskit-aer boto3 mlflow
+      - name: Run small Braket job (sandbox)
+        run: |
+          CIRCUIT=examples/quantum/sample_circuits/simple_bell.py
+          S3_OUT="${S3_OUTPUT_PREFIX}/nightly/${{ github.run_id }}"
+          python3 quantum/braket_job.py --circuit-file "$CIRCUIT" --run-type braket --device "${BRAKET_DEVICE:-arn:aws:braket:::device/quantum-simulator/amazon/sv1}" --s3-output "$S3_OUT" --mlflow-tracking-uri "$MLFLOW_URI"
+      - name: Cost guard check (best-effort)
+        run: |
+          # Placeholder for cost guard: for providers with cost APIs, query usage or estimate
+          echo "Ensure SANDBOX_QPU_BUDGET_USD is set for this workflow to enforce budget guard."
+          BUDGET=${{ secrets.SANDBOX_QPU_BUDGET_USD }}
+          if [ -n "$BUDGET" ] && [ "$BUDGET" != "0" ]; then
+            echo "Budget guard enabled: $BUDGET USD"
+          else
+            echo "No budget guard set; operator should monitor spend"
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_nightly_sandbox_ibm.yml
+name: Nightly Sandbox QPU (IBM Quantum)
+
+on:
+  schedule:
+    - cron: "30 4 * * *" # daily at 04:30 UTC
+  workflow_dispatch:
+
+jobs:
+  sandbox-ibm:
+    runs-on: ubuntu-latest
+    env:
+      S3_OUTPUT_PREFIX: ${{ secrets.SANDBOX_QPU_S3_PREFIX }}
+      MLFLOW_URI: ${{ secrets.SANDBOX_MLFLOW_URI }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install qiskit-ibm-provider qiskit qiskit-aer boto3 mlflow
+      - name: Run small IBM job (sandbox)
+        run: |
+          CIRCUIT=examples/quantum/sample_circuits/ibm_simple.py
+          S3_OUT="${S3_OUTPUT_PREFIX}/nightly/ibm/${{ github.run_id }}"
+          python3 quantum/braket_job.py --circuit-file "$CIRCUIT" --run-type ibm --device "${{ secrets.SANDBOX_IBM_DEVICE }}" --s3-output "$S3_OUT" --mlflow-tracking-uri "${MLFLOW_URI}"
+      - name: Cost guard placeholder
+        run: echo "IBM cost guard: configure monitoring and budgets per your IBM account."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_nightly_sandbox_azure.yml
+name: Nightly Sandbox QPU (Azure Quantum)
+
+on:
+  schedule:
+    - cron: "0 5 * * *" # daily at 05:00 UTC
+  workflow_dispatch:
+
+jobs:
+  sandbox-azure:
+    runs-on: ubuntu-latest
+    env:
+      S3_OUTPUT_PREFIX: ${{ secrets.SANDBOX_QPU_S3_PREFIX }}
+      MLFLOW_URI: ${{ secrets.SANDBOX_MLFLOW_URI }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install azure-quantum qiskit qiskit-aer boto3 mlflow
+      - name: Run small Azure job (sandbox)
+        run: |
+          CIRCUIT=examples/quantum/sample_circuits/azure_simple.py
+          S3_OUT="${S3_OUTPUT_PREFIX}/nightly/azure/${{ github.run_id }}"
+          python3 quantum/braket_job.py --circuit-file "$CIRCUIT" --run-type azure --device "${{ secrets.SANDBOX_AZURE_PROVIDER }}" --s3-output "$S3_OUT" --mlflow-tracking-uri "${MLFLOW_URI}"
+      - name: Cost guard placeholder
+        run: echo "Azure cost guard: configure Azure billing alerts and budget."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/qpu_approval_and_dispatch.yml
+name: QPU Run Dispatcher (guarded)
+
+on:
+  workflow_dispatch:
+    inputs:
+      workflow:
+        description: 'Which provider workflow to launch (braket|ibm|azure)'
+        required: true
+      circuit_s3:
+        description: 'S3 path to circuit file'
+        required: true
+  pull_request:
+    types: [closed]
+
+jobs:
+  dispatch:
+    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'pull_request' && github.event.action == 'closed' && github.event.pull_request.merged == true)
+    runs-on: ubuntu-latest
+    steps:
+      - name: Validate request (must include qpu-approved label on PR if applicable)
+        uses: actions/github-script@v6
+        with:
+          script: |
+            const wf = context.payload.inputs ? context.payload.inputs.workflow : null;
+            // If triggered from PR merge, confirm PR had qpu-approved label
+            if (context.payload.pull_request) {
+              const labels = context.payload.pull_request.labels.map(l => l.name);
+              if (!labels.includes('qpu-approved')) {
+                core.setFailed("Merged PR did not have 'qpu-approved' label. Aborting guarded QPU dispatch.");
+              }
+            }
+            core.setOutput("workflow", wf);
+      - name: Trigger Argo workflow via kubectl (operator-controlled)
+        env:
+          CIRCUIT_S3: ${{ github.event.inputs.circuit_s3 || '' }}
+          PROVIDER: ${{ github.event.inputs.workflow || '' }}
+        run: |
+          echo "This step is intentionally left as an operator action. It will create Argo workflow with provider ${PROVIDER} and circuit ${CIRCUIT_S3}"
+          # Example: argo submit -n aegis argo/quantum_braket_workflow.yaml -p circuit-file-s3=$CIRCUIT_S3 -p run-type=braket -p image=ghcr.io/yourorg/aegis-quantum:latest
+          echo "Operator must run the argo submit command in a secure environment (not from this workflow)."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_require_qpu_approved_annotation.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: qpuapprovedannotation
+spec:
+  crd:
+    spec:
+      names:
+        kind: QPUApprovedAnnotation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package qpuapprovedannotation
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Job"
+          # require an annotation quantum.approved == "true" on Job
+          not input.review.object.metadata.annotations["quantum.approved"]
+          msg := "Kubernetes Job must include metadata.annotations['quantum.approved']='true' to run QPU workloads."
+        }
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_qpu_approved.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: QPUApprovedAnnotation
+metadata:
+  name: require-qpu-approved
+spec:
+  match:
+    kinds:
+      - apiGroups: ["batch"]
+        kinds: ["Job"]
+    namespaces: ["aegis"]
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/qpu_cost_estimator_example.py
+"""
+Simple example to estimate QPU job cost for Braket (very rough heuristic).
+This script demonstrates reading Braket task metadata (if available) and estimating cost,
+then pushing a Prometheus metric via a pushgateway or logging for the exporter to pick up.
+Operator must adapt to provider billing APIs for accurate cost estimates.
+"""
+import os, json, time
+import boto3
+
+def estimate_braket_cost(task_arn):
+    # Placeholder: real cost estimation requires provider price list & task specs
+    # Here we return a small constant for demo
+    return 2.50
+
+def main():
+    task_arn = os.environ.get("BRAKET_TASK_ARN")
+    if not task_arn:
+        print("No BRAKET_TASK_ARN set; nothing to do.")
+        return
+    cost = estimate_braket_cost(task_arn)
+    print(f"Estimated cost for {task_arn}: ${cost:.2f}")
+    # TODO: push metric to Prometheus pushgateway or store in MLflow artifact
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/qpu_approval_and_ci_runbook.md
+# QPU Approval & CI Runbook
+
+Purpose
+- Describe how GitHub checks, Gatekeeper constraints and nightly sandbox CI work together to ensure safe, approved QPU usage.
+
+Key pieces
+- .github/workflows/qpu_approval_check.yml — PR-time check that fails PRs containing quantum artifacts unless label 'qpu-approved' is present and an approving review exists. Make this a required status check on protected branches.
+- .github/workflows/qpu_approval_and_dispatch.yml — guarded dispatcher workflow (manual dispatch) that checks approvals/labels; operator triggers Argo submit from secure workstation.
+- Gatekeeper constraint (require-qpu-approved) — prevents Kubernetes Jobs that run QPU workloads from being admitted unless annotation metadata 'quantum.approved=true' is present.
+- Nightly sandbox workflows (.github/workflows/qpu_nightly_sandbox_*.yml) — daily small test runs against simulator or sandbox QPU to validate the end-to-end plumbing.
+- Cost guard placeholders — nightly workflows and monitoring scripts include budget guard placeholders; integrate with provider billing APIs for strict enforcement.
+
+Operator setup checklist
+1. Add 'qpu-approved' label to repo and limit who can apply it (GitHub team permissions).
+2. Set GitHub branch protection to require the "QPU Approval Check" status for merges that change quantum artifacts.
+3. Install Gatekeeper and apply the constraint templates/constraints in k8s/gatekeeper/.
+4. Create ExternalSecrets for provider credentials (k8s/externalsecrets/*).
+5. Create IRSA roles and annotate SAs (ops/annotate_serviceaccounts.sh).
+6. Populate secrets in GitHub for nightly sandbox runs (SANDBOX_* secrets).
+7. Build & publish quantum image and update Argo templates image references.
+
+How to run an approved QPU job safely
+1. Create a PR with circuit and Argo workflow changes.
+2. Obtain review & add 'qpu-approved' label by an approver.
+3. Merge when PR checks pass (including QPU Approval Check).
+4. Operator (authorized) triggers the Argo workflow submission from a secure environment (see qpu_approval_and_dispatch.yml notes).
+5. The Job manifest applied to Kubernetes must include metadata.annotations['quantum.approved']="true" to pass Gatekeeper at admission time.
+
+Notes on cost enforcement
+- Nightly workflows include cost-guard placeholders. For production, integrate with provider billing APIs (Braket cost APIs, IBM billing, Azure Cost Management) and implement an enforcement step that aborts job submission if projected cost > budget.
+
*** End Patch
*** End Patch
