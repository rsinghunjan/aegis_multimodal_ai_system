*** Begin Patch
*** Add File: k8s/cert-manager/create_webhook_certs.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Generates a CA, signs a server certificate for the attestation service,
+# creates Kubernetes TLS secret and prints base64 CA bundle for ValidatingWebhookConfiguration.
+#
+# Usage: ./create_webhook_certs.sh <namespace> <service-name> <secret-name>
+#
+NS=${1:-ops}
+SVC=${2:-attestation}
+SECRET=${3:-attestation-tls}
+
+CA_KEY=ca.key.pem
+CA_CERT=ca.cert.pem
+SERVER_KEY=server.key.pem
+SERVER_CSR=server.csr.pem
+SERVER_CERT=server.cert.pem
+
+echo "Generating CA..."
+openssl genpkey -algorithm RSA -out $CA_KEY -pkeyopt rsa_keygen_bits:2048
+openssl req -x509 -new -nodes -key $CA_KEY -sha256 -days 3650 -subj "/CN=aegis-webhook-ca" -out $CA_CERT
+
+echo "Generating server key & CSR..."
+openssl genpkey -algorithm RSA -out $SERVER_KEY -pkeyopt rsa_keygen_bits:2048
+cat > csr.conf <<EOF
+[req]
+distinguished_name = req_distinguished_name
+req_extensions = v3_req
+[req_distinguished_name]
+[v3_req]
+subjectAltName = @alt_names
+[alt_names]
+DNS.1 = ${SVC}
+DNS.2 = ${SVC}.${NS}
+DNS.3 = ${SVC}.${NS}.svc
+EOF
+openssl req -new -key $SERVER_KEY -subj "/CN=${SVC}.${NS}.svc" -out $SERVER_CSR -config csr.conf
+
+echo "Signing server certificate with CA..."
+openssl x509 -req -in $SERVER_CSR -CA $CA_CERT -CAkey $CA_KEY -CAcreateserial -out $SERVER_CERT -days 365 -sha256 -extensions v3_req -extfile csr.conf
+
+echo "Create Kubernetes TLS secret..."
+kubectl create secret tls $SECRET -n $NS --cert=$SERVER_CERT --key=$SERVER_KEY --dry-run=client -o yaml | kubectl apply -f -
+
+CA_BUNDLE=$(base64 -w0 < $CA_CERT)
+echo
+echo "CA bundle (base64) to put into ValidatingWebhookConfiguration caBundle field:"
+echo
+echo "$CA_BUNDLE"
+
+echo
+echo "Cleanup local files if desired."
+
*** End Patch
*** Begin Patch
*** Add File: k8s/webhook/attestation-webhook-validating.yaml
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  name: aegis-attestation-webhook
+webhooks:
+  - name: attestation.aegis.local
+    sideEffects: None
+    admissionReviewVersions: ["v1"]
+    clientConfig:
+      service:
+        name: attestation
+        namespace: ops
+        path: /verify
+      caBundle: "" # fill with CA bundle from create_webhook_certs.sh
+    rules:
+      - operations: ["CREATE","UPDATE"]
+        apiGroups: ["apps",""]
+        apiVersions: ["v1"]
+        resources: ["deployments","pods"]
+    failurePolicy: Fail
+    timeoutSeconds: 5
+    matchPolicy: Equivalent
+
*** End Patch
*** Begin Patch
*** Add File: terraform/aws/oidc_kms_cosign.tf
+/*
+Terraform example to provision:
+ - IAM role for GitHub OIDC federation to assume
+ - KMS key for cosign signing (AWS KMS)
+
+Note: customize vars (organization, repository, kms key policy) and run Terraform in your infra account.
+*/
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_iam_role" "github_actions_oidc" {
+  name = "aegis-github-actions-oidc"
+  assume_role_policy = data.aws_iam_policy_document.github_oidc_assume_role_policy.json
+}
+
+data "aws_iam_policy_document" "github_oidc_assume_role_policy" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(var.oidc_provider_url, "https://", "")}:sub"
+      values   = ["repo:${var.github_org}/${var.github_repo}:ref:refs/heads/main"]
+    }
+  }
+}
+
+resource "aws_kms_key" "cosign_key" {
+  description             = "KMS key for cosign signing by CI"
+  deletion_window_in_days = 30
+  policy                  = data.aws_iam_policy_document.kms_policy.json
+}
+
+data "aws_iam_policy_document" "kms_policy" {
+  statement {
+    sid    = "Allow use by GitHubActionsRole"
+    actions = [
+      "kms:Sign",
+      "kms:DescribeKey",
+      "kms:GetPublicKey"
+    ]
+    principals {
+      type = "AWS"
+      identifiers = [aws_iam_role.github_actions_oidc.arn]
+    }
+  }
+}
+
+output "cosign_kms_key_arn" {
+  value = aws_kms_key.cosign_key.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File: helm/vault/values.yaml
+# Helm values for deploying Vault via HashiCorp Helm chart (example)
+server:
+  extraEnvironmentVars:
+    VAULT_LOCAL_CONFIG: |
+      ui = true
+      storage "raft" {
+        path = "/vault/data"
+      }
+      listener "tcp" {
+        address     = "0.0.0.0:8200"
+        tls_disable = 1
+      }
+  dataStorage:
+    enabled: true
+  service:
+    type: ClusterIP
+
+injector:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: services/vault/vault_k8s_auth_setup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example script to configure Vault Kubernetes auth for controller to request short-lived secrets.
+# Requires vault CLI and kubectl configured, and Vault server up.
+VAULT_ADDR=${VAULT_ADDR:-http://vault.ops.svc.cluster.local:8200}
+VAULT_K8S_SA=${VAULT_K8S_SA:-agent-controller-sa}
+VAULT_K8S_NS=${VAULT_K8S_NS:-staging}
+vault login $VAULT_TOKEN
+vault auth enable kubernetes || true
+vault write auth/kubernetes/config \
+  token_reviewer_jwt="$(kubectl get secret $(kubectl get sa $VAULT_K8S_SA -n $VAULT_K8S_NS -o jsonpath='{.secrets[0].name}') -n $VAULT_K8S_NS -o go-template='{{ .data.token }}' | base64 -d)" \
+  kubernetes_host="$(kubectl config view --raw --minify --flatten -o jsonpath='{.clusters[0].cluster.server}')" \
+  kubernetes_ca_cert="@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
+
+vault policy write agent-sandbox - <<EOF
+path "secret/data/agent/*" {
+  capabilities = ["read"]
+}
+EOF
+
+vault write auth/kubernetes/role/agent-sandbox \
+    bound_service_account_names=$VAULT_K8S_SA \
+    bound_service_account_namespaces=$VAULT_K8S_NS \
+    policies=agent-sandbox \
+    ttl=1h
+
+echo "Vault kubernetes auth configured for role agent-sandbox"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/postgres-ha-recommendation.md
+# Postgres HA recommendation
+
+- For production, use a managed Postgres (Cloud SQL / RDS) or the Postgres Operator (CrunchyData / Zalando Patroni).
+- The repository includes a statefulset example; in production prefer managed service with automated backups and HA.
+- Ensure credentials are stored in Vault and referenced by secrets, not checked in.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/milvus-ha-helm-values.yaml
+# Example values to deploy Milvus with Helm chart in HA mode (placeholder)
+replicaCount: 3
+minIO:
+  enabled: true
+persistentVolume:
+  enabled: true
+  size: 100Gi
+resources:
+  limits:
+    cpu: 4
+    memory: 8Gi
+
*** End Patch
*** Begin Patch
*** Add File: services/edge_fleet/enroll_service.py
+"""
+Edge fleet enrollment & attestation service (lightweight).
+- Devices POST their CSR to /enroll; server stores device metadata and issues enrollment record.
+- Simple TPM/attestation stub: in production integrate vendor/TPM attestation flow.
+"""
+import os, json, time, uuid
+from flask import Flask, request, jsonify
+
+DB = os.environ.get("EDGE_ENROLL_DB", "/data/edge_devices.json")
+
+app = Flask("edge-enroll")
+
+def load_db():
+    if os.path.exists(DB):
+        return json.load(open(DB))
+    return {}
+
+def save_db(db):
+    os.makedirs(os.path.dirname(DB) or ".", exist_ok=True)
+    with open(DB, "w") as f:
+        json.dump(db, f, indent=2)
+
+@app.post("/enroll")
+def enroll():
+    body = request.get_json()
+    device_id = body.get("device_id") or f"dev-{uuid.uuid4().hex[:8]}"
+    cert = body.get("device_cert")
+    meta = body.get("meta", {})
+    db = load_db()
+    db[device_id] = {"device_cert": cert, "meta": meta, "enrolled_at": int(time.time()), "status":"enrolled"}
+    save_db(db)
+    return jsonify({"ok": True, "device_id": device_id})
+
+@app.get("/devices")
+def list_devices():
+    db = load_db()
+    return jsonify(db)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8086)
+
*** End Patch
*** Begin Patch
*** Add File: services/edge_fleet/ota_rollout_controller.py
+"""
+Simple OTA rollout controller for edge devices (demo).
+- Reads device list from enroll_service and applies signed artifact to devices via Ansible or device agent.
+- Supports canary rollout and rollback via approval service.
+"""
+import os, time, requests, json
+
+ENROLL_URL = os.environ.get("ENROLL_URL", "http://edge-enroll:8086")
+APPROVAL_URL = os.environ.get("APPROVAL_URL", "http://approval:8096")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def get_devices():
+    r = requests.get(f"{ENROLL_URL}/devices", timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def start_rollout(artifact_s3_uri, canary_count=2):
+    devices = list(get_devices().keys())
+    if not devices:
+        print("No devices enrolled")
+        return
+    # pick canary devices
+    canaries = devices[:canary_count]
+    rest = devices[canary_count:]
+    print("Canary targets:", canaries)
+    # apply to canaries (placeholder: call Ansible playbook or device agent)
+    for d in canaries:
+        print("Applying to canary", d)
+        # in prod: run ansible playbook verifying cosign signature on device
+    # wait for canary period and verify via health checks -> omitted
+    # request human approval to continue
+    apr = requests.post(f"{APPROVAL_URL}/request", json={"action": f"ota_promote:{artifact_s3_uri}", "required":2}).json()
+    print("Approval requested:", apr.get("id"))
+    # Poll approval
+    for _ in range(60):
+        st = requests.get(f"{APPROVAL_URL}/status/{apr['id']}").json()
+        if st.get("status") == "approved":
+            print("Approved; promoting to rest of fleet")
+            for d in rest:
+                print("Applying to", d)
+            return
+        time.sleep(10)
+    print("Approval timeout; rolling back canaries")
+    # rollback logic here
+
+if __name__ == "__main__":
+    start_rollout(os.environ.get("ARTIFACT_URI","s3://bucket/edge/art.tflite"))
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/distributed_tpu_realdata.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: distributed-tpu-real
+  namespace: staging
+spec:
+  entrypoint: dist-train
+  templates:
+  - name: dist-train
+    inputs:
+      parameters:
+      - name: dataset_uri
+        value: "s3://aegis-datasets/imagenet-100/"
+      - name: replicas
+        value: "16"
+    container:
+      image: ghcr.io/yourorg/aegis-train:latest
+      command: ["/bin/sh","-c"]
+      args:
+        - |
+          python /app/distributed_train.py --dataset {{inputs.parameters.dataset_uri}} --replicas {{inputs.parameters.replicas}} --use-tpu True --upload-checkpoint True
+      resources:
+        limits:
+          cpu: "32"
+          memory: "128Gi"
+    ttlStrategy:
+      secondsAfterCompletion: 7200
+
*** End Patch
*** Begin Patch
*** Add File: services/training/distributed_train.py
+"""
+Distributed training harness (placeholder)
+- In production this script should launch distributed processes across TPU VMs or worker pods
+- It writes checkpoints atomically to COMPLIANCE_BUCKET and emits Prometheus metrics via simple HTTP /metrics endpoint
+"""
+import argparse, time, os, json, random
+from prometheus_client import Counter, Gauge, start_http_server
+
+CKPT_COUNTER = Counter("aegis_checkpoint_uploaded_total", "Total checkpoints uploaded")
+TRAIN_STEPS = Counter("aegis_training_steps_total", "Training steps")
+TRAIN_RUNNING = Gauge("aegis_training_running", "Training running")
+
+def upload_checkpoint(bucket, key, data):
+    # placeholder: use boto3 or gsutil depending on bucket scheme
+    CKPT_COUNTER.inc()
+    return True
+
+def run(dataset, replicas, use_tpu):
+    start_http_server(9200)
+    TRAIN_RUNNING.set(1)
+    for step in range(20):
+        loss = random.random()
+        print("step", step, "loss", loss)
+        TRAIN_STEPS.inc()
+        # write atomic checkpoint: write to tmp then rename to final
+        ckpt = f"/tmp/ckpt_step_{step}.json"
+        with open(ckpt, "w") as f:
+            json.dump({"step": step, "loss": loss}, f)
+        if os.environ.get("COMPLIANCE_BUCKET"):
+            upload_checkpoint(os.environ["COMPLIANCE_BUCKET"], f"checkpoints/run-{int(time.time())}/ckpt_{step}.json", open(ckpt).read())
+        time.sleep(5)
+    TRAIN_RUNNING.set(0)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset", required=True)
+    p.add_argument("--replicas", default="8")
+    p.add_argument("--use-tpu", action="store_true")
+    args = p.parse_args()
+    run(args.dataset, int(args.replicas), args.use_tpu)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/milvus_tune.py
+#!/usr/bin/env python3
+"""
+Script to rebuild Milvus indexes and tune index params for large collections.
+Run this post-ingest as part of scale pipeline.
+"""
+from pymilvus import connections, utility, Collection
+import os
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST","localhost")
+MILVUS_PORT = os.environ.get("MILVUS_PORT","19530")
+COLLECTION = os.environ.get("MILVUS_COLLECTION","rag_demo")
+
+def tune():
+    connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)
+    if utility.has_collection(COLLECTION):
+        col = Collection(COLLECTION)
+        print("Dropping existing index if present")
+        try:
+            col.drop_index("vector")
+        except Exception:
+            pass
+        print("Creating IVF_SQ8 index with nlist=2048")
+        col.create_index("vector", {"index_type":"IVF_SQ8", "metric_type":"L2", "params":{"nlist":2048}})
+        col.load()
+        print("Index rebuilt and loaded")
+    else:
+        print("Collection not found:", COLLECTION)
+
+if __name__ == "__main__":
+    tune()
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/leader_election.py
+"""
+Kubernetes Lease-based leader election helper for the agent controller.
+Simplified pattern: attempts to acquire a Lease resource to become leader.
+If not leader, runs in standby and polls lease.
+"""
+from kubernetes import client, config
+import socket, time, uuid, os
+
+LEASE_NAME = os.environ.get("LEADER_LEASE_NAME", "agent-controller-lease")
+NAMESPACE = os.environ.get("LEADER_LEASE_NS", "staging")
+ID = f"{socket.gethostname()}-{uuid.uuid4().hex[:6]}"
+
+def load_kube():
+    try:
+        config.load_incluster_config()
+    except:
+        config.load_kube_config()
+
+def try_acquire_lease():
+    load_kube()
+    coord = client.CoordinationV1Api()
+    now = client.V1MicroTime()
+    body = client.V1Lease(
+        metadata=client.V1ObjectMeta(name=LEASE_NAME, namespace=NAMESPACE),
+        spec=client.V1LeaseSpec(holder_identity=ID)
+    )
+    try:
+        coord.create_namespaced_lease(namespace=NAMESPACE, body=body)
+        return True
+    except client.exceptions.ApiException as e:
+        # if exists, attempt to update if expired - simplified
+        return False
+
+def leader_loop(on_become_leader, on_standby, poll=5):
+    while True:
+        if try_acquire_lease():
+            on_become_leader()
+            # refresh loop while leader
+            time.sleep(poll)
+        else:
+            on_standby()
+            time.sleep(poll)
+
*** End Patch
*** Begin Patch
*** Add File: services/cost/budget_watcher_gcp.py
+"""
+Budget watcher using GCP Cloud Billing Budget API.
+Requires GOOGLE_APPLICATION_CREDENTIALS env var with service account that has billing.viewer and budget access.
+"""
+import os, time, google.auth
+from googleapiclient.discovery import build
+
+PROJECT_ID = os.environ.get("GCP_PROJECT")
+BUDGET_NAME = os.environ.get("GCP_BUDGET_NAME")
+THRESHOLD = float(os.environ.get("BUDGET_THRESHOLD", "90"))
+CHECK_INTERVAL = int(os.environ.get("CHECK_INTERVAL", "300"))
+
+def get_budget_percent(project, budget_name):
+    service = build('billingbudgets', 'v1')
+    # This is a placeholder: implement proper Budget API calls using the billing account and budget id.
+    # For demo we return 50.0
+    return 50.0
+
+def teardown_tpu_workflows():
+    os.system("kubectl -n staging delete workflow -l aegis/tpu=true || true")
+
+if __name__ == "__main__":
+    while True:
+        pct = get_budget_percent(PROJECT_ID, BUDGET_NAME)
+        print("budget pct", pct)
+        if pct >= THRESHOLD:
+            print("threshold exceeded - tearing down TPU workflows")
+            teardown_tpu_workflows()
+        time.sleep(CHECK_INTERVAL)
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_require_attested.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequiredAttestation
+metadata:
+  name: require-attestation
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["staging","ops"]
+  parameters:
+    # Require that the Pod template includes the annotation "aegis.attested=true"
+    annotation_key: "aegis.attested"
+    annotation_value: "true"
+
*** End Patch
*** Begin Patch
*** Add File: services/attestation/annotator_hook.py
+"""
+Annotator that can be called post-verify to add annotation to manifests (optional).
+In a CI flow, after attestation passes we can patch the deployment manifest to include aegis.attested=true.
+"""
+import subprocess, sys, json
+
+def annotate_manifest(manifest_file):
+    # naive: patch YAML to add annotation to metadata.template.metadata.annotations
+    # In CI prefer kubectl patch once the Deployment is created.
+    pass
+
*** End Patch
*** Begin Patch
*** Add File: tests/pen_test_runner.py
+#!/usr/bin/env python3
+"""
+Run a subset of the pen-test checklist automatically for basic validations.
+This script attempts:
+ - Deploy an unsigned test deployment and expect webhook/Gatekeeper to reject it
+ - Create a tool job that attempts egress to an external host and check NetworkPolicy (best-effort)
+ - Attempt to submit a model promotion without Rekor signature and expect OPA to deny
+"""
+import requests, subprocess, json, time, os
+
+API = os.environ.get("CONTROLLER_URL", "http://localhost:8200")
+ATT = os.environ.get("ATTESTATION_SERVICE", "http://localhost:8443/verify")
+
+def test_unsigned_deploy():
+    print("Attempting unsigned deploy (expected to be rejected)...")
+    # create minimal deployment YAML
+    out = subprocess.run(["kubectl","apply","-f","tests/unsigned_deploy.yaml"], capture_output=True, text=True)
+    print(out.stdout, out.stderr)
+
+def test_promotion_without_rekor():
+    print("Testing promotion without Rekor signature (expect OPA deny)")
+    body = {"artifact":{"image":"ghcr.io/yourorg/model:unsigned","sbom":{}},"action":"promote_model"}
+    r = requests.post("http://opa.ops.svc.cluster.local:8181/v1/data/aegis/policies/allow", json=body, timeout=5)
+    print("OPA resp:", r.status_code, r.text)
+
+if __name__ == "__main__":
+    test_unsigned_deploy()
+    test_promotion_without_rekor()
+
*** End Patch
*** Begin Patch
*** Add File: tests/unsigned_deploy.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: unsigned-test
+  namespace: staging
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: unsigned-test
+  template:
+    metadata:
+      labels:
+        app: unsigned-test
+    spec:
+      containers:
+      - name: app
+        image: ghcr.io/yourorg/unsigned-image:latest
+        command: ["sleep","3600"]
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/production_alerts.yaml
+groups:
+- name: aegis.production
+  rules:
+  - alert: CheckpointUploadFailure
+    expr: increase(aegis_checkpoint_uploaded_total[1h]) == 0 and increase(aegis_training_steps_total[1h]) > 0
+    for: 30m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Checkpoint uploads failing for training jobs"
+
+  - alert: MilvusSearchLatencyHigh
+    expr: histogram_quantile(0.95, sum(rate(aegis_milvus_search_latency_seconds_bucket[5m])) by (le)) > 1.0
+    for: 5m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Milvus search p95 > 1s"
+
+  - alert: AgentToolEgressAttempt
+    expr: increase(aegis_agent_egress_blocked_total[5m]) > 0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Blocked agent egress attempts observed"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/production_deploy_runbook.md
+# Production Deploy & Validation Runbook (high level)
+
+1) Pre-requisites
+ - Ensure KMS key ARN available (cosign), Rekor URL, Vault configured, OIDC role for GitHub Actions.
+ - Ensure staging cluster has namespaces: staging, ops, agent-exec.
+
+2) Deploy infra
+ - Install cert-manager.
+ - Deploy Vault via Helm (helm install hashicorp/vault -f helm/vault/values.yaml).
+ - Deploy Postgres (managed or operator), Redis cluster and Milvus via Helm using milvus-ha values.
+ - Deploy OPA, Gatekeeper, Argo, Prometheus, Grafana.
+
+3) Attestation webhook
+ - Run scripts/create_webhook_certs.sh ops attestation attestation-tls and apply caBundle to ValidatingWebhookConfiguration.
+ - Deploy attestation service and expose TLS secret.
+
+4) CI wiring
+ - Configure GitHub Actions secrets: REGISTRY_HOST, REGISTRY_USER, REGISTRY_TOKEN, REKOR_URL, AWS_OIDC_ROLE_ARN, AWS_KMS_KEY_ID, AWS_REGION, VAULT_ADDR, VAULT_TOKEN (bootstrap only).
+ - Enable workflows/ci_cosign_kms_rekor.yml
+
+5) Smoke tests
+ - Run tests/pen_test_runner.py and tests/scale_e2e_checklist.md items.
+ - Verify attestation rejects unsigned images, Gatekeeper constraints enforced, attestation service returns ok for signed images.
+
*** End Patch
*** End Patch
