*** Begin Patch
*** Add File:quantum/hsm/ansible/install_pkcs11.yml
+---
+# Ansible playbook to install PKCS#11 module (SoftHSM for staging or vendor module)
+# Usage:
+# ansible-playbook -i inventory install_pkcs11.yml --extra-vars "pkcs11_module=/usr/lib/softhsm/libsofthsm2.so use_softhsm=true"
+
+- hosts: signing_hosts
+  become: true
+  vars:
+    use_softhsm: true
+    softhsm_version: "2.8.0"
+    pkcs11_module: "/usr/lib/softhsm/libsofthsm2.so"
+  tasks:
+    - name: Ensure apt cache updated (Debian/Ubuntu)
+      apt:
+        update_cache: yes
+      when: ansible_os_family == "Debian"
+
+    - name: Install SoftHSM & tools (Debian)
+      apt:
+        name:
+          - softhsm2
+          - pkcs11-tool
+          - opensc
+        state: present
+      when:
+        - ansible_os_family == "Debian"
+        - use_softhsm | bool
+
+    - name: Create SoftHSM token dir
+      file:
+        path: /var/lib/softhsm/tokens
+        state: directory
+        owner: root
+        mode: 0755
+      when: use_softhsm | bool
+
+    - name: Ensure PKCS#11 module path (placeholder)
+      file:
+        path: "{{ pkcs11_module }}"
+        state: touch
+
+    - name: Initialize SoftHSM token (idempotent)
+      command: softhsm2-util --init-token --free --label "aegis-pq-token" --so-pin 123456 --pin 123456
+      args:
+        creates: /var/lib/softhsm/tokens
+      when: use_softhsm | bool
+
+    - name: Create README note for vendor module installation
+      copy:
+        dest: /etc/aegis-pkcs11/README.txt
+        content: |
+          If you are using a vendor HSM, install their PKCS#11 module (shared object) to the signing host and set pkcs11_module variable accordingly.
+
*** End Patch
*** Begin Patch
*** Add File:quantum/hsm/vendor_validation/run_hsm_end_to_end_playbook.md
+HSM End-to-end Provisioning & Validation Playbook
+-------------------------------------------------
+
+Run this playbook after vendor HSM is provisioned and network connectivity to signing hosts established.
+
+Steps:
+1. Install PKCS#11 module on signing hosts using Ansible:
+   ansible-playbook -i inventory quantum/hsm/ansible/install_pkcs11.yml --extra-vars "use_softhsm=false pkcs11_module=/opt/vendor/lib/vendor_pkcs11.so"
+
+2. Copy vendor PKCS#11 module path and slot info to Vault:
+   ./quantum/vault/write_hsm_config_and_pubkey.sh --vault-path secret/data/hsm/config --pkcs11-lib /opt/vendor/lib/vendor_pkcs11.so --slot 0 --token-label vendor-token --pubkey /tmp/vendor_pub.pem
+
+3. Create test artifact and sign:
+   echo "hsm test" > /tmp/hsm-test.bin
+   ./quantum/hsm/validate_hsm_end_to_end.sh --artifact /tmp/hsm-test.bin --pkcs11-lib /opt/vendor/lib/vendor_pkcs11.so --pkcs11-slot 0 --pkcs11-pin 1234 --pkcs11-keylabel pqkey --s3-bucket my-hsm-audit-bucket --rekor
+
+4. Verify audit logs in S3:
+   python3 quantum/hsm/vendor_validation/verify_hsm_audit.py --s3-bucket my-hsm-audit-bucket --prefix hsm-audit/ --timeout 600
+
+5. Test rotation:
+   Use quantum/hsm/hsm_rotation.sh to rotate keys (operator creates new key in vendor HSM per vendor tools), then publish new public key to Vault and verify signing with new key.
+
+Evidence to collect:
+- hybrid-signature.json (from sign step)
+- Rekor upload output (if enabled)
+- HSM audit logs from S3
+- Vault entry with public key
+
*** End Patch
*** Begin Patch
*** Add File:quantum/vault/templates/provider_creds_example.md
+Vault provider credentials template (example)
+------------------------------------------
+
+Paths and keys (suggested):
+- secret/data/quantum/providers/braket
+  - braket_device: arn:aws:braket:...
+  - aws_access_key_id: ...
+  - aws_secret_access_key: ...
+
+- secret/data/quantum/providers/ibm
+  - ibm_token: <QISKIT_IBM_TOKEN>
+  - ibm_instance: <optional instance id>
+
+Operator should use quantum/vault/scripts/publish_provider_creds.sh to write secrets.
+
*** End Patch
*** Begin Patch
*** Add File:quantum/pilot/postprocess/fetch_and_playback.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Fetch pilot artifacts (meta and noise) from S3 and run simulator playback
+# Usage:
+#   ./fetch_and_playback.sh --s3-bucket my-bucket --prefix braket/results/<job_id>/ --qasm /path/to/qasm
+
+S3_BUCKET=""
+PREFIX=""
+QASM=""
+TMPDIR="/tmp/aegis_pilot"
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --prefix) PREFIX="$2"; shift 2;;
+    --qasm) QASM="$2"; shift 2;;
+    *) echo "Unknown $1"; exit 1;;
+  esac
+done
+
+: "${S3_BUCKET:?--s3-bucket required}"
+: "${PREFIX:?--prefix required}"
+: "${QASM:?--qasm required}"
+
+mkdir -p "${TMPDIR}"
+echo "Downloading objects under s3://${S3_BUCKET}/${PREFIX} to ${TMPDIR}"
+aws s3 cp "s3://${S3_BUCKET}/${PREFIX}" "${TMPDIR}" --recursive
+
+# Find noise snapshot (heuristic)
+NOISE_JSON=$(ls ${TMPDIR}/*noise*.json 2>/dev/null | head -n1 || true)
+if [ -z "${NOISE_JSON}" ]; then
+  echo "No noise JSON found; looking for backend meta..."
+  NOISE_JSON=$(ls ${TMPDIR}/*meta*.json 2>/dev/null | head -n1 || true)
+fi
+
+if [ -n "${NOISE_JSON}" ]; then
+  echo "Found noise/meta file: ${NOISE_JSON}"
+  python3 repro/simulator_playback.py --qasm "${QASM}" --noise "${NOISE_JSON}"
+else
+  echo "No noise snapshot available; cannot perform deterministic playback."
+fi
+
*** End Patch
*** Begin Patch
*** Add File:broker/terraform/provision_rds_and_outputs.tf
+/*
+Example Terraform wrapper to provision managed Postgres (RDS) and output connection string for Kubernetes secret.
+Operator must populate variables in broker/terraform/terraform.tfvars.
+*/
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_db_subnet_group" "aegis" {
+  name       = "aegis-db-subnet-group"
+  subnet_ids = var.db_subnet_ids
+}
+
+resource "aws_db_instance" "aegis_postgres" {
+  allocated_storage    = 100
+  engine               = "postgres"
+  engine_version       = "13.7"
+  instance_class       = "db.m6g.large"
+  name                 = var.db_name
+  username             = var.db_username
+  password             = var.db_password
+  multi_az             = true
+  db_subnet_group_name = aws_db_subnet_group.aegis.name
+  skip_final_snapshot  = true
+}
+
+output "job_database_url" {
+  value = "postgresql://${aws_db_instance.aegis_postgres.username}:${var.db_password}@${aws_db_instance.aegis_postgres.address}:${aws_db_instance.aegis_postgres.port}/${aws_db_instance.aegis_postgres.name}"
+}
+
+variable "aws_region" { type = string }
+variable "db_subnet_ids" { type = list(string) }
+variable "db_username" { type = string }
+variable "db_password" { type = string, sensitive = true }
+variable "db_name" { type = string, default = "aegis_jobs" }
+
*** End Patch
*** Begin Patch
*** Add File:broker/runbooks/broker_cutover_steps.md
+Broker production cutover runbook
+--------------------------------
+
+Prereqs:
+- Terraform variables for RDS prepared and reviewed.
+- Cluster admin credentials and cert-manager installed or available for install.
+- Vault or external-secrets configured to deliver DB and JWT secrets.
+
+Steps:
+1. Provision RDS:
+   pushd broker/terraform && terraform init && terraform apply -var-file=prod.tfvars
+   Note the job_database_url output.
+
+2. Create Kubernetes secrets (prefer external-secrets or Vault CSI):
+   kubectl create secret generic aegis-db-secret -n aegis --from-literal=JOB_DATABASE_URL="<job_database_url>"
+   kubectl create secret generic aegis-broker-secret -n aegis --from-literal=BROKER_API_JWT_SECRET="<jwt_secret>"
+
+3. Install cert-manager (if not present):
+   kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
+   kubectl apply -f broker/k8s/cert-manager-issuer.yaml
+
+4. Deploy Helm chart with production values:
+   helm upgrade --install aegis-quantum-broker broker/helm -n aegis --values broker/helm/values-production.yaml
+
+5. Deploy worker and HPA:
+   kubectl apply -f broker/k8s/worker-deployment-autoscale.yaml
+
+6. Validate:
+   - Run broker/scripts/validate_broker_prod.sh
+   - Use broker/k8s/validate_mtls.sh with operator-provided certs to verify mTLS
+   - Trigger jwt rotation: quantum/job_broker/jwt_rotate.sh
+   - Produce load via broker/runbooks/load_test_hpa.sh to exercise autoscaling
+
+7. Smoke tests:
+   - Submit a small job via broker API with valid JWT and ensure job flows to worker and completes.
+
*** End Patch
*** Begin Patch
*** Add File:broker/runbooks/load_test_hpa.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Simple load generator to create many job submissions to the broker to trigger autoscaling.
+BROKER_URL=${1:-http://aegis-quantum-broker.aegis.svc.cluster.local/submit}
+TOKEN=${2:-}
+COUNT=${3:-50}
+if [ -z "$TOKEN" ]; then
+  echo "Provide JWT token as second argument"
+  exit 2
+fi
+for i in $(seq 1 $COUNT); do
+  curl -s -X POST "$BROKER_URL" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"circuit_qasm":"OPENQASM 2.0;","shots":1024,"backend":"aer_simulator"}' || true
+done
+echo "Submitted ${COUNT} jobs to broker to exercise HPA"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/rekor_pr_gate.yml
+name: Rekor PR Gate (Enforce MLflow Rekor tags for recent pilot runs)
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+jobs:
+  enforce-rekor:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: pip install mlflow
+      - name: Check MLflow Rekor tags
+        env:
+          MLFLOW_URL: ${{ secrets.MLFLOW_URL }}
+        run: |
+          python3 quantum/rekor/check_mlflow_rekor.py --mlflow-url "${MLFLOW_URL}" --experiment quantum-pilots --threshold 5
+
*** End Patch
*** Begin Patch
*** Add File:compliance/auditor/deterministic_signing_README.md
+Deterministic Signing & Rekor Evidence for Auditors
+--------------------------------------------------
+
+Goal: provide deterministic, reproducible evidence that signing + Rekor submission is performed for promoted artifacts.
+
+Contents:
+- Steps to reproduce signing:
+  1. Retrieve artifact (hash) from CI/CD pipeline.
+  2. Use cosign key (stored in Vault) to run: cosign sign-blob --key <key> --output-signature /tmp/art.sig <artifact>
+  3. Upload signature + artifact to Rekor: rekor-cli upload --artifact <artifact> --signature /tmp/art.sig --output json
+
+- CI enforcement:
+  - The repository has .github/workflows/rekor_pr_gate.yml that fails PRs if recent pilot runs lack Rekor entries.
+
+- Auditor evidence bundle:
+  - Run compliance/packager.py to collect broker logs, Rekor outputs and HSM audit logs.
+  - Provide the produced tar.gz to the auditor with an evidence manifest.
+
+Contact operators for access to Vault keys and to run the deterministic steps.
+
*** End Patch
*** Begin Patch
*** Add File:observability/cur/aws_cur_integration_notes.md
+AWS CUR Integration Notes
+-------------------------
+
+1. Create AWS Cost & Usage Report in Billing console and deliver to S3 bucket.
+2. Ensure CUR is configured to include resource tags used to identify jobs (e.g., "aegis:job-id" or team tags).
+3. Create an ingestion job (Lambda or scheduled ECS task) that:
+   - Downloads latest CUR files
+   - Maps usage lines to job IDs/tags and aggregates cost per job or per team
+   - Writes simplified CSV consumed by broker/quotas/chargeback_connector.py
+4. Use observability/cur/ingest_cur.sh as a placeholder driver for ingestion and transformation.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/legal/final_steps_checklist.md
+Legal & Compliance Final Steps Checklist
+---------------------------------------
+
+1. Vendor Contracts
+   - Ensure HSM vendor contract signed with audit log access and PKCS#11 support clause.
+   - Ensure QPU vendors (Braket/IBM/Azure) terms permit data handling and provide required SLAs.
+
+2. Export Control
+   - Legal to review cryptography inventory and QPU provider jurisdictions.
+   - Record whether export licenses are required and keep approvals in compliance/evidence/.
+
+3. SOC2 & Auditor
+   - Schedule auditor engagement and provide the deterministic_signing_README and evidence bundle.
+   - Provide HSM audit logs, Rekor entries, MLflow runs for auditor review.
+
+4. Evidence Collection
+   - Run compliance/packager.py or compliance/audit_packager_enhanced.sh to produce tar.gz for auditors.
+
*** End Patch
*** End Patch
