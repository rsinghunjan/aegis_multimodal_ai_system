*** Begin Patch
*** Add File:demo/README_DEMO.md
+# Aegis End-to-End Staging Demo
+
+This demo runs a full staging flow:
+1. Ingest sample dataset
+2. Run Great Expectations (data quality)
+3. Run Feast offline ingestion (Parquet to S3)
+4. Submit Spark training job (SparkOperator)
+5. Upload model artifact to S3 and sign it (HSM-first, cosign fallback)
+6. Create MCP JSON and record signature metadata
+7. Deploy A/B rollout (two Deployments + TrafficSplit)
+8. Generate synthetic traffic to both variants
+9. Run Argo analysis workflow to compute A/B metrics and produce evidence bundle
+
+Assumptions / defaults
+- Kubernetes namespace: `aegis`
+- SparkOperator installed
+- Argo installed
+- Prometheus available at `http://prometheus:9090`
+- S3 bucket for artifacts set via env var `MODEL_ARTIFACT_BUCKET`
+- MLflow tracking via env `MLFLOW_URL` (optional)
+- Sign helper available (production.policy.signing.sign_with_retry in repo)
+
+How to run (quick)
+1. Ensure namespace exists: `kubectl create ns aegis || true`
+2. Apply demo manifests:
+   kubectl apply -f demo/argo/demo_pipeline.yaml -n aegis
+3. Submit the Argo workflow:
+   argo submit demo/argo/demo_pipeline.yaml -n aegis --watch
+
+CI
+- A GitHub Actions workflow is provided at `.github/workflows/demo_run.yml` to submit the demo (requires kubeconfig secret).
+
+Acceptance criteria
+- Training completes and writes artifact to S3.
+- Artifact is signed and MCP JSON is created and stored in `/tmp/mcp` (or uploaded).
+- A/B analysis produces result JSON and evidence bundle created at `/tmp/aegis_demo_evidence.tar.gz`.
+
*** End Patch
*** Begin Patch
*** Add File:demo/data/sample_data.csv
+id,feature_1,feature_2,label
+0,0.12,1,0
+1,0.75,0,1
+2,0.33,1,0
+3,0.90,0,1
+4,0.11,1,0
+5,0.44,0,1
+6,0.55,1,1
+7,0.22,0,0
+8,0.99,1,1
+9,0.01,0,0
+
*** End Patch
*** Begin Patch
*** Add File:demo/argo/demo_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-demo-pipeline-
+  namespace: aegis
+spec:
+  entrypoint: demo
+  templates:
+    - name: demo
+      steps:
+        - - name: ingest
+            template: ingest
+        - - name: ge-check
+            template: ge-check
+        - - name: feast-ingest
+            template: feast-ingest
+        - - name: spark-train
+            template: spark-train
+        - - name: sign-and-mcp
+            template: sign-and-mcp
+        - - name: deploy-ab
+            template: deploy-ab
+        - - name: traffic
+            template: gen-traffic
+        - - name: analysis
+            template: ab-analysis
+        - - name: evidence
+            template: evidence-collect
+
+    - name: ingest
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - mkdir -p /tmp/data && cp demo/data/sample_data.csv /tmp/data/sample_data.csv && echo "Data copied"
+
+    - name: ge-check
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install great_expectations pandas pyarrow && python3 dq/run_ge_check.py --data /tmp/data/sample_data.csv || true
+
+    - name: feast-ingest
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pandas pyarrow boto3 && python3 feast/ingest.py
+
+    - name: spark-train
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - |
+            echo "Applying SparkApplication..."
+            kubectl apply -f spark/full/sparkapplication.yaml -n aegis
+            # wait for completion (simple loop)
+            for i in $(seq 1 120); do
+              phase=$(kubectl -n aegis get sparkapplication aegis-pyspark-train -o jsonpath='{.status.applicationState.state}' 2>/dev/null || echo "")
+              echo "Spark phase: $phase"
+              if [ "$phase" = "COMPLETED" ]; then exit 0; fi
+              if [ "$phase" = "FAILED" ]; then exit 2; fi
+              sleep 10
+            done
+            echo "Timed out" && exit 1
+
+    - name: sign-and-mcp
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 demo/scripts/create_mcp.py --artifact-prefix ${MODEL_ARTIFACT_BUCKET:-""}/feast/model_output/
+
+    - name: deploy-ab
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - kubectl apply -f examples/experiment_split.yaml -n aegis || true
+
+    - name: gen-traffic
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests && python3 demo/scripts/generate_traffic.py --target http://model-proxy.aegis.svc:80 --duration 60
+
+    - name: ab-analysis
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests && python3 argo/analysis_workflow.py --experiment churn-model-ab --window 1h
+
+    - name: evidence-collect
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 compliance/evidence_collector_enhanced.py --out /tmp/aegis_demo_evidence.tar.gz && ls -l /tmp/aegis_demo_evidence.tar.gz
+
*** End Patch
*** Begin Patch
*** Add File:demo/scripts/generate_traffic.py
+#!/usr/bin/env python3
+"""
+Generate synthetic traffic to a model endpoint. Sends JSON POSTs with user_id and input features.
+Usage:
+  python3 generate_traffic.py --target http://model-proxy.aegis.svc:80 --duration 60
+"""
+import argparse, requests, time, random, threading
+
+def send_loop(target, duration, qps=5):
+    end = time.time() + duration
+    count = 0
+    while time.time() < end:
+        user = random.randint(1,1000)
+        payload = {"user_id": str(user), "features": {"f1": random.random(), "f2": random.randint(0,1)}}
+        try:
+            r = requests.post(target, json=payload, timeout=2)
+            # optionally print
+        except Exception:
+            pass
+        count += 1
+        time.sleep(1.0 / qps)
+    print("Sent", count, "requests")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--target", required=True)
+    p.add_argument("--duration", type=int, default=30)
+    p.add_argument("--qps", type=int, default=5)
+    args = p.parse_args()
+    send_loop(args.target, args.duration, args.qps)
+
*** End Patch
*** Begin Patch
*** Add File:demo/scripts/create_mcp.py
+#!/usr/bin/env python3
+"""
+Create a simple MCP JSON for the trained artifact and sign it using sign_with_retry (HSM-first, cosign fallback).
+This script expects the model artifact to be uploaded to S3 by the Spark job.
+It will create /tmp/mcp/<model_id>.json and a .sig file next to it.
+"""
+import os, json, time, tempfile, boto3
+from production.policy.signing.sign_with_retry import sign_payload
+
+MODEL_BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET","")
+OUT_DIR = "/tmp/mcp"
+os.makedirs(OUT_DIR, exist_ok=True)
+
+def find_latest_model_prefix(prefix="feast/model_output/"):
+    if not MODEL_BUCKET:
+        return None
+    s3 = boto3.client("s3")
+    resp = s3.list_objects_v2(Bucket=MODEL_BUCKET, Prefix=prefix)
+    if "Contents" not in resp:
+        return None
+    # pick most recent object key
+    objs = sorted(resp["Contents"], key=lambda x: x["LastModified"], reverse=True)
+    return objs[0]["Key"]
+
+def create_mcp(model_uri):
+    mcp = {
+        "model_id": "demo-churn",
+        "model_version": str(int(time.time())),
+        "artifact_uri": model_uri,
+        "created_at": time.time()
+    }
+    payload = json.dumps(mcp, indent=2).encode()
+    # sign
+    sighex, meta = sign_payload(payload, None)
+    if sighex:
+        mcp["signature"] = sighex
+        mcp["signed_by"] = meta.get("keylabel", meta.get("method"))
+    else:
+        mcp["signature"] = ""
+        mcp["signed_by"] = "none"
+        mcp["_signature_meta"] = meta
+    out = os.path.join(OUT_DIR, f"{mcp['model_id']}_{mcp['model_version']}.json")
+    with open(out, "w") as f:
+        json.dump(mcp, f, indent=2)
+    print("Wrote MCP to", out)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--artifact-prefix", default="")
+    args = p.parse_args()
+    # This demo simply uses prefix or env to generate a stub model_uri
+    model_uri = f"s3://{MODEL_BUCKET}/feast/model_output/" if MODEL_BUCKET else "/tmp/model_out"
+    create_mcp(model_uri)
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/demo_run.yml
+name: Run Aegis E2E Demo
+on:
+  workflow_dispatch:
+
+jobs:
+  run-demo:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Setup kubectl
+        uses: azure/setup-kubectl@v3
+        with:
+          version: 'v1.27.3'
+      - name: Install Argo CLI
+        run: |
+          curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.5.6/argo-linux-amd64.gz
+          gunzip argo-linux-amd64.gz
+          chmod +x argo-linux-amd64
+          sudo mv argo-linux-amd64 /usr/local/bin/argo
+      - name: Configure kubeconfig
+        env:
+          KUBECONFIG_DATA: ${{ secrets.KUBECONFIG }}
+        run: |
+          echo "$KUBECONFIG_DATA" | base64 --decode > kubeconfig
+          export KUBECONFIG=$PWD/kubeconfig
+          kubectl config current-context
+      - name: Submit Argo demo
+        run: |
+          argo submit demo/argo/demo_pipeline.yaml -n aegis --watch
+
*** End Patch
*** Begin Patch
*** Add File:feast/helm/values.yaml
+# Minimal Feast Helm values (staging)
+feast:
+  onlineStore:
+    redis:
+      host: redis.aegis.svc
+      port: 6379
+  offlineStore:
+    s3:
+      bucket: ${MODEL_ARTIFACT_BUCKET:-"your-bucket"}
+      prefix: feast/offline
+  cache:
+    enabled: false
+  job:
+    resources:
+      limits:
+        cpu: "500m"
+        memory: "1Gi"
+
*** End Patch
*** Begin Patch
*** Add File:redis/redis-cluster.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: redis
+  namespace: aegis
+spec:
+  ports:
+    - port: 6379
+  selector:
+    app: redis
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: redis
+  namespace: aegis
+spec:
+  serviceName: "redis"
+  replicas: 1
+  selector:
+    matchLabels:
+      app: redis
+  template:
+    metadata:
+      labels:
+        app: redis
+    spec:
+      containers:
+        - name: redis
+          image: redis:6.2
+          ports:
+            - containerPort: 6379
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "256Mi"
+            limits:
+              cpu: "500m"
+              memory: "1Gi"
+          volumeMounts:
+            - name: data
+              mountPath: /data
+          args: ["redis-server","--appendonly","yes"]
+  volumeClaimTemplates:
+    - metadata:
+        name: data
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 1Gi
+
*** End Patch
*** Begin Patch
*** Add File:feast/ci/redis_bench.py
+#!/usr/bin/env python3
+"""
+Simple benchmark script to validate online lookup latency against Redis.
+Usage: python3 redis_bench.py --host redis.aegis.svc --port 6379 --duration 30
+"""
+import argparse, time, statistics, redis, json
+
+def run(host, port, duration, qps=50):
+    r = redis.StrictRedis(host=host, port=port, decode_responses=False)
+    latencies = []
+    end = time.time() + duration
+    while time.time() < end:
+        t0 = time.time()
+        try:
+            r.get("nonexistent:key")
+        except Exception:
+            pass
+        latencies.append((time.time() - t0) * 1000.0)
+        time.sleep(1.0 / qps)
+    p95 = statistics.quantiles(latencies, n=100)[94] if len(latencies) >= 100 else max(latencies)
+    print(json.dumps({"p95_ms": p95, "samples": len(latencies)}))
+    return p95
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--host", default="redis.aegis.svc")
+    p.add_argument("--port", type=int, default=6379)
+    p.add_argument("--duration", type=int, default=30)
+    args = p.parse_args()
+    p95 = run(args.host, args.port, args.duration)
+    if p95 > 50:
+        print("P95 exceeded 50ms:", p95)
+        raise SystemExit(2)
+    print("P95 OK:", p95)
+
*** End Patch
*** Begin Patch
*** Add File:serving/k8s/model_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-model
+  namespace: aegis
+  labels:
+    app: aegis-model
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-model
+  template:
+    metadata:
+      labels:
+        app: aegis-model
+    spec:
+      containers:
+        - name: model
+          image: registry.example.com/aegis/model:staging
+          ports:
+            - containerPort: 8080
+          readinessProbe:
+            httpGet:
+              path: /ready
+              port: 8080
+            initialDelaySeconds: 5
+            periodSeconds: 5
+          livenessProbe:
+            httpGet:
+              path: /healthz
+              port: 8080
+            initialDelaySeconds: 20
+            periodSeconds: 10
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+            limits:
+              cpu: "1000m"
+              memory: "1Gi"
+
+---
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-model-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-model
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
+---
+apiVersion: policy/v1
+kind: PodDisruptionBudget
+metadata:
+  name: aegis-model-pdb
+  namespace: aegis
+spec:
+  minAvailable: 1
+  selector:
+    matchLabels:
+      app: aegis-model
+
*** End Patch
*** Begin Patch
*** Add File:serving/argo_rollouts/rollout.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Rollout
+metadata:
+  name: aegis-model-rollout
+  namespace: aegis
+spec:
+  replicas: 4
+  strategy:
+    canary:
+      steps:
+        - setWeight: 10
+        - pause: {duration: 60s}
+        - setWeight: 50
+        - pause: {duration: 120s}
+  selector:
+    matchLabels:
+      app: aegis-model
+  template:
+    metadata:
+      labels:
+        app: aegis-model
+    spec:
+      containers:
+        - name: model
+          image: registry.example.com/aegis/model:staging
+          ports:
+            - containerPort: 8080
+          readinessProbe:
+            httpGet:
+              path: /ready
+              port: 8080
+            initialDelaySeconds: 5
+            periodSeconds: 5
+
*** End Patch
*** Begin Patch
*** Add File:serving/prometheus/rules.yaml
+groups:
+- name: aegis-slo
+  rules:
+    - record: job:request_latency_seconds:histogram_quantile_95
+      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
+    - alert: ModelLatencyHigh
+      expr: job:request_latency_seconds:histogram_quantile_95 > 0.5
+      for: 2m
+      labels:
+        severity: page
+      annotations:
+        summary: "Model latency P95 > 0.5s"
+        description: "Model latency P95 exceeded threshold"
+
*** End Patch
*** Begin Patch
*** Add File:serving/bench/load_test.py
+#!/usr/bin/env python3
+"""
+Simple load generator that posts inference requests and records latencies.
+Usage: python3 load_test.py --target http://model-proxy.aegis.svc:80 --duration 30 --qps 50
+"""
+import argparse, time, requests, random, statistics, json
+
+def run(target, duration, qps):
+    end = time.time() + duration
+    lat = []
+    while time.time() < end:
+        payload = {"user_id": str(random.randint(1,100000)), "features": {"f1": random.random()}}
+        t0 = time.time()
+        try:
+            r = requests.post(target, json=payload, timeout=2)
+            t = (time.time() - t0) * 1000.0
+            lat.append(t)
+        except Exception:
+            lat.append(2000.0)
+        time.sleep(1.0/qps)
+    p50 = statistics.median(lat)
+    p95 = sorted(lat)[int(len(lat)*0.95)-1] if lat else None
+    print(json.dumps({"p50_ms": p50, "p95_ms": p95, "samples": len(lat)}))
+    return p50, p95
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--target", required=True)
+    p.add_argument("--duration", type=int, default=30)
+    p.add_argument("--qps", type=int, default=50)
+    args = p.parse_args()
+    run(args.target, args.duration, args.qps)
+
*** End Patch
*** Begin Patch
*** Add File:argo/analysis_workflow.py
+#!/usr/bin/env python3
+"""
+Wrapper to run the AB analysis: queries Prometheus for counts per variant and runs ab_analysis.py.
+This script is used by the demo Argo pipeline post-traffic.
+"""
+import os, json, requests, time, subprocess, tempfile
+
+PROM = os.environ.get("PROM_URL","http://prometheus:9090")
+EXP = os.environ.get("EXPERIMENT","churn-model-ab")
+
+def query_count(metric, window="1h"):
+    q = f"sum(increase({metric}{{app='exp-{EXP}'}}[{window}])) by (variant)"
+    r = requests.get(PROM + "/api/v1/query", params={"query": q}, timeout=20).json()
+    m = {}
+    if r.get("status") == "success":
+        for item in r["data"]["result"]:
+            variant = item["metric"].get("variant","unknown")
+            value = int(float(item["value"][1]))
+            m[variant] = value
+    return m
+
+def build_input():
+    totals = query_count("events_requests_total")
+    successes = query_count("events_success_total")
+    variants = {}
+    for v,tot in totals.items():
+        variants[v] = {"total": tot, "success": successes.get(v,0)}
+    return {"variants": variants, "alpha": 0.05}
+
+if __name__ == "__main__":
+    inp = build_input()
+    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
+    json.dump(inp, open(tmp.name, "w"))
+    out = subprocess.check_output(["python3", "tools/ab_analysis.py", tmp.name])
+    # store analysis result for evidence
+    os.makedirs("/tmp/aegis_analysis", exist_ok=True)
+    fn = f"/tmp/aegis_analysis/result_{int(time.time())}.json"
+    open(fn,"w").write(out.decode())
+    print("Wrote analysis to", fn)
+
*** End Patch
*** Begin Patch
*** Add File:compliance/evidence_collector_enhanced.py
+#!/usr/bin/env python3
+"""
+Collect evidence for demo: includes MCP files, HSM health, GE outputs, AB analysis outputs, and bundle into tar.gz
+"""
+import os, glob, tarfile, time
+
+OUT = "/tmp/aegis_demo_evidence.tar.gz"
+SRC_DIRS = ["/tmp/mcp", "/tmp/hsm_health", "/tmp/ge", "/tmp/aegis_analysis"]
+
+def collect():
+    with tarfile.open(OUT,"w:gz") as tf:
+        for d in SRC_DIRS:
+            if os.path.exists(d):
+                for f in glob.glob(os.path.join(d,"**"), recursive=True):
+                    if os.path.isfile(f):
+                        arcname = os.path.relpath(f, "/")
+                        tf.add(f, arcname=arcname)
+    print("Wrote evidence bundle to", OUT)
+
+if __name__ == "__main__":
+    collect()
+
*** End Patch
*** Begin Patch
*** Add File:docs/DEMO_PLAYBOOK.md
+# Demo Playbook: End-to-End Aegis Staging Demo
+
+Pre-reqs
+- Kubernetes cluster with `aegis` namespace
+- SparkOperator installed
+- Argo installed
+- Prometheus available
+- S3 bucket for MODEL_ARTIFACT_BUCKET (or leave empty for local)
+- (Optional) MLflow server and MLFLOW_URL
+
+Run
+1. Apply required infra (Redis, etc):
+   kubectl apply -f redis/redis-cluster.yaml -n aegis
+2. Deploy necessary CRDs/helm charts (SparkOperator, Argo)
+3. Submit demo Argo workflow:
+   argo submit demo/argo/demo_pipeline.yaml -n aegis --watch
+
+Validation
+- Confirm SparkApplication completed
+- Confirm /tmp/mcp contains MCP JSON files on the worker where create_mcp ran (or adapt script to upload to S3)
+- Confirm experiment was created and analysis result contains JSON under /tmp/aegis_analysis
+- Download evidence: /tmp/aegis_demo_evidence.tar.gz
+
+Cleanup
+- Remove demo deployments and SparkApplication:
+  kubectl delete -f demo/argo/demo_pipeline.yaml -n aegis
+
*** End Patch
*** Begin Patch
*** Update File:charts/aegis-enterprise/values.yaml
@@
 agent:
   enabled: true
   image:
     repository: registry.example.com/aegis/agent
     tag: latest
   resources:
     requests:
       cpu: "200m"
       memory: "256Mi"
     limits:
       cpu: "1000m"
       memory: "1Gi"
+  env:
+    - name: MODEL_ARTIFACT_BUCKET
+      value: ""
+
*** End Patch
*** Begin Patch
*** Add File:feast/README_PRODUCTIONIZE.md
+# Productionize Feast & Redis (quick guidance)
+
+1. Deploy Redis as an HA cluster (Redis Cluster or managed service). The provided `redis/redis-cluster.yaml` is a minimal StatefulSet for staging; use Bitnami Redis Helm or AWS Elasticache for production.
+2. Configure Feast Helm chart with S3 offline store and the Redis host as online store.
+3. Provision monitoring: Prometheus exporter for Redis, set SLO: P95 < 50ms for lookups.
+4. Add k8s readiness/liveness to Feast jobs and alerting rules for offline job failures.
+
*** End Patch
*** Begin Patch
*** Add File:serving/README_SERVING_HARDEN.md
+# Hardened Model Serving Guidance and Benchmarks
+
+Contents:
+- Deployment manifest with HPA, PDB and probes (serving/k8s/model_deployment.yaml)
+- Argo Rollout manifest (serving/argo_rollouts/rollout.yaml)
+- Prometheus SLO rules (serving/prometheus/rules.yaml)
+- Load test script (serving/bench/load_test.py)
+
+How to run benchmark:
+1. Ensure model-proxy or service is accessible (model-proxy service should route to Rollout/Deployment)
+2. Run: python3 serving/bench/load_test.py --target http://model-proxy.aegis.svc:80 --duration 60 --qps 50
+3. Compare p95 to SLO and tune HPA or resource requests accordingly.
+
*** End Patch
*** Begin Patch
*** Add File:ci/feast_redis_lookup_check.yml
+name: Feast Redis Lookup Check
+on:
+  workflow_dispatch:
+jobs:
+  redis-bench:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run redis benchmark
+        run: |
+          pip install redis
+          python3 feast/ci/redis_bench.py --host redis.aegis.svc --port 6379 --duration 20
+
*** End Patch
*** End Patch
