*** Begin Patch
*** Add File:security/hsm/vault_auto_unseal.tf
+/*
+ Example Terraform snippet to provision AWS KMS key for Vault auto-unseal and attach policy for Vault IAM principal.
+  - Operator must adapt to their account (key rotation, alias naming, and IAM principal).
+  - This is a best-effort scaffold; do not apply in production without review.
+*/
+provider "aws" {
+  region = var.aws_region
+}
+
+variable "aws_region" {
+  default = "us-east-1"
+}
+
+variable "vault_role_arn" {
+  description = "IAM role ARN that Vault (or the Vault operator) will assume to access the KMS key"
+  type        = string
+  default     = ""
+}
+
+resource "aws_kms_key" "vault_auto_unseal" {
+  description             = "Vault auto-unseal KMS key for Aegis"
+  deletion_window_in_days = 30
+  enable_key_rotation     = true
+  tags = {
+    Name    = "aegis-vault-auto-unseal"
+    project = "aegis"
+  }
+}
+
+resource "aws_kms_alias" "vault_alias" {
+  name          = "alias/aegis-vault-auto-unseal"
+  target_key_id = aws_kms_key.vault_auto_unseal.key_id
+}
+
+resource "aws_kms_key_rotation" "rotation" {
+  key_id = aws_kms_key.vault_auto_unseal.key_id
+}
+
+# Grant decrypt and generate-data-key to the Vault role (if provided)
+resource "aws_kms_key_policy" "vault_role_policy" {
+  key_id = aws_kms_key.vault_auto_unseal.key_id
+
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Id      = "key-default-1"
+    Statement = [
+      {
+        Sid      = "Allow administration of the key"
+        Effect   = "Allow"
+        Principal = {
+          AWS = ["arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"]
+        }
+        Action   = "kms:*"
+        Resource = "*"
+      },
+    ] ++ (length(var.vault_role_arn) > 0 ? [
+      {
+        Sid      = "Allow Vault role use of the key"
+        Effect   = "Allow"
+        Principal = {
+          AWS = [var.vault_role_arn]
+        }
+        Action = [
+          "kms:Decrypt",
+          "kms:GenerateDataKey*",
+          "kms:DescribeKey"
+        ]
+        Resource = "*"
+      }
+    ] : [])
+  })
+}
+
+data "aws_caller_identity" "current" {}
+
+output "kms_key_id" {
+  value = aws_kms_key.vault_auto_unseal.key_id
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/gatekeeper/constraint-template-require-cosign.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequiredcosign
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequiredCosign
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8srequiredcosign
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Pod"  # apply to pods (works for deployments via podTemplate)
+          containers := input.review.object.spec.containers
+          some i
+          container := containers[i]
+          image := container.image
+          # Expect image annotation or label that indicates cosign signature presence; operators should populate via CI/CD
+          not input.review.object.metadata.annotations["aegis.cosign.signed"]
+          msg := sprintf("Container image %v lacks cosign signature annotation (aegis.cosign.signed). Blocked by policy.", [image])
+        }
+
*** End Patch
*** Begin Patch
*** Add File:security/gatekeeper/constraint-require-cosign.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequiredCosign
+metadata:
+  name: require-cosign-signature
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["staging", "production"]
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cosign_sign_and_verify.yml
+name: Build, SBOM, Cosign Sign and Rekor Log
+
+on:
+  push:
+    branches:
+      - main
+  pull_request:
+
+jobs:
+  build-sign:
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+      packages: write
+      id-token: write
+
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+
+      - name: Build image
+        run: |
+          IMAGE=ghcr.io/${{ github.repository_owner }}/aegis-agent-controller:pr-${{ github.run_id }}
+          docker build -t $IMAGE ./agents/controller
+          echo "IMAGE=$IMAGE" >> $GITHUB_ENV
+
+      - name: Generate SBOM (syft)
+        run: |
+          pip install syft
+          syft $IMAGE -o json > sbom.json
+          cat sbom.json
+
+      - name: Push image
+        run: |
+          echo "${{ secrets.GHCR_PAT }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
+          docker push $IMAGE
+
+      - name: Cosign sign image (Vault or key)
+        env:
+          COSIGN_EXPERIMENTAL: "1"
+        run: |
+          # Operator: configure signing method; here we assume Cosign key in repo secret (for demo only)
+          COSIGN_PASSWORD=${{ secrets.COSIGN_PASSWORD }}
+          echo "$COSIGN_PASSWORD" | cosign sign --key cosign.key $IMAGE
+          cosign verify --key cosign.pub $IMAGE
+
+      - name: Annotate manifest example (for Gatekeeper)
+        run: |
+          echo "Operator note: CI should annotate manifests with aegis.cosign.signed=true so Gatekeeper allows deployment."
+
*** End Patch
*** Begin Patch
*** Add File:vector/qdrant/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: qdrant
+  namespace: staging
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: qdrant
+  template:
+    metadata:
+      labels:
+        app: qdrant
+    spec:
+      containers:
+        - name: qdrant
+          image: qdrant/qdrant:latest
+          ports:
+            - containerPort: 6333
+          env:
+            - name: QDRANT__SERVICE__GRPC_PORT
+              value: "6334"
+
*** End Patch
*** Begin Patch
*** Add File:rag/ingest.py
+#!/usr/bin/env python3
+"""
+RAG ingestion helper: chunk documents, call embedder, push vectors to vector DB (Qdrant example).
+Operator: replace embedder_url and vector upsert endpoint with your service endpoints.
+"""
+import requests, os, json, sys
+from pathlib import Path
+
+EMBED_URL = os.environ.get("EMBEDDER_URL","http://vector:8000/embed")
+QDRANT_UPSERT = os.environ.get("QDRANT_UPSERT","http://qdrant:6333/collections/aegis/points?wait=true")
+
+def chunk_text(text, chunk_size=1000):
+    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
+
+def embed_texts(texts):
+    r = requests.post(EMBED_URL, json={"texts": texts}, timeout=30)
+    r.raise_for_status()
+    return r.json().get("vectors", [])
+
+def upsert_vectors(ids, vectors, payloads):
+    data = {"points": [{"id": ids[i],"vector": vectors[i],"payload": payloads[i]} for i in range(len(ids))]}
+    r = requests.put(QDRANT_UPSERT, json=data, timeout=30)
+    r.raise_for_status()
+    return r.json()
+
+def ingest_file(path):
+    text = Path(path).read_text()
+    chunks = chunk_text(text)
+    vecs = embed_texts(chunks)
+    ids = [f"{Path(path).stem}-{i}" for i in range(len(chunks))]
+    payloads = [{"source": str(path), "chunk_index": i} for i in range(len(chunks))]
+    res = upsert_vectors(ids, vecs, payloads)
+    print("Upsert result:", res)
+
+if __name__ == "__main__":
+    for p in sys.argv[1:]:
+        ingest_file(p)
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_aggregation/README.md
+# Secure aggregation note (scaffold)
+
+This directory contains notes and a small scaffold to integrate secure aggregation for federated learning:
+- use existing Flower server & clients (federated/)
+- for secure aggregation use a library such as "phe" (Paillier) or integrate threshold-based MPC.
+
+Operator action:
+- Evaluate threat model and choose DP or MPC approach.
+- Use TLS mutual auth (SPIRE) to ensure clients are genuine before contributing updates.
+
*** End Patch
*** Begin Patch
*** Add File:dr/vault_snapshot.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create a Vault snapshot using CLI (operator-run).
+# Requires VAULT_ADDR and appropriate token with snapshot permissions.
+#
+OUT=${1:-/tmp/vault-snapshot.snap}
+echo "Creating Vault snapshot to $OUT"
+vault operator raft snapshot save "$OUT"
+echo "Snapshot saved. Operator should push it to secure object store and limit access."
+
*** End Patch
*** Begin Patch
*** Add File:dr/vault_restore.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Restore a Vault snapshot (operator-run; destructive).
+# Usage:
+#  ./vault_restore.sh /path/to/snapshot.snap
+#
+SNAP=${1:-}
+if [ -z "$SNAP" ]; then
+  echo "Usage: $0 /path/to/snapshot.snap"
+  exit 2
+fi
+echo "Restoring Vault from snapshot: $SNAP"
+vault operator raft snapshot restore "$SNAP"
+echo "Restore requested. Follow Vault docs for unseal and recovery."
+
*** End Patch
*** Begin Patch
*** Add File:carbon/controller/scheduler_extender.py
+"""
+Simple carbon-aware scheduler extender (prototype).
+Watches pending pods (via Kubernetes API) and annotates them with preferred nodeSelector based on carbon.json.
+This is a prototype; in production implement a real scheduling extender or use cluster autoscaler hooks.
+"""
+import json, time, os
+from kubernetes import client, config
+
+CARBON_FILE="/var/lib/aegis/carbon.json"
+
+def load_carbon():
+    if not os.path.exists(CARBON_FILE):
+        return {}
+    with open(CARBON_FILE) as f:
+        return json.load(f).get("regions", {})
+
+def pick_low_carbon_region(regions):
+    best = min(regions.items(), key=lambda kv: kv[1].get("intensity_g_per_kwh", 9999))
+    return best[0]
+
+def main():
+    config.load_incluster_config()
+    v1 = client.CoreV1Api()
+    while True:
+        regions = load_carbon()
+        if not regions:
+            time.sleep(30)
+            continue
+        preferred = pick_low_carbon_region(regions)
+        pods = v1.list_pod_for_all_namespaces(field_selector="status.phase=Pending")
+        for p in pods.items:
+            ns = p.metadata.namespace
+            name = p.metadata.name
+            # operator: annotate or patch pod template; here we just print
+            print(f"Consider scheduling pod {ns}/{name} to region {preferred}")
+        time.sleep(30)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:cost/cost_watcher.py
+#!/usr/bin/env python3
+"""
+Cost watcher prototype: given cloud billing exports or quick estimates, alert when monthly spend > threshold.
+Operator: wire to cloud billing APIs and alerting (e.g., via Prometheus pushgateway or Alertmanager).
+"""
+import os, json, smtplib
+THRESHOLD_USD=float(os.environ.get("COST_THRESHOLD_USD","1000"))
+def estimate_current_month_cost():
+    # Placeholder: operator should query cloud billing APIs
+    return float(os.environ.get("CURRENT_ESTIMATE", "0"))
+
+def main():
+    cost = estimate_current_month_cost()
+    print("Current estimated cost:", cost)
+    if cost > THRESHOLD_USD:
+        print("ALERT: cost threshold exceeded. Notify finance and throttle non-critical jobs.")
+        # operator: push alert to Alertmanager or create incident ticket
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:observability/grafana/aegis-dashboard.json
+{
+  "dashboard": {
+    "id": null,
+    "title": "Aegis Overview",
+    "panels": [
+      {
+        "type": "stat",
+        "title": "Vault Up",
+        "targets": [ { "expr": "up{job=\"vault\"}" } ],
+        "gridPos": { "x": 0, "y": 0, "w": 6, "h": 4 }
+      },
+      {
+        "type": "graph",
+        "title": "Carbon Intensity (sample)",
+        "targets": [ { "expr": "aegis_carbon_intensity" } ],
+        "gridPos": { "x": 6, "y": 0, "w": 12, "h": 6 }
+      }
+    ],
+    "schemaVersion": 27,
+    "version": 0
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:enterprise/slo/SLO_TEMPLATE.yaml
+# Example SLO template for Aegis service
+service: aegis-control-plane
+objective:
+  availability_percentage: 99.9
+  latency_target_ms: 2000
+measurement:
+  source: prometheus
+  query: up{job="quantum-controller"}
+alerting:
+  notify: oncall@example.com
+  runbook: docs/runbook_slo_incident.md
+
*** End Patch
*** Begin Patch
*** Add File:devops/drift_detector.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple drift detection: compare desired image tags in git manifests vs running images in cluster
+# Operator: extend to compare Terraform state and actual infrastructure.
+DESIRED_MANIFESTS_DIR=${1:-manifests}
+KNS=${2:-staging}
+
+echo "Scanning desired manifests for images..."
+grep -R --exclude-dir=.git "image:" $DESIRED_MANIFESTS_DIR | awk '{print $2}' | sort -u > /tmp/desired_images.txt
+kubectl get pods -n $KNS -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{"\n"}{end}{end}' | sort -u > /tmp/running_images.txt
+
+echo "Desired images:"
+cat /tmp/desired_images.txt
+echo "Running images:"
+cat /tmp/running_images.txt
+
+echo "Images present in running but not desired (possible drift):"
+comm -23 /tmp/running_images.txt /tmp/desired_images.txt || true
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/advanced_compliance_and_ops.yml
+name: Advanced Ops & Compliance (nightly)
+
+on:
+  schedule:
+    - cron: '0 3 * * *' # nightly at 03:00 UTC
+
+jobs:
+  nightly:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Run DLP scan
+        run: |
+          python3 compliance/dsar/tools/search_pii.py --path . --json > artifacts/dlp_scan.json || true
+          cat artifacts/dlp_scan.json
+
+      - name: Run cost watcher
+        run: |
+          python3 cost/cost_watcher.py || true
+
+      - name: Run drift detection (requires kubeconfig as secret)
+        run: |
+          ./devops/drift_detector.sh manifests staging || true
+
+      - name: Run retention dry-run
+        run: |
+          ./compliance/retention/enforcer.sh || true
+
*** End Patch
*** End Patch
