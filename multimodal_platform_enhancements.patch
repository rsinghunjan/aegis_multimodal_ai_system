*** Begin Patch
*** Add File: preprocessing/image/preprocess.py
+#!/usr/bin/env python3
+"""
+Image preprocessing pipeline:
+- Reads images from an input dir
+- Applies resizing, normalization, augmentations (flip, color jitter)
+- Writes processed images to output dir and emits a manifest (JSONL)
+"""
+import argparse
+import json
+import os
+from pathlib import Path
+import hashlib
+from PIL import Image, ImageOps, ImageEnhance
+
+
+def md5(path: Path):
+    h = hashlib.md5()
+    with path.open("rb") as fh:
+        while True:
+            b = fh.read(8192)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def process_image(infile: Path, outdir: Path, size=(224, 224)):
+    img = Image.open(infile).convert("RGB")
+    img = ImageOps.fit(img, size)
+    # simple augmentations
+    if infile.stat().st_size % 2 == 0:
+        img = ImageOps.mirror(img)
+    img = ImageEnhance.Color(img).enhance(1.0)
+    outp = outdir / infile.name
+    img.save(outp, format="JPEG", quality=90)
+    return outp
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input-dir", required=True)
+    p.add_argument("--output-dir", required=True)
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--size", default="224,224")
+    args = p.parse_args()
+
+    size = tuple(map(int, args.size.split(",")))
+    inp = Path(args.input_dir)
+    out = Path(args.output_dir)
+    out.mkdir(parents=True, exist_ok=True)
+
+    with open(args.manifest, "w") as mf:
+        for f in sorted(inp.iterdir()):
+            if f.suffix.lower() not in [".jpg", ".jpeg", ".png", ".bmp"]:
+                continue
+            outp = process_image(f, out, size=size)
+            entry = {
+                "id": outp.stem,
+                "modality": "image",
+                "original_path": str(f),
+                "path": str(outp),
+                "md5": md5(outp),
+                "width": size[0],
+                "height": size[1],
+            }
+            mf.write(json.dumps(entry) + "\n")
+            print("WROTE", entry["id"])
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/audio/preprocess_audio.py
+#!/usr/bin/env python3
+"""
+Audio preprocessing pipeline:
+- Resamples audio, trims/pads, computes mel spectrogram and saves numpy artifact
+Requires: torchaudio
+"""
+import argparse
+from pathlib import Path
+import numpy as np
+import torchaudio
+
+
+def process_audio(path: Path, outdir: Path, sample_rate=16000, n_mels=80, duration=10.0):
+    waveform, sr = torchaudio.load(str(path))
+    if sr != sample_rate:
+        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)
+    # mono
+    if waveform.shape[0] > 1:
+        waveform = waveform.mean(dim=0, keepdim=True)
+    max_len = int(sample_rate * duration)
+    if waveform.shape[1] < max_len:
+        pad = max_len - waveform.shape[1]
+        waveform = torch.nn.functional.pad(waveform, (0, pad))
+    else:
+        waveform = waveform[:, :max_len]
+    mel = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)(waveform)
+    mel_db = torchaudio.transforms.AmplitudeToDB()(mel)
+    outp = outdir / (path.stem + ".npy")
+    np.save(str(outp), mel_db.numpy())
+    return outp
+
+
+def main():
+    import torch
+    p = argparse.ArgumentParser()
+    p.add_argument("--input-dir", required=True)
+    p.add_argument("--output-dir", required=True)
+    args = p.parse_args()
+    inp = Path(args.input_dir)
+    out = Path(args.output_dir)
+    out.mkdir(parents=True, exist_ok=True)
+    for f in sorted(inp.iterdir()):
+        if f.suffix.lower() not in [".wav", ".mp3", ".flac"]:
+            continue
+        print("Processing", f)
+        outp = process_audio(f, out)
+        print("WROTE", outp)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/video/extract_frames.py
+#!/usr/bin/env python3
+"""
+Simple FFmpeg wrapper to extract frames from a video at a target FPS.
+Requires ffmpeg binary in PATH.
+"""
+import argparse
+import subprocess
+from pathlib import Path
+
+
+def extract(video_path: Path, outdir: Path, fps=1):
+    outdir.mkdir(parents=True, exist_ok=True)
+    cmd = [
+        "ffmpeg",
+        "-i", str(video_path),
+        "-vf", f"fps={fps}",
+        "-q:v", "2",
+        str(outdir / (video_path.stem + "_%06d.jpg")),
+    ]
+    subprocess.check_call(cmd)
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--video", required=True)
+    p.add_argument("--outdir", required=True)
+    p.add_argument("--fps", default=1, type=int)
+    args = p.parse_args()
+    extract(Path(args.video), Path(args.outdir), fps=args.fps)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: preprocessing/dicom/dicom_preprocess.py
+#!/usr/bin/env python3
+"""
+DICOM preprocessing:
+- Load DICOM files, de-identify (basic), convert to PNG for downstream pipelines
+Requires: pydicom, Pillow
+"""
+import argparse
+from pathlib import Path
+import pydicom
+from pydicom.pixel_data_handlers.util import apply_voi_lut
+from PIL import Image
+import json
+
+
+def dicom_to_image(ds):
+    arr = ds.pixel_array
+    arr = apply_voi_lut(arr, ds)
+    img = Image.fromarray(arr).convert("L")
+    return img
+
+
+def deid(ds):
+    # Basic deidentification: remove common patient-identifying tags
+    tags = [
+        (0x0010, 0x0010),  # PatientName
+        (0x0010, 0x0020),  # PatientID
+        (0x0010, 0x0030),  # PatientBirthDate
+        (0x0010, 0x0040),  # PatientSex
+    ]
+    for tag in tags:
+        if tag in ds:
+            del ds[tag]
+    return ds
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input-dir", required=True)
+    p.add_argument("--output-dir", required=True)
+    p.add_argument("--manifest", required=True)
+    args = p.parse_args()
+    inp = Path(args.input_dir)
+    out = Path(args.output_dir)
+    out.mkdir(parents=True, exist_ok=True)
+    manifest = []
+    for f in inp.glob("*.dcm"):
+        ds = pydicom.dcmread(str(f))
+        ds = deid(ds)
+        img = dicom_to_image(ds)
+        outp = out / (f.stem + ".png")
+        img.save(outp)
+        manifest.append({"id": f.stem, "path": str(outp)})
+    with open(args.manifest, "w") as fh:
+        json.dump(manifest, fh)
+    print("WROTE manifest", args.manifest)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: annotation/supervisely/README.md
+Supervisely integration (notes)
+------------------------------
+
+This folder describes how to integrate Supervisely (annotation + dataset management) with Aegis.
+
+1) Deploy Supervisely as a hosted service or use Supervisely Cloud.
+2) Export annotated datasets as a standard layout:
+   - images/ (*.jpg)
+   - annots/ (*.json) where each JSON contains bounding boxes / tags
+3) Use scripts/generate_manifest.py (in data/manifest) to convert exports into Aegis manifest JSONL
+4) Optionally push artifacts to object storage (S3/MinIO) and reference the object URI in manifests.
+
+Supervisely is a commercial tool; the integration pattern is standard import/export + manifest conversion.
+
*** End Patch
*** Begin Patch
*** Add File: data/manifest/generate_manifest.py
+#!/usr/bin/env python3
+"""
+Generate dataset manifest JSONL for multimodal datasets.
+Fields:
+ - id, modality, uri, md5, size_bytes, labels (optional)
+"""
+import argparse
+from pathlib import Path
+import hashlib
+import json
+
+
+def md5(path: Path):
+    h = hashlib.md5()
+    with path.open("rb") as fh:
+        while True:
+            b = fh.read(8192)
+            if not b:
+                break
+            h.update(b)
+    return h.hexdigest()
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input-dir", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--prefix", default="s3://bucket/datasets/aegis")
+    args = p.parse_args()
+    inp = Path(args.input_dir)
+    with open(args.out, "w") as out:
+        for f in sorted(inp.rglob("*")):
+            if f.is_dir():
+                continue
+            rec = {
+                "id": f.stem,
+                "uri": f"{args.prefix}/{f.relative_to(inp)}",
+                "md5": md5(f),
+                "size_bytes": f.stat().st_size,
+                "modality": f.suffix.lstrip("."),
+            }
+            out.write(json.dumps(rec) + "\n")
+    print("WROTE", args.out)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/register_feature.py
+#!/usr/bin/env python3
+"""
+Register a feature artifact (e.g., precomputed embedding file) into the artifact store
+and write a feature metadata record (S3 URI, schema, columns) to a registry JSON file.
+"""
+import argparse
+import json
+from pathlib import Path
+import boto3
+import os
+
+
+def upload_to_s3(local_path: Path, bucket: str, key: str):
+    s3 = boto3.client("s3")
+    s3.upload_file(str(local_path), bucket, key)
+    return f"s3://{bucket}/{key}"
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--local-file", required=True)
+    p.add_argument("--bucket", required=True)
+    p.add_argument("--key", required=True)
+    p.add_argument("--registry", default="feature_registry.json")
+    args = p.parse_args()
+    uri = upload_to_s3(Path(args.local_file), args.bucket, args.key)
+    meta = {"uri": uri, "columns": [], "type": "embedding", "owner": os.getenv("USER", "unknown")}
+    reg = []
+    if Path(args.registry).exists():
+        reg = json.load(open(args.registry))
+    reg.append(meta)
+    with open(args.registry, "w") as fh:
+        json.dump(reg, fh, indent=2)
+    print("Registered feature", uri)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: embeddings/audio/audio_encoder.py
+#!/usr/bin/env python3
+"""
+Lightweight audio embedding example:
+- Loads mel spectrogram (from preprocess_audio), applies a small CNN to produce embedding
+This is a demonstration; replace with production encoder (YAMNet, VGGish, or transformer).
+"""
+import argparse
+import numpy as np
+import torch
+import torch.nn as nn
+from pathlib import Path
+
+
+class SmallAudioEncoder(nn.Module):
+    def __init__(self, n_mels=80, emb_dim=512):
+        super().__init__()
+        self.net = nn.Sequential(
+            nn.Conv2d(1, 8, kernel_size=3, padding=1),
+            nn.ReLU(),
+            nn.AdaptiveAvgPool2d((1, 1)),
+            nn.Flatten(),
+            nn.Linear(8, emb_dim),
+        )
+
+    def forward(self, x):
+        return self.net(x)
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--mel-npy", required=True)
+    p.add_argument("--out-npy", required=True)
+    args = p.parse_args()
+    x = np.load(args.mel_npy)
+    # expect shape (1, n_mels, T) or (n_mels, T)
+    if x.ndim == 2:
+        x = x[None, :, :]
+    x = torch.tensor(x).float().unsqueeze(0)  # Bx1xMxt
+    model = SmallAudioEncoder(n_mels=x.shape[2])
+    with torch.no_grad():
+        emb = model(x).numpy()
+    np.save(args.out_npy, emb)
+    print("WROTE", args.out_npy)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: vector_db/weaviate_client.py
+#!/usr/bin/env python3
+"""
+Simple Weaviate client helpers (upsert & query). Requires WEAVIATE_URL env var and API key if needed.
+"""
+import os
+import json
+from weaviate import Client
+
+
+def client():
+    url = os.environ.get("WEAVIATE_URL", "http://weaviate:8080")
+    api_key = os.environ.get("WEAVIATE_API_KEY")
+    if api_key:
+        return Client(url, auth_client_secret=api_key)
+    return Client(url)
+
+
+def upsert(vector, meta, class_name="AegisMultimodal"):
+    c = client()
+    properties = {"meta": json.dumps(meta)}
+    return c.data_object.create(properties, class_name, vector=vector)
+
+
+def query(vector, k=5, class_name="AegisMultimodal"):
+    c = client()
+    near_vector = {"vector": vector}
+    res = c.query.get(class_name, ["meta"]).with_near_vector(near_vector).with_limit(k).do()
+    return res
+
*** End Patch
*** Begin Patch
*** Add File: vector_db/pinecone_client.py
+#!/usr/bin/env python3
+"""
+Pinecone helper (upsert & query). Requires PINECONE_API_KEY and PINECONE_INDEX env vars.
+"""
+import os
+import pinecone
+
+
+def init():
+    api_key = os.environ.get("PINECONE_API_KEY")
+    env = os.environ.get("PINECONE_ENV", "us-west1-gcp")
+    pinecone.init(api_key=api_key, environment=env)
+    return pinecone.Index(os.environ["PINECONE_INDEX"])
+
+
+def upsert(id, vector, metadata=None):
+    idx = init()
+    idx.upsert([(id, vector, metadata or {})])
+
+
+def query(vector, top_k=5):
+    idx = init()
+    return idx.query(vector=vector, top_k=top_k, include_metadata=True)
+
*** End Patch
*** Begin Patch
*** Add File: inference/kserve/multimodal_inference.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "multimodal-service"
+  namespace: "aegis"
+spec:
+  predictor:
+    custom:
+      # This can point to a Triton or custom predictor image that accepts image+text
+      container:
+        image: aegis/multimodal-predictor:latest
+        command: ["python", "app.py"]
+        ports:
+          - containerPort: 8080
+  transformer:
+    custom:
+      container:
+        image: aegis/multimodal-transformer:latest
+        command: ["python", "transformer.py"]
+        env:
+          - name: TRANSFORMER_MODE
+            value: "image_text"
+
+  # Optional: autoscaler and resources
+  autoscaler:
+    minReplicas: 1
+    maxReplicas: 3
+
*** End Patch
*** Begin Patch
*** Add File: model_conversion/onnx_export.py
+#!/usr/bin/env python3
+"""
+Export a PyTorch model to ONNX and run a sanity check (torch.onnx).
+"""
+import argparse
+import torch
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--pt-model", required=True)
+    p.add_argument("--onnx-out", required=True)
+    args = p.parse_args()
+    # example: load a simple model; in real cases load your model architecture and weights
+    model = torch.load(args.pt_model, map_location="cpu")
+    model.eval()
+    dummy = torch.randn(1, 3, 224, 224)
+    torch.onnx.export(model, dummy, args.onnx_out, opset_version=13)
+    print("Exported ONNX to", args.onnx_out)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: model_conversion/tensorrt_optimize.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple wrapper to call trtexec (TensorRT) to optimize an ONNX model
+# Requires NVIDIA TensorRT & trtexec in PATH
+if [ $# -lt 2 ]; then
+  echo "Usage: $0 model.onnx out.plan"
+  exit 2
+fi
+ONNX="$1"
+OUT="$2"
+trtexec --onnx="$ONNX" --saveEngine="$OUT" --explicitBatch --workspace=4096 --fp16 || true
+echo "Attempted TensorRT optimization, output: $OUT"
+
*** End Patch
*** Begin Patch
*** Add File: validators/schema_validator.py
+#!/usr/bin/env python3
+"""
+Validate incoming JSON request payloads for multimodal endpoints.
+Uses jsonschema to validate structure and check for required fields.
+"""
+import argparse
+import json
+from jsonschema import validate, ValidationError
+
+SCHEMA = {
+    "type": "object",
+    "properties": {
+        "image_uri": {"type": "string"},
+        "text": {"type": "string"},
+        "patient_id": {"type": "string"},
+        "request_id": {"type": "string"},
+    },
+    "required": ["image_uri", "text"],
+}
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--payload", required=True, help="JSON payload file")
+    args = p.parse_args()
+    data = json.load(open(args.payload))
+    try:
+        validate(instance=data, schema=SCHEMA)
+        print("VALID")
+    except ValidationError as e:
+        print("INVALID:", e.message)
+        raise SystemExit(2)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: privacy/phi_scrubber.py
+#!/usr/bin/env python3
+"""
+Basic PHI scrubber:
+- For free text: redacts patterns that look like SSN, MRN, phone numbers, simple names heuristics
+- For DICOM: uses pydicom to remove common patient tags (this is a simple example - use HIPAA certified tools for production)
+"""
+import re
+import argparse
+from pathlib import Path
+
+SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")
+PHONE_RE = re.compile(r"\b\d{3}[-.\s]\d{3}[-.\s]\d{4}\b")
+MRN_RE = re.compile(r"\bMRN[:#]?\s*[0-9A-Za-z-]{4,}\b", re.IGNORECASE)
+
+
+def scrub_text(s: str) -> str:
+    s = SSN_RE.sub("[REDACTED_SSN]", s)
+    s = PHONE_RE.sub("[REDACTED_PHONE]", s)
+    s = MRN_RE.sub("[REDACTED_MRN]", s)
+    # naive name scrub: redact capitalized words followed by capitalized words (First Last)
+    s = re.sub(r"\b([A-Z][a-z]+)\s+([A-Z][a-z]+)\b", "[REDACTED_NAME]", s)
+    return s
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--infile", required=True)
+    p.add_argument("--outfile", required=True)
+    args = p.parse_args()
+    txt = Path(args.infile).read_text()
+    out = scrub_text(txt)
+    Path(args.outfile).write_text(out)
+    print("WROTE", args.outfile)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: tests/e2e/multimodal_end_to_end_test.py
+import requests, time, json, os
+from pathlib import Path
+
+PREDICT_URL = os.environ.get("PREDICT_URL", "http://localhost:8080/predict")
+
+def make_image_bytes():
+    from PIL import Image, ImageDraw
+    from io import BytesIO
+    img = Image.new("RGB", (224,224), color=(120,120,120))
+    d = ImageDraw.Draw(img)
+    d.text((10,10), "e2e", fill=(255,255,255))
+    b = BytesIO()
+    img.save(b, format="PNG")
+    b.seek(0)
+    return b
+
+def test_predictor_smoke():
+    files = {"image": ("e2e.png", make_image_bytes(), "image/png")}
+    data = {"text": "a test"}
+    r = requests.post(PREDICT_URL, files=files, data=data, timeout=15)
+    assert r.status_code == 200
+    j = r.json()
+    assert "similarity" in j
+    assert isinstance(j["similarity"], float)
+
+def test_latency():
+    files = {"image": ("e2e.png", make_image_bytes(), "image/png")}
+    data = {"text": "a test latency"}
+    t0 = time.time()
+    r = requests.post(PREDICT_URL, files=files, data=data, timeout=15)
+    t = time.time() - t0
+    print("latency", t)
+    assert r.status_code == 200
+    # warn if too slow but don't fail hard
+    assert t < 5.0
+
*** End Patch
*** Begin Patch
*** Add File: chaos/kill_random_pod.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS="${1:-aegis}"
+LABEL="${2:-app=multimodal-predictor}"
+POD=$(kubectl get pods -n "$NS" -l "$LABEL" -o jsonpath='{.items[0].metadata.name}')
+echo "Killing pod $POD in namespace $NS"
+kubectl delete pod "$POD" -n "$NS"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/multimodal-e2e-chaos.yml
+name: Multimodal E2E + Chaos
+on:
+  workflow_dispatch:
+
+jobs:
+  e2e:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.9"
+      - name: Install deps for tests
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests pytest pillow
+      - name: Run e2e tests (requires cluster kubeconfig in secrets)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          # assume predictor svc is available in cluster
+          pip install -r requirements.txt || true
+          pytest -q tests/e2e/multimodal_end_to_end_test.py::test_predictor_smoke -q
+      - name: Run chaos (optional)
+        if: always()
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          # kill a pod (requires kubectl)
+          kubectl get pods -n aegis -l app=multimodal-predictor -o wide || true
+          ./chaos/kill_random_pod.sh aegis app=multimodal-predictor || true
+
*** End Patch
*** End Patch
