*** Begin Patch
*** Add File:llm/training/deepspeed_accelerate_finetune.py
+#!/usr/bin/env python3
+"""
+Distributed fine-tuning entrypoint using Hugging Face Transformers + Accelerate + DeepSpeed + PEFT (LoRA).
+ - Loads a model (HF hub or local)
+ - Applies LoRA via PEFT (optional)
+ - Uses Accelerate to run distributed training and supports DeepSpeed config
+ - Logs artifacts to MLflow and signs checkpoint with cosign (best-effort)
+
+Environment (examples):
+ - MODEL_NAME_OR_PATH (e.g., facebook/llama-7b-hf or local path)
+ - TRAIN_FILE (jsonl)
+ - HF_TOKEN (if required)
+ - USE_PEFT=1 to enable LoRA
+ - DS_CONFIG (path to deepspeed config json) optional
+ - MLFLOW_TRACKING_URI, MLFLOW_EXPERIMENT
+ - COSIGN_KMS_KEY_ARN optional (if present cosign will be invoked on checkpoint)
+
+Usage:
+  accelerate launch --config_file accelerate_config.yaml deepspeed_accelerate_finetune.py
+"""
+import os
+import subprocess
+import json
+from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
+from datasets import load_dataset
+
+USE_PEFT = os.environ.get("USE_PEFT", "") == "1"
+MODEL = os.environ.get("MODEL_NAME_OR_PATH", "gpt2")
+TRAIN_FILE = os.environ.get("TRAIN_FILE", "data/train.jsonl")
+MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
+MLFLOW_EXPERIMENT = os.environ.get("MLFLOW_EXPERIMENT", "aegis-llm-finetune")
+COSIGN_KMS = os.environ.get("COSIGN_KMS_KEY_ARN", "")
+DS_CONFIG = os.environ.get("DS_CONFIG", "")
+
+def prepare_dataset(train_file, tokenizer, block_size=1024):
+    ds = load_dataset("json", data_files={"train": train_file})
+    def tokenize(ex):
+        return tokenizer(ex["text"], truncation=True, max_length=block_size)
+    ds = ds.map(tokenize, batched=True, remove_columns=["text"])
+    ds.set_format(type="torch")
+    return ds["train"]
+
+def apply_peft(model, tokenizer):
+    try:
+        from peft import LoraConfig, get_peft_model
+    except Exception:
+        raise SystemExit("peft library required for LoRA; pip install peft")
+    lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj","v_proj"], lora_dropout=0.05, bias="none")
+    model = get_peft_model(model, lora_config)
+    return model
+
+def main():
+    tokenizer = AutoTokenizer.from_pretrained(MODEL)
+    model = AutoModelForCausalLM.from_pretrained(MODEL)
+    if USE_PEFT:
+        print("Applying LoRA/PEFT")
+        model = apply_peft(model, tokenizer)
+
+    train_dataset = prepare_dataset(TRAIN_FILE, tokenizer)
+
+    training_args = TrainingArguments(
+        output_dir="outputs",
+        per_device_train_batch_size=1,
+        gradient_accumulation_steps=8,
+        max_steps=1000,
+        logging_steps=10,
+        save_steps=200,
+        fp16=True,
+    )
+
+    if DS_CONFIG:
+        training_args.deepspeed = DS_CONFIG
+
+    trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)
+    trainer.train()
+    trainer.save_model("outputs/checkpoint")
+
+    # MLflow logging (best-effort)
+    if MLFLOW_TRACKING_URI:
+        try:
+            import mlflow
+            mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
+            mlflow.set_experiment(MLFLOW_EXPERIMENT)
+            with mlflow.start_run():
+                mlflow.log_param("model", MODEL)
+                mlflow.log_artifacts("outputs", artifact_path="model")
+        except Exception as e:
+            print("MLflow logging failed:", e)
+
+    # Optional cosign sign checkpoint (best-effort)
+    if COSIGN_KMS:
+        try:
+            ckpt_tar = "outputs/checkpoint.tar.gz"
+            subprocess.run(["tar", "-czf", ckpt_tar, "-C", "outputs", "checkpoint"], check=True)
+            subprocess.run(["cosign", "sign", "--key", f"awskms://{COSIGN_KMS}", ckpt_tar], check=True)
+            print("Signed checkpoint with cosign")
+        except Exception as e:
+            print("cosign signing failed (best-effort):", e)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:llm/training/accelerate_config_template.yaml
+compute_environment_type: LOCAL_MACHINE
+deepspeed_config: ds_config.json  # Replace with path to DeepSpeed config if used
+mixed_precision: fp16
+num_processes: 1
+main_process_ip: 127.0.0.1
+main_process_port: 29500
+num_machines: 1
+
*** End Patch
*** Begin Patch
*** Add File:llm/training/ds_config_template.json
+{
+  "train_micro_batch_size_per_gpu": 1,
+  "gradient_accumulation_steps": 8,
+  "optimizer": {
+    "type": "AdamW",
+    "params": {
+      "lr": 1e-5
+    }
+  },
+  "fp16": {
+    "enabled": true
+  },
+  "zero_optimization": {
+    "stage": 2
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:llm/serving/kserve_triton_inference.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-llm"
+  namespace: "aegis"
+spec:
+  predictor:
+    custom:
+      container:
+        image: "ghcr.io/yourorg/aegis-llm-server:latest"
+        args: ["--model-dir=/models", "--max-tokens=1024"]
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            cpu: "4"
+            memory: "16Gi"
+
+---
+# Notes:
+# - Requires KServe / Triton / NVIDIA GPU nodes.
+# - Replace image with optimized model server image (Triton or HF optimized runtime).
+
*** End Patch
*** Begin Patch
*** Add File:embeddings/milvus/milvus_deploy_placeholder.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: milvus
+
+---
+# Placeholder StatefulSet/Deployment for Milvus. For production, use Milvus Helm chart and persistent volumes.
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: milvus
+  namespace: milvus
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: milvus
+  template:
+    metadata:
+      labels:
+        app: milvus
+    spec:
+      containers:
+        - name: milvus
+          image: milvusdb/milvus:2.2.6
+          ports:
+            - containerPort: 19530
+          env:
+            - name: "TZ"
+              value: "UTC"
+
*** End Patch
*** Begin Patch
*** Add File:embeddings/ingest/embed_and_upsert.py
+#!/usr/bin/env python3
+"""
+Document ingestion pipeline:
+ - Chunk documents
+ - Embed using sentence-transformers or an external embedding API (OpenAI)
+ - Upsert vectors to Milvus (pymilvus)
+
+Environment:
+ - EMBED_MODEL (sentence-transformers model or 'openai')
+ - OPENAI_API_KEY (if using OpenAI)
+ - MILVUS_HOST, MILVUS_PORT
+ - COLLECTION_NAME
+"""
+import os, json, math, uuid
+from typing import List
+
+EMBED_MODEL = os.environ.get("EMBED_MODEL", "all-MiniLM-L6-v2")
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "milvus.milvus.svc.cluster.local")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", "19530"))
+COLLECTION = os.environ.get("COLLECTION_NAME", "aegis_docs")
+
+def chunk_text(text, chunk_size=512, overlap=64):
+    tokens = text.split()
+    chunks = []
+    i = 0
+    while i < len(tokens):
+        chunk = " ".join(tokens[i:i+chunk_size])
+        chunks.append(chunk)
+        i += chunk_size - overlap
+    return chunks
+
+def embed_texts(texts: List[str]):
+    if EMBED_MODEL == "openai":
+        import openai
+        openai.api_key = os.environ.get("OPENAI_API_KEY")
+        res = [openai.Embedding.create(input=t, model="text-embedding-3-small")["data"][0]["embedding"] for t in texts]
+        return res
+    else:
+        from sentence_transformers import SentenceTransformer
+        model = SentenceTransformer(EMBED_MODEL)
+        return model.encode(texts).tolist()
+
+def upsert_milvus(ids, vectors, metas):
+    from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    # Define collection if not exists (best-effort)
+    schema = CollectionSchema([FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
+                              FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=len(vectors[0])),
+                              FieldSchema(name="meta", dtype=DataType.VARCHAR, max_length=1024)])
+    try:
+        coll = Collection(COLLECTION)
+    except Exception:
+        coll = Collection(name=COLLECTION, schema=schema)
+    coll.insert([ids, vectors, metas])
+    coll.flush()
+
+def ingest_doc(doc_json):
+    text = doc_json.get("text","")
+    chunks = chunk_text(text)
+    vectors = embed_texts(chunks)
+    ids = [str(uuid.uuid4()) for _ in chunks]
+    metas = [json.dumps({"source": doc_json.get("source"), "chunk_index": i}) for i in range(len(chunks))]
+    upsert_milvus(ids, vectors, metas)
+    return len(chunks)
+
+if __name__ == "__main__":
+    import sys
+    j = json.load(open(sys.argv[1]))
+    n = ingest_doc(j)
+    print("Upserted chunks:", n)
+
*** End Patch
*** Begin Patch
*** Add File:rag/service/fastapi_rag_service.py
+#!/usr/bin/env python3
+"""
+RAG microservice (FastAPI) that:
+ - Accepts a query
+ - Retrieves top-k docs from Milvus
+ - Calls an LLM (HF local/remote or OpenAI) to generate an answer with retrieved context
+ - Implements session/state management and simple Redis caching and rate-limiting hooks
+ - Exposes Prometheus metrics for latency, token usage
+
+Env:
+ - MILVUS_HOST, MILVUS_PORT, COLLECTION_NAME
+ - EMBED_MODEL (for query embedding)
+ - LLM_PROVIDER: openai | hf
+ - OPENAI_API_KEY or HF_TOKEN
+ - REDIS_URL (for caching/quotas)
+"""
+from fastapi import FastAPI, HTTPException, Request
+from pydantic import BaseModel
+import os, time, json
+from typing import List
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "milvus.milvus.svc.cluster.local")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", "19530"))
+COLLECTION = os.environ.get("COLLECTION_NAME", "aegis_docs")
+EMBED_MODEL = os.environ.get("EMBED_MODEL", "all-MiniLM-L6-v2")
+LLM_PROVIDER = os.environ.get("LLM_PROVIDER", "openai")
+
+app = FastAPI()
+
+# Prometheus
+from prometheus_client import Counter, Histogram, start_http_server
+REQUESTS = Counter("aegis_rag_requests_total", "Total RAG requests")
+LATENCY = Histogram("aegis_rag_latency_seconds", "RAG request latency")
+TOKENS = Counter("aegis_rag_tokens_total", "Total tokens used (approx)")
+start_http_server(8001)
+
+# simple in-memory cache fallback
+try:
+    import redis
+    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
+    r = redis.from_url(REDIS_URL)
+except Exception:
+    r = None
+
+class Query(BaseModel):
+    q: str
+    top_k: int = 5
+    session_id: str = None
+
+def embed_query(text):
+    from sentence_transformers import SentenceTransformer
+    model = SentenceTransformer(EMBED_MODEL)
+    return model.encode([text])[0].tolist()
+
+def milvus_search(vec, top_k=5):
+    from pymilvus import connections, Collection
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    coll = Collection(COLLECTION)
+    res = coll.search([vec], "embedding", params={"metric_type":"L2"}, limit=top_k)
+    docs = []
+    for hits in res:
+        for h in hits:
+            meta = json.loads(h.entity.get("meta"))
+            docs.append({"id": h.entity.get("id"), "score": h.distance, "meta": meta})
+    return docs
+
+def call_llm(prompt):
+    # Minimal wrapper: call OpenAI or HF Inference
+    if LLM_PROVIDER == "openai":
+        import openai
+        openai.api_key = os.environ.get("OPENAI_API_KEY")
+        res = openai.ChatCompletion.create(model="gpt-4o-mini", messages=[{"role":"user","content":prompt}], max_tokens=512)
+        text = res.choices[0].message.content
+        TOKENS.inc(res.usage.total_tokens if hasattr(res,"usage") else 0)
+        return text
+    else:
+        # HF Inference (best-effort)
+        from transformers import pipeline
+        pipe = pipeline("text-generation", model=os.environ.get("HF_MODEL","gpt2"))
+        out = pipe(prompt, max_length=512, do_sample=False)
+        TOKENS.inc(0)
+        return out[0]["generated_text"]
+
+@app.post("/query")
+@LATENCY.time()
+def query(q: Query, request: Request):
+    REQUESTS.inc()
+    # Basic rate-limiting (per session)
+    if r and q.session_id:
+        key = f"quota:{q.session_id}"
+        used = r.get(key)
+        if used and int(used) > 10000:
+            raise HTTPException(status_code=429, detail="quota exceeded")
+        r.incrby(key, 1)
+        r.expire(key, 3600)
+    # Caching key
+    cache_key = f"cache:{q.q}:{q.top_k}"
+    if r:
+        cached = r.get(cache_key)
+        if cached:
+            return json.loads(cached)
+    vec = embed_query(q.q)
+    docs = milvus_search(vec, top_k=q.top_k)
+    context = "\n\n".join([d["meta"].get("source","") for d in docs])
+    prompt = f"Answer the question using only the following context:\n\n{context}\n\nQuestion: {q.q}"
+    answer = call_llm(prompt)
+    res = {"answer": answer, "docs": docs}
+    if r:
+        r.set(cache_key, json.dumps(res), ex=60*5)
+    return res
+
*** End Patch
*** Begin Patch
*** Add File:llm/ops/prompt_store/prompt_store.py
+#!/usr/bin/env python3
+"""
+Simple prompt store to version prompts/system templates for LLM Ops.
+Stores metadata + SHA256 hash + author in S3 and records MLflow-like run metadata in local sqlite.
+"""
+import os, json, hashlib, time
+import sqlite3
+import boto3
+
+DB = os.environ.get("PROMPT_DB", "prompts.db")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+
+def init_db():
+    conn = sqlite3.connect(DB)
+    conn.execute("""CREATE TABLE IF NOT EXISTS prompts (id TEXT PRIMARY KEY, name TEXT, sha TEXT, s3key TEXT, author TEXT, ts INTEGER)""")
+    conn.commit()
+    return conn
+
+def store_prompt(name, content, author="unknown"):
+    sha = hashlib.sha256(content.encode()).hexdigest()
+    key = f"llm/prompts/{name}_{sha}.json"
+    s3 = boto3.client("s3")
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps({"name":name,"content":content,"author":author,"sha":sha,"ts":int(time.time())}).encode())
+    conn = init_db()
+    conn.execute("INSERT OR REPLACE INTO prompts (id,name,sha,s3key,author,ts) VALUES (?,?,?,?,?,?)",(sha,name,sha,key,author,int(time.time())))
+    conn.commit()
+    return sha, key
+
+if __name__ == "__main__":
+    import sys
+    name = sys.argv[1]
+    author = sys.argv[2] if len(sys.argv)>2 else "unknown"
+    content = sys.stdin.read()
+    sha, key = store_prompt(name, content, author)
+    print("Stored", sha, key)
+
*** End Patch
*** Begin Patch
*** Add File:llm/ops/safety/safety_filters.py
+#!/usr/bin/env python3
+"""
+Safety filter utilities:
+ - simple classifiers / regex-based filters for PII and harmful content
+ - can be extended to call external safety models
+"""
+import re
+
+# naive PII patterns (examples)
+EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")
+SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")
+
+def scrub_pii(text):
+    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
+    text = SSN_RE.sub("[REDACTED_SSN]", text)
+    return text
+
+def basic_harm_filter(text):
+    # placeholder; return True if allowed, False if flagged
+    blocked = ["terrorist", "explode", "bomb"]
+    lower = text.lower()
+    for b in blocked:
+        if b in lower:
+            return False
+    return True
+
*** End Patch
*** Begin Patch
*** Add File:llm/ops/usage/usage_middleware.py
+"""
+FastAPI middleware for token usage accounting and per-tenant quotas (approximate).
+ - Increments Prometheus counters and persists usage to Redis for quotas/billing.
+"""
+from starlette.middleware.base import BaseHTTPMiddleware
+from prometheus_client import Counter
+import os
+
+TOKENS = Counter("aegis_llm_tokens_total", "Approx token usage")
+REDIS_URL = os.environ.get("REDIS_URL")
+try:
+    import redis
+    redis_client = redis.from_url(REDIS_URL) if REDIS_URL else None
+except Exception:
+    redis_client = None
+
+class UsageMiddleware(BaseHTTPMiddleware):
+    async def dispatch(self, request, call_next):
+        tenant = request.headers.get("X-Tenant-Id","global")
+        resp = await call_next(request)
+        # Example: increment token count by a header set by downstream (X-Tokens)
+        tokens = int(request.headers.get("X-Tokens","0"))
+        if tokens:
+            TOKENS.inc(tokens)
+            if redis_client:
+                redis_client.incrby(f"usage:{tenant}:tokens", tokens)
+        return resp
+
*** End Patch
*** Begin Patch
*** Add File:llm/privacy/prompt_provenance.py
+#!/usr/bin/env python3
+"""
+Prompt provenance & privacy helpers:
+ - redact/scrub PII before storing prompts or sending to third-party LLMs
+ - compute prompt hash for provenance without storing the plain prompt
+ - enforce retention rules (TTL) via S3 object tagging (best-effort)
+"""
+import hashlib, json, time, os
+from safety_filters import scrub_pii
+import boto3
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "REPLACE_WITH_EVIDENCE_BUCKET")
+S3 = boto3.client("s3")
+
+def hash_prompt(prompt):
+    return hashlib.sha256(prompt.encode()).hexdigest()
+
+def store_prompt_provenance(prompt, meta, ttl_days=30):
+    clean = scrub_pii(prompt)
+    h = hash_prompt(clean)
+    key = f"llm/prompts/provenance/{h}.json"
+    payload = {"hash": h, "meta": meta, "ts": int(time.time())}
+    S3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(payload).encode())
+    # Note: set object lifecycle policy in bucket to delete after ttl_days
+    return h, key
+
*** End Patch
*** Begin Patch
*** Add File:llm/connectors/openai_wrapper.py
+#!/usr/bin/env python3
+"""
+Minimal OpenAI connector wrapper with retry and cost estimation (approx tokens * cost_per_token).
+"""
+import os, time
+import openai
+
+openai.api_key = os.environ.get("OPENAI_API_KEY")
+COST_PER_1K_TOKENS = float(os.environ.get("OPENAI_COST_PER_1K", "0.02"))
+
+def chat_completion(messages, model="gpt-4o-mini", max_tokens=512):
+    res = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=max_tokens)
+    usage = getattr(res, "usage", None)
+    total_tokens = usage.get("total_tokens") if usage else None
+    cost = (total_tokens or 0)/1000.0 * COST_PER_1K_TOKENS
+    return res.choices[0].message.content, total_tokens, cost
+
*** End Patch
*** Begin Patch
*** Add File:llm/monitoring/grafana_dashboard_stub.json
+{
+  "dashboard": {
+    "title": "Aegis LLM Ops (stub)",
+    "panels": [
+      {"title":"RAG Latency","type":"graph","targets":[{"expr":"aegis_rag_latency_seconds"}]},
+      {"title":"LLM Tokens","type":"graph","targets":[{"expr":"aegis_llm_tokens_total"}]},
+      {"title":"RAG Requests","type":"graph","targets":[{"expr":"aegis_rag_requests_total"}]}
+    ]
+  }
+}
+
*** End Patch
*** End Patch
