*** Begin Patch
*** Add File:staging/README_STAGING_RUNBOOK.md
````markdown
# Aegis Staging: Calibration & Validation Runbook

This runbook guides an operator through deploying the staging components and running the calibration & validation pipeline to reach the acceptance goal:

Acceptance target:
- median(|measured - estimated| / measured) <= 10% on representative workloads

Overview (high level)
1. Deploy supporting infra in the `aegis` / `observability` namespaces: telemetry collector, Prometheus ServiceMonitor, runtime predictor, webhook/controller (staging) and Redis.
2. Collect at least one week of telemetry (power and job runtime) or generate representative historical CSVs for initial training.
3. Run calibration pipeline to produce `power_profiles.yaml` and publish as ConfigMap `aegis-power-profiles`.
4. Train runtime predictor from collected job runtime CSVs and publish model to ConfigMap `runtime-model-cm`.
5. Deploy runtime predictor and webhook in staging configured to call the predictor.
6. Run test workflows (pilot NLP/CV training jobs) through Argo in staging and gather reconciliation reports.
7. Run `validate_accuracy.py` to compute accuracy metrics; iterate until acceptance.

File locations in this repo
- Manifests:
  - `staging/manifests/telemetry-deployment.yaml` (DaemonSet exporter)
  - `staging/manifests/prometheus-servicemonitor.yaml` (ServiceMonitor)
  - `staging/manifests/runtime-predictor-deployment.yaml` (predictor)
  - `staging/manifests/calibration-cronjob.yaml` (CronJob to run calibration)
  - `staging/manifests/enforcer-rbac.yaml` (RBAC for controller/enforcer)
- Scripts:
  - `staging/scripts/run_staging_pipeline.sh` — orchestrates pipeline end-to-end (operator-run)
  - `staging/scripts/validate_accuracy.py` — computes median relative error and emits report
- Support:
  - Ensure you have `kubectl`, `helm`, and `aws` CLI access configured with operator permissions.
  - Ensure `cert-manager` is installed, Redis is deployed, and `prometheus` (Prometheus Operator) scrapes the ServiceMonitor.

Step-by-step operator procedure (detailed)

Prereqs
- Access to cluster with `kubectl` that can create ClusterRoleBindings.
- Operator AWS creds if using EKS Terraform automation for spot pools (not required for this runbook).
- Prometheus Operator installed and scraping `observability` namespace.
- Redis deployed: `aegis-redis.aegis.svc:6379` (or edit manifests to point at your Redis).
- cert-manager installed and functioning for webhook TLS (if you plan to exercise webhook in staging).

1) Deploy telemetry exporter & ServiceMonitor
- kubectl apply -f staging/manifests/telemetry-deployment.yaml
- Verify pods are Running: kubectl -n observability get pods -l app=node-power-exporter
- Verify metrics are exposed: curl http://<node-ip>:9104/metrics (or check Prometheus target)

2) Deploy Prometheus ServiceMonitor (if using the Operator)
- kubectl apply -f staging/manifests/prometheus-servicemonitor.yaml
- In Prometheus UI check target `node-power-exporter` exists and metrics visible.

3) Deploy runtime predictor (staging)
- Create ConfigMap `runtime-model-cm` with an initial model or empty placeholder
  - If you have historical runtime CSVs, train locally (see runtime_predictor/train_model.py) and create ConfigMap:
    - kubectl -n aegis create configmap runtime-model-cm --from-file=runtime_model.pkl=./runtime_model.pkl
- kubectl apply -f staging/manifests/runtime-predictor-deployment.yaml
- kubectl -n aegis rollout status deploy/runtime-predictor

4) Deploy webhook & enforcer (staging)
- Ensure cert-manager provisions TLS secret (or use test self-signed secret)
- kubectl apply -f webhook/aegis-carbon-webhook-deploy.yaml
- kubectl apply -f webhook/mutating_webhook_configuration.yaml (patch caBundle via cert-manager helper)
- kubectl apply -f staging/manifests/enforcer-rbac.yaml
- Deploy enforcer controller (use `controller/enforcer.py` packaged as image `ghcr.io/yourorg/aegis-enforcer:latest`) or run it in a Job for manual testing.

5) Collect telemetry (power + runtime)
- Let the system run for representative workload period (recommended 7 days), or simulate:
  - Place representative Argo Workflows in staging that exercise CPU/GPU and annotate `aegis.input_data_bytes` where possible.
- Telemetry files appear in PVC mounted at `/data/telemetry` by the telemetry collector. Copy them locally if needed.

6) Run calibration
- Ensure `calibration/calibration_pipeline.py` is available in an execution image or accessible locally.
- Run: staging/scripts/run_staging_pipeline.sh calibrate
  - This runs calibration/calibration_pipeline.py against `/data/telemetry` and uploads ConfigMap `aegis-power-profiles`.

7) Train runtime predictor
- Run: staging/scripts/run_staging_pipeline.sh train-runtime
  - This collects runtime CSVs from `/data/runtime` and runs `runtime_predictor/train_model.py` producing `runtime_model.pkl` and publishing `runtime-model-cm`.

8) Validate accuracy
- Trigger representative jobs in staging, wait for completion and then run reconciliation (reconcile_job_postrun.py).
- Run: python3 staging/scripts/validate_accuracy.py --reconciliations-dir /data/reconciliations --output /tmp/accuracy_report.json
- Review median relative error; if > 10% iterate: collect more telemetry, refine power profiles, retrain runtime predictor.

9) Promote to production (operator decision)
- When acceptance met, apply `aegis-power-profiles` ConfigMap into production namespace (after review), and schedule regular calibration runs (CronJob).

Troubleshooting
- If Prometheus targets missing: check ServiceMonitor label selectors and Prometheus scrape_configs.
- If runtime predictions are wildly off: inspect runtime CSV features (req_cpu, gpu, data_bytes) and ensure training data has representative examples.
- If webhook rejects jobs unexpectedly: check Redis policy keys and token-budget endpoint.

Safety & Audit
- All calibration outputs and reconciliation reports must be uploaded to evidence S3 and signed with cosign as part of the pipeline.
- Keep manual approval gates before auto-applying profile changes in production.
