diff --git a/argo-events/eventsource-webhook.yaml b/argo-events/eventsource-webhook.yaml
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/argo-events/eventsource-webhook.yaml
@@ -0,0 +1,22 @@
+apiVersion: argoproj.io/v1alpha1
+kind: EventSource
+metadata:
+  name: aegis-webhook-es
+  namespace: aegis-ml
+spec:
+  webhook:
+    # exposes: /drift (POST) by default on port 12000
+    endpoint: /drift
+    port: "12000"
+    method: POST
+    # optional: set allowed origins, headers etc.
+    url: ""
+    # You can secure this with a shared token header; example below shows expected header name
+    # optional:
+    #   headers:
+    #     - name: "X-AEGIS-TOKEN"
+    #       valueFrom:
+    #         secretKeyRef:
+    #           name: aegis-webhook-secret
+    #           key: token
+
diff --git a/argo-events/sensor-retrain.yaml b/argo-events/sensor-retrain.yaml
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/argo-events/sensor-retrain.yaml
@@ -0,0 +1,38 @@
+apiVersion: argoproj.io/v1alpha1
+kind: Sensor
+metadata:
+  name: aegis-retrain-sensor
+  namespace: aegis-ml
+spec:
+  dependencies:
+    - name: drift-dep
+      eventSourceName: aegis-webhook-es
+      eventName: webhook
+  triggers:
+    - template:
+        name: call-retrain-agent
+        http:
+          url: http://retrain-agent.aegis-ml.svc.cluster.local:8080/webhook
+          method: POST
+          # forward entire event body by referencing dependency
+          body:
+            content:
+              expression: '{{ (index . "body") }}'
+          # optionally add headers
+          headers:
+            Content-Type: application/json
+            # If using a secret token:
+            # X-AEGIS-TOKEN: "my-shared-secret"
+      retryStrategy:
+        retryPolicy: "Always"
+        backoff:
+          duration: "30s"
+          factor: 2
+          maxDuration: "10m"
+
diff --git a/k8s/notes/argo_events_deploy_notes.md b/k8s/notes/argo_events_deploy_notes.md
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/k8s/notes/argo_events_deploy_notes.md
@@ -0,0 +1,24 @@
+# Notes: Deploying Argo Events pieces for the Retrain Agent integration
+
+1. Install Argo Events (EventSource + Sensor controllers) in your cluster if not already present:
+   - https://argoproj.github.io/argo-events/installation/
+
+2. Apply EventSource and Sensor:
+   kubectl apply -f argo-events/eventsource-webhook.yaml
+   kubectl apply -f argo-events/sensor-retrain.yaml
+
+3. The EventSource exposes a service inside the cluster on port 12000. To test locally, port-forward:
+   kubectl -n aegis-ml port-forward svc/aegis-webhook-es 12000:12000
+
+4. This Sensor will call the Retrain Agent webhook at:
+   http://retrain-agent.aegis-ml.svc.cluster.local:8080/webhook
+   Ensure retrain-agent Deployment is running and accessible in namespace aegis-ml.
+
+5. Security:
+   - Optionally secure the EventSource with a shared header (X-AEGIS-TOKEN) and configure Retrain Agent to validate it.
+   - Use NetworkPolicies / ServiceAccount RBAC for production.
+
+6. Testing:
+   - Use tests/send_synthetic_drift.py (below) to post a synthetic Alertmanager-like payload to the EventSource (while port-forwarded).
+
diff --git a/tests/send_synthetic_drift.py b/tests/send_synthetic_drift.py
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/tests/send_synthetic_drift.py
@@ -0,0 +1,65 @@
+#!/usr/bin/env python3
+"""
+Simple test script: Post a synthetic drift alert to the Argo EventSource webhook.
+
+Usage:
+1) Port-forward the eventsource service locally:
+   kubectl -n aegis-ml port-forward svc/aegis-webhook-es 12000:12000
+
+2) Run this script:
+   python tests/send_synthetic_drift.py --model my_model
+
+This will simulate an Alertmanager-style payload that should be picked up by the Sensor and forwarded to the Retrain Agent webhook.
+"""
+import argparse
+import requests
+import json
+import time
+
+def make_alert(model):
+    return {
+      "status": "firing",
+      "alerts": [
+        {
+          "status": "firing",
+          "labels": {
+            "alertname": "ModelFeatureDrift",
+            "severity": "warning",
+            "model": model
+          },
+          "annotations": {
+            "summary": f"Feature drift detected for model {model}"
+          },
+          "startsAt": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+        }
+      ]
+    }
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--model", default="my_model")
+    parser.add_argument("--url", default="http://localhost:12000/drift")
+    args = parser.parse_args()
+    payload = make_alert(args.model)
+    print("Posting synthetic alert to", args.url)
+    r = requests.post(args.url, json=payload, timeout=10)
+    print("Response:", r.status_code, r.text)
+
+if __name__ == "__main__":
+    main()
+
diff --git a/agents/common/decision_db.py b/agents/common/decision_db.py
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/agents/common/decision_db.py
@@ -0,0 +1,77 @@
+#!/usr/bin/env python3
+"""
+Decision log helper backed by PostgreSQL.
+
+Environment variables expected:
+- DECISION_DB_HOST
+- DECISION_DB_PORT
+- DECISION_DB_NAME
+- DECISION_DB_USER
+- DECISION_DB_PASSWORD
+
+Simple insert helper and retrieval functions.
+"""
+import os
+import json
+import psycopg2
+import psycopg2.extras
+from datetime import datetime
+
+def _conn():
+    return psycopg2.connect(
+        host=os.environ.get("DECISION_DB_HOST", "postgres.aegis-ml.svc.cluster.local"),
+        port=int(os.environ.get("DECISION_DB_PORT", "5432")),
+        dbname=os.environ.get("DECISION_DB_NAME", "aegis"),
+        user=os.environ.get("DECISION_DB_USER", "aegis"),
+        password=os.environ.get("DECISION_DB_PASSWORD", "aegispass")
+    )
+
+def init_schema():
+    sql = """
+    CREATE TABLE IF NOT EXISTS decision_log (
+      id SERIAL PRIMARY KEY,
+      created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now(),
+      agent TEXT NOT NULL,
+      model TEXT,
+      env TEXT,
+      action TEXT,
+      payload JSONB,
+      evidence JSONB,
+      metadata JSONB
+    );
+    CREATE INDEX IF NOT EXISTS idx_decision_agent ON decision_log(agent);
+    CREATE INDEX IF NOT EXISTS idx_decision_model ON decision_log(model);
+    """
+    conn = _conn()
+    cur = conn.cursor()
+    cur.execute(sql)
+    conn.commit()
+    cur.close()
+    conn.close()
+
+def record_decision(agent, model, env, action, payload=None, evidence=None, metadata=None):
+    conn = _conn()
+    cur = conn.cursor()
+    cur.execute(
+        "INSERT INTO decision_log (agent, model, env, action, payload, evidence, metadata) VALUES (%s,%s,%s,%s,%s,%s,%s) RETURNING id, created_at",
+        (agent, model, env, action, json.dumps(payload) if payload else None, json.dumps(evidence) if evidence else None, json.dumps(metadata) if metadata else None)
+    )
+    row = cur.fetchone()
+    conn.commit()
+    cur.close()
+    conn.close()
+    return {"id": row[0], "created_at": row[1].isoformat()}
+
+def fetch_recent(limit=100):
+    conn = _conn()
+    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
+    cur.execute("SELECT * FROM decision_log ORDER BY created_at DESC LIMIT %s", (limit,))
+    rows = cur.fetchall()
+    cur.close()
+    conn.close()
+    return rows
+
diff --git a/sql/001_create_decision_log.sql b/sql/001_create_decision_log.sql
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/sql/001_create_decision_log.sql
@@ -0,0 +1,12 @@
+-- SQL migration for Decision Log (Postgres)
+CREATE TABLE IF NOT EXISTS decision_log (
+  id SERIAL PRIMARY KEY,
+  created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now(),
+  agent TEXT NOT NULL,
+  model TEXT,
+  env TEXT,
+  action TEXT,
+  payload JSONB,
+  evidence JSONB,
+  metadata JSONB
+);
+CREATE INDEX IF NOT EXISTS idx_decision_agent ON decision_log(agent);
+CREATE INDEX IF NOT EXISTS idx_decision_model ON decision_log(model);
+
diff --git a/k8s/postgres/postgres-deployment.yaml b/k8s/postgres/postgres-deployment.yaml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/k8s/postgres/postgres-deployment.yaml
@@ -0,0 +1,40 @@
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-postgres
+  namespace: aegis-ml
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-postgres
+  template:
+    metadata:
+      labels:
+        app: aegis-postgres
+    spec:
+      containers:
+        - name: postgres
+          image: postgres:14
+          env:
+            - name: POSTGRES_DB
+              value: "aegis"
+            - name: POSTGRES_USER
+              value: "aegis"
+            - name: POSTGRES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-postgres-secret
+                  key: password
+          ports:
+            - containerPort: 5432
+          volumeMounts:
+            - name: pgdata
+              mountPath: /var/lib/postgresql/data
+      volumes:
+        - name: pgdata
+          emptyDir: {}
+---
+apiVersion: v1
+kind: Secret
+metadata:
+  name: aegis-postgres-secret
+  namespace: aegis-ml
+type: Opaque
+stringData:
+  password: "aegispass"
+
diff --git a/agents/common/utils.py b/agents/common/utils.py
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/agents/common/utils.py
@@ -0,0 +1,210 @@
+#!/usr/bin/env python3
+"""
+Common utilities for Aegis agents (updated to use Postgres decision log helper if available).
+- Provides Prometheus query helpers, Argo helpers and wraps decision logging to prefer Postgres helper.
+"""
+import os
+import time
+import json
+import requests
+from urllib.parse import urljoin
+from datetime import datetime, timedelta
+
+# GitHub helper
+try:
+    from github import Github
+except Exception:
+    Github = None
+
+# Kubernetes client
+try:
+    from kubernetes import client as k8s_client, config as k8s_config
+    k8s_config.load_incluster_config()
+    k8s_api = k8s_client.CustomObjectsApi()
+    k8s_core = k8s_client.CoreV1Api()
+    k8s_apps = k8s_client.AppsV1Api()
+except Exception:
+    k8s_client = None
+    k8s_api = None
+    k8s_core = None
+    k8s_apps = None
+
+# Decision log - prefer Postgres backend if available
+try:
+    from agents.common.decision_db import record_decision as db_record_decision
+    _has_db = True
+except Exception:
+    db_record_decision = None
+    _has_db = False
+
+PROM = os.environ.get("PROMETHEUS_URL", "http://prometheus.monitoring.svc.cluster.local:9090")
+ARGO = os.environ.get("ARGO_SERVER", "http://argo-server.argo-workflows.svc.cluster.local:2746")
+GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")
+GITHUB_REPO = os.environ.get("GITHUB_REPO", "")  # e.g. owner/repo
+
+def query_prometheus(query, timeout=30):
+    url = urljoin(PROM, "/api/v1/query")
+    resp = requests.get(url, params={"query": query}, timeout=timeout)
+    resp.raise_for_status()
+    data = resp.json()
+    if data.get("status") != "success":
+        raise RuntimeError("Prometheus query failed: %s" % data)
+    return data["data"]["result"]
+
+def range_query_prometheus(query, start_seconds=300, step="30s"):
+    end = datetime.utcnow()
+    start = end - timedelta(seconds=start_seconds)
+    url = urljoin(PROM, "/api/v1/query_range")
+    params = {"query": query, "start": start.isoformat() + "Z", "end": end.isoformat() + "Z", "step": step}
+    resp = requests.get(url, params=params, timeout=60)
+    resp.raise_for_status()
+    data = resp.json()
+    if data.get("status") != "success":
+        raise RuntimeError("Prometheus range query failed: %s" % data)
+    return data["data"]["result"]
+
+def argo_submit_workflow(workflow_manifest: dict):
+    url = urljoin(ARGO, "/api/v1/workflows/aegis-ml")
+    headers = {"Content-Type": "application/json"}
+    resp = requests.post(url, headers=headers, json=workflow_manifest)
+    resp.raise_for_status()
+    return resp.json()
+
+def argo_get_workflow(name):
+    url = urljoin(ARGO, f"/api/v1/workflows/aegis-ml/{name}")
+    resp = requests.get(url)
+    resp.raise_for_status()
+    return resp.json()
+
+def wait_for_argo_workflow(name, timeout_minutes=60, poll_seconds=10):
+    deadline = time.time() + timeout_minutes * 60
+    while time.time() < deadline:
+        wf = argo_get_workflow(name)
+        phase = wf.get("status", {}).get("phase", "")
+        if phase in ("Succeeded", "Failed", "Error", "Running", "Pending"):
+            if phase == "Succeeded":
+                return wf
+            if phase in ("Failed", "Error"):
+                raise RuntimeError(f"Workflow {name} failed: {phase}")
+        time.sleep(poll_seconds)
+    raise TimeoutError("Timed out waiting for workflow %s" % name)
+
+def create_github_pr(title, body, head_branch, base_branch="main", reviewers=None, labels=None):
+    if not GITHUB_TOKEN or not GITHUB_REPO:
+        raise RuntimeError("GITHUB_TOKEN and GITHUB_REPO must be set for GitHub operations")
+    if Github is None:
+        raise RuntimeError("PyGithub not installed in agent image")
+    gh = Github(GITHUB_TOKEN)
+    repo = gh.get_repo(GITHUB_REPO)
+    pr = repo.create_pull(title=title, body=body, head=head_branch, base=base_branch)
+    if reviewers:
+        try:
+            pr.create_review_request(reviewers=reviewers)
+        except Exception:
+            pass
+    if labels:
+        try:
+            pr.add_to_labels(*labels)
+        except Exception:
+            pass
+    return pr.html_url
+
+def create_github_issue(title, body, assignees=None, labels=None):
+    if not GITHUB_TOKEN or not GITHUB_REPO:
+        raise RuntimeError("GITHUB_TOKEN and GITHUB_REPO must be set for GitHub operations")
+    if Github is None:
+        raise RuntimeError("PyGithub not installed in agent image")
+    gh = Github(GITHUB_TOKEN)
+    repo = gh.get_repo(GITHUB_REPO)
+    issue = repo.create_issue(title=title, body=body, assignees=assignees or [], labels=labels or [])
+    return issue.html_url
+
+def patch_istio_virtualservice(namespace, name, new_spec):
+    if k8s_api is None:
+        raise RuntimeError("Kubernetes client not available (are you running in-cluster?)")
+    group = "networking.istio.io"
+    version = "v1beta1"
+    plural = "virtualservices"
+    return k8s_api.patch_namespaced_custom_object(group=group, version=version, namespace=namespace, plural=plural, name=name, body=new_spec)
+
+def record_decision(record):
+    """
+    Unified recorder: attempts to write to Postgres decision log if available, otherwise falls back to writing
+    to DECISION_LOG_PATH file or DECISION_LOG_URL endpoint or stdout.
+    Expected record: dict containing keys like agent, model, env, action, payload, evidence, metadata
+    """
+    try:
+        if _has_db and db_record_decision:
+            res = db_record_decision(record.get("agent"), record.get("model"), record.get("env"), record.get("action"), payload=record.get("payload"), evidence=record.get("evidence"), metadata=record.get("metadata"))
+            print("Recorded decision to DB:", res)
+            return res
+    except Exception as e:
+        print("Postgres decision log failed:", e)
+
+    # fallback to file or http URL
+    path = os.environ.get("DECISION_LOG_PATH")
+    url = os.environ.get("DECISION_LOG_URL")
+    try:
+        if path:
+            with open(path, "a") as fh:
+                fh.write(json.dumps(record) + "\n")
+            return {"written": path}
+        elif url:
+            requests.post(url, json=record, timeout=5)
+            return {"posted": url}
+    except Exception as e:
+        print("Fallback decision log failed:", e)
+
+    # last resort stdout
+    print("DECISION_LOG:", json.dumps(record))
+    return {"stdout": True}
+
diff --git a/tests/requirements-tests.txt b/tests/requirements-tests.txt
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/tests/requirements-tests.txt
@@ -0,0 +1,4 @@
+pytest
+requests
+responses
+psycopg2-binary
+pyyaml
+
diff --git a/tests/test_canary_agent.py b/tests/test_canary_agent.py
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/tests/test_canary_agent.py
@@ -0,0 +1,24 @@
+import json
+import time
+import pytest
+from unittest import mock
+import agents.canary_controller.main as canary
+
+# Mock range_query_prometheus to return stable values
+@pytest.fixture(autouse=True)
+def patch_prom(monkeypatch):
+    def fake_range(q, start_seconds=300, step="30s"):
+        # return a value representing a stable p95 of 0.1, no errors, low drift
+        return [
+            {"values":[[int(time.time()), "0.1"]]},
+        ]
+    monkeypatch.setattr("agents.common.utils.range_query_prometheus", fake_range)
+
+def test_evaluate_window_pass():
+    res = canary.evaluate_window()
+    assert "p95" in res
+    assert float(res["p95"]) == 0.1
+
diff --git a/tests/test_retrain_agent.py b/tests/test_retrain_agent.py
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/tests/test_retrain_agent.py
@@ -0,0 +1,27 @@
+import json
+import pytest
+from unittest import mock
+from flask import Flask
+from agents.retrain_agent import main as retrain_app
+
+def test_webhook_minimal(monkeypatch):
+    client = retrain_app.app.test_client()
+    payload = {"model":"test-model","alerts":[{"labels":{"model":"test-model"}}]}
+    # Patch argo_submit_workflow to return a fake workflow name
+    monkeypatch.setattr("agents.common.utils.argo_submit_workflow", lambda wf: {"metadata":{"name":"retrain-test-wf"}})
+    monkeypatch.setattr("agents.common.utils.wait_for_argo_workflow", lambda name: {"status":{"phase":"Succeeded"}})
+    monkeypatch.setattr("agents.common.utils.create_github_pr", lambda *a, **k: "http://fake.pr")
+    rv = client.post("/webhook", json=payload)
+    assert rv.status_code in (200,202)
+
diff --git a/tests/test_sbom_bot.py b/tests/test_sbom_bot.py
new file mode 100644
index 0000000..cccccccc
--- /dev/null
+++ b/tests/test_sbom_bot.py
@@ -0,0 +1,16 @@
+from agents.sbom_bot import main as sbom_app
+import json
+
+def test_sbom_webhook(monkeypatch):
+    client = sbom_app.app.test_client()
+    payload = {"image":"my-image","vulnerabilities":[{"id":"CVE-1","severity":"HIGH"}]}
+    # Patch create_github_issue to avoid network
+    monkeypatch.setattr("agents.common.utils.create_github_issue", lambda *a, **k: "http://fake.issue")
+    rv = client.post("/webhook", json=payload)
+    assert rv.status_code == 201
+
diff --git a/docs/github_app_setup.md b/docs/github_app_setup.md
new file mode 100644
index 0000000..dddddddd
--- /dev/null
+++ b/docs/github_app_setup.md
@@ -0,0 +1,50 @@
+# GitHub App setup for Aegis agent PRs (summary)
+
+This document explains how to set up a GitHub App to allow agents to create PRs with least privilege.
+
+1. Create GitHub App (in your org)
+   - Name: aegis-agent-bot
+   - Homepage & callback not required for basic automation
+   - Permissions:
+     - Repository permissions:
+       - Contents: Read & Write (to create branches/files)
+       - Pull requests: Read & Write
+       - Issues: Read & Write
+       - Metadata: Read-only
+     - No unnecessary org-level perms.
+   - Subscribe to webhook events: pull_request, push (optional)
+
+2. Install the App on target repositories.
+
+3. Generate a private key and download it (pem file). Store in cluster secret (aegis-github-app-key).
+
+4. For your agents, use JWT authentication:
+   - Create a JWT signed with the App private key
+   - Exchange JWT for an installation access token via:
+     POST https://api.github.com/app/installations/:installation_id/access_tokens
+
+5. Use the installation token (short lived) as GITHUB_TOKEN in agents to create PRs/issues.
+
+6. Example helpers are provided in agents/common/github_app.py for generating tokens and creating PRs.
+
+Security notes:
+- Use the App's installation token only; do not reuse a personal token.
+- Rotate the App's private key if compromised.
+
diff --git a/agents/common/github_app.py b/agents/common/github_app.py
new file mode 100644
index 0000000..eeeeeeee
--- /dev/null
+++ b/agents/common/github_app.py
@@ -0,0 +1,56 @@
+#!/usr/bin/env python3
+"""
+Lightweight GitHub App helper using PyJWT and PyGithub.
+
+Env:
+- GITHUB_APP_ID
+- GITHUB_APP_PEM (or mount the private key file)
+- GITHUB_INSTALLATION_ID
+- GITHUB_REPO
+"""
+import os
+import time
+import jwt
+import requests
+from github import Github
+
+def make_jwt(app_id, private_key_pem):
+    now = int(time.time())
+    payload = {"iat": now - 60, "exp": now + (10 * 60), "iss": int(app_id)}
+    token = jwt.encode(payload, private_key_pem, algorithm="RS256")
+    return token
+
+def get_installation_token(app_id, private_key_pem, installation_id):
+    jwt_token = make_jwt(app_id, private_key_pem)
+    url = f"https://api.github.com/app/installations/{installation_id}/access_tokens"
+    headers = {"Authorization": f"Bearer {jwt_token}", "Accept": "application/vnd.github.v3+json"}
+    r = requests.post(url, headers=headers)
+    r.raise_for_status()
+    return r.json()["token"]
+
+def create_pr_with_app(installation_token, repo_full_name, head_branch, base_branch, title, body):
+    gh = Github(installation_token)
+    repo = gh.get_repo(repo_full_name)
+    pr = repo.create_pull(title=title, body=body, head=head_branch, base=base_branch)
+    return pr.html_url
+
diff --git a/.github/PULL_REQUEST_TEMPLATE/auto_promotion.md b/.github/PULL_REQUEST_TEMPLATE/auto_promotion.md
new file mode 100644
index 0000000..ffffffff
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE/auto_promotion.md
@@ -0,0 +1,12 @@
+<!-- Auto-promotion PR template created by Aegis agent -->
+## Promotion candidate: {{ model_name }}
+
+- MLflow run: {{ run_id }}
+- LakeFS snapshot: {{ lakefs_commit }}
+- Image digest: {{ image_digest }}
+- Git SHA: {{ git_sha }}
+- Validation: {{ validation_summary }}
+
+Notes:
+- This PR was created automatically by the Aegis agent. It requires CODEOWNERS review and passing SBOM/Trivy checks before merge.
+- Do not merge unless SRE and Security have signed off (add reviewers).
+
diff --git a/README_AGENTS_NEXT_STEPS.md b/README_AGENTS_NEXT_STEPS.md
new file mode 100644
index 0000000..10101010
--- /dev/null
+++ b/README_AGENTS_NEXT_STEPS.md
@@ -0,0 +1,36 @@
+# Next steps & how to run the demo (synthetic drift -> retrain)
+
+1. Deploy the Retrain Agent (ensure retrain-agent deployment exists and is reachable at retrain-agent.aegis-ml.svc.cluster.local:8080)
+2. Install Argo Events (EventSource + Sensor) and apply:
+   kubectl apply -f argo-events/eventsource-webhook.yaml
+   kubectl apply -f argo-events/sensor-retrain.yaml
+
+3. Deploy Postgres (for decision log) if you want durable logging:
+   kubectl apply -f k8s/postgres/postgres-deployment.yaml
+   Then initialize schema:
+   kubectl exec -n aegis-ml deploy/aegis-postgres -- bash -c "psql -U aegis -d aegis -c '$(cat /sql/001_create_decision_log.sql)'" 
+   (or run the SQL locally against the service)
+
+4. Port-forward the EventSource to your laptop:
+   kubectl -n aegis-ml port-forward svc/aegis-webhook-es 12000:12000
+
+5. Run the synthetic drift script:
+   python tests/send_synthetic_drift.py --model my_model
+
+6. Observe:
+   - Sensor should forward the event to the retrain agent
+   - Retrain agent should submit an Argo retrain workflow (check argo UI)
+   - On workflow success the retrain agent will create a promotion PR for review (check GitHub)
+
+Notes:
+- For production, secure the webhook with secrets and run EventSource controllers with proper RBAC.
+- If Argo server is not accessible in-cluster, set ARGO_SERVER env accordingly in agents.
+
