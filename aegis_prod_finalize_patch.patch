*** Begin Patch
*** Add File: vault/enable_audit_and_roles.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Usage:
+# VAULT_ADDR=https://vault.example VAULT_TOKEN=... ./enable_audit_and_roles.sh
+
+: "${VAULT_ADDR:?Please set VAULT_ADDR}"
+: "${VAULT_TOKEN:?Please set VAULT_TOKEN}"
+
+echo "Configuring Vault audit device (file) and policies..."
+export VAULT_ADDR
+vault login "${VAULT_TOKEN}" >/dev/null
+
+# enable file audit device (appends)
+if ! vault audit list | grep -q file/ ; then
+  vault audit enable file file_path=/var/log/vault_audit.log
+  echo "Enabled file audit device at /var/log/vault_audit.log"
+else
+  echo "File audit device already enabled"
+fi
+
+# Install policy for signing (example)
+vault policy write aegis-hsm-policy - <<'HCL'
+path "secret/data/aegis/llm/*" {
+  capabilities = ["read"]
+}
+path "secret/data/aegis/cosign/*" {
+  capabilities = ["read"]
+}
+HCL
+
+echo "Ensure Kubernetes auth role exists (operator-signing-sa in aegis namespace)..."
+if ! vault auth list | grep -q kubernetes; then
+  vault auth enable kubernetes
+fi
+
+cat > /tmp/aegis-k8s-role.json <<EOF
+{
+  "bound_service_account_names": ["operator-signing-sa"],
+  "bound_service_account_namespaces": ["aegis"],
+  "policies": ["aegis-hsm-policy"],
+  "ttl": "24h"
+}
+EOF
+vault write auth/kubernetes/role/aegis-operator-role @/tmp/aegis-k8s-role.json
+
+echo "Vault configured: policy wrote and kubernetes role created. Audit logs will be written to /var/log/vault_audit.log on Vault host(s)."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/verify_no_signing_access.yml
+name: Verify CI cannot access operator signing endpoint
+on:
+  workflow_dispatch:
+
+jobs:
+  verify:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Attempt access to operator-signing service (must fail)
+        run: |
+          set -e
+          OP_URL="${{ secrets.OPERATOR_SIGNING_URL }}"
+          echo "Testing access to ${OP_URL}"
+          # Expect non-2xx (403 or connection refused). If 200, fail the workflow.
+          HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "${OP_URL}/health" || echo "000")
+          echo "HTTP status: $HTTP_STATUS"
+          if [ "$HTTP_STATUS" = "200" ]; then
+            echo "ERROR: CI can reach operator-signing endpoint. This must be blocked."
+            exit 2
+          fi
+          echo "OK: CI cannot reach operator-signing endpoint (expected)."
+
*** End Patch
*** Begin Patch
*** Add File: k8s/operator/kata-operator-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: kata-operator
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: kata-operator
+  template:
+    metadata:
+      labels:
+        app: kata-operator
+    spec:
+      serviceAccountName: kata-operator-sa
+      containers:
+        - name: kata-operator
+          image: aegis/kata-operator:latest
+          env:
+            - name: WATCH_NAMESPACE
+              value: "aegis-sandbox"
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+            limits:
+              cpu: "500m"
+              memory: "512Mi"
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: kata-operator-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: kata-operator-role
+  namespace: aegis
+rules:
+- apiGroups: [""]
+  resources: ["pods","nodes"]
+  verbs: ["get","list","watch","create","delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: kata-operator-binding
+  namespace: aegis
+subjects:
+- kind: ServiceAccount
+  name: kata-operator-sa
+  namespace: aegis
+roleRef:
+  kind: Role
+  name: kata-operator-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/sandbox_isolation_ci.yml
+name: Sandbox Isolation CI
+on:
+  workflow_dispatch:
+
+jobs:
+  isolation-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install kubectl
+        run: |
+          curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
+          chmod +x kubectl; sudo mv kubectl /usr/local/bin/
+      - name: Run isolation test script against cluster
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_INTEGRATION }}
+        run: |
+          chmod +x agents/sandbox_isolation_test.sh
+          ./agents/sandbox_isolation_test.sh
+
*** End Patch
*** Begin Patch
*** Add File: rl/ppo_trainer_trl.py
+#!/usr/bin/env python3
+"""
+PPO Trainer using TRL (trl) library.
+This script demonstrates a small RLHF PPO loop using a supervised model and a reward model.
+Requirements: pip install trl transformers accelerate datasets peft
+Operator: run on GPU node(s) with accelerate config for distributed runs.
+"""
+import os, json
+import torch
+from transformers import AutoTokenizer, AutoModelForCausalLM
+from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
+import mlflow
+from rl.checkpoint_manager import upload_checkpoint
+
+MODEL_NAME = os.environ.get("BASE_MODEL", "gpt2")
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI")
+WORKDIR = "/tmp/trl_ppo"
+os.makedirs(WORKDIR, exist_ok=True)
+
+def load_models():
+    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
+    model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_NAME)
+    return tokenizer, model
+
+def train_ppo():
+    tokenizer, model = load_models()
+    ppo_config = PPOConfig(model_name=MODEL_NAME, batch_size=1, forward_batch_size=1)
+    trainer = PPOTrainer(ppo_config, model, tokenizer)
+    # toy dataset: list of prompts and references (operator replace with real)
+    prompts = ["Q: 2+2?","Q: Capital of France?"]
+    reward_fn = lambda gen: [1.0 if "4" in gen or "Paris" in gen else -1.0]
+    with mlflow.start_run():
+        for epoch in range(1):
+            for p in prompts:
+                # generate baseline response
+                response = trainer.generate(p, max_new_tokens=32)
+                rewards = reward_fn(response[0])
+                stats = trainer.step([p], response, rewards)
+                print("PPO step stats:", stats)
+        # save model checkpoint
+        ckpt = os.path.join(WORKDIR, "policy_final.pt")
+        model.save_pretrained(os.path.join(WORKDIR, "policy_model"))
+        rec = upload_checkpoint(ckpt, f"rlhf/trl_policy_{int(__import__('time').time())}.pt", metadata={"method":"trl_ppo"})
+        mlflow.log_param("checkpoint", rec.get("s3_path") if rec else "none")
+    print("PPO training complete")
+
+if __name__=="__main__":
+    train_ppo()
+
*** End Patch
*** Begin Patch
*** Add File: rl/checkpoint_validate.py
+#!/usr/bin/env python3
+"""
+Validate model checkpoint integrity and basic loadability.
+ - compute SHA256
+ - attempt to load model weights (lightweight check)
+ - register metadata to MLflow
+"""
+import os, hashlib, json
+import mlflow
+from transformers import AutoConfig, AutoModel
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI")
+
+def sha256(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as fh:
+        for chunk in iter(lambda: fh.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def validate(checkpoint_path):
+    s = sha256(checkpoint_path)
+    # basic load test: try to load config
+    try:
+        cfg = AutoConfig.from_pretrained(os.path.dirname(checkpoint_path))
+        m = AutoModel.from_config(cfg)
+        load_ok = True
+    except Exception as e:
+        load_ok = False
+        err = str(e)
+    # log to MLflow
+    if MLFLOW_URI:
+        mlflow.set_tracking_uri(MLFLOW_URI)
+        with mlflow.start_run():
+            mlflow.log_param("checksum", s)
+            mlflow.log_param("load_ok", load_ok)
+    return {"checksum": s, "load_ok": load_ok}
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--ckpt", required=True)
+    args = p.parse_args()
+    print(validate(args.ckpt))
+
*** End Patch
*** Begin Patch
*** Add File: safety/hallu_dataset_manager.py
+#!/usr/bin/env python3
+"""
+Manage hallucination labeled datasets:
+ - merge new labeled samples (JSONL with fields text,label)
+ - sample holdout sets for validation
+ - produce training JSONL for retraining jobs
+"""
+import os, json, random
+
+DATA_DIR = os.environ.get("HALLU_DATA_DIR", "/data/hallucination")
+TRAIN_OUT = os.path.join(DATA_DIR, "train.jsonl")
+VAL_OUT = os.path.join(DATA_DIR, "val.jsonl")
+
+def merge_new(path):
+    if not os.path.exists(DATA_DIR):
+        os.makedirs(DATA_DIR)
+    target = os.path.join(DATA_DIR, "all.jsonl")
+    with open(target, "a") as fh, open(path) as src:
+        for l in src:
+            fh.write(l)
+    print("Merged new labeled data into", target)
+
+def split_holdout(seed=42, val_pct=0.1):
+    allp = os.path.join(DATA_DIR, "all.jsonl")
+    if not os.path.exists(allp):
+        print("No data found")
+        return
+    with open(allp) as fh:
+        lines = [l.strip() for l in fh if l.strip()]
+    random.Random(seed).shuffle(lines)
+    split = int(len(lines)*(1-val_pct))
+    with open(TRAIN_OUT,"w") as t, open(VAL_OUT,"w") as v:
+        for l in lines[:split]:
+            t.write(l+"\n")
+        for l in lines[split:]:
+            v.write(l+"\n")
+    print("Wrote train/val sets:", TRAIN_OUT, VAL_OUT)
+
+if __name__=="__main__":
+    import argparse
+    p=argparse.ArgumentParser()
+    p.add_argument("--merge")
+    p.add_argument("--split", action="store_true")
+    args=p.parse_args()
+    if args.merge:
+        merge_new(args.merge)
+    if args.split:
+        split_holdout()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/adversarial_gate.yml
+name: Adversarial Promotion Gate
+on:
+  repository_dispatch:
+    types: [model-prepromotion]
+  workflow_dispatch:
+
+jobs:
+  adversarial:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run adversarial harness
+        env:
+          LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }}
+          ADV_MANIFEST: tests/adversarial_prompts.jsonl
+          MODEL_REGISTRY_API: ${{ secrets.MODEL_REGISTRY_API }}
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          set -e
+          python safety/adversarial_harness.py
+      - name: If success, promote model via Model Registry
+        if: success()
+        run: |
+          echo "Promotion allowed - operator or auto_manager should promote model"
+      - name: On failure, trigger rollback action (post to registry)
+        if: failure()
+        env:
+          MODEL_REGISTRY_API: ${{ secrets.MODEL_REGISTRY_API }}
+        run: |
+          echo "Adversarial tests failed. Triggering rollback in model registry."
+          curl -s -XPOST -H "Content-Type: application/json" -d "{\"action\":\"rollback\",\"reason\":\"adversarial_fail\"}\" $MODEL_REGISTRY_API/models/actions || true
+
*** End Patch
*** Begin Patch
*** Add File: observability/opensearch/index_template_aegis.json
+{
+  "index_patterns": ["aegis-audit*"],
+  "settings": {
+    "number_of_shards": 1,
+    "number_of_replicas": 0
+  },
+  "mappings": {
+    "properties": {
+      "kind": { "type": "keyword" },
+      "ts": { "type": "date" },
+      "record.tenant": { "type": "keyword" },
+      "record.model_id": { "type": "keyword" },
+      "record.prompt_id": { "type": "keyword" },
+      "record.action": { "type": "keyword" },
+      "record.prompt": { "type": "text" },
+      "record.response": { "type": "text" }
+    }
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: ci/auto_canary_manager_es.py
+#!/usr/bin/env python3
+"""
+Auto Canary Manager using OpenSearch/Elasticsearch queries
+ - computes hallucination rate by querying 'aegis-audit' index for 'hallu_check' or 'agent_plan' records
+ - computes cost by querying billing indices (placeholder)
+ - acts to promote/rollback models via model_registry API
+"""
+import os, requests, json
+from elasticsearch import Elasticsearch
+from datetime import datetime, timedelta
+
+ES_HOST = os.environ.get("ES_HOST")
+MODEL_REGISTRY_API = os.environ.get("MODEL_REGISTRY_API")
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+es = Elasticsearch([ES_HOST])
+
+def hallucination_rate(model_id, minutes=30):
+    now = datetime.utcnow()
+    start = (now - timedelta(minutes=minutes)).isoformat()
+    q = {
+      "query": {
+        "bool": {
+          "must": [
+            {"range": {"ts": {"gte": start}}},
+            {"term": {"kind": "hallu_check"}},
+            {"term": {"record.model_id": model_id}}
+          ]
+        }
+      },
+      "size": 0,
+      "aggs": {
+        "hallu_count": {"sum": {"field": "record.is_hallucination"}},
+        "total": {"value_count": {"field": "record.is_hallucination"}}
+      }
+    }
+    res = es.search(index="aegis-audit*", body=q)
+    agg = res.get("aggregations", {})
+    hallu = agg.get("hallu_count", {}).get("value", 0)
+    total = agg.get("total", {}).get("value", 0)
+    rate = (hallu / total) if total else 0.0
+    return rate
+
+def cost_per_hour(model_id, minutes=60):
+    # placeholder: requires ingestion of cost metrics into ES; return dummy
+    return 1.0
+
+def decide(model_id):
+    hallu = hallucination_rate(model_id, minutes=30)
+    cost = cost_per_hour(model_id)
+    print(f"Metrics for {model_id}: hallu={hallu}, cost={cost}")
+    if hallu < float(os.environ.get("CANARY_HALLU_THRESH", "0.02")) and cost <= float(os.environ.get("CANARY_COST_THRESH","10.0")):
+        print("Promoting model")
+        requests.post(f"{MODEL_REGISTRY_API}/models/promote", json={"model": model_id}, timeout=10)
+        action="promote"
+    else:
+        print("Rolling back model")
+        requests.post(f"{MODEL_REGISTRY_API}/models/rollback", json={"model": model_id}, timeout=10)
+        action="rollback"
+    if OPERATOR_WEBHOOK:
+        try:
+            requests.post(OPERATOR_WEBHOOK, json={"model": model_id, "action": action, "hallu": hallu, "cost": cost}, timeout=5)
+        except Exception:
+            pass
+    return {"model": model_id, "action": action, "hallu": hallu, "cost": cost}
+
+if __name__=="__main__":
+    import sys
+    if len(sys.argv) < 2:
+        print("usage: auto_canary_manager_es.py <model_id>")
+        raise SystemExit(2)
+    print(decide(sys.argv[1]))
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile.py
+#!/usr/bin/env python3
+"""
+Billing reconciliation:
+ - compare aggregated token counters (Redis) vs recorded invoices (S3 or DB)
+ - produce reconciliation report with anomalies
+"""
+import os, json, redis, boto3
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/3")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+r = redis.from_url(REDIS_URL)
+
+def fetch_invoices_from_s3(prefix="billing/invoices/"):
+    if not COMPLIANCE_BUCKET:
+        return []
+    s3 = boto3.client("s3")
+    objs = s3.list_objects_v2(Bucket=COMPLIANCE_BUCKET, Prefix=prefix)
+    invoices = []
+    for o in objs.get("Contents", []):
+        key = o["Key"]
+        tmp = "/tmp/" + key.replace("/","_")
+        s3.download_file(COMPLIANCE_BUCKET, key, tmp)
+        invoices.extend(json.load(open(tmp)))
+    return invoices
+
+def aggregate_redis_tokens():
+    data = r.hgetall("llm:tokens") or {}
+    return {k.decode(): int(v.decode()) for k,v in data.items()}
+
+def reconcile():
+    redis_vals = aggregate_redis_tokens()
+    invoices = fetch_invoices_from_s3()
+    # naive check: ensure tenants in invoices exist in redis or not
+    anomalies = []
+    for inv_batch in invoices:
+        for inv in inv_batch:
+            tenant = inv["tenant"]
+            tokens = inv["tokens"]
+            if tenant not in redis_vals:
+                anomalies.append({"tenant":tenant,"invoice_tokens": tokens, "redis_tokens": 0})
+    out = {"anomalies": anomalies}
+    print("Reconcile report:", json.dumps(out, indent=2))
+    return out
+
+if __name__=="__main__":
+    reconcile()
+
*** End Patch
*** Begin Patch
*** Add File: security/oauth2_proxy_deploy.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: oauth2-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: oauth2-proxy
+  template:
+    metadata:
+      labels:
+        app: oauth2-proxy
+    spec:
+      containers:
+        - name: oauth2-proxy
+          image: quay.io/pusher/oauth2_proxy:latest
+          args:
+            - --provider=oidc
+            - --oidc-issuer-url=$(OIDC_ISSUER)
+            - --client-id=$(OIDC_CLIENT_ID)
+            - --client-secret=$(OIDC_CLIENT_SECRET)
+            - --cookie-secret=$(COOKIE_SECRET)
+            - --upstream=http://tenant-ui:80
+          env:
+            - name: OIDC_ISSUER
+              valueFrom:
+                secretKeyRef:
+                  name: oidc-secret
+                  key: issuer
+            - name: OIDC_CLIENT_ID
+              valueFrom:
+                secretKeyRef:
+                  name: oidc-secret
+                  key: client-id
+            - name: OIDC_CLIENT_SECRET
+              valueFrom:
+                secretKeyRef:
+                  name: oidc-secret
+                  key: client-secret
+            - name: COOKIE_SECRET
+              valueFrom:
+                secretKeyRef:
+                  name: oidc-secret
+                  key: cookie-secret
+
+---
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: aegis-ingress
+  namespace: aegis
+  annotations:
+    nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.aegis.svc/oauth2/auth"
+    nginx.ingress.kubernetes.io/auth-signin: "http://oauth2-proxy.aegis.svc/oauth2/start?rd=$request_uri"
+    nginx.ingress.kubernetes.io/enable-modsecurity: "true"
+    nginx.ingress.kubernetes.io/modsecurity-snippet: |
+      SecRuleEngine On
+spec:
+  rules:
+    - host: aegis.example.com
+      http:
+        paths:
+          - path: /
+            pathType: Prefix
+            backend:
+              service:
+                name: tenant-ui
+                port:
+                  number: 80
+
*** End Patch
*** Begin Patch
*** Add File: infra/vllm/vllm_config.json
+{
+  "model": "gpt-like-vllm",
+  "max_batch_size": 32,
+  "max_batch_latency": 0.02,
+  "num_workers_per_model": 1,
+  "prefill": {
+    "max_prefill_batch_size": 16
+  },
+  "tokenizer": {
+    "type": "hf"
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File: load_test/ingest_load_results.py
+#!/usr/bin/env python3
+"""
+Ingest load test results into OpenSearch and S3 for analysis.
+"""
+import os, json
+from datetime import datetime
+try:
+    from elasticsearch import Elasticsearch
+except Exception:
+    Elasticsearch = None
+import boto3
+
+ES_HOST = os.environ.get("ES_HOST")
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+
+def ingest_to_es(path):
+    if not Elasticsearch or not ES_HOST:
+        print("ES not configured; skipping")
+        return
+    es = Elasticsearch([ES_HOST])
+    data = json.load(open(path))
+    for i,rec in enumerate(data):
+        es.index(index="aegis-load-test", body={"ts": datetime.utcnow().isoformat(), "rec": rec})
+    print("Indexed to ES")
+
+def upload_to_s3(path):
+    if not COMPLIANCE_BUCKET:
+        return
+    s3 = boto3.client("s3")
+    key = f"loadtests/{os.path.basename(path)}"
+    s3.upload_file(path, COMPLIANCE_BUCKET, key)
+    print("Uploaded to S3", key)
+
+if __name__=="__main__":
+    import sys
+    p = sys.argv[1] if len(sys.argv)>1 else "/tmp/stress_results.json"
+    ingest_to_es(p)
+    upload_to_s3(p)
+
*** End Patch
*** End Patch
