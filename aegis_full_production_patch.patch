*** Begin Patch
*** Add File:artifacts/boot_manifest_template.json
+{
+  "artifact": "model.onnx",
+  "version": "0.0.0",
+  "format": "onnx",
+  "size": 0,
+  "sha256": "",
+  "signature": "",
+  "sbom": "sbom.spdx.json",
+  "trivy_report": "trivy_report.json",
+  "provenance": {
+    "git_sha": "",
+    "build_time": "",
+    "builder": "aegis-ci"
+  },
+  "apply_instructions": {
+    "type": "delta",
+    "rollback": "preserve_previous_artifact"
+  },
+  "acceptance_criteria": {
+    "latency_ms": 50,
+    "accuracy_threshold": 0.90,
+    "deadline_misses_allowed": 0
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/ci_build_sign.yml
+name: Build → SBOM → SCA → Sign → Upload
+on:
+  push:
+    branches:
+      - main
+  pull_request:
+    branches:
+      - main
+
+jobs:
+  build_and_sign:
+    runs-on: ubuntu-latest
+    env:
+      EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+      MODEL_ARTIFACT_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+      REKOR_URL: ${{ secrets.REKOR_URL }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Build model artifact (placeholder)
+        run: |
+          mkdir -p /tmp/artifacts
+          echo "{}" > /tmp/artifacts/model.onnx
+
+      - name: Generate SBOM (syft)
+        run: |
+          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+          syft packages dir:/tmp/artifacts -o spdx-json > sbom.spdx.json || true
+
+      - name: Run Trivy SCA
+        run: |
+          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+          trivy fs --format json --output trivy_report.json --severity HIGH,CRITICAL /tmp/artifacts || true
+
+      - name: Compute SHA256 and populate manifest
+        run: |
+          sha=$(sha256sum /tmp/artifacts/model.onnx | awk '{print $1}')
+          jq --arg sha "$sha" '.sha256=$sha | .version="1.0.0" | .provenance.git_sha=env.GITHUB_SHA | .provenance.build_time=(now|tostring)' artifacts/boot_manifest_template.json > /tmp/manifest.json
+          cp /tmp/manifest.json /tmp/artifacts/manifest.json
+
+      - name: Upload artifact to S3
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          aws s3 cp /tmp/artifacts/model.onnx s3://${{ secrets.MODEL_ARTIFACT_BUCKET }}/artifacts/model.onnx
+          aws s3 cp /tmp/artifacts/manifest.json s3://${{ secrets.MODEL_ARTIFACT_BUCKET }}/artifacts/manifest.json
+
+      - name: Cosign sign manifest via KMS
+        env:
+          COSIGN_EXPERIMENTAL: "1"
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          # sign manifest with AWS KMS key (repo secret COSIGN_KMS_KEY_ARN required)
+          cosign sign --key "awskms://${{ secrets.COSIGN_KMS_KEY_ARN }}" --rekor-url "${{ secrets.REKOR_URL }}" /tmp/manifest.json || true
+          # attach signature to manifest location (store in evidence)
+          aws s3 cp /tmp/manifest.json s3://${{ secrets.EVIDENCE_BUCKET }}/manifests/${{ github.sha }}-manifest.json
+
+      - name: Upload SBOM and Trivy to evidence
+        run: |
+          aws s3 cp sbom.spdx.json s3://${{ secrets.EVIDENCE_BUCKET }}/sboms/${{ github.sha }}-sbom.spdx.json || true
+          aws s3 cp trivy_report.json s3://${{ secrets.EVIDENCE_BUCKET }}/sca/${{ github.sha }}-trivy.json || true
+
*** End Patch
*** Begin Patch
*** Add File:argo/promotion/promotion_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: artifact-promotion-
+  namespace: aegis
+spec:
+  entrypoint: promote-artifact
+  templates:
+    - name: promote-artifact
+      inputs:
+        parameters:
+          - name: manifest-s3
+      steps:
+        - - name: download-manifest
+            template: download-manifest
+            arguments:
+              parameters:
+                - name: manifest-s3
+                  value: "{{inputs.parameters.manifest-s3}}"
+        - - name: verify-signature
+            template: verify-signature
+        - - name: run-verification
+            template: run-verification
+        - - name: attach-evidence
+            template: attach-evidence
+            arguments:
+              parameters:
+                - name: manifest
+                  value: "{{steps.download-manifest.outputs.parameters.manifest}}"
+
+    - name: download-manifest
+      inputs:
+        parameters:
+          - name: manifest-s3
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 || true
+            python3 - <<PY
+import boto3,sys
+src="{{inputs.parameters.manifest-s3}}"
+parts=src[5:].split("/",1)
+bucket,key=parts[0],parts[1]
+boto3.client("s3").download_file(bucket,key,"/tmp/manifest.json")
+print("/tmp/manifest.json")
+PY
+      outputs:
+        parameters:
+          - name: manifest
+            valueFrom:
+              path: /tmp/manifest.json
+
+    - name: verify-signature
+      container:
+        image: sigstore/cosign:latest
+        command: [sh, -c]
+        args:
+          - echo "Verifying cosign attestation for manifest (operator: adjust to your schema)"; sleep 1
+            # In production run cosign verify-blob with Rekor
+
+    - name: run-verification
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - echo "Running verification pipelines (SIL->PIL->HIL) for artifact and collecting results"; sleep 2
+            # In production invoke Argo verification workflow(s)
+
+    - name: attach-evidence
+      inputs:
+        parameters:
+          - name: manifest
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/evidence/attach_and_sign.py --manifest "{{inputs.parameters.manifest}}" --out /tmp/evidence_bundle.json || true
+            cat /tmp/evidence_bundle.json || true
+
*** End Patch
*** Begin Patch
*** Add File:argo/verification/full_pipeline_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: verification-full-
+  namespace: aegis
+spec:
+  entrypoint: verification
+  templates:
+    - name: verification
+      inputs:
+        parameters:
+          - name: artifact-s3
+      steps:
+        - - name: run-sil
+            template: run-sil
+            arguments:
+              parameters:
+                - name: artifact
+                  value: "{{inputs.parameters.artifact-s3}}"
+        - - name: run-montecarlo
+            template: run-montecarlo
+        - - name: train
+            template: train
+        - - name: compute-coverage
+            template: compute-coverage
+        - - name: run-pil
+            template: run-pil
+        - - name: evaluate-safety
+            template: evaluate-safety
+
+    - name: run-sil
+      inputs:
+        parameters:
+          - name: artifact
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - echo "Running SIL (sim) using artifact {{inputs.parameters.artifact}} (stub)"; sleep 2
+
+    - name: run-montecarlo
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/sim/run_monte_carlo.py --runs 100 --out /tmp/sim_samples.npy || true
+      outputs:
+        artifacts:
+          - name: sim-samples
+            path: /tmp/sim_samples.npy
+
+    - name: train
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/train/train_and_checkpoint.py --input /tmp/sim_samples.npy --out /tmp/checkpoint.pt || true
+      outputs:
+        artifacts:
+          - name: checkpoint
+            path: /tmp/checkpoint.pt
+
+    - name: compute-coverage
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/sim/coverage_report.py --samples /tmp/sim_samples.npy --out /tmp/sim_coverage.json || true
+      outputs:
+        artifacts:
+          - name: sim-coverage
+            path: /tmp/sim_coverage.json
+
+    - name: run-pil
+      container:
+        image: registry.example.com/aegis/flight-pil:latest
+        command: [sh, -c]
+        args:
+          - /usr/local/bin/pil_harness --artifact /tmp/checkpoint.pt --out /tmp/pil_result.json || true
+      outputs:
+        artifacts:
+          - name: pil-result
+            path: /tmp/pil_result.json
+
+    - name: evaluate-safety
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - python3 scripts/safety/safety_oracle.py --sim /tmp/sim_coverage.json --pil /tmp/pil_result.json --out /tmp/safety_eval.json || true
+            cat /tmp/safety_eval.json || true
+
*** End Patch
*** Begin Patch
*** Add File:scripts/safety/safety_oracle.py
+#!/usr/bin/env python3
+"""
+Simple safety oracle that consumes sim coverage and PIL results and returns pass/fail.
+Production: replace with domain-specific oracles and formal checks.
+"""
+import argparse, json, sys
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--sim", default="/tmp/sim_coverage.json")
+    p.add_argument("--pil", default="/tmp/pil_result.json")
+    p.add_argument("--out", default="/tmp/safety_eval.json")
+    args = p.parse_args()
+    sim = {}
+    pil = {}
+    try:
+        sim = json.load(open(args.sim))
+    except:
+        pass
+    try:
+        pil = json.load(open(args.pil))
+    except:
+        pass
+    # simple rules: require at least N samples and all pil tests passed
+    n = sim.get("n", 0)
+    pil_ok = pil.get("timing_ok", False)
+    status = "failed"
+    reason = ""
+    if n >= 50 and pil_ok:
+        status = "success"
+    else:
+        reason = f"n={n} pil_ok={pil_ok}"
+    out = {"status": status, "reason": reason, "sim": sim.get("n"), "pil_ok": pil_ok}
+    open(args.out, "w").write(json.dumps(out))
+    print(json.dumps(out))
+    if status != "success":
+        sys.exit(2)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/evidence/attach_and_sign.py
+#!/usr/bin/env python3
+"""
+Collect evidence artifacts (SBOM, sim coverage, pil logs, manifest), bundle, sign (cosign KMS) and upload to EVIDENCE_BUCKET.
+"""
+import argparse, json, subprocess, os, tempfile
+
+def bundle_evidence(manifest_path, out):
+    tmp = tempfile.mkdtemp()
+    # copy referenced artifacts (best effort)
+    try:
+        m = json.load(open(manifest_path))
+    except:
+        m = {}
+    # copy manifest
+    subprocess.run(["cp", manifest_path, os.path.join(tmp, "manifest.json")])
+    # optionally include SBOM, trivy, sim_coverage, pil_result if present locally
+    for p in ["sbom.spdx.json","trivy_report.json","/tmp/sim_coverage.json","/tmp/pil_result.json"]:
+        if os.path.exists(p):
+            subprocess.run(["cp", p, tmp])
+    tar = out
+    subprocess.run(["tar","czf",tar,"-C",tmp,"."], check=True)
+    return tar
+
+def sign_and_upload(tar_path, evidence_bucket):
+    # sign using cosign KMS if COSIGN_KMS_KEY_ARN present, else just upload
+    if os.environ.get("COSIGN_KMS_KEY_ARN"):
+        subprocess.run(["cosign","sign","--key",f"awskms://{os.environ['COSIGN_KMS_KEY_ARN']}",tar_path], check=False)
+    if evidence_bucket:
+        subprocess.run(["aws","s3","cp",tar_path,f"s3://{evidence_bucket}/evidence/"], check=False)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--out", default="/tmp/evidence_bundle.tgz")
+    args = p.parse_args()
+    tar = bundle_evidence(args.manifest, args.out)
+    sign_and_upload(tar, os.environ.get("EVIDENCE_BUCKET",""))
+    print("Wrote evidence bundle", tar)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/ha-values-raft.yaml
+# Vault Helm values for HA using Raft storage with KMS auto-unseal and TLS via cert-manager
+server:
+  ha:
+    enabled: true
+  dataStorage:
+    enabled: false
+  standalone:
+    enabled: false
+  extraEnvironmentVars:
+    - name: VAULT_LOG_LEVEL
+      value: "info"
+tls:
+  enabled: true
+  certManager:
+    enabled: true
+    issuerRef:
+      name: vault-ca-issuer
+      kind: ClusterIssuer
+unsealer:
+  aws:
+    enabled: true
+    region: "us-west-2"
+    kmsKeyId: "" # fill with KMS ARN / let terraform output
+server:
+  raft:
+    enabled: true
+resources:
+  requests:
+    cpu: 250m
+    memory: 512Mi
+  limits:
+    cpu: 1
+    memory: 2Gi
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/cert-manager-issuer-and-ca.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: vault-ca-issuer
+spec:
+  selfSigned: {}
+
+---
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: vault-server-tls
+  namespace: aegis
+spec:
+  secretName: vault-server-tls
+  commonName: vault.aegis.svc
+  dnsNames:
+    - vault.aegis.svc
+  issuerRef:
+    name: vault-ca-issuer
+    kind: ClusterIssuer
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/init-vault-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: vault-init
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: vault
+      restartPolicy: OnFailure
+      containers:
+        - name: init
+          image: hashicorp/vault:1.14.0
+          env:
+            - name: VAULT_ADDR
+              value: "https://vault.aegis.svc:8200"
+          command:
+            - sh
+            - -c
+            - |
+              set -e
+              if vault status >/dev/null 2>&1; then
+                echo "Vault reachable"
+              else
+                echo "Vault not reachable - ensure service and TLS"
+              fi
+      restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/postgres-statefulset.yaml
+apiVersion: v1
+kind: Secret
+metadata:
+  name: rekor-postgres-secret
+  namespace: aegis
+type: Opaque
+stringData:
+  postgres-user: rekor
+  postgres-password: "REPLACE_WITH_STRONG_PASSWORD"
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  serviceName: rekor-postgres
+  replicas: 1
+  selector:
+    matchLabels:
+      app: rekor-postgres
+  template:
+    metadata:
+      labels:
+        app: rekor-postgres
+    spec:
+      containers:
+        - name: postgres
+          image: postgres:15
+          env:
+            - name: POSTGRES_USER
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-user
+            - name: POSTGRES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-postgres-secret
+                  key: postgres-password
+          ports:
+            - containerPort: 5432
+          volumeMounts:
+            - name: pgdata
+              mountPath: /var/lib/postgresql/data
+  volumeClaimTemplates:
+    - metadata:
+        name: pgdata
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 10Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor-postgres
+  namespace: aegis
+spec:
+  ports:
+    - port: 5432
+      targetPort: 5432
+  selector:
+    app: rekor-postgres
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/rekor-deployment-postgres.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          env:
+            - name: REKOR_STORAGE_TYPE
+              value: "postgres"
+            - name: REKOR_DATABASE_URL
+              value: "postgres://rekor:REPLACE_WITH_STRONG_PASSWORD@rekor-postgres.aegis.svc:5432/rekordb"
+            - name: REKOR_API_PORT
+              value: "3000"
+          ports:
+            - containerPort: 3000
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  selector:
+    app: rekor
+  ports:
+    - name: http
+      port: 3000
+      targetPort: 3000
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/hardened-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: signing-proxy
+  template:
+    metadata:
+      labels:
+        app: signing-proxy
+    spec:
+      serviceAccountName: signing-proxy-sa
+      containers:
+        - name: signing-proxy
+          image: registry.example.com/aegis/signing-proxy:latest
+          args:
+            - "--port=8443"
+            - "--tls-cert=/tls/tls.crt"
+            - "--tls-key=/tls/tls.key"
+            - "--client-ca=/tls/ca.crt"
+            - "--vault-addr=https://vault.aegis.svc:8200"
+            - "--rekor-url=https://rekor.aegis.svc:3000"
+            - "--audit-bucket=${EVIDENCE_BUCKET}"
+          ports:
+            - containerPort: 8443
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: signing-proxy-tls
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/staging/enable_dryrun_constraint.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-staging
+  labels:
+    env: staging
+
+---
+# Deploy require cosign constraint in dryrun mode for staging
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireCosignAttestation
+metadata:
+  name: cosign-required-staging
+spec:
+  enforcementAction: dryrun
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces:
+      - "aegis-staging"
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/automation/gatekeeper_dryrun_check_and_promote.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Usage:
+#  ./gatekeeper_dryrun_check_and_promote.sh --constraint cosign-required-staging --namespace aegis-staging [--promote]
+PROMOTE=false
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --constraint) CONSTRAINT="$2"; shift 2;;
+    --namespace) NS="$2"; shift 2;;
+    --promote) PROMOTE=true; shift;;
+    *) shift;;
+  esac
+done
+
+if [ -z "${CONSTRAINT:-}" ] || [ -z "${NS:-}" ]; then
+  echo "Require --constraint and --namespace"
+  exit 2
+fi
+
+echo "Running Gatekeeper dry-run evaluation for constraint ${CONSTRAINT} in namespace ${NS}"
+cat > /tmp/gk_test_pod.yaml <<EOF
+apiVersion: v1
+kind: Pod
+metadata:
+  name: gatekeeper-test-pod
+  namespace: ${NS}
+spec:
+  containers:
+    - name: nginx
+      image: nginx:alpine
+EOF
+
+set +e
+kubectl apply -f /tmp/gk_test_pod.yaml
+RC=$?
+set -e
+
+if [ $RC -eq 0 ]; then
+  echo "Test pod created successfully in dry-run. Gatekeeper NOT blocking. Please review constraint or workload."
+  kubectl delete pod -n "${NS}" gatekeeper-test-pod || true
+else
+  echo "Pod was rejected (good) — Gatekeeper constraint likely effective in dry-run."
+fi
+
+if [ "${PROMOTE}" = true ]; then
+  echo "Promoting constraint ${CONSTRAINT} to enforce mode"
+  kubectl patch constraint "${CONSTRAINT}" --type='merge' -p '{"spec":{"enforcementAction":"deny"}}' || { echo "Patch failed"; exit 1; }
+  echo "Constraint promoted to deny"
+fi
+
+rm -f /tmp/gk_test_pod.yaml
+
*** End Patch
*** Begin Patch
*** Add File:flagger/canary/flagger_canary_template.yaml
+apiVersion: flagger.app/v1beta1
+kind: Canary
+metadata:
+  name: model-serving
+  namespace: aegis
+spec:
+  targetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: model-serving
+  service:
+    port: 80
+  analysis:
+    interval: 1m
+    threshold: 10
+    iterations: 10
+    metrics:
+      - name: request-success-rate
+        template: success-rate
+      - name: request-duration
+        template: request-duration
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/alert_rules_canary.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-canary-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: canary.rules
+      rules:
+        - alert: CanaryLatencyHigh
+          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 0.5
+          for: 2m
+          labels:
+            severity: page
+          annotations:
+            summary: "Canary latency high"
+
*** End Patch
*** Begin Patch
*** Add File:falco/sidekick/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: falco-sidekick
+  namespace: kube-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: falco-sidekick
+  template:
+    metadata:
+      labels:
+        app: falco-sidekick
+    spec:
+      containers:
+        - name: sidekick
+          image: falcosecurity/falco-sidekick:latest
+          env:
+            - name: DRIVER
+              value: "elasticsearch"
+            - name: ELASTICSEARCH_URL
+              value: "https://elasticsearch.logging.svc:9200"
+            - name: ELASTICSEARCH_USER
+              valueFrom:
+                secretKeyRef:
+                  name: elastic-credentials
+                  key: user
+            - name: ELASTICSEARCH_PASS
+              valueFrom:
+                secretKeyRef:
+                  name: elastic-credentials
+                  key: password
+          ports:
+            - containerPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File:terraform/aegis_infra/main.tf
+terraform {
+  required_version = ">= 1.2"
+  required_providers {
+    aws = {
+      source  = "hashicorp/aws"
+      version = "~> 5.0"
+    }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_kms_key" "aegis" {
+  description             = "Aegis KMS key for auto-unseal and cosign"
+  deletion_window_in_days = 30
+}
+
+resource "aws_s3_bucket" "evidence" {
+  bucket = var.evidence_bucket
+  acl    = "private"
+  versioning {
+    enabled = true
+  }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm     = "aws:kms"
+        kms_master_key_id = aws_kms_key.aegis.arn
+      }
+    }
+  }
+}
+
+resource "aws_s3_bucket" "models" {
+  bucket = var.model_bucket
+  acl    = "private"
+  versioning {
+    enabled = true
+  }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm     = "aws:kms"
+        kms_master_key_id = aws_kms_key.aegis.arn
+      }
+    }
+  }
+}
+
+output "kms_key_arn" {
+  value = aws_kms_key.aegis.arn
+}
+output "evidence_bucket" {
+  value = aws_s3_bucket.evidence.bucket
+}
+output "model_bucket" {
+  value = aws_s3_bucket.models.bucket
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/aegis_infra/variables.tf
+variable "aws_region" {
+  type    = string
+  default = "us-west-2"
+}
+
+variable "evidence_bucket" {
+  type = string
+}
+
+variable "model_bucket" {
+  type = string
+}
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ops/run_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NAMESPACE=${1:-aegis}
+BACKUP_NAME="drill-$(date +%s)"
+echo "Creating Velero backup ${BACKUP_NAME} for namespace ${NAMESPACE}"
+velero backup create "${BACKUP_NAME}" --include-namespaces "${NAMESPACE}"
+echo "Waiting for backup to complete..."
+velero backup wait "${BACKUP_NAME}" --for=complete --timeout 10m || { echo "Backup did not complete"; exit 2; }
+echo "Creating temporary restore namespace: ${NAMESPACE}-restore"
+kubectl create ns "${NAMESPACE}-restore" || true
+RESTORE_NAME="${BACKUP_NAME}-restore"
+velero restore create --from-backup "${BACKUP_NAME}" --namespace-mappings "${NAMESPACE}:${NAMESPACE}-restore" "${RESTORE_NAME}"
+echo "Waiting for restore to complete..."
+velero restore wait "${RESTORE_NAME}" --for=complete --timeout 15m || { echo "Restore did not complete"; exit 2; }
+echo "Restore completed. Run smoke tests against ${NAMESPACE}-restore and then delete it."
+echo "Cleanup: velero restore delete ${RESTORE_NAME} ; kubectl delete ns ${NAMESPACE}-restore ; velero backup delete ${BACKUP_NAME}"
+
*** End Patch
*** Begin Patch
*** Add File:docs/PRODUCTION_RUNBOOK_SUMMARY.md
+# Aegis — Production Runbook Summary (Factory, Lab, Vault, Control Tower)
+
+This document summarizes the apply order and key operator actions required to turn the scaffolds into a hardened production platform.
+
+1) Infra provisioning (Terraform)
+   - terraform init && terraform apply -var="evidence_bucket=<your-evidence>" -var="model_bucket=<your-model-bucket>"
+   - record KMS ARN, bucket names and export to CI and Helm values.
+
+2) Vault HA & Cert-Manager
+   - install cert-manager (CRDs) and apply deploy/vault/cert-manager-issuer-and-ca.yaml
+   - helm upgrade --install vault hashicorp/vault -n aegis -f deploy/vault/ha-values-raft.yaml (fill KMS ARN)
+   - run deploy/vault/init-vault-job.yaml if needed and run policy loader scripts.
+
+3) Rekor + Signing-proxy
+   - apply deploy/rekor/postgres-statefulset.yaml and deploy/rekor/rekor-deployment-postgres.yaml (set DB password)
+   - create TLS secrets for signing-proxy and apply ops/signing-proxy/hardened-deployment.yaml
+   - configure CI secrets and test mTLS signing flow using .github/workflows/ci_build_sign.yml or cosign workflow
+
+4) Gatekeeper & Flagger (Control Tower)
+   - apply gatekeeper staging constraint: gatekeeper/staging/enable_dryrun_constraint.yaml
+   - run gatekeeper/automation/gatekeeper_dryrun_check_and_promote.sh to collect dry-run results; tune constraints
+   - deploy Flagger canaries for model-serving and Prometheus rule for canary metrics
+
+5) Lab (Verification)
+   - submit Argo verification workflow argo/verification/full_pipeline_workflow.yaml for artifacts in staging
+   - tune safety_oracle rules to block promotion where acceptance criteria are not met
+
+6) Evidence & Backup
+   - run scripts/ops/run_restore_drill.sh to validate Velero backups and sign proof of restore into evidence bucket
+   - run scripts/evidence/attach_and_sign.py during promotions to bundle evidence
+
+7) Monitoring & Alerting
+   - deploy Falco + falco-sidekick and forward to Elastic; tune suppression list to reduce noise
+   - ensure Prometheus endpoints are accessible and Flagger can query metrics for canary evaluations
+
+Notes:
+- All secrets (DB passwords, TLS certs, KMS key ARNs) must be provisioned prior to applying manifests.
+- Gatekeeper constraints should remain in dry-run for at least one release cycle in staging.
+- Signing-proxy must be audited and logs stored in evidence bucket.
+
*** End Patch
*** End Patch
