*** Begin Patch
*** Add File:milvus/embedding_ingest.py
+#!/usr/bin/env python3
+"""
+Embedding ingest pipeline for Milvus.
+
+Reads a CSV/Parquet dataset containing a text column, computes embeddings using
+sentence-transformers, and ingests vectors into Milvus. Creates collection and index if needed.
+
+Usage:
+  export MILVUS_HOST=milvus.aegis.svc.cluster.local
+  export MILVUS_PORT=19530
+  python milvus/embedding_ingest.py --input s3://bucket/path/texts.csv --text-col text --collection-name docs --batch 512
+
+Requirements:
+  pip install sentence-transformers pymilvus boto3 pandas pyarrow
+"""
+import os
+import argparse
+import tempfile
+import boto3
+import pandas as pd
+from sentence_transformers import SentenceTransformer
+from pymilvus import (
+    connections,
+    FieldSchema, CollectionSchema, DataType, Collection, utility, Index,
+)
+
+def download_s3(s3uri):
+    parts = s3uri[5:].split("/", 1)
+    bucket, key = parts[0], parts[1]
+    s3 = boto3.client("s3")
+    tmp = tempfile.mktemp(suffix=os.path.basename(key))
+    s3.download_file(bucket, key, tmp)
+    return tmp
+
+def connect_milvus(host, port):
+    connections.connect(host=host, port=str(port))
+
+def create_collection(name, dim, metric="IP"):
+    if utility.has_collection(name):
+        print(f"Collection {name} exists")
+        return Collection(name)
+    id_field = FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True)
+    emb_field = FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim)
+    meta_field = FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535)
+    schema = CollectionSchema([id_field, emb_field, meta_field], description="Embeddings collection")
+    coll = Collection(name, schema=schema)
+    print(f"Created collection {name}")
+    return coll
+
+def create_index(collection, index_params=None):
+    if index_params is None:
+        index_params = {"index_type": "IVF_FLAT", "metric_type": "IP", "params": {"nlist": 1024}}
+    collection.create_index(field_name="embedding", index_params=index_params)
+    print("Index created:", index_params)
+
+def ingest(collection, texts, model, batch=256):
+    dim = model.get_sentence_embedding_dimension()
+    for i in range(0, len(texts), batch):
+        chunk = texts[i:i+batch]
+        emb = model.encode(chunk, show_progress_bar=False, convert_to_numpy=True, batch_size=64)
+        # store [embedding, text]; id auto-generated
+        mr = collection.insert([[], emb.tolist(), chunk])
+        print(f"Ingested {len(chunk)} vectors")
+    collection.flush()
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--text-col", default="text")
+    p.add_argument("--collection-name", default="aegis_docs")
+    p.add_argument("--model", default="all-MiniLM-L6-v2")
+    p.add_argument("--batch", type=int, default=512)
+    args = p.parse_args()
+
+    host = os.environ.get("MILVUS_HOST", "localhost")
+    port = os.environ.get("MILVUS_PORT", "19530")
+    connect_milvus(host, port)
+    print(f"Connected to Milvus at {host}:{port}")
+
+    # Load data
+    if args.input.startswith("s3://"):
+        local = download_s3(args.input)
+    else:
+        local = args.input
+    if local.endswith(".csv"):
+        df = pd.read_csv(local)
+    else:
+        df = pd.read_parquet(local)
+    if args.text_col not in df.columns:
+        raise RuntimeError(f"Text column {args.text_col} not found")
+    texts = df[args.text_col].astype(str).tolist()
+
+    # Embedding model
+    model = SentenceTransformer(args.model)
+    dim = model.get_sentence_embedding_dimension()
+    coll = create_collection(args.collection_name, dim)
+    # create index if not exists
+    if not coll.has_index():
+        create_index(coll)
+    ingest(coll, texts, model, batch=args.batch)
+    print("Ingest complete. Collection stats:", coll.num_entities)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:milvus/query_cluster_examples.py
+#!/usr/bin/env python3
+"""
+Example queries and clustering using vectors stored in Milvus.
+
+Operations:
+ - nearest neighbor search for a query text
+ - export all vectors (or sample) and run HDBSCAN to find clusters
+
+Usage:
+  python milvus/query_cluster_examples.py --collection docs --op search --query "example text" --topk 5
+  python milvus/query_cluster_examples.py --collection docs --op cluster --sample 10000
+
+Requirements:
+  pip install pymilvus sentence-transformers hdbscan numpy
+"""
+import os
+import argparse
+import numpy as np
+from sentence_transformers import SentenceTransformer
+from pymilvus import connections, Collection, utility
+
+def connect(host, port):
+    connections.connect(host=host, port=str(port))
+
+def search(collection_name, text, model, topk=5):
+    coll = Collection(collection_name)
+    emb = model.encode([text], convert_to_numpy=True)
+    results = coll.search(emb.tolist(), "embedding", params={"metric_type":"IP","params":{"nprobe":16}}, limit=topk)
+    for res in results[0]:
+        print(f"score={res.score}, id={res.id}, text={res.entity.get('text')[:200]}")
+
+def cluster(collection_name, model, sample=5000):
+    coll = Collection(collection_name)
+    n = coll.num_entities
+    print(f"Collection {collection_name} has approx {n} entities")
+    # sample ids
+    import random
+    ids = []
+    if n <= sample:
+        # retrieve all using query (may be heavy)
+        expr = ""
+    else:
+        # use range sampling via offsetting: not optimal; better to track ids at ingest time
+        ids = random.sample(range(int(n)), sample)
+    # fetch vector field (use query if available)
+    datas = coll.query(expr="", output_fields=["embedding","text"]) if n <= sample else coll.query(expr="", output_fields=["embedding","text"])
+    vectors = np.array([d["embedding"] for d in datas])
+    texts = [d["text"] for d in datas]
+    # clustering
+    import hdbscan
+    clusterer = hdbscan.HDBSCAN(min_cluster_size=20)
+    labels = clusterer.fit_predict(vectors)
+    print("Found clusters:", set(labels.tolist()))
+    # print top examples per cluster
+    for lbl in sorted(set(labels)):
+        if lbl == -1:
+            continue
+        idxs = [i for i,l in enumerate(labels) if l==lbl][:5]
+        print(f"Cluster {lbl} examples:")
+        for i in idxs:
+            print("-", texts[i][:200])
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--collection", required=True)
+    p.add_argument("--op", choices=["search","cluster"], required=True)
+    p.add_argument("--query", default="")
+    p.add_argument("--topk", type=int, default=5)
+    p.add_argument("--sample", type=int, default=5000)
+    args = p.parse_args()
+    host = os.environ.get("MILVUS_HOST","localhost")
+    port = os.environ.get("MILVUS_PORT","19530")
+    connect(host, port)
+    model = SentenceTransformer("all-MiniLM-L6-v2")
+    if args.op == "search":
+        search(args.collection, args.query, model, topk=args.topk)
+    else:
+        cluster(args.collection, model, sample=args.sample)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:milvus/README.md
+# Milvus embedding pipeline
+
+Files:
+- embedding_ingest.py — ingest texts into Milvus using sentence-transformers
+- query_cluster_examples.py — search and clustering examples (HDBSCAN)
+
+Quick start:
+1. Deploy Milvus (Helm recommended). Example chart: https://milvus.io/docs/install_milvus-operator.md
+2. Build/push an image with sentence-transformers and pymilvus or use your aegis-flink/engine image.
+3. Run ingest:
+   MILVUS_HOST=milvus.aegis.svc.cluster.local MILVUS_PORT=19530 python milvus/embedding_ingest.py --input s3://... --collection-name docs
+4. Run search / cluster:
+   MILVUS_HOST=... python milvus/query_cluster_examples.py --collection docs --op search --query "sample"
+
+Notes:
+- For production use, shard collections, configure indexes (IVF/PQ/ANNOY/HNSW), and enable backups of Milvus data.
+
*** End Patch
*** Begin Patch
*** Add File:gnn/train_gnn.py
+#!/usr/bin/env python3
+"""
+Simple GNN training script using PyTorch Geometric for node classification.
+
+Inputs:
+ - edges CSV (src, dst)
+ - node features CSV (node_id, feat_0,...feat_n, label)
+
+Saves model locally and logs to MLflow. Uploads model artifact to S3 and optionally signs with cosign (KMS).
+
+Requirements:
+  pip install torch torchvision torchaudio torch-geometric mlflow boto3 pandas scikit-learn
+"""
+import os
+import argparse
+import tempfile
+import tarfile
+import subprocess
+import boto3
+import pandas as pd
+import torch
+import mlflow
+from torch_geometric.data import Data
+from torch_geometric.nn import GCNConv
+from sklearn.model_selection import train_test_split
+
+class SimpleGCN(torch.nn.Module):
+    def __init__(self, in_channels, hidden=64, out_channels=2):
+        super().__init__()
+        self.conv1 = GCNConv(in_channels, hidden)
+        self.conv2 = GCNConv(hidden, out_channels)
+
+    def forward(self, x, edge_index):
+        x = self.conv1(x, edge_index).relu()
+        x = self.conv2(x, edge_index)
+        return x
+
+def download_s3(s3uri):
+    parts = s3uri[5:].split("/",1)
+    bucket, key = parts[0], parts[1]
+    s3 = boto3.client("s3")
+    tmp = tempfile.mktemp(suffix=os.path.basename(key))
+    s3.download_file(bucket, key, tmp)
+    return tmp
+
+def tar_dir(src, outpath):
+    with tarfile.open(outpath, "w:gz") as tar:
+        tar.add(src, arcname=os.path.basename(src))
+
+def cosign_sign(local_file, kms_arn):
+    try:
+        subprocess.check_call(["cosign","sign","--key",kms_arn, local_file])
+        return local_file + ".sig"
+    except Exception as e:
+        print("cosign sign failed", e)
+        return None
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--edges", required=True)
+    p.add_argument("--nodes", required=True)
+    p.add_argument("--epochs", type=int, default=5)
+    p.add_argument("--batch-size", type=int, default=32)
+    p.add_argument("--output-dir", default="/tmp/gnn_out")
+    p.add_argument("--evidence-bucket", default=os.environ.get("EVIDENCE_BUCKET"))
+    p.add_argument("--cosign-kms-arn", default=os.environ.get("COSIGN_KMS_ARN"))
+    p.add_argument("--mlflow-tracking-uri", default=os.environ.get("MLFLOW_TRACKING_URI"))
+    args = p.parse_args()
+
+    if args.mlflow_tracking_uri:
+        mlflow.set_tracking_uri(args.mlflow_tracking_uri)
+
+    # download inputs if s3
+    edges_path = download_s3(args.edges) if args.edges.startswith("s3://") else args.edges
+    nodes_path = download_s3(args.nodes) if args.nodes.startswith("s3://") else args.nodes
+    edges = pd.read_csv(edges_path)
+    nodes = pd.read_csv(nodes_path)
+
+    # prepare torch_geometric Data
+    node_idx_map = {nid: i for i, nid in enumerate(nodes["node_id"].tolist())}
+    edge_index = torch.tensor([[node_idx_map[s] for s in edges.src.tolist()],
+                               [node_idx_map[d] for d in edges.dst.tolist()]], dtype=torch.long)
+    feat_cols = [c for c in nodes.columns if c.startswith("feat_")]
+    x = torch.tensor(nodes[feat_cols].values, dtype=torch.float)
+    y = torch.tensor(nodes["label"].values, dtype=torch.long)
+    data = Data(x=x, edge_index=edge_index, y=y)
+
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    model = SimpleGCN(in_channels=x.shape[1], out_channels=int(y.max().item()+1)).to(device)
+    data = data.to(device)
+    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
+    loss_fn = torch.nn.CrossEntropyLoss()
+
+    with mlflow.start_run() as run:
+        for epoch in range(1, args.epochs+1):
+            model.train()
+            optimizer.zero_grad()
+            out = model(data.x, data.edge_index)
+            loss = loss_fn(out, data.y)
+            loss.backward()
+            optimizer.step()
+            mlflow.log_metric("loss", loss.item(), step=epoch)
+            print(f"Epoch {epoch} loss {loss.item():.4f}")
+
+        # Save model
+        os.makedirs(args.output_dir, exist_ok=True)
+        model_path = os.path.join(args.output_dir, "gnn_model.pt")
+        torch.save(model.state_dict(), model_path)
+        mlflow.log_artifact(model_path, artifact_path="gnn")
+
+        # Archive and upload
+        archive = tempfile.mktemp(suffix=".tgz")
+        tar_dir(args.output_dir, archive)
+        if not args.evidence_bucket:
+            raise RuntimeError("EVIDENCE_BUCKET required to upload artifacts")
+        parts = args.evidence_bucket[5:].split("/",1) if args.evidence_bucket.startswith("s3://") else args.evidence_bucket.split("/",1)
+        s3 = boto3.client("s3")
+        bucket = parts[0] if args.evidence_bucket.startswith("s3://") else parts[0]
+        prefix = parts[1] if len(parts)>1 else "gnn"
+        key = f"{prefix.rstrip('/')}/gnn_{run.info.run_id}.tgz"
+        s3.upload_file(archive, bucket, key)
+        s3uri = f"s3://{bucket}/{key}"
+        mlflow.set_tag("checkpoint_s3", s3uri)
+
+        # cosign sign (optional)
+        if args.cosign_kms_arn:
+            sig = cosign_sign(archive, args.cosign_kms_arn)
+            if sig:
+                s3.upload_file(sig, bucket, f"{prefix.rstrip('/')}/{os.path.basename(sig)}")
+
+    print("Training complete. Model archived to", s3uri)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/gnn_training_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: gnn-train-
+  namespace: aegis
+spec:
+  entrypoint: train-gnn
+  arguments:
+    parameters:
+      - name: edges
+        value: "s3://REPLACE_EVIDENCE_BUCKET/graph/edges.csv"
+      - name: nodes
+        value: "s3://REPLACE_EVIDENCE_BUCKET/graph/nodes_features.csv"
+      - name: image
+        value: "{{IMAGE_REGISTRY}}/aegis-gnn:latest"
+  templates:
+    - name: train-gnn
+      inputs:
+        parameters:
+          - name: edges
+          - name: nodes
+          - name: image
+      container:
+        image: "{{inputs.parameters.image}}"
+        command: ["/bin/bash","-lc"]
+        args:
+          - |
+            set -euo pipefail
+            python /opt/gnn/train_gnn.py \
+              --edges "{{inputs.parameters.edges}}" \
+              --nodes "{{inputs.parameters.nodes}}" \
+              --epochs 5 \
+              --output-dir /tmp/gnn_out
+        resources:
+          limits:
+            nvidia.com/gpu: "1"
+            cpu: "4000m"
+            memory: "16Gi"
+          requests:
+            nvidia.com/gpu: "1"
+            cpu: "2000m"
+            memory: "8Gi"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gnn/gnn_inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-gnn-service"
+  namespace: "aegis"
+spec:
+  predictor:
+    custom:
+      container:
+        image: "{{IMAGE_REGISTRY}}/aegis-gnn-serve:latest"
+        name: gnn-predictor
+        env:
+          - name: MODEL_S3_URI
+            value: "s3://REPLACE_EVIDENCE_BUCKET/gnn_models/latest/"
+          - name: MLFLOW_TRACKING_URI
+            value: "{{MLFLOW_TRACKING_URI}}"
+        resources:
+          limits:
+            cpu: "2000m"
+            memory: "4Gi"
+          requests:
+            cpu: "1000m"
+            memory: "2Gi"
+    logger:
+      mode: "all"
+      url: "http://mcpx-logger.aegis.svc.cluster.local:8080/log"
+  transformer:
+    containers:
+      - name: gnn-transformer
+        image: "{{IMAGE_REGISTRY}}/aegis-transformer:latest"
+        env:
+          - name: MCPX_AUTH_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: mcpx-auth
+                key: token
+
+annotations:
+  serving.kserve.io/ingress-class: "istio"
+
*** End Patch
*** Begin Patch
*** Add File:docker/gnn/Dockerfile
+FROM python:3.10-slim
+RUN apt-get update && apt-get install -y build-essential git curl && rm -rf /var/lib/apt/lists/*
+RUN pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
+RUN pip install --no-cache-dir torch-geometric==2.2.0 torch-scatter torch-sparse pymilvus mlflow boto3 pandas scikit-learn sentence-transformers
+WORKDIR /opt/gnn
+COPY gnn/ /opt/gnn/
+CMD ["bash"]
+
*** End Patch
*** Begin Patch
*** Add File:demo/fp_growth_demo_notebook.ipynb
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# FP-Growth demo: viz and Superset import\n",
+    "This notebook loads FP-Growth/association rule outputs (Parquet) from S3, visualizes top co-occurrence pairs as a network, and demonstrates uploading a Superset chart via the REST API."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import os\n",
+    "import pandas as pd\n",
+    "import networkx as nx\n",
+    "import matplotlib.pyplot as plt\n",
+    "from urllib.parse import urljoin\n",
+    "\n",
+    "EVIDENCE_BUCKET = os.environ.get('EVIDENCE_BUCKET','REPLACE_EVIDENCE_BUCKET')\n",
+    "FP_RULES_PATH = f's3://{EVIDENCE_BUCKET}/mining/fp_growth/latest/assocRules.parquet'\n",
+    "\n",
+    "print('This notebook expects AWS creds in environment or IRSA to access S3.')\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Load assoc rules (using pandas reading from s3 via s3fs if configured)\n",
+    "try:\n",
+    "    df = pd.read_parquet(FP_RULES_PATH, engine='pyarrow')\n",
+    "except Exception as e:\n",
+    "    print('Unable to read parquet directly; try downloading locally or configure s3fs:', e)\n",
+    "    df = pd.DataFrame({'items': [['a','b'],['b','c']], 'confidence':[0.8,0.6], 'freq':[100,80]})\n",
+    "\n",
+    "df['item_a'] = df['items'].apply(lambda x: x[0] if isinstance(x, (list,tuple)) and len(x)>0 else None)\n",
+    "df['item_b'] = df['items'].apply(lambda x: x[1] if isinstance(x, (list,tuple)) and len(x)>1 else None)\n",
+    "top = df.sort_values('freq', ascending=False).head(50)\n",
+    "print('Loaded top pairs', len(top))\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Build networkx graph\n",
+    "G = nx.Graph()\n",
+    "for _, r in top.iterrows():\n",
+    "    a = r['item_a']\n",
+    "    b = r['item_b']\n",
+    "    w = float(r.get('freq',1))\n",
+    "    if a and b:\n",
+    "        G.add_edge(a,b,weight=w)\n",
+    "\n",
+    "plt.figure(figsize=(10,10))\n",
+    "pos = nx.spring_layout(G, k=0.3, seed=42)\n",
+    "weights = [G[u][v]['weight'] for u,v in G.edges()]\n",
+    "nx.draw_networkx_nodes(G, pos, node_size=100)\n",
+    "nx.draw_networkx_edges(G, pos, width=[max(0.5, w/50) for w in weights])\n",
+    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
+    "plt.title('Top co-occurring items network')\n",
+    "plt.axis('off')\n",
+    "plt.show()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Example: prepare a Superset chart payload and call Superset REST API to create a dataset/chart\n",
+    "import requests, json\n",
+    "SUPERSET_URL = os.environ.get('SUPERSET_URL','https://superset.aegis.example.com')\n",
+    "API_TOKEN = os.environ.get('SUPERSET_API_TOKEN','')\n",
+    "if API_TOKEN:\n",
+    "    headers = {'Authorization': f'Bearer {API_TOKEN}', 'Content-Type': 'application/json'}\n",
+    "    payload = {\n",
+    "      'sql': 'SELECT item_a, item_b, freq FROM mining.fp_rules ORDER BY freq DESC LIMIT 100',\n",
+    "      'schema': 'public',\n",
+    "      'table_name': 'fp_rules'\n",
+    "    }\n",
+    "    try:\n",
+    "        resp = requests.post(SUPERSET_URL + '/api/v1/dataset/', headers=headers, data=json.dumps(payload), verify=False)\n",
+    "        print('Superset dataset create status', resp.status_code, resp.text[:300])\n",
+    "    except Exception as e:\n",
+    "        print('Superset API call failed', e)\n",
+    "else:\n",
+    "    print('No SUPSERSET_API_TOKEN set; skipping superset import step')\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "name": "python",
+   "version": "3.10"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
+
*** End Patch
*** Begin Patch
*** Add File:demo/superset_import.py
+#!/usr/bin/env python3
+"""
+Simple Superset REST API importer for a dashboard/chart JSON.
+Usage:
+  python demo/superset_import.py --url https://superset.aegis.example.com --token $TOKEN --file superset/dashboards/top_cooccurrences_chart.json
+"""
+import argparse, json, requests
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--url", required=True)
+    p.add_argument("--token", required=True)
+    p.add_argument("--file", required=True)
+    args = p.parse_args()
+    headers = {"Authorization": f"Bearer {args.token}", "Content-Type": "application/json"}
+    with open(args.file) as f:
+        payload = json.load(f)
+    # create chart/dashboard via Superset API - this is a simplified example and may need adjustment per Superset version
+    resp = requests.post(f"{args.url}/api/v1/dashboard/", headers=headers, json=payload, verify=False)
+    print("Status:", resp.status_code, resp.text[:400])
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:README_PATTERN_RECOGNITION.md
+# Pattern Recognition Artifacts
+
+This patch provides three integrated capabilities:
+
+1) Embedding → Milvus pipeline
+   - milvus/embedding_ingest.py: ingest texts into Milvus using sentence-transformers
+   - milvus/query_cluster_examples.py: examples to search & run HDBSCAN clustering
+   - milvus/README.md: quick start and notes
+
+2) GNN training & serving
+   - gnn/train_gnn.py: PyG training script for node classification, MLflow logging, S3 upload + optional cosign signing
+   - argo/gnn_training_workflow.yaml: Argo Workflow template to run GNN training on GPU nodepool
+   - k8s/gnn/gnn_inferenceservice.yaml: KServe InferenceService template to serve the trained GNN model
+   - docker/gnn/Dockerfile: image for GNN training + inference (build & push to GHCR/ECR via CI)
+
+3) FP-Growth demo notebook & Superset import
+   - demo/fp_growth_demo_notebook.ipynb: notebook to read association rules, visualize as a network and (optionally) register a Superset dataset
+   - demo/superset_import.py: small helper to import dashboard/chart JSON via Superset REST API
+
+How to run (operator checklist)
+- Milvus:
+  - Deploy Milvus (Helm recommended). Configure MILVUS_HOST/MILVUS_PORT.
+  - Build or use your job image with sentence-transformers/pymilvus and run embedding_ingest.py.
+
+- GNN:
+  - Build and push docker/gnn image (CI workflows recommended).
+  - Submit Argo workflow: argo submit --parameter IMAGE_REGISTRY=ghcr.io/yourorg argo/gnn_training_workflow.yaml
+  - Deploy KServe InferenceService (edit image names and MODEL_S3_URI).
+
+- Demo notebook:
+  - Run the notebook in JupyterLab with cluster credentials (IRSA or env AWS creds).
+  - Set SUPSERSET_API_TOKEN env var to import dashboards if desired.
+
+Security & production notes
+- Ensure IRSA roles for pods that access S3/KMS and for any job that performs cosign signing.
+- Configure Milvus persistence and backups before ingesting production vectors.
+- Use vector similarity index parameters (IVF/HNSW/PQ) appropriate for scale and latency.
+- For GNN production serving, wrap model in a fast predictor that loads model weights and exposes a batch endpoint; consider TorchScript/ONNX for speed.
+
+If you want, I can:
+- generate CI workflows to build/push the GNN/Milvus images and wire them into kustomize overlays,
+- add a KServe + readiness/liveness probe example for the GNN service,
+- produce a sample notebook that goes end-to-end: ingest small sample, run FP-growth, generate network visualization, push Superset chart.
+
*** End Patch
*** End Patch
