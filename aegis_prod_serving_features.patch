*** Begin Patch
*** Add File: k8s/serving/kserve_production.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-model
+  namespace: aegis
+  annotations:
+    # KServe + Knative tuning hints
+    autoscaling.knative.dev/target: "200"                # concurrency target per pod
+    serving.kserve.io/enable-batcher: "true"             # enable server-side batching if supported
+spec:
+  predictor:
+    serviceAccountName: aegis-model-sa
+    timeout: 300s
+    container:
+      image: ghcr.io/yourorg/aegis-model-server:latest
+      name: kserve-container
+      env:
+        - name: MODEL_STORE_URI
+          value: "s3://MODEL_BUCKET/model-archives/aegis-model/"
+      resources:
+        limits:
+          cpu: "4"
+          memory: "8Gi"
+          # request GPU when available; cluster must have GPU nodes and drivers
+          nvidia.com/gpu: "1"
+        requests:
+          cpu: "1"
+          memory: "2Gi"
+      ports:
+        - containerPort: 8080
+    # allow concurrency and batching configuration via container args/env or KServe config
+
+---
+# ServiceAccount + RBAC for model predictor to access S3 and secrets (minimal)
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-model-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: aegis-model-role
+  namespace: aegis
+rules:
+  - apiGroups: [""]
+    resources: ["secrets"]
+    verbs: ["get"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: aegis-model-rolebinding
+  namespace: aegis
+subjects:
+  - kind: ServiceAccount
+    name: aegis-model-sa
+    namespace: aegis
+roleRef:
+  kind: Role
+  name: aegis-model-role
+  apiGroup: rbac.authorization.k8s.io
+
+# Notes:
+# - Replace MODEL_BUCKET and container image with real values.
+# - Tune resources, concurrency & batching according to model characteristics and latency SLOs.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ingress/istio_gateway.yaml
+apiVersion: networking.istio.io/v1beta1
+kind: Gateway
+metadata:
+  name: aegis-ingress-gateway
+  namespace: aegis
+spec:
+  selector:
+    istio: ingressgateway
+  servers:
+    - port:
+        number: 80
+        name: http
+        protocol: HTTP
+      hosts:
+        - "models.aegis.example"
+      # enable HTTP/2 / gRPC for low-latency serving when using V2 protocol
+
+---
+apiVersion: networking.istio.io/v1beta1
+kind: VirtualService
+metadata:
+  name: aegis-model-vs
+  namespace: aegis
+spec:
+  hosts:
+    - "models.aegis.example"
+  gateways:
+    - aegis-ingress-gateway
+  http:
+    - name: "model-route"
+      match:
+        - uri:
+            prefix: "/v1/models"
+      route:
+        - destination:
+            host: aegis-model-predictor-default.aegis.svc.cluster.local
+            port:
+              number: 8080
+      timeout: 10s
+      retries:
+        attempts: 2
+        perTryTimeout: 2s
+
+# Notes:
+# - Istio virtualservice routes should point at the KServe predictor service name pattern for your cluster.
+# - This config sets short timeouts and retries to favour low-latency inference.
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/model_alerts_rules.yml
+groups:
+- name: aegis-modeling.rules
+  rules:
+  - alert: AegisModelHighLatency
+    expr: histogram_quantile(0.95, sum(rate(aegis_model_request_latency_seconds_bucket[5m])) by (le, model)) > 1.0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "High 95th percentile latency for model {{ $labels.model }}"
+      description: "95th percentile latency > 1s for model {{ $labels.model }} over 5m"
+
+  - alert: AegisModelAUCDrop
+    expr: (avg_over_time(aegis_model_auc[1h]) < (avg_over_time(aegis_model_auc[6h]) * 0.9))
+    for: 15m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Model AUC drop detected"
+      description: "Current 1h avg AUC dropped >10% from 6h baseline for model"
+
+  - alert: AegisPredictionSkew
+    expr: increase(aegis_model_prediction_positive_rate[30m]) > 0.15
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Prediction positive rate spike"
+      description: "Model positive prediction rate increased by more than 15% in 30m"
+
+  - alert: AegisDRFailing
+    expr: increase(restore_test_failure_total[1h]) > 0
+    for: 1m
+    labels:
+      severity: page
+    annotations:
+      summary: "DR Restore test failures detected"
+      description: "A DR restore rehearsal reported failure in last hour"
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/grafana_model_dashboard.json
+{
+  "title": "Aegis Model Ops",
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Request rate",
+      "targets": [
+        { "expr": "sum(rate(aegis_model_request_count_total[5m])) by (model)", "legendFormat": "{{model}}" }
+      ],
+      "id": 1
+    },
+    {
+      "type": "graph",
+      "title": "Latency (P95)",
+      "targets": [
+        { "expr": "histogram_quantile(0.95, sum(rate(aegis_model_request_latency_seconds_bucket[5m])) by (le, model))", "legendFormat": "{{model}}" }
+      ],
+      "id": 2
+    },
+    {
+      "type": "graph",
+      "title": "Positive prediction rate",
+      "targets": [
+        { "expr": "aegis_model_prediction_positive_rate", "legendFormat": "{{model}}" }
+      ],
+      "id": 3
+    },
+    {
+      "type": "graph",
+      "title": "Model AUC (recent)",
+      "targets": [
+        { "expr": "aegis_model_auc", "legendFormat": "{{model}}" }
+      ],
+      "id": 4
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/redis_online_store.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: feast-redis
+  template:
+    metadata:
+      labels:
+        app: feast-redis
+    spec:
+      containers:
+        - name: redis
+          image: redis:6.2-alpine
+          ports:
+            - containerPort: 6379
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: feast-redis
+  namespace: feast
+spec:
+  ports:
+    - port: 6379
+  selector:
+    app: feast-redis
+
+# Notes:
+# - For production, use a Redis Cluster or managed Redis with persistence and high availability.
+# - Configure Feast to point to this service as online store.
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/feast_materialize.sh
+#!/usr/bin/env bash
+#
+# Materialize features from offline store to online store for Feast.
+# Usage:
+#   METADATA_REPO=./feature_repo START_DATE=2023-01-01 END_DATE=2023-01-02 ./feature_store/feast_materialize.sh
+set -euo pipefail
+
+FEAST_REPO="${FEAST_REPO:-./feature_repo}"
+START="${START_DATE:-}"
+END="${END_DATE:-}"
+
+if [ -z "$START" ] || [ -z "$END" ]; then
+  echo "Usage: START_DATE and END_DATE env variables required (YYYY-MM-DD)" >&2
+  exit 2
+fi
+
+echo "Materializing features from $START to $END using repo $FEAST_REPO"
+pushd "$FEAST_REPO" >/dev/null
+feast materialize "$START" "$END"
+popd >/dev/null
+echo "Materialize complete"
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/retrain_with_validation.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-retrain-
+  namespace: aegis
+spec:
+  entrypoint: retrain-and-validate
+  templates:
+  - name: retrain-and-validate
+    steps:
+      - - name: drift-check
+          template: drift-check
+      - - name: retrain
+          template: retrain
+          when: "{{steps.drift-check.outputs.parameters.drift}} == true"
+      - - name: validate
+          template: validate
+          when: "{{steps.retrain.outputs.parameters.retrained}} == true"
+      - - name: package-and-attest
+          template: package
+          when: "{{steps.validate.outputs.parameters.passed}} == true"
+      - - name: request-approval
+          template: request-approval
+          when: "{{steps.package-and-attest.outputs.parameters.attested}} == true"
+
+  - name: drift-check
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          aws s3 cp s3://$METRICS_BUCKET/reference.csv /tmp/reference.csv
+          aws s3 cp s3://$METRICS_BUCKET/production_recent.csv /tmp/production.csv
+          python3 scripts/drift_check.py /tmp/reference.csv /tmp/production.csv || exit 2
+          if [ $? -eq 2 ]; then echo "true" > /tmp/out; else echo "false" > /tmp/out; fi
+    outputs:
+      parameters:
+        - name: drift
+          valueFrom:
+            path: /tmp/out
+
+  - name: retrain
+    container:
+      image: ghcr.io/yourorg/aegis-train:latest
+      command: [sh, -c]
+      args:
+        - |
+          ./train_and_package.sh
+          echo "true" > /tmp/retrained
+    outputs:
+      parameters:
+        - name: retrained
+          valueFrom:
+            path: /tmp/retrained
+
+  - name: validate
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          # run validation that computes metrics and ensures they meet thresholds
+          python3 scripts/validate_model_metrics.py --pred predictions.csv --labels labels.csv --min_auc 0.85 --out validation.json
+          python3 - <<'PY'
+import json,sys
+with open('validation.json') as fh:
+  r=json.load(fh)
+print(r)
+sys.exit(0 if r.get('auc',0)>=0.85 else 1)
+PY
+    outputs:
+      parameters:
+        - name: passed
+          valueFrom:
+            path: /tmp/validation_passed
+
+  - name: package
+    container:
+      image: ghcr.io/yourorg/aegis-tools:latest
+      command: [sh, -c]
+      args:
+        - |
+          ./scripts/package_and_attest_rekor_canonical.sh model_dir ./artifact.tar.gz
+          echo "true" > /tmp/attested
+    outputs:
+      parameters:
+        - name: attested
+          valueFrom:
+            path: /tmp/attested
+
+  - name: request-approval
+    container:
+      image: curlimages/curl:7.85.0
+      command: [sh, -c]
+      args:
+        - |
+          # POST to governance API to request a human approval
+          curl -sS -X POST -H "Content-Type: application/json" -H "X-API-KEY: ${GOV_API_KEY}" -d '{"artifact_key":"model-archives/artifact.tar.gz","approver":"auto","notes":"retrained"}' ${GOV_URL}/approve || true
+
+# Notes:
+# - METRICS_BUCKET, GOV_API_KEY, GOV_URL should be provided as workflow env or through cluster secrets.
+#
*** End Patch
*** Begin Patch
*** Add File: scripts/validate_model_metrics.py
+#!/usr/bin/env python3
+"""
+Validate model metrics from prediction output.
+Writes validation.json with metrics like auc, accuracy, and a simple pass/fail.
+"""
+import argparse, json
+from sklearn.metrics import roc_auc_score, accuracy_score
+import pandas as pd
+
+ap = argparse.ArgumentParser()
+ap.add_argument("--pred", required=True)
+ap.add_argument("--labels", required=True)
+ap.add_argument("--min_auc", type=float, default=0.80)
+ap.add_argument("--out", default="validation.json")
+args = ap.parse_args()
+
+pred = pd.read_csv(args.pred)
+labels = pd.read_csv(args.labels)
+y_true = labels.iloc[:,0]
+if "score" in pred.columns:
+    y_score = pred["score"]
+else:
+    y_score = pred.iloc[:,0]
+
+auc = float(roc_auc_score(y_true, y_score))
+pred_label = (y_score > 0.5).astype(int)
+acc = float(accuracy_score(y_true, pred_label))
+result = {"auc": auc, "accuracy": acc}
+result["passed"] = auc >= args.min_auc
+with open(args.out, "w") as fh:
+    json.dump(result, fh)
+print("Validation result:", result)
+exit(0 if result["passed"] else 1)
+
*** End Patch
*** Begin Patch
*** Add File: hpo/katib_result_handler.py
+#!/usr/bin/env python3
+"""
+Katib result handler: find best trial, download best model, package & attest, and optionally trigger governance approval.
+Requires katib experiments accessible via k8s API and kubectl/gh/curl.
+"""
+import subprocess, json, tempfile, os, sys
+
+EXP_NAME = os.environ.get("KATIB_EXPERIMENT", "aegis-hpo-example")
+GOV_URL = os.environ.get("GOV_URL", "")
+GOV_API_KEY = os.environ.get("GOV_API_KEY", "")
+
+def get_best_trial(exp):
+    out = subprocess.check_output(["kubectl", "get", "experiment", exp, "-n", "aegis", "-o", "json"])
+    data = json.loads(out)
+    # Simplified: inspect status.bestTrial or completedTrials
+    bt = data.get("status",{}).get("currentOptimalTrial",{}) or data.get("status",{}).get("bestTrial",{})
+    if bt:
+        return bt.get("name") or bt.get("trialName")
+    return None
+
+def main():
+    trial = get_best_trial(EXP_NAME)
+    if not trial:
+        print("No best trial found for", EXP_NAME); sys.exit(2)
+    print("Best trial:", trial)
+    # Placeholder: copy model from trial output location to staging and package
+    # This must be customized per trial output conventions
+    # After packaging & attestation, call governance API to request approval
+    print("Packaging & attesting best trial (manual steps required)")
+    if GOV_URL and GOV_API_KEY:
+        payload = json.dumps({"artifact_key":"model-archives/best-trial-artifact.tar.gz","approver":"hpo-bot","notes":"best katib trial"})
+        subprocess.check_call(["curl","-sS","-X","POST","-H",f"Content-Type: application/json","-H",f"X-API-KEY: {GOV_API_KEY}", "-d", payload, f"{GOV_URL}/approve"])
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: explainability/shap_dashboard.py
+#!/usr/bin/env python3
+"""
+Simple Flask app to serve SHAP explanations and fairness summaries for a model.
+Endpoints:
+ - /explanations?run_id=...
+ - /fairness?group_field=age&threshold=...
+"""
+from flask import Flask, request, jsonify
+import os, json
+
+app = Flask(__name__)
+DATA_DIR = os.environ.get("SHAP_DIR","/data/shap")
+
+@app.route("/explanations")
+def explanations():
+    run_id = request.args.get("run_id")
+    if not run_id:
+        return jsonify({"error":"run_id required"}), 400
+    path = os.path.join(DATA_DIR, f"{run_id}_shap.json")
+    if not os.path.exists(path):
+        return jsonify({"error":"not found"}), 404
+    return jsonify(json.load(open(path)))
+
+@app.route("/fairness")
+def fairness():
+    group = request.args.get("group_field")
+    if not group:
+        return jsonify({"error":"group_field required"}), 400
+    # Simple synthetic fairness check reading precomputed stats
+    path = os.path.join(DATA_DIR, "fairness_summary.json")
+    if not os.path.exists(path):
+        return jsonify({"error":"no fairness data"}), 404
+    data = json.load(open(path))
+    return jsonify(data.get(group, {}))
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8082")))
+
*** End Patch
*** Begin Patch
*** Add File: labeling/labeling_queue.py
+#!/usr/bin/env python3
+"""
+Simple Redis-backed labeling queue worker.
+- Producers push unlabeled examples to Redis list (or SQS in production).
+- Worker pops example, serves to labeler UI or writes to S3 as labeled when annotated.
+"""
+import os, json, time
+import redis
+import boto3
+
+REDIS_URL = os.environ.get("REDIS_URL","redis://localhost:6379/0")
+BUCKET = os.environ.get("LABEL_BUCKET","")
+QUEUE = os.environ.get("LABEL_QUEUE","aegis-label-queue")
+
+r = redis.from_url(REDIS_URL)
+s3 = boto3.client("s3") if BUCKET else None
+
+def process_item(item):
+    # Example: item is JSON with id and payload
+    data = json.loads(item)
+    print("Process item:", data.get("id"))
+    # For demo, mark as 'to-be-labeled' by writing to S3 a placeholder
+    key = f"labels/{data.get('id')}.json"
+    if s3:
+        s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps({"id":data.get("id"), "status":"unlabeled", "payload":data}))
+    else:
+        with open(key.replace("/","_"), "w") as fh:
+            json.dump({"id":data.get("id"), "status":"unlabeled", "payload":data}, fh)
+
+def run_worker():
+    while True:
+        item = r.lpop(QUEUE)
+        if not item:
+            time.sleep(1)
+            continue
+        try:
+            process_item(item)
+        except Exception as e:
+            print("Failed to process item:", e)
+            # push back or log to DLQ in production
+
+if __name__ == "__main__":
+    run_worker()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/build_images.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+REGISTRY="${1:-ghcr.io/yourorg}"
+
+echo "Building and pushing model server, metrics exporter, shap dashboard images"
+docker build -t "${REGISTRY}/aegis-model-server:latest" -f Dockerfile.modelserver .
+docker push "${REGISTRY}/aegis-model-server:latest"
+
+docker build -t "${REGISTRY}/aegis-metrics-exporter:latest" -f monitoring/metrics.Dockerfile .
+docker push "${REGISTRY}/aegis-metrics-exporter:latest"
+
+docker build -t "${REGISTRY}/aegis-shap-dashboard:latest" -f explainability/Dockerfile .
+docker push "${REGISTRY}/aegis-shap-dashboard:latest"
+
+echo "Images built and pushed to ${REGISTRY}"
+
*** End Patch
*** Begin Patch
*** Add File: docs/PROD_SERVING_RUNBOOK.md
+## Aegis Production Serving & ML Ops Runbook (summary)
+
+Purpose
+- Deploy a production-grade serving mesh, monitoring, retrain pipeline, HPO handler, explainability and labeling infra.
+
+Quickstart (high-level)
+1. Provision online feature store (Feast) and online store (Redis):
+   - kubectl apply -f feature_store/redis_online_store.yaml (namespace feast)
+   - Configure Feast repo to point to Redis
+   - Run feature_store/feast_materialize.sh to populate online features
+
+2. Deploy serving infra:
+   - kubectl apply -f k8s/serving/kserve_production.yaml
+   - Configure Istio ingress/virtualservice: kubectl apply -f k8s/ingress/istio_gateway.yaml
+   - Ensure GPU drivers/nvidia device plugin and DCGM exporter are installed on GPU nodes
+
+3. Deploy monitoring:
+   - Add Prometheus rule file monitoring/model_alerts_rules.yml to PrometheusRule CRD or Prometheus config
+   - Import Grafana dashboard monitoring/grafana_model_dashboard.json
+   - Deploy monitoring/collector (use monitoring/model_metrics_exporter.py packaged as container)
+
+4. Retrain & validation:
+   - Deploy Argo workflow argo/workflows/retrain_with_validation.yaml
+   - Configure METRICS_BUCKET, GOV_URL, GOV_API_KEY as workflow env
+   - Ensure scripts/validate_model_metrics.py is available in retrain image
+
+5. HPO:
+   - Deploy Katib and run hpo/katib/job_template.yaml
+   - Run hpo/katib_result_handler.py after experiments finish to package best model and request approval
+
+6. Explainability & labeling
î€€
