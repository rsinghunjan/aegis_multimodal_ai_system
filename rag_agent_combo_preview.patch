*** Begin Patch
*** Add File: services/rag/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN python -m pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt
+COPY app.py .
+COPY prompts/ prompts/
+ENV MILVUS_HOST=milvus
+ENV MILVUS_PORT=19530
+ENV PROMPT_ID=default
+EXPOSE 8000
+CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
+
*** End Patch
*** Begin Patch
*** Add File: services/rag/requirements.txt
+fastapi==0.95.2
+uvicorn[standard]==0.21.1
+pymilvus==2.3.0
+sentence-transformers==2.2.2
+openai==0.39.0
+requests==2.31.0
+
*** End Patch
*** Begin Patch
*** Add File: services/rag/app.py
+import os
+import json
+from typing import List
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
+from sentence_transformers import SentenceTransformer
+import openai
+import requests
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "localhost")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", 19530))
+COLLECTION_NAME = os.environ.get("MILVUS_COLLECTION", "rag_demo")
+PROMPT_ID = os.environ.get("PROMPT_ID", "default")
+AUDIT_COLLECTOR = os.environ.get("AUDIT_COLLECTOR", "http://audit-collector.ops.svc.cluster.local:8088/event")
+OPENAI_KEY = os.environ.get("OPENAI_API_KEY")
+
+app = FastAPI(title="Aegis RAG Service")
+
+# load prompt registry (simple file based)
+with open("prompts/prompts.yaml", "r") as f:
+    import yaml
+    PROMPTS = yaml.safe_load(f)
+
+# embedding model (small)
+EMB_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
+DIM = EMB_MODEL.get_sentence_embedding_dimension()
+
+def connect_milvus():
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    if not utility.has_collection(COLLECTION_NAME):
+        fields = [
+            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
+            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=4096),
+            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM)
+        ]
+        schema = CollectionSchema(fields, description="RAG demo collection")
+        col = Collection(COLLECTION_NAME, schema)
+        col.create_index("vector", {"index_type":"IVF_FLAT", "metric_type":"L2", "params":{"nlist":128}})
+        col.load()
+    else:
+        col = Collection(COLLECTION_NAME)
+        if not col.is_loaded:
+            col.load()
+    return col
+
+class Query(BaseModel):
+    q: str
+    k: int = 4
+
+def audit_event(actor: str, action: str, payload: dict):
+    try:
+        requests.post(AUDIT_COLLECTOR, json={"actor": actor, "action": action, "payload": payload}, timeout=5)
+    except Exception:
+        # non-fatal; we log to stdout
+        print("Audit post failed")
+
+@app.on_event("startup")
+def startup():
+    connect_milvus()
+    if OPENAI_KEY:
+        openai.api_key = OPENAI_KEY
+
+@app.post("/query")
+def query(q: Query):
+    col = connect_milvus()
+    query_text = q.q
+    k = q.k
+    # embed
+    emb = EMB_MODEL.encode([query_text])[0].tolist()
+    # search
+    expr = [{"vector": emb}]
+    results = col.search(expr, "vector", param={"metric_type":"L2", "params":{"nprobe":10}}, limit=k)
+    docs = []
+    for res in results[0]:
+        # each res.entity.get("text") not available directly; use result ids
+        eid = res.id
+        out = col.query(expr=None, output_fields=["id","text"], expr=f"id == {eid}")
+        if out:
+            docs.append(out[0])
+    # assemble context
+    context = "\n\n".join([f"Doc {d['id']}: {d['text']}" for d in docs])
+    prompt_template = PROMPTS.get(PROMPT_ID, PROMPTS.get("default", {})).get("template", "{{context}}\n\nQ: {{query}}\nA:")
+    prompt = prompt_template.replace("{{context}}", context).replace("{{query}}", query_text)
+
+    # call LLM if available
+    if OPENAI_KEY:
+        resp = openai.Completion.create(model="text-davinci-003", prompt=prompt, max_tokens=256, temperature=0.0)
+        answer = resp.choices[0].text.strip()
+    else:
+        # local fallback: echo retrieved context + query (deterministic)
+        answer = f"Context:\n{context}\n\nAnswer (simulated): Based on retrieved documents, see above."
+
+    audit_event(actor="rag-service", action="query", payload={"query": query_text, "k": k, "result_count": len(docs)})
+    return {"answer": answer, "docs": docs}
+
*** End Patch
*** Begin Patch
*** Add File: services/embeddings/embedding_runner.py
+"""
+Simple embedding runner: reads data/manifest.jsonl (id, text, modality, uri) and pushes vectors to Milvus.
+For demo purposes we only embed text entries. Image embedding hook is present but not implemented in this small demo.
+"""
+import os
+import json
+from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
+from sentence_transformers import SentenceTransformer
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "localhost")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", 19530))
+COLLECTION_NAME = os.environ.get("MILVUS_COLLECTION", "rag_demo")
+MANIFEST = os.environ.get("MANIFEST_PATH", "data/manifest.jsonl")
+
+EMB_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
+
+def ensure_collection():
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+    if not utility.has_collection(COLLECTION_NAME):
+        DIM = EMB_MODEL.get_sentence_embedding_dimension()
+        fields = [
+            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
+            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=4096),
+            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIM)
+        ]
+        schema = CollectionSchema(fields, description="RAG demo collection")
+        col = Collection(COLLECTION_NAME, schema)
+        col.create_index("vector", {"index_type":"IVF_FLAT", "metric_type":"L2", "params":{"nlist":128}})
+        col.load()
+    else:
+        col = Collection(COLLECTION_NAME)
+        if not col.is_loaded:
+            col.load()
+    return col
+
+def run():
+    col = ensure_collection()
+    ids = []
+    texts = []
+    vecs = []
+    with open(MANIFEST, "r") as f:
+        for line in f:
+            obj = json.loads(line)
+            if obj.get("modality","text") != "text":
+                # image embedding placeholder
+                continue
+            ids.append(int(obj["id"]))
+            texts.append(obj.get("text",""))
+            emb = EMB_MODEL.encode([obj.get("text","")])[0]
+            vecs.append(emb.tolist())
+    if not ids:
+        print("No text entries found in manifest.")
+        return
+    # upsert
+    col.insert([ids, texts, vecs])
+    col.flush()
+    print(f"Inserted {len(ids)} entries into Milvus collection {COLLECTION_NAME}")
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: data/manifest.jsonl
+{"id": 1, "modality": "text", "text": "The transaction manager records commits and rollbacks. It retries up to 3 times on transient errors.", "uri": ""}
+{"id": 2, "modality": "text", "text": "Edge TPU devices require libedgetpu to be installed and drivers loaded. Use the Ansible role for provisioning.", "uri": ""}
+{"id": 3, "modality": "text", "text": "Checkpoint uploads must be atomic and uploaded to the COMPLIANCE_BUCKET to enable replay and restore validation.", "uri": ""}
+
*** End Patch
*** Begin Patch
*** Add File: prompts/prompts.yaml
+default:
+  id: default
+  description: "Default prompt template that asks the model to answer using supplied context and to indicate sources."
+  template: |
+    Use the following retrieved documents to answer the question. Cite document ids inline.
+    {{context}}
+
+    Q: {{query}}
+
+    A:
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_orchestrator/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN python -m pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt
+COPY app.py tool_runner.py .
+EXPOSE 8100
+CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8100"]
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_orchestrator/requirements.txt
+fastapi==0.95.2
+uvicorn[standard]==0.21.1
+requests==2.31.0
+pymilvus==2.3.0
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_orchestrator/tool_runner.py
+"""
+Simple tool runner that supports a limited set of safe actions.
+This is intentionally constrained for demo purposes.
+"""
+import json
+from pymilvus import Collection
+
+def fetch_doc_text(doc_id: int, milvus_collection_name: str = "rag_demo"):
+    try:
+        col = Collection(milvus_collection_name)
+        res = col.query(expr=f"id == {doc_id}", output_fields=["id","text"])
+        if res:
+            return {"ok": True, "id": res[0]["id"], "text": res[0]["text"]}
+        return {"ok": False, "error": "not found"}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+ALLOWED_TOOLS = {
+    "fetch_doc": fetch_doc_text
+}
+
+def run_tool(action: dict):
+    """
+    action example: {"tool": "fetch_doc", "args": {"doc_id": 2}}
+    """
+    tool = action.get("tool")
+    args = action.get("args", {})
+    if tool not in ALLOWED_TOOLS:
+        return {"ok": False, "error": "tool_not_allowed"}
+    fn = ALLOWED_TOOLS[tool]
+    return fn(**args)
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_orchestrator/app.py
+import os
+import json
+import requests
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+from tool_runner import run_tool
+
+OPA_URL = os.environ.get("OPA_URL", "http://opa.ops.svc.cluster.local:8181/v1/data/aegis/policies/allow")
+AUDIT_COLLECTOR = os.environ.get("AUDIT_COLLECTOR", "http://audit-collector.ops.svc.cluster.local:8088/event")
+
+app = FastAPI(title="Aegis Agent Orchestrator (demo)")
+
+class AgentRequest(BaseModel):
+    actor: str
+    action: dict  # expected to contain tool & args
+
+def opa_check(action: dict, approvers=[] , required=1):
+    payload = {"action": action.get("tool", "unknown"), "approvers": approvers, "required": required}
+    try:
+        res = requests.post(OPA_URL, json=payload, timeout=5)
+        if res.status_code != 200:
+            return False, "opa_error"
+        body = res.json()
+        allow = body.get("result", {}).get("allow", False)
+        return allow, body
+    except Exception as e:
+        return False, str(e)
+
+def audit_event(actor: str, action: str, payload: dict):
+    try:
+        requests.post(AUDIT_COLLECTOR, json={"actor": actor, "action": action, "payload": payload}, timeout=5)
+    except Exception:
+        print("Audit post failed")
+
+@app.post("/execute")
+def execute(req: AgentRequest):
+    # Check OPA
+    allowed, opa_resp = opa_check(req.action)
+    audit_event(req.actor, "preflight", {"action": req.action, "allowed": allowed})
+    if not allowed:
+        raise HTTPException(status_code=403, detail={"reason": "opa_denied", "opa": opa_resp})
+    # Execute tool
+    res = run_tool(req.action)
+    audit_event(req.actor, "execute_tool", {"action": req.action, "result": res})
+    return {"result": res, "opa": opa_resp}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/rag_smoke.yml
+name: RAG + Agent Smoke Test
+on:
+  workflow_dispatch:
+
+jobs:
+  smoke:
+    runs-on: ubuntu-latest
+    services:
+      docker:
+        image: docker:20.10.16
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+
+      - name: Install python deps for smoke harness
+        run: python -m pip install --upgrade pip && pip install sentence-transformers pymilvus requests
+
+      - name: Start Milvus standalone (docker)
+        run: |
+          docker pull milvusdb/milvus:v2.2.9-standalone
+          docker run -d --name milvus -p 19530:19530 -p 19121:19121 milvusdb/milvus:v2.2.9-standalone
+          # wait for milvus to start
+          for i in $(seq 1 30); do nc -z localhost 19530 && break || sleep 2; done
+
+      - name: Run embedding runner (insert demo vectors)
+        run: |
+          python3 services/embeddings/embedding_runner.py
+
+      - name: Build RAG service image
+        run: |
+          docker build -t ghcr.io/yourorg/rag-service:smoke -f services/rag/Dockerfile .
+
+      - name: Run RAG service container
+        run: |
+          docker run -d --name rag -p 8000:8000 --env MILVUS_HOST=host.docker.internal ghcr.io/yourorg/rag-service:smoke
+          sleep 4
+
+      - name: Smoke query (expect non-empty answer)
+        run: |
+          sleep 2
+          curl -sS -X POST "http://localhost:8000/query" -H "Content-Type: application/json" -d '{"q":"How are checkpoints uploaded?","k":2}' | tee /tmp/rag_out.json
+          if grep -q "Answer" /tmp/rag_out.json; then echo "OK"; else (echo "RAG service did not return expected answer" && exit 1); fi
+
+      - name: Build Agent Orchestrator image
+        run: |
+          docker build -t ghcr.io/yourorg/agent-orch:smoke -f services/agent_orchestrator/Dockerfile services/agent_orchestrator
+
+      - name: Run Agent Orchestrator container
+        run: |
+          docker run -d --name agent -p 8100:8100 --env OPA_URL=http://opa.ops.svc.cluster.local:8181/v1/data/aegis/policies/allow ghcr.io/yourorg/agent-orch:smoke
+          sleep 3
+
+      - name: Test Agent Tool (expect 403 since OPA unreachable)
+        run: |
+          set -e
+          curl -sS -X POST "http://localhost:8100/execute" -H "Content-Type: application/json" -d '{"actor":"test","action":{"tool":"fetch_doc","args":{"doc_id":1}}}' || true
+
+      - name: Cleanup
+        if: always()
+        run: |
+          docker rm -f rag agent milvus || true
+
*** End Patch
*** Begin Patch
*** Add File: k8s/milvus/standalone-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: milvus-standalone
+  namespace: staging
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: milvus-standalone
+  template:
+    metadata:
+      labels:
+        app: milvus-standalone
+    spec:
+      containers:
+      - name: milvus
+        image: milvusdb/milvus:v2.2.9-standalone
+        ports:
+        - containerPort: 19530
+        - containerPort: 19121
+        env:
+        - name: TZ
+          value: "UTC"
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: milvus
+  namespace: staging
+spec:
+  selector:
+    app: milvus-standalone
+  ports:
+    - port: 19530
+      targetPort: 19530
+    - port: 19121
+      targetPort: 19121
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/rag_runbook.md
+# RAG + Agent demo runbook
+
+This runbook explains how to run the RAG + Agent demo locally (or in CI).
+
+Prerequisites
+- Docker or a k8s cluster.
+- (Optional) OPENAI_API_KEY set as environment variable to enable OpenAI calls.
+- (For production) MILVUS deployed in `staging` namespace or reachable at MILVUS_HOST:19530.
+
+Quick local run (docker):
+1. Start Milvus (standalone):
+   docker run -d --name milvus -p 19530:19530 -p 19121:19121 milvusdb/milvus:v2.2.9-standalone
+
+2. Insert demo vectors:
+   python3 services/embeddings/embedding_runner.py
+
+3. Build and run RAG service:
+   docker build -t ghcr.io/yourorg/rag-service:dev -f services/rag/Dockerfile .
+   docker run -d --name rag -p 8000:8000 --env MILVUS_HOST=host.docker.internal ghcr.io/yourorg/rag-service:dev
+
+4. Test a query:
+   curl -X POST "http://localhost:8000/query" -H "Content-Type: application/json" -d '{"q":"How are checkpoints uploaded?","k":2}'
+
+5. Agent test:
+   docker build -t ghcr.io/yourorg/agent-orch:dev -f services/agent_orchestrator/Dockerfile services/agent_orchestrator
+   docker run -d --name agent -p 8100:8100 ghcr.io/yourorg/agent-orch:dev
+   curl -X POST "http://localhost:8100/execute" -H "Content-Type: application/json" -d '{"actor":"tester","action":{"tool":"fetch_doc","args":{"doc_id":1}}}'
+
+Notes & acceptance criteria
+- The RAG query should return an answer string and list of retrieved docs.
+- The Agent orchestrator will call OPA preflight; in a demo environment OPA may be unavailable and the orchestrator will return 403 (expected). Wire OPA_URL to a reachable OPA to enable full flow.
+- Audit events are posted to AUDIT_COLLECTOR if the audit_collector service is deployed and reachable.
+
+Extending the demo
+- Add image embedding support in services/embeddings/embedding_runner.py (use CLIP or OpenAI embeddings).
+- Replace OpenAI calls with a local LLM by changing the RAG service LLM call.
+- Harden tools: run tool execution inside sandboxed containers; limit allowed tools via OPA policies.
+
*** End Patch
*** Begin Patch
*** Add File: services/rag/notes.txt
+Notes:
+- The RAG service uses SentenceTransformers for embeddings by default (all-MiniLM-L6-v2).
+- For production at scale consider:
+  - OpenAI embeddings or a larger embedding model.
+  - Managed vector DB (Pinecone) or Milvus cluster with HA.
+  - Model signing (cosign) and OPA policies for any agent tool calls.
+
*** End Patch
*** End Patch
