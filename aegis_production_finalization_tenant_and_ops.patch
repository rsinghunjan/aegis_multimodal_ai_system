*** Begin Patch
*** Add File:tenant-finalization/azure/finalize_azure_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_azure_tenant.sh
+# Operator-driven script to finalize Azure tenant wiring:
+#  - create app registration (if missing) and federated credential for GitHub Actions
+#  - create user-assigned managed identity (UAI) for CI workloads
+#  - create Key Vault key (if desired) and set least-privilege Key Vault policy for the UAI
+#  - print the values to add as GitHub repository secrets
+#
+# WARNING: This script performs changes in Azure. Review and run in a staging subscription first.
+#
+usage() {
+  cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --resource-group <rg> --location <loc> --vault-name <vault>
+
+Outputs (copy/paste to GitHub secrets after running):
+  - AZURE_OIDC_CLIENT_ID
+  - AZURE_TENANT_ID
+  - AZURE_KEY_URI
+
+EOF
+  exit 1
+}
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --resource-group) RG="$2"; shift 2;;
+    --location) LOC="$2"; shift 2;;
+    --vault-name) VAULT="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${RG:-}" ] || usage
+[ -n "${LOC:-}" ] || usage
+[ -n "${VAULT:-}" ] || usage
+
+echo "Ensure you are logged in: az login and have sufficient Graph rights"
+
+echo "1) Create App registration (if not exists)"
+APP_NAME="aegis-github-federation-${ORG}-${REPO}"
+EXISTING=$(az ad app list --display-name "$APP_NAME" --query 'length([])' -o tsv)
+if [ "$EXISTING" -eq "0" ]; then
+  APP_JSON=$(az ad app create --display-name "$APP_NAME" --query "{appId:appId,id:id}" -o json)
+  APP_ID=$(echo "$APP_JSON" | jq -r .appId)
+  APP_OBJ_ID=$(echo "$APP_JSON" | jq -r .id)
+  echo "Created app: $APP_ID"
+else
+  APP_JSON=$(az ad app list --display-name "$APP_NAME" -o json | jq '.[0]')
+  APP_ID=$(echo "$APP_JSON" | jq -r .appId)
+  APP_OBJ_ID=$(echo "$APP_JSON" | jq -r .id)
+  echo "Found existing app: $APP_ID"
+fi
+
+echo "2) Create federated credential for GitHub Actions (subject repo:${ORG}/${REPO}:ref:refs/heads/main)"
+SUB="repo:${ORG}/${REPO}:ref:refs/heads/main"
+if az ad app federated-credential list --id "$APP_ID" -o json | jq -e ".[] | select(.subject==\"$SUB\")" >/dev/null 2>&1; then
+  echo "Federated credential already exists for $SUB"
+else
+  if az ad app federated-credential create -h >/dev/null 2>&1; then
+    az ad app federated-credential create --id "$APP_ID" --parameters "{\"name\":\"gha-${ORG}-${REPO}\",\"issuer\":\"https://token.actions.githubusercontent.com\",\"subject\":\"$SUB\",\"audiences\":[\"api://AzureADTokenExchange\"]}"
+    echo "Federated credential created."
+  else
+    echo "Fallback: please create federated credential in the Azure Portal for app $APP_ID with issuer https://token.actions.githubusercontent.com and subject $SUB"
+  fi
+fi
+
+echo "3) Create User Assigned Managed Identity for CI to access Key Vault"
+MI_NAME="aegis-ci-identity"
+MI_JSON=$(az identity create -g "$RG" -n "$MI_NAME" --location "$LOC" -o json)
+MI_CLIENT_ID=$(echo "$MI_JSON" | jq -r .clientId)
+MI_PRINCIPAL_ID=$(echo "$MI_JSON" | jq -r .principalId)
+echo "Created UAI: clientId=$MI_CLIENT_ID principalId=$MI_PRINCIPAL_ID"
+
+echo "4) Ensure Key Vault exists (or create minimal vault placeholder)"
+if az keyvault show -n "$VAULT" -g "$RG" >/dev/null 2>&1; then
+  echo "Key Vault $VAULT exists"
+else
+  echo "Creating Key Vault $VAULT (soft-delete enabled)"
+  az keyvault create -n "$VAULT" -g "$RG" -l "$LOC" --enable-soft-delete true
+fi
+
+echo "5) Create Key in Key Vault for cosign/keystore usage (optional: operator may choose existing key)"
+KEY_NAME="aegis-signing-key"
+az keyvault key create --vault-name "$VAULT" --name "$KEY_NAME" --kty EC -o json >/dev/null
+KEY_URI="azurekms://${VAULT}.vault.azure.net/keys/${KEY_NAME}"
+echo "Created KeyVault key: $KEY_URI"
+
+echo "6) Grant Key Vault permissions to the UAI (wrapKey, unwrapKey, get)"
+az keyvault set-policy -n "$VAULT" --object-id "$MI_PRINCIPAL_ID" --key-permissions wrapKey unwrapKey get
+
+echo
+echo "=== GitHub secrets to create (output) ==="
+echo "AZURE_OIDC_CLIENT_ID=$APP_ID"
+echo "AZURE_TENANT_ID=$(az account show --query tenantId -o tsv)"
+echo "AZURE_KEY_URI=$KEY_URI"
+echo "AZURE_MANAGED_IDENTITY_CLIENT_ID=$MI_CLIENT_ID"
+echo
+echo "Store those values in your repo secrets and configure azure/login action to use the federated client id."
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/oci/finalize_oci_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_oci_tenant.sh
+# Operator-guided helper to produce the OCI console steps and CLI snippets to finalize OIDC federation, Dynamic Group, and KMS policy.
+#
+usage() {
+  cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --compartment <compartment-ocid>
+Outputs textual artifacts for manual creation in OCI Console or CLI.
+EOF
+  exit 1
+}
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --compartment) COMP="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${COMP:-}" ] || usage
+
+ISSUER="https://token.actions.githubusercontent.com"
+SUB="repo:${ORG}/${REPO}:ref:refs/heads/main"
+
+cat <<EOF
+OCI Tenant Finalization - Steps & artifacts
+
+1) Create an OIDC Identity Provider in OCI Console:
+   - Issuer URL: $ISSUER
+   - Name: aegis-github-oidc-${ORG}-${REPO}
+
+2) Create a Dynamic Group (in Console):
+   - Name: aegis-github-oidc-${ORG}-${REPO}
+   - Matching rule (example):
+       ALL {request.principal.claims["iss"] = "$ISSUER" && request.principal.claims["sub"] = "$SUB"}
+
+3) Create IAM policy (example) to allow dynamic group to use specific key(s) in compartment $COMP:
+   Policy statements:
+     Allow dynamic-group aegis-github-oidc-${ORG}-${REPO} to use keys in compartment $COMP
+     Allow dynamic-group aegis-github-oidc-${ORG}-${REPO} to inspect keys in compartment $COMP
+
+4) Example EXCHANGE_CMD to place in GitHub secret OCI_EXCHANGE_CMD:
+   OCI_EXCHANGE_CMD=\"oci sts create-session-token --assume-role-with-web-identity --web-identity-token '<<TOKEN>>' --role-arn ocid1.role.oc1..example --duration-seconds 3600 --output json\"
+
+5) Example OCI_KEY_URI: oci-kms://<region>/<key-ocid>
+   (Provide actual key OCID after creating key in OCI Vault / KMS)
+
+After creating the Identity Provider and Dynamic Group, test a workflow using the provided exchange scripts in scripts/ci/.
+
*** End Patch
*** Begin Patch
*** Add File:tenant-finalization/alibaba/finalize_alibaba_tenant.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# finalize_alibaba_tenant.sh
+# Emits guidance & a sample RAM trust policy and EXCHANGE_CMD for Alibaba Cloud OIDC + RAM role.
+#
+usage() {
+  cat <<EOF
+Usage: $0 --org <github-org> --repo <github-repo> --role-name <role-name>
+EOF
+  exit 1
+}
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --role-name) ROLE="$2"; shift 2;;
+    -h|--help) usage;;
+    *) echo "Unknown $1"; usage;;
+  esac
+done
+
+[ -n "${ORG:-}" ] || usage
+[ -n "${REPO:-}" ] || usage
+[ -n "${ROLE:-}" ] || usage
+
+SUB="repo:${ORG}/${REPO}:ref:refs/heads/main"
+
+cat <<EOF
+Alibaba Cloud Tenant Finalization
+
+1) Create a RAM role with Web Identity Federation trust:
+   Trust policy example (JSON):
+{
+  "Statement": [
+    {
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Effect": "Allow",
+      "Principal": { "Federated": ["https://token.actions.githubusercontent.com"] },
+      "Condition": {
+        "StringEquals": { "token.actions.githubusercontent.com:sub": "$SUB" }
+      }
+    }
+  ],
+  "Version": "1"
+}
+
+2) Attach a RAM policy to allow kms:Sign and kms:GetPublicKey on your KMS key resource.
+3) Example ALICLOUD_EXCHANGE_CMD to store in GitHub secrets (replace RoleArn and parameters):
+   ALICLOUD_EXCHANGE_CMD=\"aliyun sts AssumeRoleWithWebIdentity --WebIdentityToken '<<TOKEN>>' --RoleArn 'acs:ram::1234567890123456:role/$ROLE' --RoleSessionName gha-oidc --Output json\"
+
+4) Set ALICLOUD_KEY_URI to the KMS key identifier (provider-specific)
+
+After you create the role and policy, test the exchange using scripts/ci/alibaba_oidc_exchange.sh in the repo.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/vault/README.md
+# Managed HA Vault Terraform (RAFT) scaffolds
+
+This folder contains example Terraform that provisions the infrastructure for HA Vault (store/backing, security group, IAM) and helper modules to create a persistent, encrypted storage layer and supporting resources.
+
+Operator: adapt the cloud-specific provider and ensure proper networking and IAM.
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/vault/aws/main.tf
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws" }
+  }
+}
+provider "aws" {
+  region = var.region
+}
+
+resource "aws_ebs_volume" "vault_data" {
+  availability_zone = var.availability_zone
+  size              = var.volume_size_gb
+  type              = "gp3"
+  encrypted         = true
+}
+
+resource "aws_instance" "vault" {
+  ami           = var.ami
+  instance_type = var.instance_type
+  subnet_id     = var.subnet_id
+  tags = { Name = "${var.stage}-vault-instance" }
+}
+
+output "vault_instance_ip" { value = aws_instance.vault.public_ip }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/managed/vault/variables.tf
+variable "region" { type = string default = "us-east-1" }
+variable "availability_zone" { type = string default = "us-east-1a" }
+variable "volume_size_gb" { type = number default = 50 }
+variable "ami" { type = string }
+variable "instance_type" { type = string default = "t3.large" }
+variable "subnet_id" { type = string }
+variable "stage" { type = string default = "staging" }
+
*** End Patch
*** Begin Patch
*** Add File:prod/vault/audit_forwarding.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Configure Vault audit devices to forward to a remote syslog/siem collector (via file or socket).
+# Requires VAULT_ADDR and VAULT_TOKEN env vars.
+#
+SIEM_HOST=${SIEM_HOST:-"syslog.example.com"}
+SIEM_PORT=${SIEM_PORT:-514}
+DEVICE_NAME="siem-audit"
+
+if ! command -v vault >/dev/null 2>&1; then
+  echo "vault CLI required"
+  exit 1
+fi
+
+echo "Enabling file audit device locally and/or syslog forwarding"
+vault audit enable file file_path=/var/log/vault_audit.log
+
+# If vault supports syslog device via plugin or socket, configure accordingly. Fallback: tail and forward.
+echo "Setting up forwarder (tail -> logger) to $SIEM_HOST:$SIEM_PORT (operator: replace with secured channel)"
+nohup bash -c "tail -F /var/log/vault_audit.log | logger -n $SIEM_HOST -P $SIEM_PORT -t vault_audit" >/dev/null 2>&1 &
+echo "Audit forwarding started (background). Verify SIEM receipt."
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/mtls/generate_ca_and_secrets.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Generate a simple self-signed CA and a TLS cert for internal services; load into Kubernetes secret for mTLS.
+#
+NS=${NS:-ops}
+SECRET_NAME=${SECRET_NAME:-aegis-internal-tls}
+CA_DIR=${CA_DIR:-/tmp/aegis-ca}
+mkdir -p "$CA_DIR"
+
+echo "Generating CA private key and cert"
+openssl genrsa -out "$CA_DIR/ca.key" 4096
+openssl req -x509 -new -nodes -key "$CA_DIR/ca.key" -subj "/CN=aegis-root-CA" -days 3650 -out "$CA_DIR/ca.crt"
+
+echo "Generate server key and CSR"
+openssl genrsa -out "$CA_DIR/server.key" 2048
+openssl req -new -key "$CA_DIR/server.key" -subj "/CN=aegis-internal.svc.cluster.local" -out "$CA_DIR/server.csr"
+
+openssl x509 -req -in "$CA_DIR/server.csr" -CA "$CA_DIR/ca.crt" -CAkey "$CA_DIR/ca.key" -CAcreateserial -out "$CA_DIR/server.crt" -days 365
+
+kubectl create ns "$NS" || true
+kubectl create secret tls "$SECRET_NAME" --cert="$CA_DIR/server.crt" --key="$CA_DIR/server.key" -n "$NS" --dry-run=client -o yaml | kubectl apply -f -
+kubectl create secret generic aegis-ca --from-file=ca.crt="$CA_DIR/ca.crt" -n "$NS" --dry-run=client -o yaml | kubectl apply -f -
+
+echo "mTLS secrets created in namespace $NS (secret: $SECRET_NAME, aegis-ca)"
+
*** End Patch
*** Begin Patch
*** Add File:prod/security/networkpolicies/namespace-restrictive-policies.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: default-deny-all
+  namespace: ops
+spec:
+  podSelector: {}
+  policyTypes:
+  - Ingress
+  - Egress
+
+---
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: allow-monitoring
+  namespace: ops
+spec:
+  podSelector:
+    matchLabels:
+      app: aegis-ops
+  ingress:
+  - from:
+    - namespaceSelector:
+        matchLabels:
+          name: monitoring
+    ports:
+    - protocol: TCP
+      port: 443
+
*** End Patch
*** Begin Patch
*** Add File:prod/reliability/drill/run_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Automated DR drill:
+# 1) snapshot Vault
+# 2) snapshot lakeFS objects to backup bucket
+# 3) simulate restore to recovery cluster (dry-run unless RECOVER=1)
+#
+RECOVER=${RECOVER:-0}
+BACKUP_PREFIX=${BACKUP_PREFIX:-aegis-drill-$(date -u +%Y%m%dT%H%M%SZ)}
+echo "Starting DR drill: backup prefix $BACKUP_PREFIX"
+
+echo "1) Vault snapshot"
+SNAP="/tmp/vault-snapshot-$(date -u +%s).snap"
+vault operator raft snapshot save "$SNAP" || { echo "Vault snapshot failed"; exit 1; }
+echo "Vault snapshot: $SNAP"
+
+echo "2) lakeFS backup (sync to backup bucket - operator must configure AWS cli)"
+aws s3 sync s3://aegis-lakefs s3://aegis-lakefs-backups/${BACKUP_PREFIX}
+echo "lakeFS sync complete"
+
+if [ "$RECOVER" = "1" ]; then
+  echo "=== Performing recovery (destructive) - ensure operator approval and correct cluster ==="
+  vault operator raft snapshot restore "$SNAP"
+  echo "Snapshot restore initiated"
+  # Optionally restore lakeFS bucket contents to recovery bucket
+fi
+
+echo "DR drill completed (dry-run unless RECOVER=1). Review artifacts and logs."
+
*** End Patch
*** Begin Patch
*** Add File:prod/autoscale/keda/gpu_scaledobject_examples.md
+# KEDA GPU ScaledObject examples
+
+KEDA can scale by custom Prometheus metrics (e.g., DCGM GPU utilization). The repo includes prod/serving/autoscaling/keda/prometheus-scaledobject.yaml.
+
+For production:
+- Ensure DCGM exporter for GPUs is deployed and writing metrics to Prometheus.
+- Tune `threshold` in the ScaledObject to target GPU utilization.
+- Use KEDA with the external scaler if you have custom logic for batching or queue length.
+
*** End Patch
*** Begin Patch
*** Add File:prod/cost/kubecost/helm-values.yaml
+# kube-cost minimal helm values example (operator: use licensed kube-cost for production)
+controller:
+  enable: true
+  resources:
+    limits:
+      cpu: 500m
+      memory: 1Gi
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/device_attestation/README.md
+# Edge & IoT: Device attestation & enrollment
+
+This folder holds a small reference implementation for device enrollment and attestation:
+- device-enroll-service (Flask) accepts device CSR, issues certificate, records device metadata.
+- example device agent that performs local attestation and publishes its signed identity artifact to Rekor (offline bundle support).
+
+Operator notes:
+- Production device attestation requires TPM/TEE attestation, a secure enrollment flow, and long-lived device identity management. Use this as a starting scaffold.
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/device_attestation/device_enroll_service.py
+#!/usr/bin/env python3
+from flask import Flask, request, jsonify
+import os, subprocess, tempfile
+
+app = Flask(__name__)
+
+@app.post("/enroll")
+def enroll():
+    data = request.get_json(force=True)
+    device_id = data.get("device_id")
+    csr = data.get("csr")
+    # operator: validate CSR & device attestation evidence here
+    # For demo, sign CSR with local CA (not production secure)
+    csr_file = "/tmp/device.csr"
+    with open(csr_file,"w") as f: f.write(csr)
+    cert_file = f"/tmp/{device_id}.crt"
+    # Use openssl to sign (operator should use secure CA)
+    subprocess.run(["openssl","x509","-req","-in",csr_file,"-signkey","/etc/aegis/ca.key","-out",cert_file,"-days","365"], check=True)
+    # Return certificate (PEM)
+    with open(cert_file) as f:
+        cert_pem = f.read()
+    return jsonify({"device_id": device_id, "cert": cert_pem})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0",port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:prod/edge/device_attestation/device_sign_and_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Example device-side script to sign a small artifact and produce an offline Rekor bundle
+ARTIFACT=${1:-/tmp/hello.txt}
+DEVICE_ID=${DEVICE_ID:-dev-001}
+OUTPUT_BUNDLE=${2:-./device-bundle.tar.gz}
+
+[ -f "$ARTIFACT" ] || echo "hello from $DEVICE_ID" > "$ARTIFACT"
+
+# Sign using cosign with a private key on device (for demo only) and create a Rekor bundle (if Rekor offline bundling supported)
+if ! command -v cosign >/dev/null 2>&1; then
+  echo "Install cosign to produce signatures"
+  exit 1
+fi
+
+echo "Signing artifact with local private key (demo). Operator: replace with secure TPM-based signing"
+cosign sign-blob --key /etc/aegis/device.key --output-signature /tmp/art.sig --output-cose /tmp/art.cose "$ARTIFACT" || true
+
+echo "Bundle artifact + sig into offline bundle"
+tar -czf "$OUTPUT_BUNDLE" "$ARTIFACT" /tmp/art.sig /tmp/art.cose || true
+echo "Bundle created: $OUTPUT_BUNDLE"
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/README.md
+# Quantum optimization: connectors & hybrid workflows (scaffold)
+
+This folder contains starter connectors and Argo workflow templates to orchestrate hybrid classical/quantum workloads:
+- qiskit_workflow.py - Python example that runs a parameterized Qiskit circuit
+- braket_workflow.py - AWS Braket SDK example
+- argo/hybrid-quantum-workflow.yaml - Argo template showing orchestration pattern
+
+Note: Quantum workloads are highly specialized. Use these as examples to integrate with your orchestration pipelines.
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/qiskit_workflow.py
+#!/usr/bin/env python3
+"""
+Example script to run a small Qiskit circuit and save results.
+This is a placeholder; production quantum integration requires account setup & provider connectors.
+"""
+from qiskit import QuantumCircuit, Aer, execute
+qc = QuantumCircuit(2,2)
+qc.h(0)
+qc.cx(0,1)
+qc.measure([0,1],[0,1])
+backend = Aer.get_backend('qasm_simulator')
+job = execute(qc, backend=backend, shots=1024)
+result = job.result()
+print("Qiskit result counts:", result.get_counts())
+
*** End Patch
*** Begin Patch
*** Add File:prod/quantum/argo/hybrid-quantum-workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-hybrid-quantum-
+  namespace: ml
+spec:
+  entrypoint: hybrid
+  templates:
+  - name: hybrid
+    steps:
+    - - name: classical-prep
+        template: run-classical
+      - name: quantum-step
+        template: run-quantum
+      - name: postprocess
+        template: run-classical
+  - name: run-classical
+    container:
+      image: python:3.10
+      command: ["python","-c"]
+      args: ["print('classical work')"]
+  - name: run-quantum
+    container:
+      image: aegis/quantum:latest
+      command: ["python","/app/qiskit_workflow.py"]
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_sdk/auth.py
+import os, json, time, webbrowser, requests
+
+class TokenStore:
+    PATH = os.path.expanduser("~/.aegis/credentials.json")
+    @classmethod
+    def save(cls, data):
+        os.makedirs(os.path.dirname(cls.PATH), exist_ok=True)
+        with open(cls.PATH,"w") as f:
+            json.dump(data,f)
+    @classmethod
+    def load(cls):
+        if not os.path.exists(cls.PATH):
+            return {}
+        return json.load(open(cls.PATH))
+
+def oauth2_device_flow(client_id, auth_url, token_url, scope="openid"):
+    # Simplified device flow: operator may prefer proper oauthlib flow
+    resp = requests.post(auth_url, data={"client_id": client_id, "scope": scope})
+    print("Open the following URL and enter user code:", resp.json())
+    # For brevity, this is a placeholder - implement standard device auth flow
+    return {}
+
+def refresh_token(refresh_token, token_url, client_id):
+    r = requests.post(token_url, data={"grant_type":"refresh_token","refresh_token":refresh_token,"client_id":client_id})
+    return r.json()
+
*** End Patch
*** Begin Patch
*** Add File:prod/devtools/aegis_cli/auth_integration.md
+# aegisctl authentication integration
+
+This document explains how aegisctl will authenticate developers:
+- Use OIDC device-flow or browser-based OAuth to obtain short-lived tokens.
+- Tokens are stored in ~/.aegis/credentials.json and aegisctl will refresh them via refresh_token.
+- For CI, use GitHub OIDC to get short-lived tokens which can be exchanged to Vault tokens (Vault JWT auth).
+
+Security notes:
+- Do not store long-lived secrets in the workspace; always use short-lived tokens and refresh flows.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/tenant_iam_finalization.md
+# Tenant & IAM finalization runbook (operator)
+
+This runbook consolidates cross-cloud tenant finalization steps:
+- Azure: run tenant-finalization/azure/finalize_azure_tenant.sh and create the GitHub secrets printed by script.
+- OCI: follow tenant-finalization/oci/finalize_oci_tenant.sh output and create Identity Provider, Dynamic Group and policy in OCI Console. Put resulting OCI_EXCHANGE_CMD and OCI_KEY_URI in GitHub secrets.
+- Alibaba: follow tenant-finalization/alibaba/finalize_alibaba_tenant.sh output and create RAM role + policy. Put ALICLOUD_EXCHANGE_CMD and ALICLOUD_KEY_URI in GitHub secrets.
+
+After creating secrets:
+ - Test CI workflows (Azure / OCI / Alibaba) with a sample image build & sign job.
+ - Confirm cosign sign succeeds and Rekor entries appear.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/ha_and_managed_services.md
+# Managed services & HA runbook
+
+1) Provision managed Redis / Memorystore / ElastiCache for Feast (Terraform modules in terraform/managed/*).
+2) Provision a hardened S3 bucket with versioning & KMS encryption for lakeFS (terraform/managed/s3 provided).
+3) Deploy lakeFS in HA with Postgres backing (use prod/feature-store/helm/lakefs-values-production.yaml).
+4) For Vault use HA storage (raft or cloud KMS-backed auto-unseal) and ensure backup/restore scripts in prod/backup are scheduled and tested.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/security_and_pentest.md
+# Security hardening & external validation runbook
+
+1) Run prod/compliance/pentest/run_pentest_precheck.sh as an initial scan.
+2) Commission external pentest vendor and run full tests (network, app, infra).
+3) Triage and remediate findings. Re-run scans until passed.
+4) Enable Vault audit forwarding (prod/vault/audit_forwarding.sh) and push logs to SIEM.
+5) Install cert-manager and create mTLS secrets (prod/security/mtls/generate_ca_and_secrets.sh).
+6) Enforce network policies (prod/security/networkpolicies).
+7) Harden RBAC: use least privilege roles; rotate any service account keys.
+
*** End Patch
*** Begin Patch
*** Add File:prod/runbooks/sre_and_dr.md
+# Reliability, SRE and DR runbook
+
+1) Schedule weekly snapshots for Vault and lakeFS using scripts in prod/backup/.
+2) Run quarterly DR drills using prod/reliability/drill/run_drill.sh (start dry-run, then recovery test).
+3) Maintain SLOs and execution playbooks in prod/sre/.
+4) Run incident response drills with on-call rotation and a postmortem process.
+
*** End Patch
*** Begin Patch
*** Add File:prod/validation/perf_and_smoke.md
+# Load & performance validation runbook
+
+1) Use prod/serving/perf/run_triton_perf.sh to run perf_analyzer or k6 tests against the KServe endpoint.
+2) Sweep batch size and concurrency and capture RPS/latency graphs.
+3) Tune KEDA threshold and KServe batching config accordingly.
+4) Run smoke validation via prod/validation/run_all_validations.sh after adjustments.
+
*** End Patch
*** End Patch
