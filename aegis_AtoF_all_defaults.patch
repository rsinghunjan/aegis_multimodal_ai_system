*** Begin Patch
*** Add File:controllers/modelcontrol-operator/pyproject.toml
+[build-system]
+requires = ["setuptools", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "aegis-modelcontrol-operator"
+version = "0.1.0"
+dependencies = [
+  "kopf>=1.38.0",
+  "kubernetes>=23.0.0",
+  "requests",
+  "python-dotenv"
+]
+
*** End Patch
*** Begin Patch
*** Add File:controllers/modelcontrol-operator/operator.py
+#!/usr/bin/env python3
+"""
+Kopf-based Model Control Plane operator (skeleton)
+ - Watches ModelControlPlane CRs (aegis.ai/v1alpha1) and reconciles lifecycle state.
+ - Validates MCP presence (uses tools/mcp_validate_cli.py)
+ - Submits Argo workflows by calling `argo` CLI when configured (simple approach)
+ - Updates CR status with reconciliation results and emits events
+
+Deploy this operator into the aegis namespace. It expects `ARGO_CMD` env var set to "argo" or path to argo CLI.
+"""
+import os
+import kopf
+import subprocess
+import logging
+
+LOGGER = logging.getLogger("modelcontrol-operator")
+ARGO_CMD = os.environ.get("ARGO_CMD", "argo")
+
+@kopf.on.startup()
+def configure(settings: kopf.OperatorSettings, **_):
+    # increase default timeouts for long-running operations (if needed)
+    settings.posting.level = logging.INFO
+
+def validate_mcp_uri(mcp_uri: str) -> bool:
+    try:
+        ret = subprocess.call(["python3", "tools/mcp_validate_cli.py", "--mcp-uri", mcp_uri])
+        return ret == 0
+    except Exception as e:
+        LOGGER.exception("mcp_validate_cli failed: %s", e)
+        return False
+
+def submit_argo_workflow(workflow_yaml: str, params: dict = None) -> str:
+    cmd = [ARGO_CMD, "submit", workflow_yaml, "--watch"]
+    if params:
+        for k, v in params.items():
+            cmd += ["--parameter", f"{k}={v}"]
+    LOGGER.info("Submitting Argo workflow: %s", cmd)
+    out = subprocess.check_output(cmd).decode()
+    # best-effort parse of workflow name
+    for line in out.splitlines():
+        if line.strip().startswith("workflow"):
+            name = line.split()[-1]
+            return name
+    return ""
+
+@kopf.on.create("aegis.ai", "v1alpha1", "modelcontrolplanes")
+@kopf.on.update("aegis.ai", "v1alpha1", "modelcontrolplanes")
+def reconcile(spec, name, namespace, status, **kwargs):
+    mcp_uri = spec.get("mcp_uri")
+    lifecycle_state = spec.get("lifecycle_state", "Dev")
+    LOGGER.info("Reconciling ModelControlPlane %s/%s state=%s", namespace, name, lifecycle_state)
+
+    if mcp_uri:
+        ok = validate_mcp_uri(mcp_uri)
+        if ok:
+            # if Certified and not yet promoted, optionally run promotion workflow
+            if lifecycle_state == "Certified":
+                # example: call an argopipeline that promotes model (workflow YAML must exist)
+                wf_name = submit_argo_workflow("production/pipeline/argo_model_lifecycle.yaml", params={
+                    "MODEL_ID": spec.get("model_id", ""),
+                    "MODEL_VERSION": spec.get("model_version", ""),
+                    "IMAGE_TAG": spec.get("model_version", "staging")
+                })
+                msg = f"MCP validated. Submitted promotion workflow {wf_name}"
+                kopf.event({"apiVersion": "aegis.ai/v1alpha1", "kind": "ModelControlPlane", "metadata": {"name": name, "namespace": namespace}}, type="Normal", reason="PromoteSubmitted", message=msg)
+                return {"state": "PromotionWorkflowSubmitted", "message": msg}
+            else:
+                return {"state": "MCPValidated", "message": "MCP validated"}
+        else:
+            return {"state": "MCPInvalid", "message": "MCP validation failed"}
+    else:
+        # If no MCP and lifecycle_state requires MCP, set status accordingly
+        if lifecycle_state in ("Certified", "Production"):
+            return {"state": "NoMCP", "message": "MCP missing for certified/production state"}
+        return {"state": "Pending", "message": "Waiting for MCP or next action"}
+
*** End Patch
*** Begin Patch
*** Add File:controllers/modelcontrol-operator/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY pyproject.toml /app/
+RUN pip install --no-cache-dir -r <(python - <<PY
+import tomllib,sys,subprocess
+data=tomllib.load(open('pyproject.toml','rb'))
+deps=data['project']['dependencies']
+print('\\n'.join(deps))
+PY)
+COPY operator.py /app/operator.py
+CMD ["python3","/app/operator.py"]
+
*** End Patch
*** Begin Patch
*** Add File:controllers/modelcontrol-operator/k8s/operator_rbac.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: modelcontrol-operator
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: modelcontrol-operator-role
+  namespace: aegis
+rules:
+  - apiGroups: ["aegis.ai"]
+    resources: ["modelcontrolplanes"]
+    verbs: ["get","list","watch","update","patch"]
+  - apiGroups: [""]
+    resources: ["events"]
+    verbs: ["create","patch","update"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: modelcontrol-operator-rb
+  namespace: aegis
+subjects:
+  - kind: ServiceAccount
+    name: modelcontrol-operator
+    namespace: aegis
+roleRef:
+  kind: Role
+  name: modelcontrol-operator-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File:controllers/modelcontrol-operator/k8s/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: modelcontrol-operator
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: modelcontrol-operator
+  template:
+    metadata:
+      labels:
+        app: modelcontrol-operator
+    spec:
+      serviceAccountName: modelcontrol-operator
+      containers:
+        - name: operator
+          image: registry.example.com/aegis/modelcontrol-operator:latest
+          env:
+            - name: ARGO_CMD
+              value: "argo"
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+
*** End Patch
*** Begin Patch
*** Add File:webhook/bootstrap_certs.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Bootstrap self-signed CA and server cert for validating webhook in staging.
+# Produces a k8s secret 'aegis-admission-tls' with tls.crt/tls.key and patches the webhook's caBundle.
+#
+NAMESPACE="${1:-aegis}"
+SERVICE_NAME="${2:-aegis-admission}"
+WEBHOOK_NAME="${3:-aegis-mcp-validator}"
+
+CA_KEY=$(mktemp)
+CA_CRT=$(mktemp)
+SERVER_KEY=$(mktemp)
+SERVER_CSR=$(mktemp)
+SERVER_CRT=$(mktemp)
+
+echo "Generating CA..."
+openssl genrsa -out "$CA_KEY" 2048
+openssl req -x509 -new -nodes -key "$CA_KEY" -sha256 -days 3650 -subj "/CN=aegis-ca" -out "$CA_CRT"
+
+echo "Generating server key and CSR..."
+openssl genrsa -out "$SERVER_KEY" 2048
+openssl req -new -key "$SERVER_KEY" -subj "/CN=${SERVICE_NAME}.${NAMESPACE}.svc" -out "$SERVER_CSR"
+
+echo "Signing server certificate..."
+openssl x509 -req -in "$SERVER_CSR" -CA "$CA_CRT" -CAkey "$CA_KEY" -CAcreateserial -out "$SERVER_CRT" -days 365 -sha256 -extfile <(cat <<EOF
+subjectAltName = DNS:${SERVICE_NAME}.${NAMESPACE}.svc, DNS:${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local
+EOF
+)
+
+echo "Creating k8s secret ${SERVICE_NAME}-tls in namespace ${NAMESPACE}"
+kubectl create secret tls ${SERVICE_NAME}-tls -n ${NAMESPACE} --cert="$SERVER_CRT" --key="$SERVER_KEY" --dry-run=client -o yaml | kubectl apply -f -
+
+CA_BUNDLE=$(base64 -w 0 < "$CA_CRT")
+echo "Patching ValidatingWebhookConfiguration ${WEBHOOK_NAME} with CA bundle"
+kubectl patch validatingwebhookconfiguration ${WEBHOOK_NAME} --type='json' -p="[ {\"op\": \"replace\", \"path\": \"/webhooks/0/clientConfig/caBundle\", \"value\": \"${CA_BUNDLE}\" } ]"
+
+echo "Bootstrap complete."
+
*** End Patch
*** Begin Patch
*** Add File:webhook/validatingwebhook.yaml
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  name: aegis-mcp-validator
+webhooks:
+  - name: mcp.validation.aegis.ai
+    admissionReviewVersions: ["v1"]
+    sideEffects: None
+    timeoutSeconds: 10
+    clientConfig:
+      service:
+        name: aegis-admission
+        namespace: aegis
+        path: "/validate"
+      caBundle: "" # populated by bootstrap_certs.sh
+    rules:
+      - apiGroups: [""]
+        apiVersions: ["v1"]
+        operations: ["CREATE","UPDATE"]
+        resources: ["deployments"]
+    failurePolicy: Fail
+    namespaceSelector:
+      matchExpressions:
+        - key: "aegis/enforce-mcp"
+          operator: Exists
+
*** End Patch
*** Begin Patch
*** Add File:webhook/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-admission
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-admission
+  template:
+    metadata:
+      labels:
+        app: aegis-admission
+    spec:
+      serviceAccountName: aegis-agent
+      containers:
+        - name: admission
+          image: registry.example.com/aegis/aegis-admission:latest
+          ports:
+            - containerPort: 8443
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: aegis-admission-tls
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-admission
+  namespace: aegis
+spec:
+  ports:
+    - port: 443
+      targetPort: 8443
+  selector:
+    app: aegis-admission
+
*** End Patch
*** Begin Patch
*** Add File:webhook/app.py
+#!/usr/bin/env python3
+from flask import Flask, request, jsonify
+import ssl, os
+from admission_server import validate
+
+app = Flask(__name__)
+app.register_blueprint(validate, url_prefix="/") if False else None
+
+@app.route("/validate", methods=["POST"])
+def validate_wrapper():
+    # This imports the simple validation logic from controllers/admission_server.py if present,
+    # fallback to local logic if not.
+    from controllers import admission_server as adm
+    return adm.app.full_dispatch_request()
+
+if __name__ == "__main__":
+    tls_cert = "/tls/tls.crt"
+    tls_key = "/tls/tls.key"
+    app.run(host="0.0.0.0", port=8443, ssl_context=(tls_cert, tls_key))
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_agg/server.py
+#!/usr/bin/env python3
+"""
+PoC Secure Aggregation coordinator using additive secret-sharing.
+Clients POST masked updates; coordinator reconstructs aggregate without learning individual updates.
+NOT production cryptography; educational PoC only. Replace with vetted libraries for production.
+"""
+from flask import Flask, request, jsonify
+import numpy as np
+import json
+
+app = Flask(__name__)
+round_state = {}
+
+def vec_from_json(j):
+    return np.array(j, dtype=float)
+
+@app.route("/register", methods=["POST"])
+def register():
+    body = request.json or {}
+    cid = body.get("client_id")
+    round_id = body.get("round", "r1")
+    round_state.setdefault(round_id, {"parts": {}, "n": body.get("n", 0)})
+    return jsonify({"status":"ok","round":round_id})
+
+@app.route("/submit", methods=["POST"])
+def submit():
+    body = request.json or {}
+    round_id = body.get("round","r1")
+    client = body["client_id"]
+    share = vec_from_json(body["share"])
+    # store share
+    rd = round_state.setdefault(round_id, {"parts": {}, "n": body.get("n",1)})
+    rd["parts"][client] = share.tolist()
+    return jsonify({"status":"ok"})
+
+@app.route("/aggregate", methods=["POST"])
+def aggregate():
+    body = request.json or {}
+    round_id = body.get("round","r1")
+    rd = round_state.get(round_id)
+    if not rd:
+        return jsonify({"error":"no such round"}), 400
+    parts = [np.array(v) for v in rd["parts"].values()]
+    agg = sum(parts)
+    return jsonify({"aggregate": agg.tolist(), "count": len(parts)})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=7000)
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_agg/client.py
+#!/usr/bin/env python3
+"""
+PoC client that masks an update into additive shares and posts to coordinator.
+"""
+import requests, numpy as np, uuid, sys
+
+COORD="http://localhost:7000"
+
+def create_shares(vec, n):
+    # create n shares such that sum(shares)=vec
+    shares = []
+    for i in range(n-1):
+        shares.append(np.random.normal(size=vec.shape))
+    last = vec - sum(shares)
+    shares.append(last)
+    return shares
+
+def run_client(vec, client_id=None, round_id="r1", n=5):
+    client_id = client_id or str(uuid.uuid4())
+    # each client sends its own share (this PoC uses same coordinator receiving all shares)
+    share = vec.tolist()
+    requests.post(f"{COORD}/submit", json={"round": round_id, "client_id": client_id, "share": share, "n": n})
+    print("Submitted share for client", client_id)
+
+if __name__ == "__main__":
+    v = np.array([1.0,2.0,3.0])
+    run_client(v)
+
*** End Patch
*** Begin Patch
*** Add File:agent/runtime.py
+#!/usr/bin/env python3
+"""
+Minimal agent runtime skeleton (planner + executor)
+ - Accepts multimodal input (text + image path), performs simple tool calls, returns action.
+ - Plugin-based tool adapters can be registered (retrieval, search, rest-tool).
+ - Logs decisions to stdout and to a local file to be picked up by incident bundler.
+"""
+import json, os
+from tools.retrieval import simple_search
+
+class Agent:
+    def __init__(self, name="aegis-agent"):
+        self.name = name
+        self.tools = {"search": simple_search}
+
+    def plan(self, input):
+        # Very simple plan: if input contains "find", call search tool
+        if "find" in input.get("text","").lower():
+            return {"tool":"search", "query": input["text"]}
+        return {"tool":"echo", "text": input.get("text","")}
+
+    def execute(self, plan):
+        tool = plan.get("tool")
+        if tool == "search":
+            return {"result": self.tools["search"](plan.get("query"))}
+        if tool == "echo":
+            return {"result": plan.get("text")}
+        return {"error": "unknown tool"}
+
+def run_demo():
+    agent = Agent()
+    inp = {"text": "Find recent models for customer churn"}
+    plan = agent.plan(inp)
+    out = agent.execute(plan)
+    print("Plan:", plan)
+    print("Out:", out)
+    # write trace (for MCP/evidence collection)
+    os.makedirs("/tmp/agent_traces", exist_ok=True)
+    with open("/tmp/agent_traces/trace.json", "w") as f:
+        json.dump({"input":inp, "plan":plan, "out":out}, f)
+
+if __name__ == "__main__":
+    run_demo()
+
*** End Patch
*** Begin Patch
*** Add File:agent/tools/retrieval.py
+def simple_search(query):
+    # placeholder retrieval tool — return deterministic pseudo-results
+    return {"hits": [{"id":"m1","score":0.9, "snippet":"Model for churn with XGBoost"}, {"id":"m2","score":0.7, "snippet":"Baseline logistic model"}]}
+
*** End Patch
*** Begin Patch
*** Add File:qa/argo/agent_multimodal.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-agent-multimodal-
+spec:
+  entrypoint: agent-demo
+  templates:
+    - name: agent-demo
+      steps:
+        - - name: run-agent
+            template: run-script
+            arguments:
+              parameters:
+                - name: script
+                  value: "python3 agent/runtime.py"
+
+    - name: run-script
+      inputs:
+        parameters:
+          - name: script
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args: ["{{inputs.parameters.script}}"]
+
*** End Patch
*** Begin Patch
*** Add File:playbooks/linkerd_spire.md
+# Linkerd + SPIRE Playbook (defaults, staging-friendly)
+
+Goal:
+- Deploy Linkerd for automatic mTLS across pods.
+- Deploy SPIRE server and agents to issue SPIFFE IDs to workloads and enable workload attestation.
+
+Quick staging steps (assumes kubectl context points to your cluster):
+1. Install Linkerd (CLI: https://linkerd.io/):
+   - curl -sL https://run.linkerd.io/install | sh
+   - linkerd check --pre
+   - linkerd install | kubectl apply -f -
+   - linkerd check
+
+2. Install SPIRE (simplified staging manifests):
+   - kubectl apply -f https://raw.githubusercontent.com/spiffe/spire/main/examples/k8s/manifests/spire-all.yaml
+   - Ensure SPIRE server is reachable and register the K8s cluster plugin (see SPIRE docs).
+
+3. Integrate Linkerd + SPIRE:
+   - Use Linkerd's automatic injection; ensure SPIRE agent runs on nodes and issues workload SVIDs.
+   - For production, follow HA recommendations for Linkerd control plane and SPIRE (etcd, storage).
+
+Production notes:
+- Use external CA or cert-manager-backed issuance for SPIRE server CA.
+- Harden RBAC, deploy SPIRE server behind strict network policies, and enable logging to SIEM.
+
*** End Patch
*** Begin Patch
*** Add File:carbon/collector.py
+#!/usr/bin/env python3
+"""
+Simple carbon intensity collector (ElectricityMap-style PoC).
+Default: returns a made-up intensity per region. In production, wire to ElectricityMap or cloud provider carbon APIs.
+"""
+import time, json, os
+
+CACHE="/tmp/carbon_cache.json"
+
+def get_intensity(region="us-west-2"):
+    # PoC: deterministic synthetic values
+    value = {"us-west-2": 150, "us-east-1": 200, "europe-west1": 100}.get(region, 180)
+    return {"region": region, "intensity_gCO2_per_kWh": value, "ts": int(time.time())}
+
+def main():
+    region = os.environ.get("REGION", "us-west-2")
+    out = get_intensity(region)
+    with open(CACHE, "w") as f:
+        json.dump(out, f)
+    print("Wrote carbon intensity to", CACHE)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:autoschedulers/hpc_scheduler_integration.py
+"""
+Extended scheduler helper with carbon-awareness tie-in.
+Adds a 'carbon_score' to candidate partitions when picking placement.
+"""
+from cost_optimizer.placement import choose_placement
+import json, os
+
+def pick_partition_with_carbon(job_requirements, partitions, carbon_map, cost_delta_pct=0.10):
+    """
+    partitions: [{name, cpu_avail, mem_avail, spot_price, region}]
+    carbon_map: dict region -> intensity
+    Strategy: prefer partitions within cost_delta_pct of cheapest that also have low carbon intensity.
+    """
+    # annotate partitions
+    for p in partitions:
+        p["carbon"] = carbon_map.get(p.get("region"), 999)
+    # find cheapest cost
+    costs = [p["spot_price"] for p in partitions if p["available"]]
+    if not costs:
+        return None
+    cheapest = min(costs)
+    allowed = [p for p in partitions if p["spot_price"] <= cheapest * (1 + cost_delta_pct) and p["available"]]
+    # pick the one with lowest carbon among allowed
+    allowed.sort(key=lambda x: (x["carbon"], x["spot_price"]))
+    return allowed[0] if allowed else None
+
*** End Patch
*** Begin Patch
*** Add File:carbon/cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: carbon-collector
+  namespace: aegis
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: carbon
+              image: python:3.10-slim
+              command: ["bash", "-lc"]
+              args: ["pip install boto3 && python3 carbon/collector.py"]
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:tools/README_DEPLOY_A_TO_F.md
+# Deploy A–F (All defaults) — Quick guide (staging)
+
+This document describes how to deploy the artifacts included in this patch into a staging cluster using the default values.
+
+Assumptions:
+- kubectl context points to target cluster
+- namespace 'aegis' exists (create: kubectl create ns aegis)
+- You have built/pushed images to registry.example.com/aegis or adjust image names before deploy.
+
+Steps (high-level):
+1. Apply CRDs (ModelControlPlane CRD already present in repo earlier). If not, apply k8s/crd/modelcontrolplane_crd.yaml
+2. Deploy the modelcontrol operator:
+   - kubectl apply -f controllers/modelcontrol-operator/k8s/operator_rbac.yaml
+   - kubectl apply -f controllers/modelcontrol-operator/k8s/deployment.yaml
+3. Deploy admission webhook:
+   - kubectl apply -f webhook/deployment.yaml
+   - kubectl apply -f webhook/validatingwebhook.yaml
+   - ./webhook/bootstrap_certs.sh aegis aegis-admission aegis-mcp-validator
+4. Deploy agent demo (Argo workflows):
+   - argo submit qa/argo/agent_multimodal.yaml --watch
+5. Start PoC federated secure-agg:
+   - kubectl run --attach --restart=Never --image=python:3.10-slim fed-coord -- python3 federated/secure_agg/server.py
+   - Run few client processes locally: python3 federated/secure_agg/client.py
+6. Deploy carbon collector CronJob:
+   - kubectl apply -f carbon/cronjob.yaml
+
+Notes:
+- These are staging defaults. For production, replace self-signed bootstrap with cert-manager, use a real image registry and hardened RBAC.
+- For the operator and admission webhook, build Docker images and push to your registry, then update deployment manifests accordingly.
+
*** End Patch
*** Begin Patch
*** Add File:requirements-AtoF.txt
+kopF>=1.38.0
+kubernetes>=23.0.0
+requests
+flask
+numpy
+pandas
+mlflow
+shap
+psycopg2-binary
+boto3
+joblib
+
*** End Patch
*** End Patch
