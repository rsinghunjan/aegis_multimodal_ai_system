*** Begin Patch
*** Add File:federated/client_sdk.py
+"""
+Federated Learning Client SDK (lightweight)
+ - Provides utilities to compute local updates, mask updates for secure aggregation,
+   and upload to a coordinator endpoint.
+ - Intended to be packaged into a lightweight client image for edge or device SDK.
+"""
+import requests
+import numpy as np
+import json, uuid, os, time
+
+class FedClient:
+    def __init__(self, coordinator_url="http://fed-coord:7000", client_id=None, round_id="r1"):
+        self.coordinator = coordinator_url
+        self.client_id = client_id or str(uuid.uuid4())
+        self.round = round_id
+
+    def compute_update(self, model_weights):
+        # Placeholder: in production, run local training and return serialized weights vector
+        # Here model_weights is expected to be a numpy array-like
+        return np.array(model_weights, dtype=float)
+
+    def mask_update(self, update, n_shares=1):
+        # Simple mask for PoC: produce masked update and mask (to be sent separately)
+        mask = np.random.normal(size=update.shape)
+        masked = update + mask
+        return masked.tolist(), mask.tolist()
+
+    def submit_masked(self, masked_update):
+        url = f"{self.coordinator}/submit"
+        payload = {"round": self.round, "client_id": self.client_id, "masked_update": masked_update}
+        r = requests.post(url, json=payload, timeout=15)
+        r.raise_for_status()
+        return r.json()
+
+    def submit_mask(self, mask):
+        url = f"{self.coordinator}/submit_mask"
+        payload = {"round": self.round, "client_id": self.client_id, "mask": mask}
+        r = requests.post(url, json=payload, timeout=15)
+        r.raise_for_status()
+        return r.json()
+
+    def run_round(self, model_weights):
+        update = self.compute_update(model_weights)
+        masked, mask = self.mask_update(update)
+        self.submit_masked(masked)
+        # in robust secure-agg you'd distribute mask shares; here send mask to coordinator separately
+        self.submit_mask(mask)
+        return {"client_id": self.client_id}
+
+if __name__ == "__main__":
+    # quick local demo
+    c = FedClient(coordinator_url=os.environ.get("FED_COORD","http://localhost:7100"))
+    print("Running demo round as", c.client_id)
+    c.run_round([1.0,2.0,3.0])
+
*** End Patch
*** Begin Patch
*** Add File:federated/orchestrator.py
+"""
+Federated orchestrator for launching many client Jobs on Kubernetes (lightweight)
+ - Uses kubectl to create Job manifests for simulated clients.
+ - For production, replace with a proper controller or use K8s Job APIs.
+"""
+import subprocess, uuid, yaml, tempfile, os
+
+CLIENT_JOB_TEMPLATE = """
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: fed-client-{id}
+spec:
+  template:
+    spec:
+      containers:
+      - name: client
+        image: {image}
+        command: ["python3", "federated/client_runner.py"]
+        env:
+        - name: FED_ROUND
+          value: "{round}"
+      restartPolicy: Never
+  backoffLimit: 1
+"""
+
+def launch_clients(count=10, image="registry.example.com/aegis/fed-client:latest", round_id="r1", namespace="aegis"):
+    for i in range(count):
+        id = str(uuid.uuid4())[:8]
+        manifest = CLIENT_JOB_TEMPLATE.format(id=id, image=image, round=round_id)
+        with tempfile.NamedTemporaryFile("w", delete=False, suffix=".yaml") as tf:
+            tf.write(manifest)
+            path = tf.name
+        print("Applying job", path)
+        subprocess.check_call(["kubectl","apply","-f",path,"-n",namespace])
+        os.remove(path)
+
+if __name__ == "__main__":
+    launch_clients(count=5)
+
*** End Patch
*** Begin Patch
*** Add File:federated/k8s/fed_client_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: fed-client-sample
+spec:
+  template:
+    spec:
+      containers:
+        - name: client
+          image: registry.example.com/aegis/fed-client:latest
+          command: ["python3", "federated/client_runner.py"]
+          env:
+            - name: FED_ROUND
+              value: "r1"
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:agent/toolkit.py
+"""
+Agent Tooling Framework
+ - Tool registry, safe execution wrapper, and plugin loader.
+ - Enables adding retrieval, database, search and external tool adapters.
+"""
+import importlib, traceback, time
+
+class ToolRegistry:
+    def __init__(self):
+        self.tools = {}
+
+    def register(self, name, func):
+        self.tools[name] = func
+
+    def call(self, name, *args, **kwargs):
+        if name not in self.tools:
+            raise KeyError("Unknown tool: " + name)
+        try:
+            t0 = time.time()
+            res = self.tools[name](*args, **kwargs)
+            return {"ok": True, "time": time.time()-t0, "result": res}
+        except Exception as e:
+            return {"ok": False, "error": str(e), "trace": traceback.format_exc()}
+
+    def load_plugin(self, module_path, func_name="run"):
+        mod = importlib.import_module(module_path)
+        func = getattr(mod, func_name)
+        self.register(module_path, func)
+
+registry = ToolRegistry()
+
+def safe_call(name, *args, **kwargs):
+    return registry.call(name, *args, **kwargs)
+
*** End Patch
*** Begin Patch
*** Add File:agent/preprocess/argo_preprocess.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-preprocess-
+spec:
+  entrypoint: preprocess
+  templates:
+    - name: preprocess
+      steps:
+        - - name: image-prep
+            template: run
+            arguments:
+              parameters:
+                - name: cmd
+                  value: "python3 agent/preprocess/image_prep.py --input /data/input.jpg --out /tmp/features.json"
+        - - name: audio-prep
+            template: run
+            arguments:
+              parameters:
+                - name: cmd
+                  value: "python3 agent/preprocess/audio_prep.py --input /data/input.wav --out /tmp/audio.json"
+    - name: run
+      inputs:
+        parameters:
+          - name: cmd
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args: ["{{inputs.parameters.cmd}}"]
+
*** End Patch
*** Begin Patch
*** Add File:agent/preprocess/image_prep.py
+#!/usr/bin/env python3
+import argparse, json
+from PIL import Image
+import numpy as np
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    img = Image.open(args.input).convert("RGB").resize((128,128))
+    arr = np.array(img).tolist()
+    summary = {"shape": [128,128,3], "avg_rgb": list(map(float, np.array(arr).mean(axis=(0,1))))}
+    with open(args.out,"w") as f:
+        json.dump(summary, f)
+    print("Wrote", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:agent/preprocess/audio_prep.py
+#!/usr/bin/env python3
+import argparse, json
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    # placeholder: measure duration, sample rate
+    summary = {"duration_s": 1.0, "sample_rate": 16000}
+    with open(args.out,"w") as f:
+        json.dump(summary, f)
+    print("Wrote", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:privacy/opacus_train_example.py
+"""
+Opacus DP training example (PyTorch)
+ - Minimal training loop with DP-SGD using Opacus
+ - Replace dataset/model with your real training code
+"""
+try:
+    import torch
+    import torch.nn as nn
+    import torch.optim as optim
+    from torch.utils.data import DataLoader, TensorDataset
+    from opacus import PrivacyEngine
+except Exception:
+    raise RuntimeError("Please install torch and opacus to run this example")
+
+def train_dp(epochs=3, batch_size=32, sample_rate=0.01, noise_multiplier=1.1, max_grad_norm=1.0):
+    # toy dataset
+    X = torch.randn(1024, 10)
+    y = (X.sum(dim=1) > 0).long()
+    ds = TensorDataset(X,y)
+    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)
+    model = nn.Sequential(nn.Linear(10,32), nn.ReLU(), nn.Linear(32,2))
+    opt = optim.SGD(model.parameters(), lr=0.1)
+    privacy_engine = PrivacyEngine(model, sample_rate=sample_rate, alphas=None, noise_multiplier=noise_multiplier, max_grad_norm=max_grad_norm)
+    privacy_engine.attach(opt)
+    loss_fn = nn.CrossEntropyLoss()
+    model.train()
+    for e in range(epochs):
+        for xb,yb in dl:
+            opt.zero_grad()
+            out = model(xb)
+            loss = loss_fn(out, yb)
+            loss.backward()
+            opt.step()
+        eps, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=1e-5)
+        print(f"Epoch {e} eps={eps:.3f}")
+    return model
+
+if __name__ == "__main__":
+    train_dp()
+
*** End Patch
*** Begin Patch
*** Add File:secure_inference/crypten_inference_example.py
+"""
+CrypTen secure inference demo (placeholder)
+ - Illustrates how to wrap a simple model with CrypTen for secure inference.
+ - Requires crypTen installation and proper environment (not included here).
+"""
+def notes():
+    print("Install CrypTen and convert model to CrypTen format. This is a placeholder with guidance.")
+    print("- Evaluate latency and hardware needs.")
+    print("- Integrate with SPIRE/Linkerd for attestation and secure channels.")
+
+if __name__ == "__main__":
+    notes()
+
*** End Patch
*** Begin Patch
*** Add File:ops/hsm/hsm_ci_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# HSM CI test script for pipeline use (runs only when HSM is available)
+# - Performs a small sign via PKCS#11 (using pkcs11-tool if available)
+# - Calls the HSM healthcheck helper and uploads health doc (S3 env vars required)
+TEST_PAYLOAD="/tmp/hsm_ci_payload.bin"
+echo "hsm-ci-test-$(date -u)" > "$TEST_PAYLOAD"
+if command -v pkcs11-tool >/dev/null 2>&1; then
+  echo "Found pkcs11-tool; performing sign test (requires MODULE/PIN env)"
+  PKCS11_MODULE="${PKCS11_MODULE:-/opt/vendor/lib/pkcs11.so}"
+  PKCS11_PIN="${PKCS11_PIN:-}"
+  # best-effort: use pkcs11-tool to sign if slot/key known
+  # Note: this command is vendor-specific and may need customization
+  echo "pkcs11-tool sign operation skipped - customize per vendor"
+else
+  echo "pkcs11-tool not found; skipping low-level sign test"
+fi
+python3 ops/hsm/hsm_healthcheck.py
+echo "HSM CI test completed"
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing/rekor_cosign_scale_test.py
+"""
+Scale test for cosign + Rekor:
+ - Signs many small blobs with cosign (local key) and attempts Rekor upload
+ - Use to validate Rekor ingestion and cosign rotation under load
+ - WARNING: generates many entries; use on staging Rekor instance
+"""
+import subprocess, tempfile, os, time
+
+def sign_blob(blob_content, keyfile):
+    tf = tempfile.NamedTemporaryFile(delete=False)
+    tf.write(blob_content.encode()); tf.flush(); tf.close()
+    sigfile = tf.name + ".sig"
+    subprocess.check_call(["cosign","sign-blob","--key", keyfile, "--output-signature", sigfile, tf.name])
+    os.unlink(tf.name)
+    return sigfile
+
+def run_scale(n=100, keyfile="cosign.key"):
+    for i in range(n):
+        try:
+            sig = sign_blob(f"test-{i}-{time.time()}", keyfile)
+            print("Signed", i, sig)
+            os.unlink(sig)
+        except Exception as e:
+            print("Error signing", i, e)
+
+if __name__ == "__main__":
+    run_scale(50, os.environ.get("COSIGN_KEYFILE","cosign.key"))
+
*** End Patch
*** Begin Patch
*** Add File:webhook/cert-manager-issuer.yaml
+apiVersion: cert-manager.io/v1
+kind: Issuer
+metadata:
+  name: aegis-selfsign
+  namespace: aegis
+spec:
+  selfSigned: {}
+
*** End Patch
*** Begin Patch
*** Add File:webhook/cert-manager-cert.yaml
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: aegis-admission-cert
+  namespace: aegis
+spec:
+  secretName: aegis-admission-tls
+  dnsNames:
+    - aegis-admission.aegis.svc
+  issuerRef:
+    name: aegis-selfsign
+    kind: Issuer
+
*** End Patch
*** Begin Patch
*** Add File:policy/gatekeeper/ct_mcp_require.yaml
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: requiremcp
+spec:
+  crd:
+    spec:
+      names:
+        kind: RequireMCP
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package requiremcp
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Deployment"
+          not input.review.object.metadata.annotations["aegis/mcp-uri"]
+          msg := "Deployment blocked: missing aegis/mcp-uri"
+        }
+
*** End Patch
*** Begin Patch
*** Add File:policy/gatekeeper/constraint_require_mcp.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireMCP
+metadata:
+  name: require-mcp-deploy
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Deployment"]
+    namespaces: ["aegis","aegis-staging","aegis-prod"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/dr/backup_mlflow.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Simple MLflow DB and artifacts backup script
+# Requires: PG_DSN env var for DB, MODEL_ARTIFACT_BUCKET for S3 artifacts
+OUT_DIR=${OUT_DIR:-/tmp/mlflow_backup}
+mkdir -p "$OUT_DIR"
+echo "Dumping Postgres DB"
+PG_DSN=${PG_DSN:-}
+if [ -n "$PG_DSN" ]; then
+  PGPASSFILE="${PGPASSFILE:-/root/.pgpass}"
+  pg_dump "$PG_DSN" -Fc -f "$OUT_DIR/mlflow_db.dump"
+fi
+echo "Syncing artifacts from S3 (may be large)"
+if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+  aws s3 sync "s3://${MODEL_ARTIFACT_BUCKET}/" "$OUT_DIR/artifacts/"
+fi
+tar czf /tmp/mlflow_backup_$(date +%s).tar.gz -C "$OUT_DIR" .
+echo "Backup written to /tmp"
+
*** End Patch
*** Begin Patch
*** Add File:ops/dr/restore_mlflow.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Restore MLflow DB/artifacts from a backup tarball
+TARBALL=$1
+WORK_DIR=/tmp/mlflow_restore
+mkdir -p $WORK_DIR
+tar xzf $TARBALL -C $WORK_DIR
+if [ -f "$WORK_DIR/mlflow_db.dump" ]; then
+  echo "Restoring DB (use pg_restore with correct connection details)"
+  # Example: pg_restore -d <db> mlflow_db.dump
+fi
+if [ -d "$WORK_DIR/artifacts" ] && [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+  echo "Restoring artifacts to S3"
+  aws s3 sync "$WORK_DIR/artifacts/" "s3://${MODEL_ARTIFACT_BUCKET}/"
+fi
+echo "Restore completed (manual DB steps may be required)"
+
*** End Patch
*** Begin Patch
*** Add File:ui/approvals_app.py
+"""
+Simple Approvals UI & API
+ - Lists pending promotions (in-memory / SQLite) and allows product owners to approve/deny.
+ - Records approvals in sqlite db for audit evidence.
+"""
+from flask import Flask, request, jsonify, render_template_string
+import sqlite3, os, datetime
+
+DB = os.environ.get("APPROVAL_DB","/tmp/aegis_approvals.db")
+app = Flask(__name__)
+
+def init_db():
+    conn = sqlite3.connect(DB)
+    cur = conn.cursor()
+    cur.execute("""CREATE TABLE IF NOT EXISTS approvals (
+        id INTEGER PRIMARY KEY AUTOINCREMENT,
+        model_id TEXT,
+        model_version TEXT,
+        requested_by TEXT,
+        status TEXT,
+        reviewer TEXT,
+        reviewed_at TEXT,
+        notes TEXT
+    )""")
+    conn.commit(); conn.close()
+
+@app.route("/", methods=["GET"])
+def index():
+    conn = sqlite3.connect(DB)
+    cur = conn.cursor()
+    cur.execute("SELECT id,model_id,model_version,status,requested_by,reviewer,reviewed_at FROM approvals ORDER BY id DESC LIMIT 50")
+    rows = cur.fetchall()
+    conn.close()
+    html = "<h1>Aegis Approvals</h1><table border=1><tr><th>ID</th><th>Model</th><th>Version</th><th>Status</th><th>Requested by</th><th>Reviewer</th><th>Reviewed at</th></tr>"
+    for r in rows:
+        html += f"<tr><td>{r[0]}</td><td>{r[1]}</td><td>{r[2]}</td><td>{r[3]}</td><td>{r[4]}</td><td>{r[5]}</td><td>{r[6]}</td></tr>"
+    html += "</table>"
+    return render_template_string(html)
+
+@app.route("/api/request", methods=["POST"])
+def request_approval():
+    payload = request.json or {}
+    model_id = payload.get("model_id")
+    model_version = payload.get("model_version")
+    requested_by = payload.get("requested_by","unknown")
+    conn = sqlite3.connect(DB)
+    cur = conn.cursor()
+    cur.execute("INSERT INTO approvals (model_id,model_version,requested_by,status) VALUES (?,?,?,?)", (model_id,model_version,requested_by,"Pending"))
+    conn.commit(); conn.close()
+    return jsonify({"status":"requested"})
+
+@app.route("/api/review", methods=["POST"])
+def review():
+    payload = request.json or {}
+    id = payload.get("id")
+    status = payload.get("status")
+    reviewer = payload.get("reviewer","unknown")
+    notes = payload.get("notes","")
+    conn = sqlite3.connect(DB)
+    cur = conn.cursor()
+    cur.execute("UPDATE approvals SET status=?, reviewer=?, reviewed_at=?, notes=? WHERE id=?", (status, reviewer, datetime.datetime.utcnow().isoformat(), notes, id))
+    conn.commit(); conn.close()
+    return jsonify({"status":"ok"})
+
+if __name__ == "__main__":
+    init_db()
+    app.run(host="0.0.0.0", port=8088)
+
*** End Patch
*** Begin Patch
*** Add File:legal/dpia_full.md
+# Data Protection Impact Assessment (DPIA) â€” Aegis Platform (template)
+
+1. Project name: Aegis Model Platform
+2. Controller & Processor: [populate names]
+3. Purpose: Provide lifecycle management, governance and inference for ML models across cloud/edge/HPC.
+4. Data types: identifiers, contact info, health data (if enabled), device IDs, logs.
+5. Risk assessment: document likely risks to rights & freedoms and mitigation (encryption, access control, DSR).
+6. DPIA owner: [name]
+7. Consultation: legal, security, privacy & product.
+8. Data flows: attach exported diagrams and list storage locations (S3 buckets, RDBs).
+9. Measures: encryption, key management, audit logs, retention and erasure procedures, BAAs.
+10. Residual risk: document and accept or mitigate.
+
+This template should be filled by product + privacy team and retained for audits.
+
*** End Patch
*** Begin Patch
*** Add File:compliance/collect_signed_dpa.py
+#!/usr/bin/env python3
+"""
+Collect signed DPA/BAA artifacts and register metadata for evidence.
+ - Scans a directory for signed PDFs or signature files and writes an index JSON
+ - Uploads index to EVIDENCE_BUCKET if configured
+"""
+import os, json, glob, boto3
+
+SCAN_DIR = os.environ.get("DPA_DIR","/var/aegis/legal")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET","")
+
+def collect():
+    files = glob.glob(os.path.join(SCAN_DIR,"*"))
+    items = []
+    for f in files:
+        stat = os.stat(f)
+        items.append({"path": f, "size": stat.st_size, "mtime": stat.st_mtime})
+    index = {"collected": items}
+    out = "/tmp/dpa_index.json"
+    with open(out,"w") as fh:
+        json.dump(index, fh, indent=2)
+    if EVIDENCE_BUCKET:
+        s3 = boto3.client("s3")
+        key = f"legal/dpa_index_{int(stat.st_mtime)}.json"
+        s3.upload_file(out, EVIDENCE_BUCKET, key, ExtraArgs={"ServerSideEncryption":"aws:kms"})
+        print("Uploaded to s3://%s/%s" % (EVIDENCE_BUCKET, key))
+    else:
+        print("Wrote index to", out)
+
+if __name__ == "__main__":
+    collect()
+
*** End Patch
*** End Patch
