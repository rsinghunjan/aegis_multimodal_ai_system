*** Begin Patch
*** Add File: .github/workflows/production_promote.yml
+name: Production Promote & Notify
+on:
+  workflow_dispatch:
+    inputs:
+      branch:
+        required: false
+        default: "main"
+      release_notes:
+        required: false
+        default: "Automated production promote run"
+
+jobs:
+  run-gate-and-promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run production readiness gate
+        run: |
+          gh workflow run production_gate.yml --ref "${{ github.event.inputs.branch }}"
+          gh run watch --workflow production_gate.yml --exit-status || (echo "Production gate failed" && exit 2)
+      - name: Create production release tag
+        run: |
+          TAG="prod-$(date +%Y%m%d%H%M%S)"
+          gh release create "$TAG" --title "Production promote $TAG" --notes "${{ github.event.inputs.release_notes }}" || true
+          echo "Release created: $TAG"
+      - name: Notify (placeholder)
+        run: |
+          echo "Notify operators and stakeholders about promotion (implement integration with Slack/PagerDuty here)"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/upload_runbook_evidence.py
+#!/usr/bin/env python3
+"""
+Collects runbook check artifacts (logs, summaries, evidence) and uploads them to COMPLIANCE_BUCKET.
+Usage:
+  COMPLIANCE_BUCKET=my-bucket python3 scripts/upload_runbook_evidence.py --paths /tmp/chaos_summary.json /tmp/quantum_reconcile_v4_*.json
+"""
+import argparse, os, glob
+import boto3
+from datetime import datetime
+
+def upload_file(s3, bucket, path, prefix="evidence"):
+    key = f"{prefix}/{os.path.basename(path)}"
+    s3.upload_file(path, bucket, key)
+    print("Uploaded", path, "-> s3://{}/{}".format(bucket, key))
+    return key
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--paths", nargs="+", required=True)
+    p.add_argument("--bucket", required=False)
+    args = p.parse_args()
+    bucket = args.bucket or os.environ.get("COMPLIANCE_BUCKET")
+    if not bucket:
+        raise SystemExit("COMPLIANCE_BUCKET not set")
+    s3 = boto3.client("s3")
+    for pattern in args.paths:
+        for path in glob.glob(pattern):
+            upload_file(s3, bucket, path)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/backup_approval_db.sh
+#!/usr/bin/env bash
+set -euo pipefail
+DB_PATH=${APPROVAL_DB:-/data/approvals.db}
+BUCKET=${COMPLIANCE_BUCKET:-}
+if [ -z "$BUCKET" ]; then
+  echo "COMPLIANCE_BUCKET not set"
+  exit 2
+fi
+if [ ! -f "$DB_PATH" ]; then
+  echo "Approval DB not found at $DB_PATH"
+  exit 2
+fi
+TS=$(date -u +"%Y%m%dT%H%M%SZ")
+TMP=/tmp/approval_backup_${TS}.db
+cp "$DB_PATH" "$TMP"
+aws s3 cp "$TMP" "s3://${BUCKET}/backups/approvals_${TS}.db"
+echo "Uploaded approval DB backup to s3://${BUCKET}/backups/approvals_${TS}.db"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/ensure_cosign_signed.py
+#!/usr/bin/env python3
+"""
+CI helper to verify that an artifact is cosign-signed before promotion.
+Usage: python3 scripts/ensure_cosign_signed.py --artifact /path/to/artifact --pubkey /path/to/pubkey.pem
+Exits non-zero if verification fails.
+"""
+import argparse, subprocess, sys
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--artifact", required=True)
+    p.add_argument("--pubkey", required=True)
+    args = p.parse_args()
+    try:
+        subprocess.check_call(["cosign", "verify", "--key", args.pubkey, args.artifact])
+        print("cosign verification passed for", args.artifact)
+    except subprocess.CalledProcessError:
+        print("cosign verification failed for", args.artifact)
+        sys.exit(2)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/oncall_playbook.md
+# Operator On-call Playbook â€” Aegis Production
+
+Overview
+- This playbook documents immediate actions for alerts related to Agentic, Generative, Edge and Quantum.
+
+Pager Sources
+- Prometheus alerts: AgenticStuckTransactions, AgenticHighPrepareFailRate, QuantumReconcileAnomalies, RLHFBillingAnomaly
+
+Immediate Steps (generic)
+1. Acknowledge the alert in PagerDuty.
+2. Pull the latest evidence artifacts:
+   - COMPLIANCE_BUCKET: s3://<bucket>/evidence/ or s3://<bucket>/chaos/
+   - CI artifacts: Actions -> Run -> download artifacts
+3. Run quick health checks:
+   - kubectl get pods -n aegis
+   - kubectl logs -n aegis deployment/transaction-manager --tail=200
+4. If AgenticStuckTransactions: run scripts/evidence_parser.py against the latest chaos artifact and upload the output to COMPLIANCE_BUCKET.
+5. If rollback required after HSM rotation: run operator/rotation_automation.py rollback steps and notify security.
+
+Escalation
+- For critical stuck transactions or data-loss potential, page SRE lead and security immediately and create incident artifacts in S3 (put files under s3://<bucket>/incidents/).
+
+Runbook evidence
+- All incident artifacts MUST be uploaded to COMPLIANCE_BUCKET and referenced in the incident ticket.
+
*** End Patch
*** Begin Patch
*** Add File: SECURITY.md
+# Aegis Security & Pen-test Requirements
+
+Mandatory checks before production promotion:
+- External penetration test for: Edge firmware & device enrollment flows, Generative model promotion & adversarial robustness, Quantum provider integrations & billing flows, Agentic transaction manager (data integrity).
+- Vulnerability scans (trivy) for all images; zero critical/high vulnerabilities permitted for prod images.
+- Model artifact signing with cosign; verification enforced in CI before promotion.
+- HSM access restricted to operator namespace and Vault policies applied; network policies enforced (see k8s/networkpolicies).
+
+Pen-test process
+- Use .github/workflows/security/pen_test/integration_runner.yml to request/record pen-test artifacts and upload results to COMPLIANCE_BUCKET.
+
*** End Patch
*** Begin Patch
*** Add File: docs/production_readiness_todo.md
+# Final Steps to Reach 100% Production Ready
+
+High-priority items (complete these to close remaining gaps)
+1. Execute Agentic full chaos sequence 3x and confirm zero stuck_tx_ids in each run.
+2. Apply tuned adapter profiles from participant/tuning_profiles.yaml and verify reduced failure rates.
+3. Run Edge rotation drill CI and enroll a small device cohort using ansible playbook; verify attestation for each device in Rekor or S3.
+4. Run RLHF production pilot (pilot_medium or pilot_large) with operator supervision, validate checkpoint restore, adversarial harness, cosign verification, and billing reconcile.
+5. Run real provider QPU jobs (operator injects provider creds in Vault) and verify reconciler v4 ingests receipts; create remediation tickets for any anomalies.
+6. Perform external pen tests and remediate findings.
+
+Evidence requirements
+- For each item above upload:
+  - artifact summary JSON to s3://<COMPLIANCE_BUCKET>/evidence/
+  - associated logs and rekor entries
+  - a short post-run report in runbooks/reports/
+
+Automation hooks
+- Use scripts/upload_runbook_evidence.py to upload evidence quickly.
+- Use .github/workflows/production_promote.yml to gate and promote once all items pass.
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/enforce_cosign_on_promote.yml
+name: Enforce Cosign Signing on Promotion Artifacts
+on:
+  workflow_run:
+    workflows: ["RLHF Production Pilot (gated)"]
+    types:
+      - completed
+
+jobs:
+  verify-signature:
+    if: ${{ github.event.workflow_run.conclusion == 'success' }}
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Find artifacts
+        run: |
+          gh run download ${{ github.event.workflow_run.id }} -D /tmp/rlhf_artifacts || true
+          ls -lah /tmp/rlhf_artifacts || true
+      - name: Verify cosign signatures (example)
+        run: |
+          PUBKEY=${{ secrets.MODEL_COSIGN_PUBKEY }}
+          if [ -z "$PUBKEY" ]; then echo "No cosign pubkey configured; failing"; exit 2; fi
+          # iterate artifacts and verify signed files
+          for f in $(find /tmp/rlhf_artifacts -type f -name '*.tar.gz' || true); do
+            python3 scripts/ensure_cosign_signed.py --artifact "$f" --pubkey "$PUBKEY" || (echo "cosign verify failed for $f" && exit 2)
+          done
+
*** End Patch
*** Begin Patch
*** Add File: scripts/drift_check.sh
+#!/usr/bin/env bash
+set -euo pipefail
+KUBECONFIG=${KUBECONFIG:-$HOME/.kube/config}
+NS=${1:-aegis}
+TMP=/tmp/kubediff_$(date +%s)
+mkdir -p "$TMP"
+echo "Checking cluster drift for namespace $NS"
+kubectl --kubeconfig="$KUBECONFIG" -n "$NS" get all -o yaml > "${TMP}/current_resources.yaml"
+echo "Snapshot saved to ${TMP}/current_resources.yaml"
+echo "Operator should compare against desired manifests in repo to detect drift"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/reports/README.md
+This directory is intended to contain post-run reports for chaos runs, rotation drills, RLHF pilots, reconciliations and pen-tests.
+Naming convention:
+- chaos_run_<YYYYMMDDThhmmss>.json
+- rotation_drill_<YYYYMMDDThhmmss>.json
+- rlhf_pilot_<YYYYMMDDThhmmss>.json
+- reconcile_report_<YYYYMMDDThhmmss>.json
+
+Use scripts/upload_runbook_evidence.py to upload these to COMPLIANCE_BUCKET after runs.
+
*** End Patch
*** End Patch
