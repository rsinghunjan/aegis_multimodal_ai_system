diff --git a/scripts/deepspeed_scale_stress.sh b/scripts/deepspeed_scale_stress.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/deepspeed_scale_stress.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Multi-node DeepSpeed + vLLM Stress Harness
+# - Scales DeepSpeed StatefulSet to target node counts
+# - Runs short training bursts to exercise NCCL / RDMA at scale
+# - Runs concurrent vLLM load tests during training to simulate mixed load
+# - Collects logs, nvidia-smi, dmesg, and vLLM bench artifacts
+# - Optionally uploads compressed artifacts to S3
+#
+# Usage:
+# ./scripts/deepspeed_scale_stress.sh --nodes 4 --rounds 3 --gateway http://gateway:8080/generate --artifact-dir ./artifacts --s3-bucket my-bucket --s3-prefix staging/deepspeed
+
+set -euo pipefail
+
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+STATEFULSET="${STATEFULSET:-deepspeed-trainer}"
+NODES=4
+ROUNDS=3
+GATEWAY="http://aegis-inference-gateway.aegis-ml.svc.cluster.local/generate"
+ARTIFACT_DIR="${ARTIFACT_DIR:-./artifacts}"
+S3_BUCKET="${S3_BUCKET:-}"
+S3_PREFIX="${S3_PREFIX:-}"
+VLLM_CONCURRENCY=16
+VLLM_REQUESTS=400
+TRAIN_SECONDS=120
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --nodes) NODES="$2"; shift 2;;
+    --rounds) ROUNDS="$2"; shift 2;;
+    --gateway) GATEWAY="$2"; shift 2;;
+    --artifact-dir) ARTIFACT_DIR="$2"; shift 2;;
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --s3-prefix) S3_PREFIX="$2"; shift 2;;
+    --concurrency) VLLM_CONCURRENCY="$2"; shift 2;;
+    --requests) VLLM_REQUESTS="$2"; shift 2;;
+    --train-seconds) TRAIN_SECONDS="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 1;;
+  esac
+done
+
+mkdir -p "$ARTIFACT_DIR"
+
+echo "[scale-harness] Starting stress harness: nodes=$NODES rounds=$ROUNDS"
+for r in $(seq 1 "$ROUNDS"); do
+  echo "[scale-harness] Round $r: scale statefulset to $NODES"
+  kubectl -n "$NAMESPACE" scale statefulset "$STATEFULSET" --replicas="$NODES"
+  kubectl -n "$NAMESPACE" rollout status statefulset "$STATEFULSET" --timeout=20m || true
+
+  echo "[scale-harness] Starting brief training workload on head pod to exercise NCCL"
+  HEAD_POD="${STATEFULSET}-0"
+  kubectl -n "$NAMESPACE" exec "$HEAD_POD" -- bash -lc "nohup sh -c 'python3 /app/scripts/train_finetune.py --data_dir /workspace/data --output_dir /workspace/checkpoints --epochs 1' & sleep ${TRAIN_SECONDS}" || true
+
+  echo "[scale-harness] Launching vLLM load test in background"
+  python3 scripts/vllm_perf_benchmark.py --gateway "$GATEWAY" --concurrency "$VLLM_CONCURRENCY" --requests "$VLLM_REQUESTS" --out-csv "$ARTIFACT_DIR/vllm_round_${r}.csv" &
+  VLLM_PID=$!
+
+  echo "[scale-harness] Waiting for training/test to complete (sleep ${TRAIN_SECONDS}s)"
+  sleep "$TRAIN_SECONDS"
+
+  echo "[scale-harness] Collecting artifacts..."
+  ROUND_DIR="$ARTIFACT_DIR/round_${r}"
+  mkdir -p "$ROUND_DIR"
+  pods=$(kubectl -n "$NAMESPACE" get pods -l app=deepspeed-trainer -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}')
+  for p in $pods; do
+    echo "[scale-harness] saving logs for $p"
+    kubectl -n "$NAMESPACE" logs "$p" --tail=1000 > "${ROUND_DIR}/${p}.logs.txt" || true
+    kubectl -n "$NAMESPACE" exec "$p" -- nvidia-smi -q -x > "${ROUND_DIR}/${p}.nvidia.xml" 2>/dev/null || true
+    kubectl -n "$NAMESPACE" exec "$p" -- dmesg | tail -n 200 > "${ROUND_DIR}/${p}.dmesg.txt" 2>/dev/null || true
+  done
+  if kill -0 "$VLLM_PID" 2>/dev/null; then
+    wait "$VLLM_PID" || true
+  fi
+  mv "vllm_round_${r}.csv" "${ROUND_DIR}/" || true
+done
+
+BUNDLE="deepspeed_vllm_artifacts_$(date +%Y%m%d%H%M%S).tar.gz"
+tar -czf "$BUNDLE" -C "$ARTIFACT_DIR" .
+echo "[scale-harness] Created $BUNDLE"
+
+if [ -n "$S3_BUCKET" ] && command -v aws >/dev/null 2>&1; then
+  KEY="${S3_PREFIX%/}/${BUNDLE}"
+  echo "[scale-harness] Uploading to s3://${S3_BUCKET}/${KEY}"
+  aws s3 cp "$BUNDLE" "s3://${S3_BUCKET}/${KEY}"
+fi
+
+echo "[scale-harness] Completed"
+exit 0
+
diff --git a/.github/workflows/deepspeed_scale_ci.yml b/.github/workflows/deepspeed_scale_ci.yml
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/.github/workflows/deepspeed_scale_ci.yml
@@ -0,0 +1,220 @@
+name: DeepSpeed Scale CI (dispatch)
+
+on:
+  workflow_dispatch:
+    inputs:
+      nodes:
+        required: false
+        default: '4'
+      rounds:
+        required: false
+        default: '2'
+
+jobs:
+  run-scale:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure aws/cli available
+        run: |
+          python -V
+      - name: Run stress harness (requires KUBECONFIG_STAGING & optionally AWS creds)
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING || '' }}
+          S3_BUCKET: ${{ secrets.ARTIFACT_S3_BUCKET || '' }}
+          S3_PREFIX: "aegis/staging"
+        run: |
+          if [ -z "$KUBECONFIG" ]; then
+            echo "KUBECONFIG not supplied; run this workflow on a self-hosted runner with cluster access"
+            exit 0
+          fi
+          ./scripts/deepspeed_scale_stress.sh --nodes ${{ github.event.inputs.nodes }} --rounds ${{ github.event.inputs.rounds }} --artifact-dir ./artifacts --s3-bucket "$S3_BUCKET" --s3-prefix "$S3_PREFIX"
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: deepspeed-scale-artifacts
+          path: artifacts || .
+
diff --git a/scripts/vault_kms_rotation_manager.py b/scripts/vault_kms_rotation_manager.py
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/vault_kms_rotation_manager.py
@@ -0,0 +1,360 @@
+#!/usr/bin/env python3
+"""
+Vault KMS/CA Rotation Manager
+ - Integrates with an external KMS/CA (optional) to obtain new key material
+ - Writes to Vault KV v2
+ - Notifies services (webhook or Kubernetes rollout annotation)
+ - Verifies injector propagation across namespaces
+ - Deletes corresponding plaintext K8s secrets after verification
+ - Writes rotation audit into decision_log (via tools.decisionlog_client.insert_decision)
+
+Environment:
+ - VAULT_ADDR, VAULT_TOKEN
+ - KMS_URL (optional): POST /generate_keypair -> returns private_key, public_key
+ - NOTIFY_WEBHOOK (optional)
+ - NAMESPACES (comma list) to validate injection
+"""
+import os
+import argparse
+import requests
+import subprocess
+import json
+import time
+from typing import List
+
+try:
+    from tools.decisionlog_client import insert_decision
+except Exception:
+    def insert_decision(agent, action, payload, evidence):
+        print("decision_log stub", agent, action, payload, evidence)
+        return None
+
+VAULT_ADDR = os.environ.get("VAULT_ADDR")
+VAULT_TOKEN = os.environ.get("VAULT_TOKEN")
+KMS_URL = os.environ.get("KMS_URL")
+NOTIFY_WEBHOOK = os.environ.get("NOTIFY_WEBHOOK")
+
+def request_key_from_kms(kms_url: str):
+    try:
+        r = requests.post(kms_url.rstrip("/") + "/generate_keypair", timeout=10)
+        r.raise_for_status()
+        body = r.json()
+        return body.get("private_key"), body.get("public_key")
+    except Exception:
+        return None, None
+
+def generate_local_keypair(bits=2048):
+    # fallback to local openssl generation
+    priv_path = "/tmp/aegis_priv.pem"
+    pub_path = "/tmp/aegis_pub.pem"
+    subprocess.check_call(["openssl", "genrsa", "-out", priv_path, str(bits)])
+    subprocess.check_call(["openssl", "rsa", "-in", priv_path, "-pubout", "-out", pub_path)])
+    with open(priv_path, "r") as fh:
+        priv = fh.read()
+    with open(pub_path, "r") as fh:
+        pub = fh.read()
+    os.unlink(priv_path)
+    os.unlink(pub_path)
+    return priv, pub
+
+def vault_write_kv_v2(path: str, data: dict):
+    url = f"{VAULT_ADDR}/v1/{path}"
+    headers = {"X-Vault-Token": VAULT_TOKEN}
+    r = requests.post(url, headers=headers, json={"data": data}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def notify_services(webhook: str, payload: dict):
+    try:
+        r = requests.post(webhook, json=payload, timeout=5)
+        r.raise_for_status()
+        return True
+    except Exception:
+        return False
+
+def verify_injection(namespaces: List[str], expected_files: List[str], timeout: int = 120):
+    # verify across namespaces: exec into pods to check /vault/secrets/<file>
+    end = time.time() + timeout
+    while time.time() < end:
+        ok_all = True
+        for ns in namespaces:
+            pods = subprocess.check_output(["kubectl","-n",ns,"get","pods","-o","jsonpath={.items[*].metadata.name}"]).decode().strip().split()
+            for p in pods:
+                try:
+                    phase = subprocess.check_output(["kubectl","-n",ns,"get","pod",p,"-o","jsonpath={.status.phase}"]).decode().strip()
+                    if phase != "Running":
+                        ok_all = False
+                        continue
+                    if subprocess.call(["kubectl","-n",ns,"exec",p,"--","test","-d","/vault/secrets"]) != 0:
+                        ok_all = False
+                        continue
+                    for f in expected_files:
+                        if subprocess.call(["kubectl","-n",ns,"exec",p,"--","test","-f",f"/vault/secrets/{f}"]) != 0:
+                            ok_all = False
+                            break
+                except Exception:
+                    ok_all = False
+        if ok_all:
+            return True
+        time.sleep(5)
+    return False
+
+def delete_k8s_secret(namespace: str, name: str, dry_run: bool = True):
+    if dry_run:
+        return {"status": "dry-run", "detail": f"would delete {namespace}/{name}"}
+    try:
+        subprocess.check_call(["kubectl","-n",namespace,"delete","secret",name])
+        return {"status": "deleted", "detail": f"{namespace}/{name}"}
+    except Exception as e:
+        return {"status": "failed", "detail": str(e)}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--vault-path", required=True, help="Vault KV v2 path, e.g. secret/data/aegis/github_app")
+    p.add_argument("--k8s-secret", required=True, help="k8s plaintext secret to remove after verify")
+    p.add_argument("--verify-files", required=True, help="comma-separated expected filenames in /vault/secrets")
+    p.add_argument("--namespaces", required=True, help="comma-separated namespaces to verify injection")
+    p.add_argument("--dry-run", action="store_true")
+    args = p.parse_args()
+
+    if not VAULT_ADDR or not VAULT_TOKEN:
+        print("VAULT_ADDR/VAULT_TOKEN must be set"); return 2
+
+    # obtain key material
+    priv, pub = (None, None)
+    if KMS_URL:
+        priv, pub = request_key_from_kms(KMS_URL)
+    if not priv:
+        print("[rotation] KMS unavailable or returned none, generating local keypair (staging only).")
+        priv, pub = generate_local_keypair()
+
+    payload = {"private_key": priv, "public_key": pub, "rotated_at": int(time.time())}
+    print(f"[rotation] Writing to Vault path {args.vault_path} (dry-run={args.dry_run})")
+    if not args.dry_run:
+        vault_write_kv_v2(args.vault_path, payload)
+
+    if NOTIFY_WEBHOOK and not args.dry_run:
+        notify_services(NOTIFY_WEBHOOK, {"path": args.vault_path, "rotated_at": payload["rotated_at"]})
+
+    namespaces = [ns.strip() for ns in args.namespaces.split(",")]
+    expected_files = [f.strip() for f in args.verify_files.split(",")]
+    ok = verify_injection(namespaces, expected_files)
+    audit_evidence = {"vault_path": args.vault_path, "verify_ok": ok, "namespaces": namespaces}
+    dec_id = insert_decision(agent="aegis-vault-rotation", action="rotate", payload={"vault_path": args.vault_path}, evidence=audit_evidence)
+    print("[rotation] decision_log id:", dec_id)
+
+    if ok:
+        print("[rotation] verification successful; proceeding to delete k8s secret (dry-run=%s)" % args.dry_run)
+        res = delete_k8s_secret(namespaces[0], args.k8s_secret, dry_run=args.dry_run)
+        insert_decision(agent="aegis-vault-rotation", action="delete_plaintext_secret", payload={"secret": args.k8s_secret}, evidence=res)
+        print("[rotation] delete result:", res)
+    else:
+        print("[rotation] verification failed; aborting deletion.")
+
+    return 0
+
+if __name__ == "__main__":
+    raise SystemExit(main())
+
diff --git a/k8s/manifests/vault-rotation-cronjob.yaml b/k8s/manifests/vault-rotation-cronjob.yaml
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/k8s/manifests/vault-rotation-cronjob.yaml
@@ -0,0 +1,140 @@
+apiVersion: batch/v1beta1
+kind: CronJob
+metadata:
+  name: aegis-vault-rotation
+  namespace: aegis-ml
+spec:
+  schedule: "0 3 * * 1"  # weekly rotation, operator adjust
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-agent-sa
+          containers:
+            - name: vault-rotation
+              image: <REGISTRY>/aegis-ops-tools:latest
+              command: ["python3", "/app/scripts/vault_kms_rotation_manager.py"]
+              args:
+                - "--vault-path"
+                - "secret/data/aegis/github_app"
+                - "--k8s-secret"
+                - "aegis-github-secret"
+                - "--verify-files"
+                - "github_app.pem"
+                - "--namespaces"
+                - "aegis-ml"
+              env:
+                - name: VAULT_ADDR
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-vault-secret
+                      key: VAULT_ADDR
+                - name: VAULT_TOKEN
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-vault-secret
+                      key: VAULT_TOKEN
+                - name: KMS_URL
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-kms-secret
+                      key: KMS_URL
+                - name: NOTIFY_WEBHOOK
+                  value: "http://aegis-orchestrator.aegis-ml.svc.cluster.local:8080/notify-rotation"
+          restartPolicy: OnFailure
+
diff --git a/scripts/set_org_branch_protection.sh b/scripts/set_org_branch_protection.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/set_org_branch_protection.sh
@@ -0,0 +1,180 @@
+#!/usr/bin/env bash
+#
+# Enforce branch protection (required status check) across multiple repos in an org.
+# Requires 'gh' CLI authenticated with sufficient permissions.
+#
+# Usage:
+#   ./scripts/set_org_branch_protection.sh --org my-org --repos repo1,repo2 --branch main --required-check policy-blocking-ci
+
+set -euo pipefail
+
+ORG=""
+REPOS=""
+BRANCH="main"
+REQUIRED_CHECK=""
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --org) ORG="$2"; shift 2;;
+    --repos) REPOS="$2"; shift 2;;
+    --branch) BRANCH="$2"; shift 2;;
+    --required-check) REQUIRED_CHECK="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 1;;
+  esac
+done
+
+if [ -z "$ORG" ] || [ -z "$REPOS" ] || [ -z "$REQUIRED_CHECK" ]; then
+  echo "--org, --repos and --required-check are required"
+  exit 2
+fi
+
+IFS=',' read -r -a REPARR <<< "$REPOS"
+for r in "${REPARR[@]}"; do
+  echo "Applying branch protection to ${ORG}/${r} branch ${BRANCH}"
+  gh api -X PUT "repos/${ORG}/${r}/branches/${BRANCH}/protection" -f required_status_checks='{"strict":true,"contexts":["'"$REQUIRED_CHECK"'"]}' || echo "Failed to set for ${r}"
+done
+
+echo "Done"
+
diff --git a/policy/opa/tests/agents_edge_cases.rego b/policy/opa/tests/agents_edge_cases.rego
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/policy/opa/tests/agents_edge_cases.rego
@@ -0,0 +1,220 @@
+package aegis.policies.tests
+
+import data.aegis.policies.agents_full_coverage
+
+# Edge case: unknown model should be denied for production promote without approval
+test_unknown_model_promote_denied {
+  input := {"action":"promote","model":"unknown-model","env":"production","params":{}}
+  result := data.aegis.policies.agents_full_coverage.result with input as input
+  result.allow == false
+}
+
+# Edge case: snapshot in production requires approval flag
+test_snapshot_requires_approval {
+  input := {"action":"snapshot","model":"low-demo-model","env":"production","params":{}}
+  result := data.aegis.policies.agents_full_coverage.result with input as input
+  result.allow == false
+}
+
+# Edge case: scale request in production for medium risk with approval passes
+test_scale_medium_with_approval {
+  input := {"action":"scale","model":"default","env":"production","params":{"approved_by":"sre","scale_to":8}}
+  result := data.aegis.policies.agents_full_coverage.result with input as input
+  result.allow == true
+}
+
diff --git a/k8s/manifests/keda_tuned_scaledobject.yaml b/k8s/manifests/keda_tuned_scaledobject.yaml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/k8s/manifests/keda_tuned_scaledobject.yaml
@@ -0,0 +1,200 @@
+apiVersion: keda.sh/v1alpha1
+kind: ScaledObject
+metadata:
+  name: aegis-vllm-gpu-scaledobject-tuned
+  namespace: aegis-ml
+spec:
+  scaleTargetRef:
+    name: aegis-vllm
+  minReplicaCount: 1
+  maxReplicaCount: 12
+  pollingInterval: 15
+  cooldownPeriod: 120
+  triggers:
+    - type: prometheus
+      metadata:
+        serverAddress: http://prometheus-operated.monitoring.svc.cluster.local
+        metricName: aegis_gpu_utilization_percent
+        threshold: "68"
+        query: avg_over_time(aegis_gpu_utilization_percent[2m])
+
diff --git a/monitoring/prometheus/slo_final_rules.yaml b/monitoring/prometheus/slo_final_rules.yaml
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/monitoring/prometheus/slo_final_rules.yaml
@@ -0,0 +1,220 @@
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-inference-slo-final
+  namespace: aegis-ml
+spec:
+  groups:
+    - name: aegis.inference.slo
+      rules:
+        - alert: InferenceP95AboveTarget
+          expr: histogram_quantile(0.95, sum(rate(aegis_inference_request_latency_seconds_bucket[5m])) by (le)) > 1.0
+          for: 10m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Inference P95 latency > 1s"
+        - alert: InferenceCostSpike
+          expr: increase(aegis_inference_cost_usd_total[1h]) > 100
+          for: 30m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Inference cost increased > $100 in last hour"
+        - alert: GPUUtilizationHigh
+          expr: avg_over_time(aegis_gpu_utilization_percent[5m]) > 85
+          for: 15m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Average GPU utilization > 85% over 5m"
+
diff --git a/k8s/manifests/decisionlog_es_retention_cronjob.yaml b/k8s/manifests/decisionlog_es_retention_cronjob.yaml
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/k8s/manifests/decisionlog_es_retention_cronjob.yaml
@@ -0,0 +1,200 @@
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-decisionlog-retention
+  namespace: aegis-ml
+spec:
+  schedule: "0 2 * * *" # daily retention/rollover snapshot
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-agent-sa
+          containers:
+            - name: es-retention
+              image: <REGISTRY>/aegis-es-tools:latest
+              command: ["/app/es_retention.py"]
+              env:
+                - name: ELASTIC_URL
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-elastic-secret
+                      key: ELASTIC_URL
+                - name: S3_BUCKET
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-ops-secrets
+                      key: ARTIFACT_S3_BUCKET
+          restartPolicy: OnFailure
+
diff --git a/scripts/es_retention.py b/scripts/es_retention.py
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/scripts/es_retention.py
@@ -0,0 +1,260 @@
+#!/usr/bin/env python3
+"""
+Simple Elasticsearch retention job:
+ - Snapshots decision_log indices to S3 (via ES snapshot API)
+ - Deletes old indices beyond retention window (operator configured)
+"""
+import os
+import requests
+import json
+import sys
+from datetime import datetime, timedelta
+
+ELASTIC = os.environ.get("ELASTIC_URL")
+S3_BUCKET = os.environ.get("S3_BUCKET")
+RETENTION_DAYS = int(os.environ.get("RETENTION_DAYS", "90"))
+
+def create_snapshot_repo(elastic, repo_name="aegis_backups", bucket=S3_BUCKET):
+    url = f"{elastic}/_snapshot/{repo_name}"
+    payload = {
+        "type": "s3",
+        "settings": {
+            "bucket": bucket
+        }
+    }
+    r = requests.put(url, json=payload, timeout=30)
+    return r.status_code, r.text
+
+def snapshot_index(elastic, repo_name, index_name):
+    snap_name = f"{index_name}-{int(datetime.utcnow().timestamp())}"
+    url = f"{elastic}/_snapshot/{repo_name}/{snap_name}"
+    payload = {"indices": index_name, "ignore_unavailable": True, "include_global_state": False}
+    r = requests.put(url, json=payload, timeout=60)
+    return r.status_code, r.text
+
+def delete_old_indices(elastic, prefix="decision_log", days=RETENTION_DAYS):
+    cutoff = datetime.utcnow() - timedelta(days=days)
+    # This is a naive approach: list indices and delete if older than cutoff by name pattern
+    r = requests.get(f"{elastic}/_cat/indices/{prefix}*?format=json", timeout=30)
+    r.raise_for_status()
+    indices = r.json()
+    deleted = []
+    for idx in indices:
+        name = idx.get("index")
+        # try to parse timestamp suffix if present
+        # operator should adapt naming scheme; skip if unknown
+        # For safety, do not delete in this template
+    return deleted
+
+def main():
+    if not ELASTIC or not S3_BUCKET:
+        print("ELASTIC_URL and S3_BUCKET must be set in env"); sys.exit(2)
+    print("Creating snapshot repo (if necessary)")
+    print(create_snapshot_repo(ELASTIC, "aegis_backups", S3_BUCKET))
+    print("Snapshotting current decision_log* index (best-effort)")
+    print(snapshot_index(ELASTIC, "aegis_backups", "decision_log*"))
+    print("Retention job complete (deletion disabled in template for safety).")
+
+if __name__ == "__main__":
+    main()
+
diff --git a/docs/PROD_RUNBOOK_FINAL.md b/docs/PROD_RUNBOOK_FINAL.md
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/docs/PROD_RUNBOOK_FINAL.md
@@ -0,0 +1,240 @@
+# Aegis Production Validation & Operations Runbook (Final)
+
+This runbook ties together the final validation, rotation, policy enforcement, autoscaling and auditor flows.
+
+1) Multi-node DeepSpeed + vLLM validation
+ - Use .github workflow 'DeepSpeed Scale CI' or run scripts/deepspeed_scale_stress.sh on a runner with KUBECONFIG_STAGING.
+ - Collect artifacts (logs, nvidia-smi, dmesg, vllm CSVs). Uploaded to S3 for inspection.
+ - Iterate NCCL/RDMA tuning via configmap k8s/manifests/deepspeed-tuning-configmap.yaml and re-run harness.
+
+2) Vault rotation at scale
+ - Configure KMS_URL in cluster secret (or allow local generation for staging).
+ - Deploy k8s/manifests/vault-rotation-cronjob.yaml after testing scripts/vault_kms_rotation_manager.py in dry-run.
+ - Verify that insert_decision entries are written for each rotation and deletion event.
+ - Only after verification, enable destructive deletion flag in CronJob image args.
+
+3) Policy enforcement
+ - Ensure policy_blocking_ci check exists and is added to branch protection for all repos via scripts/set_org_branch_protection.sh (run by org admin).
+ - Expand rego tests in policy/opa/tests (agents_edge_cases.rego added).
+
+4) Autoscaling & SLOs
+ - Deploy GPU exporter (scripts/gpu_metric_exporter.py) as DaemonSet on GPU nodes to populate aegis_gpu_utilization_percent.
+ - Apply k8s/manifests/keda_tuned_scaledobject.yaml and monitoring/prometheus/slo_final_rules.yaml.
+ - Perform vLLM load runs with quantization enabled via scripts/vllm_perf_benchmark.py and iterate KEDA thresholds.
+
+5) Auditor UX & retention
+ - Run scripts/metabase_provision_advanced.py to create saved questions and dashboards for auditors.
+ - Deploy decisionlog_es_retention_cronjob.yaml to snapshot indices to S3; audit retention via ES cluster.
+ - Apply RBAC (k8s/manifests/metabase_auditor_rbac.yaml) and provide auditor onboarding steps.
+
+6) Canary & rollback validation
+ - Use scripts/simulate_canary_failure.sh (or the CI dispatchable canary_rollback_test) to validate automatic rollback.
+ - Ensure rollback playbook (scripts/rollback_playbook.sh) successfully restores prior state.
+
+Runbook contacts and SLAs:
+ - SRE on-call: ops-team@example.com
+ - Security contact: sec-team@example.com
+ - ML lead: ml-team@example.com
+
+End of runbook.
+
