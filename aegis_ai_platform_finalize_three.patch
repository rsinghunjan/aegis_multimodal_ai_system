*** Begin Patch
*** Add File:serving/kserve/deploy_kserve_and_triton.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# deploy_kserve_and_triton.sh
+# Installs KServe + required components (KFServing/KNative) and deploys the example Triton InferenceService.
+#
+# Operator prerequisites:
+#  - Kubernetes cluster with GPU nodes
+#  - kubectl and helm in PATH
+#  - A container registry secret for model repository access (if private)
+#
+NAMESPACE=${NAMESPACE:-ml}
+KSERVE_VERSION=${KSERVE_VERSION:-v0.11.0}
+
+echo "Creating namespace $NAMESPACE"
+kubectl create ns "$NAMESPACE" || true
+
+echo "Installing KServe (Helm chart)"
+helm repo add kserve https://kserve.github.io/helm-charts
+helm repo update
+helm upgrade --install kserve kserve/kserve --namespace "$NAMESPACE" --version "$KSERVE_VERSION" \
+  --set server.enable=true \
+  --wait
+
+echo "Applying Triton InferenceService example"
+kubectl apply -f serving/kserve/triton-inferenceservice.yaml -n "$NAMESPACE"
+
+echo "Creating HPA fallback"
+kubectl apply -f serving/kserve/ksvc-hpa.yaml -n "$NAMESPACE"
+
+echo "Done. Check pods: kubectl get pods -n $NAMESPACE"
+
*** End Patch
*** Begin Patch
*** Add File:serving/kserve/argo-run-inference-bench.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-inference-bench-
+  namespace: ml
+spec:
+  entrypoint: bench
+  templates:
+  - name: bench
+    steps:
+    - - name: run-bench
+        template: run-bench
+
+  - name: run-bench
+    container:
+      image: bitnami/kubectl:1.26
+      command: ["/bin/sh","-c"]
+      args:
+      - |
+        HOST=$(kubectl get inferenceservice aegis-triton-model -n ml -o jsonpath='{.status.url}' | sed 's#https://##')
+        echo "Inference host: $HOST"
+        /bin/sh -c "curl -sS -o /dev/null -w '%{http_code}\n' https://$HOST || true"
+        echo "Running bench script"
+        kubectl -n ml run --rm -i aegis-bench --image=curlimages/curl --restart=Never -- /bin/sh -c "for i in \$(seq 1 20); do curl -s -X POST -H 'Content-Type: application/json' https://$HOST/v1/models/aegis-triton-model:predict -d '{\"inputs\":[{\"name\":\"input__0\",\"shape\":[1,3,224,224],\"datatype\":\"FP32\",\"data\":[0]}]}' >/dev/null; done"
+    retryStrategy:
+      limit: 1
+
+# This Argo workflow sends a simple set of requests to the model endpoint (operator: adapt payload).
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/auto_deploy_on_promotion/README.md
+# MLflow promotion -> deploy to KServe automation
+
+This folder contains resources to wire MLflow model promotion events to an automated deploy to KServe:
+- A GitHub Action (already added earlier) promotes & signs the model (mlflow_promote_and_sign.yml).
+- This folder provides an ArgoCD/Argo Workflow manifest that can be triggered after promotion to update the Triton InferenceService storage location to the promoted artifact (example patch).
+
+Operator notes:
+- You may integrate this with your CI by invoking `kubectl apply -f mlflow/auto_deploy_on_promotion/deploy_patch.yaml` or calling the Argo Workflows API.
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/auto_deploy_on_promotion/deploy_patch.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-triton-model
+  namespace: ml
+spec:
+  predictor:
+    triton:
+      storage:
+        s3:
+          endpoint: "s3.amazonaws.com"   # replace as needed
+          bucket: "aegis-model-repo"
+          region: "us-east-1"
+          accessKeyIDSecret:
+            name: aegis-s3-creds
+            key: access_key
+          secretAccessKeySecret:
+            name: aegis-s3-creds
+            key: secret_key
+
+# The operator or a CI step should patch or replace the InferenceService storage to point to the promoted model artifact path.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/mlflow_deploy_on_promotion.yml
+name: Deploy model to KServe on MLflow promotion
+
+on:
+  workflow_dispatch:
+    inputs:
+      image:
+        required: false
+        type: string
+      model-uri:
+        required: true
+        type: string
+
+jobs:
+  deploy:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Patch InferenceService with model URI
+        env:
+          KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }} # base64-encoded kubeconfig with access to cluster (operator must add)
+          MODEL_URI: ${{ github.event.inputs.model-uri }}
+        run: |
+          echo "$KUBE_CONFIG" | base64 -d > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+          # Replace the storage location in the InferenceService (operator: adapt to exact storage path)
+          kubectl -n ml patch inferenceservice aegis-triton-model --type='json' -p="[ { \"op\": \"replace\", \"path\": \"/spec/predictor/triton/storage/s3/bucket\", \"value\": \"${MODEL_URI}\" } ]" || true
+          kubectl -n ml rollout restart deployment aegis-triton-model || true
+
+      - name: Wait for rollout
+        run: |
+          export KUBECONFIG=/tmp/kubeconfig
+          kubectl -n ml rollout status deployment aegis-triton-model --timeout=2m || true
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/helm/redis-values.yaml
+# Helm values for a Redis Cluster suitable as Feast online store (example: bitnami/redis chart)
+replica:
+  replicaCount: 2
+architecture: "replication"
+resources:
+  master:
+    limits:
+      cpu: 1000m
+      memory: 1Gi
+  replica:
+    limits:
+      cpu: 500m
+      memory: 512Mi
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/helm/lakefs-values.yaml
+# lakeFS sample values (operator: adapt to your object store credentials)
+server:
+  service:
+    type: ClusterIP
+persistence:
+  enabled: false
+storage:
+  type: s3
+  s3:
+    endpoint: s3.amazonaws.com
+    bucket: aegis-data
+    region: us-east-1
+    access_key_id: "<REPLACE>"
+    secret_access_key: "<REPLACE>"
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/deploy_feast_and_lakefs.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy local Feast online store (Redis via Helm) and lakeFS for dataset versioning in the cluster.
+#
+NAMESPACE=${NAMESPACE:-ml}
+kubectl create ns "$NAMESPACE" || true
+
+echo "Installing Redis (bitnami/redis) as Feast online store"
+helm repo add bitnami https://charts.bitnami.com/bitnami
+helm repo update
+helm upgrade --install aegis-redis bitnami/redis -n "$NAMESPACE" -f feature-store/helm/redis-values.yaml --wait
+
+echo "Installing lakeFS (Helm chart) - operator must configure object store credentials"
+helm repo add lakefs https://charts.lakefs.io
+helm repo update
+helm upgrade --install lakefs lakefs/lakefs -n "$NAMESPACE" -f feature-store/helm/lakefs-values.yaml --wait
+
+echo "Feast & lakeFS deployment complete. Next: configure Feast repo and point to Redis & offline store."
+
*** End Patch
*** Begin Patch
*** Add File:feature-store/feast/deploy_and_ingest.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy Feast local demo and ingest sample features, then demonstrate dataset snapshot reproducibility.
+#
+NAMESPACE=${NAMESPACE:-ml}
+kubectl create ns "$NAMESPACE" || true
+
+echo "Prepare offline_store dir"
+mkdir -p feature-store/feast/offline_store
+
+echo "Run sample ingestion (local)"
+python3 feature-store/feast/ingest_sample.py
+
+echo "Create a lakeFS snapshot (if lakeFS is available) - operator: adapt to lakeFS CLI or API"
+echo "Operator: Use lakeFS/gateway to create a branch & commit containing offline_store; refer to runbooks for exact steps."
+
+echo "Ingestion complete. To reproduce dataset, fetch offline_store snapshot or use DVC/lakeFS snapshot reference in training."
+
*** End Patch
*** Begin Patch
*** Add File:validation/validate_serving_mlflow_feast.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# High-level validation script for Serving (KServe), MLflow promotion+deploy, and Feast ingest/repro.
+#
+echo "1) Serving: check InferenceService status"
+kubectl get inferenceservice aegis-triton-model -n ml -o yaml || true
+
+echo "2) MLflow: ensure model promotion + sign flow can be invoked (manual step)"
+echo "   - Trigger MLflow Promote & Sign GitHub Action (requires secrets configured)"
+echo "   - After CI completes, run cosign verify --rekor-server <REKOR> <image-or-blob>"
+
+echo "3) Feast: check Redis and run ingest script"
+kubectl get pods -n ml -l app.kubernetes.io/name=redis || true
+bash feature-store/feast/deploy_and_ingest.sh
+
+echo "4) Optional: run Argo inference bench workflow"
+kubectl apply -f serving/kserve/argo-run-inference-bench.yaml -n ml || true
+echo "Use 'kubectl logs' to follow the Argo Workflow pods for bench outputs."
+
+echo "Validation script completed. Inspect logs for details."
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/ai_platform_finalize_runbook.md
+# Finalization runbook for Serving, MLflow promotion & signing, and Feast + lakeFS
+
+This runbook walks the operator through getting the three capabilities fully validated in staging.
+
+Prereqs
+- A Kubernetes cluster (staging) with GPU nodes and kubectl & helm access.
+- GitHub repo secrets set: GHCR_PAT, VAULT_ADDR, REKOR_SERVER, KUBE_CONFIG (base64), MLFLOW_* tokens if using MLflow CI.
+- Object store credentials (S3/GCS/COS) for model storage and lakeFS.
+
+Steps
+1) Deploy KServe & Triton
+   - Run: ./serving/kserve/deploy_kserve_and_triton.sh
+   - Wait for inferenceservice to be Ready: kubectl get inferenceservice aegis-triton-model -n ml
+
+2) Deploy Feast & lakeFS
+   - Run: ./feature-store/deploy_feast_and_lakefs.sh
+   - Run ingestion: ./feature-store/feast/deploy_and_ingest.sh
+
+3) Configure MLflow promotion & signing
+   - Ensure Vault transit key `aegis-signing-key` exists and GitHub Actions secrets are set.
+   - Trigger GitHub Action: MLflow Promote & Sign
+   - After success, trigger Deploy model to KServe workflow (or use Argo to patch InferenceService)
+
+4) Validate end-to-end
+   - Run validation: ./validation/validate_serving_mlflow_feast.sh
+   - Confirm Rekor entries and webhook admission as described in the MLflow runbook.
+
+Notes
+- For production-grade autoscaling / GPU-aware scaling, integrate KEDA, Volcano or custom scheduler hints.
+- Replace example S3 endpoints/credentials with your provider object storage.
+
*** End Patch
*** End Patch
