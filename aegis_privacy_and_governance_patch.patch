*** Begin Patch
*** Add File:legal/BAA_template.md
+# Business Associate Agreement (BAA) — Template
+
+Parties:
+- Covered Entity: [CUSTOMER NAME]
+- Business Associate: Aegis [YOUR ORG]
+
+Overview
+- This BAA governs the handling of Protected Health Information (PHI) where Aegis processes PHI on behalf of the Covered Entity.
+
+Key clauses (fill & negotiate with legal):
+- Permitted uses and disclosures of PHI (scope limited to necessary processing).
+- Required safeguards: administrative, physical, technical (include HSM/KMS, Vault, mTLS, logging).
+- Notification obligations for breaches (48–72 hours recommended).
+- Subprocessors: list and obligations for downstream vendors.
+- Data return & deletion: upon termination, return or securely destroy PHI and provide evidence.
+- Audit rights: Covered Entity may audit compliance; Business Associate will provide evidence bundles.
+- Indemnification & liability limits (as negotiated).
+
+Operational controls (examples to include in exhibit):
+- Encryption at rest: all PHI stored in S3/DB must use KMS with automatic rotation.
+- Encryption in transit: mTLS for all service-to-service calls carrying PHI.
+- Logging: all access to PHI recorded and retained per retention policy.
+- Access controls: least privilege via RBAC & periodic access reviews.
+- DSR & Erasure: documented process to respond to Subject Access Requests and erasure for PHI when required.
+
+This is a starting template: legal must finalize and sign with each Covered Entity.
+
*** End Patch
*** Begin Patch
*** Add File:ui/backend/consent_api.py
+from flask import Blueprint, request, jsonify
+import os, json, time
+import psycopg2
+
+bp = Blueprint("consent", __name__, url_prefix="/api/consent")
+
+DB_DSN = os.environ.get("CONSENT_DB_DSN", os.environ.get("DATABASE_URL"))
+
+def db_connect():
+    if not DB_DSN:
+        raise RuntimeError("CONSENT_DB_DSN / DATABASE_URL not set")
+    return psycopg2.connect(DB_DSN, sslmode="require")
+
+@bp.route("/grant", methods=["POST"])
+def grant():
+    payload = request.json or {}
+    subject = payload.get("subject_id")
+    scope = payload.get("scope","default")
+    if not subject:
+        return jsonify({"error":"subject_id required"}), 400
+    conn = db_connect()
+    cur = conn.cursor()
+    cur.execute("INSERT INTO user_consent (subject_id, consent_scope, granted, granted_at) VALUES (%s,%s,%s,now()) ON CONFLICT (subject_id, consent_scope) DO UPDATE SET granted=TRUE,granted_at=now(),revoked_at=NULL", (subject, scope, True))
+    conn.commit()
+    cur.close()
+    conn.close()
+    return jsonify({"status":"granted","subject":subject,"scope":scope})
+
+@bp.route("/revoke", methods=["POST"])
+def revoke():
+    payload = request.json or {}
+    subject = payload.get("subject_id")
+    scope = payload.get("scope","default")
+    if not subject:
+        return jsonify({"error":"subject_id required"}), 400
+    conn = db_connect()
+    cur = conn.cursor()
+    cur.execute("INSERT INTO user_consent (subject_id, consent_scope, granted, revoked_at) VALUES (%s,%s,%s,now()) ON CONFLICT (subject_id, consent_scope) DO UPDATE SET granted=FALSE,revoked_at=now()", (subject, scope, False))
+    conn.commit()
+    cur.close()
+    conn.close()
+    return jsonify({"status":"revoked","subject":subject,"scope":scope})
+
+@bp.route("/status/<subject_id>", methods=["GET"])
+def status(subject_id):
+    conn = db_connect()
+    cur = conn.cursor()
+    cur.execute("SELECT consent_scope, granted, granted_at, revoked_at FROM user_consent WHERE subject_id=%s", (subject_id,))
+    rows = cur.fetchall()
+    cur.close()
+    conn.close()
+    out = []
+    for r in rows:
+        out.append({"scope": r[0], "granted": r[1], "granted_at": r[2].isoformat() if r[2] else None, "revoked_at": r[3].isoformat() if r[3] else None})
+    return jsonify({"subject_id": subject_id, "consents": out})
+
*** End Patch
*** Begin Patch
*** Add File:compliance/portability/export_portability.py
+#!/usr/bin/env python3
+"""
+Portability exporter — collects user-related artifacts and packages them for subject access / portability.
+This is intended as a safe, auditable export used by DSR flows.
+"""
+import argparse, os, tarfile, tempfile, shutil, json, boto3
+
+def export_local(subject, out):
+    tmp = tempfile.mkdtemp(prefix="port_")
+    # placeholder: collect DB rows, MLflow artifacts, S3 objects with predictable prefixes
+    with open(os.path.join(tmp,"metadata.json"),"w") as f:
+        json.dump({"subject":subject, "collected_at": None}, f)
+    bucket = os.environ.get("MODEL_ARTIFACT_BUCKET")
+    if bucket:
+        s3 = boto3.client("s3")
+        prefix = f"users/{subject}/"
+        resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
+        for o in resp.get("Contents", []):
+            key = o["Key"]
+            local = os.path.join(tmp, os.path.basename(key))
+            s3.download_file(bucket, key, local)
+    with tarfile.open(out, "w:gz") as tar:
+        tar.add(tmp, arcname=os.path.basename(tmp))
+    shutil.rmtree(tmp)
+    print("Portability export written to", out)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--subject", required=True)
+    p.add_argument("--out", default="/tmp/portability_export.tar.gz")
+    args = p.parse_args()
+    export_local(args.subject, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/classification/classification_report.py
+#!/usr/bin/env python3
+"""
+Run automated PII classification across local paths and S3, produce a JSON report with object keys and classification labels.
+This is a simple regex-based classifier useful for triage; in production use a DLP service.
+"""
+import argparse, json, os, tempfile
+from compliance.pii_scanner import scan_local, scan_s3
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--local", help="local dir to scan")
+    p.add_argument("--s3-bucket", help="s3 bucket to scan")
+    p.add_argument("--s3-prefix", default="")
+    p.add_argument("--out", default="/tmp/classification_report.json")
+    args = p.parse_args()
+    report = {"local":{}, "s3":{}}
+    if args.local:
+        report["local"] = scan_local(args.local)
+    if args.s3_bucket:
+        report["s3"] = scan_s3(args.s3_bucket, args.s3_prefix)
+    with open(args.out,"w") as f:
+        json.dump(report, f, indent=2)
+    print("Classification report written to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:security/terraform/kms_s3.tf
+/*
+Terraform snippet (example) for S3 bucket with KMS encryption and logging.
+Adapt to your org conventions; do NOT include secrets in files.
+*/
+terraform {
+  required_providers {
+    aws = {
+      source  = "hashicorp/aws"
+      version = "~> 4.0"
+    }
+  }
+}
+
+provider "aws" {
+  region = var.region
+}
+
+resource "aws_kms_key" "aegis_s3_key" {
+  description             = "KMS key for Aegis S3 encryption"
+  deletion_window_in_days = 30
+}
+
+resource "aws_s3_bucket" "aegis_models" {
+  bucket = var.bucket_name
+  force_destroy = false
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        kms_master_key_id = aws_kms_key.aegis_s3_key.arn
+        sse_algorithm     = "aws:kms"
+      }
+    }
+  }
+}
+
+output "s3_bucket" {
+  value = aws_s3_bucket.aegis_models.id
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/mtls/nginx_ingress_mtls.yaml
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: aegis-mtls-ingress
+  namespace: aegis
+  annotations:
+    nginx.ingress.kubernetes.io/auth-tls-secret: "aegis/ingress-client-certs"
+    nginx.ingress.kubernetes.io/auth-tls-verify-client: "on"
+spec:
+  rules:
+  - host: aegis.example.com
+    http:
+      paths:
+      - backend:
+          service:
+            name: aegis-api
+            port:
+              number: 8081
+        path: /
+        pathType: Prefix
+
+/*
+Notes:
+- Create kubernetes secret aes ingress-client-certs containing trusted CA cert(s).
+- This config enforces mTLS at the ingress; adjust for your ingress controller.
+*/
+
*** End Patch
*** Begin Patch
*** Add File:ops/access_review/generate_access_review.py
+#!/usr/bin/env python3
+"""
+Generate an access review report for Kubernetes RBAC, Vault roles and IAM users (AWS).
+Produce JSON file suitable for evidence collection.
+"""
+import subprocess, json, os, datetime
+
+OUT = os.environ.get("ACCESS_REVIEW_OUT", "/tmp/access_review.json")
+
+def kubectl_dump(ns=None):
+    cmd = ["kubectl","get","roles,rolebindings,clusterroles,clusterrolebindings","-o","json"]
+    if ns:
+        cmd = ["kubectl","get","roles,rolebindings","-n",ns,"-o","json"]
+    out = subprocess.check_output(cmd)
+    return json.loads(out)
+
+def vault_policies():
+    try:
+        out = subprocess.check_output(["vault","policy","list","-format","json"])
+        return json.loads(out)
+    except Exception:
+        return {}
+
+def aws_iam_users():
+    try:
+        out = subprocess.check_output(["aws","iam","list-users","--output","json"])
+        return json.loads(out)
+    except Exception:
+        return {}
+
+def main():
+    report = {"generated_at": datetime.datetime.utcnow().isoformat(), "k8s": None, "vault": None, "aws": None}
+    try:
+        report["k8s"] = kubectl_dump()
+    except Exception as e:
+        report["k8s_error"] = str(e)
+    try:
+        report["vault"] = vault_policies()
+    except Exception as e:
+        report["vault_error"] = str(e)
+    try:
+        report["aws"] = aws_iam_users()
+    except Exception as e:
+        report["aws_error"] = str(e)
+    with open(OUT,"w") as f:
+        json.dump(report, f, indent=2)
+    print("Access review written to", OUT)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:compliance/retention/evidence_purger.py
+#!/usr/bin/env python3
+"""
+Retention purger that deletes objects older than retention days and writes a deletion manifest (evidence).
+This script is intended to be run as a CronJob in staging and can be used to produce deletion evidence for auditors.
+"""
+import boto3, os, datetime, json
+
+BUCKET = os.environ.get("MODEL_ARTIFACT_BUCKET")
+PREFIX = os.environ.get("RETENTION_PREFIX","telemetry/")
+RETENTION_DAYS = int(os.environ.get("RETENTION_DAYS","365"))
+EVIDENCE_OUT = os.environ.get("RETENTION_EVIDENCE","/tmp/retention_deletions.json")
+
+def list_old_objects(bucket, prefix, days):
+    s3 = boto3.client("s3")
+    cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=days)
+    cont = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
+    old = []
+    for o in cont.get("Contents", []):
+        if o["LastModified"].replace(tzinfo=None) < cutoff:
+            old.append({"Key": o["Key"], "LastModified": o["LastModified"].isoformat()})
+    return old
+
+def delete_objects(bucket, keys):
+    s3 = boto3.client("s3")
+    to_del = {"Objects": [{"Key": k["Key"]} for k in keys]}
+    if to_del["Objects"]:
+        s3.delete_objects(Bucket=bucket, Delete=to_del)
+
+def main():
+    if not BUCKET:
+        print("MODEL_ARTIFACT_BUCKET not set")
+        return
+    old = list_old_objects(BUCKET, PREFIX, RETENTION_DAYS)
+    if not old:
+        print("No objects older than retention")
+        return
+    # write evidence manifest
+    manifest = {"deleted_at": datetime.datetime.utcnow().isoformat(), "bucket": BUCKET, "objects": old}
+    with open(EVIDENCE_OUT,"w") as f:
+        json.dump(manifest, f, indent=2)
+    # perform deletion
+    delete_objects(BUCKET, old)
+    print("Deleted objects and wrote evidence to", EVIDENCE_OUT)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:payments/attest_no_pan.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple attest script that scans S3 bucket for PAN-like strings and fails if any found.
+# Use as part of compliance evidence to assert the platform does not hold raw PAN data.
+BUCKET=${1:-${MODEL_ARTIFACT_BUCKET:-}}
+PREFIX=${2:-""}
+if [ -z "$BUCKET" ]; then
+  echo "Usage: $0 <s3-bucket> [prefix]"
+  exit 2
+fi
+TMPDIR=$(mktemp -d)
+echo "Downloading objects under s3://$BUCKET/$PREFIX to $TMPDIR (max 1000 objects)..."
+aws s3 cp --recursive "s3://$BUCKET/$PREFIX" "$TMPDIR" --exclude "*" --include "*" --no-progress || true
+echo "Scanning files for PAN-like patterns..."
+set +e
+grep -E -R -I --line-number "[0-9]{13,16}" "$TMPDIR" > /tmp/pan_scan.txt 2>/dev/null || true
+set -e
+if [ -s /tmp/pan_scan.txt ]; then
+  echo "PAN-like patterns found! See /tmp/pan_scan.txt"
+  exit 1
+else
+  echo "No PAN-like patterns detected (simple scan)."
+  rm -rf "$TMPDIR"
+  exit 0
+fi
+
*** End Patch
*** Begin Patch
*** Add File:compliance/evidence_collector_enhanced.py
+#!/usr/bin/env python3
+"""
+Enhanced evidence collector to produce a compliance bundle that includes:
+ - MCP documents
+ - Access review report
+ - PII classification report
+ - Retention deletion manifests
+ - DSR logs and recent exports/erasures
+ - Audit log tail
+Outputs a tar.gz bundle suitable for auditors.
+"""
+import os, tarfile, tempfile, shutil, subprocess, json
+from datetime import datetime
+
+OUT = os.environ.get("EVIDENCE_OUT","/tmp/aegis_compliance_evidence.tar.gz")
+
+def gather(files, outdir):
+    for f in files:
+        if os.path.exists(f):
+            shutil.copy(f, outdir)
+
+def main():
+    tmp = tempfile.mkdtemp(prefix="evidence_")
+    try:
+        # collect MCP store if present
+        mcp_dir = os.environ.get("MCP_STORE_DIR","/var/aegis/mcp")
+        if os.path.isdir(mcp_dir):
+            shutil.copytree(mcp_dir, os.path.join(tmp,"mcp"))
+        # run access review
+        access_out = os.path.join(tmp,"access_review.json")
+        subprocess.run(["python3","ops/access_review/generate_access_review.py"], check=False)
+        if os.path.exists("/tmp/access_review.json"):
+            shutil.copy("/tmp/access_review.json", access_out)
+        # classification
+        class_out = os.path.join(tmp,"classification.json")
+        subprocess.run(["python3","compliance/classification/classification_report.py","--out",class_out], check=False)
+        # retention manifest
+        if os.path.exists("/tmp/retention_deletions.json"):
+            shutil.copy("/tmp/retention_deletions.json", os.path.join(tmp,"retention_deletions.json"))
+        # DSR store
+        dsr = os.environ.get("DSR_STORE","/var/aegis/dsr")
+        if os.path.isdir(dsr):
+            shutil.copytree(dsr, os.path.join(tmp,"dsr"))
+        # audit log tail (last 10k lines)
+        audit = os.environ.get("AUDIT_LOG","/var/log/aegis/audit.log")
+        if os.path.exists(audit):
+            subprocess.run(["tail","-n","10000", audit], stdout=open(os.path.join(tmp,"audit_tail.log"),"w"))
+        # package
+        with tarfile.open(OUT,"w:gz") as tar:
+            tar.add(tmp, arcname=os.path.basename(tmp))
+        print("Evidence bundle created at", OUT)
+    finally:
+        shutil.rmtree(tmp, ignore_errors=True)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/compliance_evidence.yml
+name: Compliance evidence bundle
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: "0 2 * * 1"  # weekly
+
+jobs:
+  collect:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Configure AWS
+        env:
+          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
+          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: |
+          aws sts get-caller-identity
+      - name: Run evidence collector
+        env:
+          MODEL_ARTIFACT_BUCKET: ${{ secrets.MODEL_ARTIFACT_BUCKET }}
+        run: |
+          python3 compliance/evidence_collector_enhanced.py
+      - name: Upload evidence bundle
+        if: success()
+        uses: actions/upload-artifact@v4
+        with:
+          name: aegis-compliance-evidence
+          path: /tmp/aegis_compliance_evidence.tar.gz
+
*** End Patch
*** End Patch
