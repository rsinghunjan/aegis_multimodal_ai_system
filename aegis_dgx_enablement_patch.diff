diff --git a/scripts/dgx/provision_and_get_kubeconfig.sh b/scripts/dgx/provision_and_get_kubeconfig.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/dgx/provision_and_get_kubeconfig.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Helper / runbook wrapper: Provision DGX Cloud minimally and retrieve kubeconfig.
+# This script is intentionally a "guided" helper — DGX Cloud provisioning usually
+# requires interacting with NVIDIA's portal or an account team. Use this script
+# to record steps and standardize the kubeconfig retrieval + validation.
+#
+# Usage:
+#   ./scripts/dgx/provision_and_get_kubeconfig.sh --provider dgx-cloud --cluster-name my-dgx-cluster --out-kubeconfig ./kubeconfig-dgx
+#
+set -euo pipefail
+
+PROVIDER=""
+CLUSTER_NAME=""
+OUT_KUBECONFIG="${OUT_KUBECONFIG:-./kubeconfig-dgx}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --provider) PROVIDER="$2"; shift 2;;
+    --cluster-name) CLUSTER_NAME="$2"; shift 2;;
+    --out-kubeconfig) OUT_KUBECONFIG="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$PROVIDER" || -z "$CLUSTER_NAME" ]]; then
+  echo "Usage: $0 --provider dgx-cloud --cluster-name <name> [--out-kubeconfig ./kubeconfig-dgx]"
+  exit 2
+fi
+
+echo "=== DGX provisioning helper ==="
+echo "Provider: $PROVIDER"
+echo "Cluster name: $CLUSTER_NAME"
+echo
+echo "STEP 1: Provision DGX Cloud cluster (manual / vendor portal)"
+echo "- Work with NVIDIA DGX Cloud onboarding to provision the cluster."
+echo "- Choose whether you will run Kubernetes on DGX (recommended) or use Slurm/Batch."
+echo "- Confirm whether drivers are pre-installed by the vendor or if GPU Operator should install them."
+echo
+read -p "Press ENTER once DGX provisioning is complete and you have access credentials..."
+
+echo
+echo "STEP 2: Obtain kubeconfig"
+echo "- If DGX provides a kubeconfig, save it to: $OUT_KUBECONFIG"
+echo "- Example (vendor-specific): curl -o $OUT_KUBECONFIG 'https://vendor.example.com/api/kubeconfig?cluster=$CLUSTER_NAME&token=...' "
+echo
+read -p "Enter path to kubeconfig file to copy into project (or press ENTER to skip): " user_kc
+if [[ -n "$user_kc" ]]; then
+  cp "$user_kc" "$OUT_KUBECONFIG"
+  chmod 0600 "$OUT_KUBECONFIG"
+  echo "Saved kubeconfig -> $OUT_KUBECONFIG"
+fi
+
+echo
+echo "STEP 3: Validate kubeconfig"
+export KUBECONFIG="$OUT_KUBECONFIG"
+if kubectl version --short >/dev/null 2>&1; then
+  echo "kubectl can talk to the cluster:"
+  kubectl get nodes -o wide || true
+else
+  echo "kubectl failed with the provided kubeconfig. Please verify credentials and network access."
+  exit 3
+fi
+
+echo
+echo "STEP 4: Decide driver handling"
+echo "- If DGX nodes already include the exact production drivers you need, set GPU Operator autoInstall to false."
+echo "- Otherwise allow GPU Operator to install drivers (autoInstall: true)."
+echo "You can toggle this via scripts/dgx/toggle_gpu_operator_driver_install.sh"
+
+echo
+echo "STEP 5: Next ops"
+echo "- Label nodes (scripts/dgx/label_dgx_nodes.sh)"
+echo "- Build & push matching images (scripts/dgx/build_push_deepspeed_image.sh)"
+echo "- Run NCCL tuning script (scripts/dgx/nccl_tuning.sh)"
+echo "- Deploy GPU Operator / DCGM exporter and run smoke job (scripts/dgx/dgx_smoke_test.sh)"
+
+echo "Kubeconfig helper finished. KUBECONFIG set to $OUT_KUBECONFIG for this session."
+echo "Remember to add the KUBECONFIG_DGX secret to your CI with the contents of $OUT_KUBECONFIG."
+
+exit 0
+
diff --git a/scripts/dgx/label_dgx_nodes.sh b/scripts/dgx/label_dgx_nodes.sh
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/scripts/dgx/label_dgx_nodes.sh
@@ -0,0 +1,120 @@
+#!/usr/bin/env bash
+#
+# Label DGX nodes so k8s manifests with nodeSelector "node.kubernetes.io/dgx=true" will schedule there.
+#
+# Usage:
+#   ./scripts/dgx/label_dgx_nodes.sh --selector 'node-role.kubernetes.io/worker' --label node.kubernetes.io/dgx=true
+#   or to label specific nodes:
+#   ./scripts/dgx/label_dgx_nodes.sh --nodes node-1,node-2 --label node.kubernetes.io/dgx=true
+
+set -euo pipefail
+
+NODES=""
+LABEL="node.kubernetes.io/dgx=true"
+SELECTOR=""
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --label) LABEL="$2"; shift 2;;
+    --selector) SELECTOR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$NODES" && -z "$SELECTOR" ]]; then
+  echo "Either --nodes or --selector must be provided."
+  exit 2
+fi
+
+if [[ -n "$SELECTOR" ]]; then
+  echo "Selecting nodes with selector: $SELECTOR"
+  mapfile -t node_list < <(kubectl get nodes -o name --selector="$SELECTOR" | sed 's/^node\///')
+else
+  IFS=',' read -r -a node_list <<< "$NODES"
+fi
+
+echo "Nodes to label: ${node_list[*]}"
+
+for n in "${node_list[@]}"; do
+  if kubectl get node "$n" >/dev/null 2>&1; then
+    echo "Labeling $n with $LABEL"
+    kubectl label node "$n" "$LABEL" --overwrite || true
+  else
+    echo "Node $n not found in cluster; skipping."
+  fi
+done
+
+echo "Labeling complete."
+exit 0
+
diff --git a/scripts/dgx/build_push_deepspeed_image.sh b/scripts/dgx/build_push_deepspeed_image.sh
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/dgx/build_push_deepspeed_image.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Build and push a DeepSpeed-compatible image tuned for DGX.
+# You must choose matching CUDA/cuDNN/NCCL/PyTorch/DeepSpeed versions for your DGX nodes.
+# The Dockerfile template is at docker/Dockerfile.deepspeed.template
+#
+# Usage:
+#   REGISTRY=registry.example.com/namespace IMAGE_TAG=deepspeed-h100-cuda12 ./scripts/dgx/build_push_deepspeed_image.sh
+
+set -euo pipefail
+
+REGISTRY="${REGISTRY:-<REGISTRY>}"
+IMAGE_TAG="${IMAGE_TAG:-aegis-deepspeed:latest}"
+CUDA_VERSION="${CUDA_VERSION:-12.1}"
+CUDNN_VERSION="${CUDNN_VERSION:-8}"
+NCCL_VERSION="${NCCL_VERSION:-2.18.0}"
+PYTORCH_VERSION="${PYTORCH_VERSION:-2.2.0}"
+DEEPSPEED_VERSION="${DEEPSPEED_VERSION:-0.9.2}"
+BUILD_DIR="$(mktemp -d)"
+
+if [[ "$REGISTRY" == "<REGISTRY>" ]]; then
+  echo "Please set REGISTRY env var to your container registry (e.g., REGISTRY=ghcr.io/org)"
+  exit 2
+fi
+
+echo "Preparing build context in $BUILD_DIR"
+cat > "$BUILD_DIR/Dockerfile" <<DOCKER
+FROM nvidia/cuda:${CUDA_VERSION}-cudnn${CUDNN_VERSION}-devel-ubuntu22.04
+
+# Install system deps (minimal)
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    python3 python3-pip git build-essential wget ca-certificates \
+    libnuma-dev && rm -rf /var/lib/apt/lists/*
+
+# Python & Torch
+RUN python3 -m pip install --upgrade pip
+RUN pip install torch==${PYTORCH_VERSION} --extra-index-url https://download.pytorch.org/whl/cu${CUDA_VERSION/./}
+
+# Install specific NCCL / DeepSpeed versions if needed (adjust for your matrix)
+RUN pip install deepspeed==${DEEPSPEED_VERSION}
+
+# Optional: install CUDA-aware mpi or other libs if your setup requires them
+RUN pip install -U transformers datasets
+
+WORKDIR /workspace
+COPY . /workspace
+CMD ["/bin/bash"]
+DOCKER
+
+echo "Building image ${REGISTRY}/${IMAGE_TAG}"
+docker build -t "${REGISTRY}/${IMAGE_TAG}" "$BUILD_DIR"
+
+echo "Pushing image ${REGISTRY}/${IMAGE_TAG}"
+docker push "${REGISTRY}/${IMAGE_TAG}"
+
+echo "Build & push complete. Remember to update k8s manifests to use ${REGISTRY}/${IMAGE_TAG}"
+rm -rf "$BUILD_DIR"
+exit 0
+
diff --git a/scripts/dgx/toggle_gpu_operator_driver_install.sh b/scripts/dgx/toggle_gpu_operator_driver_install.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/toggle_gpu_operator_driver_install.sh
@@ -0,0 +1,160 @@
+#!/usr/bin/env bash
+#
+# Toggle the autoInstall/driver install behavior for the GPU Operator values file.
+# Usage:
+#   ./scripts/dgx/toggle_gpu_operator_driver_install.sh --values k8s/manifests/dgx/gpu-operator-values.yaml --autoInstall true
+
+set -euo pipefail
+
+VALUES_FILE=""
+AUTO_INSTALL="true"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --values) VALUES_FILE="$2"; shift 2;;
+    --autoInstall) AUTO_INSTALL="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$VALUES_FILE" ]]; then
+  echo "--values <file> is required"
+  exit 2
+fi
+
+if [[ ! -f "$VALUES_FILE" ]]; then
+  echo "Values file $VALUES_FILE not found"
+  exit 3
+fi
+
+echo "Toggling autoInstall in $VALUES_FILE -> $AUTO_INSTALL"
+# Simple YAML edit (safe for the known file structure). Use yq if available.
+if command -v yq >/dev/null 2>&1; then
+  yq e -i ".driver.autoInstall = ${AUTO_INSTALL}" "$VALUES_FILE"
+else
+  # Fallback: sed-based edit (works for the existing template which has 'autoInstall: true' line)
+  if grep -q "autoInstall:" "$VALUES_FILE"; then
+    sed -i -E "s/autoInstall: .*/autoInstall: ${AUTO_INSTALL}/" "$VALUES_FILE"
+  else
+    echo "autoInstall: ${AUTO_INSTALL}" >> "$VALUES_FILE"
+  fi
+fi
+
+echo "Done. You may helm upgrade/install GPU Operator with this values file."
+echo "Example:"
+echo "  helm repo add nvidia https://nvidia.github.io/gpu-operator"
+echo "  helm repo update"
+echo "  helm upgrade --install --namespace gpu-operator gpu-operator nvidia/gpu-operator -f $VALUES_FILE"
+
+exit 0
+
diff --git a/scripts/dgx/generate_nccl_env_suggest.sh b/scripts/dgx/generate_nccl_env_suggest.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/dgx/generate_nccl_env_suggest.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Parse nccl tuning outputs (produced by scripts/dgx/nccl_tuning.sh) and emit suggested NCCL env exports.
+# This is a best-effort helper: it looks for common NIC / RDMA device names and produces a recommended env block.
+#
+# Usage:
+#   ./scripts/dgx/generate_nccl_env_suggest.sh --nccl-output ./artifacts/nccl/all_reduce_perf.txt --out ./artifacts/nccl/env_suggest.sh
+
+set -euo pipefail
+
+NCCL_OUTPUT=""
+OUT_FILE="./nccl_env_suggest.sh"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nccl-output) NCCL_OUTPUT="$2"; shift 2;;
+    --out) OUT_FILE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$NCCL_OUTPUT" || ! -f "$NCCL_OUTPUT" ]]; then
+  echo "Provide --nccl-output path to nccl test output (e.g., all_reduce_perf.txt)"
+  exit 2
+fi
+
+echo "Analyzing $NCCL_OUTPUT..."
+
+# Heuristics: try to discover IB devices and primary ethernet interface
+IB_DEVICES=$(grep -Eo "mlx[0-9_]+" "$NCCL_OUTPUT" | sort -u | tr '\n' ' ' || true)
+ETH_IFACE=$(grep -Eo "eth[0-9]+" "$NCCL_OUTPUT" | head -n1 || true)
+if [[ -z "$ETH_IFACE" ]]; then
+  # fallback to system discovery if possible
+  if command -v ip >/dev/null 2>&1; then
+    ETH_IFACE=$(ip -o link | awk -F': ' '{print $2}' | grep -E 'eth|ens|bond' | head -n1 || true)
+  fi
+fi
+
+echo "Detected IB devices: $IB_DEVICES"
+echo "Detected ethernet iface: $ETH_IFACE"
+
+{
+  echo "#!/usr/bin/env bash"
+  echo "# Suggested NCCL env exports generated from $NCCL_OUTPUT"
+  echo "export NCCL_DEBUG=INFO"
+  echo "export NCCL_DEBUG_SUBSYS=ALL"
+  echo "export TORCH_DISTRIBUTED_DEBUG=INFO"
+  if [[ -n "$ETH_IFACE" ]]; then
+    echo "export NCCL_SOCKET_IFNAME=${ETH_IFACE}"
+  fi
+  if [[ -n "$IB_DEVICES" ]]; then
+    # pick first device for suggestion
+    FIRST_IB=$(echo "$IB_DEVICES" | awk '{print $1}')
+    echo "export NCCL_IB_HCA=${FIRST_IB}"
+    echo "export NCCL_IB_GID_INDEX=3  # example: adjust per infra"
+    echo "export NCCL_IB_DISABLE=0"
+  else
+    echo "export NCCL_IB_DISABLE=1  # no IB device detected; disable IB"
+  fi
+  echo "export NCCL_NET_GDR_LEVEL=2  # if using GDR"
+  echo "echo 'NCCL env suggested: NCCL_SOCKET_IFNAME=${ETH_IFACE} NCCL_IB_HCA=${FIRST_IB:-<none>}'"
+} > "$OUT_FILE"
+
+chmod +x "$OUT_FILE"
+echo "Wrote suggested env script to $OUT_FILE"
+echo "Review and adjust values before use; common adjustments: NCCL_IB_GID_INDEX and NCCL_SOCKET_IFNAME."
+exit 0
+
diff --git a/k8s/manifests/dgx/checkpoints-pvc.yaml b/k8s/manifests/dgx/checkpoints-pvc.yaml
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/k8s/manifests/dgx/checkpoints-pvc.yaml
@@ -0,0 +1,84 @@
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: dgx-checkpoints-pvc
+  namespace: aegis-ml
+spec:
+  accessModes:
+    - ReadWriteOnce
+  resources:
+    requests:
+      storage: 2Ti
+  storageClassName: nvme-fast   # replace with your fast storage StorageClass (Lustre/NVMe)
+
+# Notes:
+# - Replace 'nvme-fast' with the fast StorageClass available on your DGX cluster (Lustre / NVMe / specialized CSI).
+# - If using S3 for checkpoints, consider an init/upload sidecar or use a PVC-backed cache + periodic sync to S3.
+
diff --git a/.github/workflows/dgx_run_with_secrets.yml b/.github/workflows/dgx_run_with_secrets.yml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/.github/workflows/dgx_run_with_secrets.yml
@@ -0,0 +1,220 @@
+name: DGX Smoke/Run (self-hosted)
+
+on:
+  workflow_dispatch:
+    inputs:
+      job:
+        description: "Job to run (smoke|benchmark)"
+        required: true
+        default: "smoke"
+
+jobs:
+  run-dgx:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup environment
+        run: |
+          python -m pip install --upgrade pip
+          pip install yq kubernetes boto3 || true
+
+      - name: Restore KUBECONFIG from secret
+        env:
+          KUBECONFIG_DGX: ${{ secrets.KUBECONFIG_DGX }}
+        run: |
+          if [ -z "$KUBECONFIG_DGX" ]; then
+            echo "KUBECONFIG_DGX secret not set"; exit 1
+          fi
+          echo "$KUBECONFIG_DGX" > "$HOME/.kube/config_dgx"
+          export KUBECONFIG="$HOME/.kube/config_dgx"
+          kubectl version --short || (echo "kubectl cannot access DGX cluster"; exit 2)
+
+      - name: Login to registry (if needed)
+        if: ${{ secrets.DGX_REGISTRY_USER != '' && secrets.DGX_REGISTRY_PASS != '' }}
+        env:
+          REG_USER: ${{ secrets.DGX_REGISTRY_USER }}
+          REG_PASS: ${{ secrets.DGX_REGISTRY_PASS }}
+          REG_HOST: ${{ secrets.DGX_REGISTRY_HOST }}
+        run: |
+          echo "$REG_PASS" | docker login "$REG_HOST" -u "$REG_USER" --password-stdin
+
+      - name: Run smoke or benchmark
+        env:
+          KUBECONFIG: ${{ github.workspace }}/kubeconfig_temp  # placeholder; we set above in previous step
+        run: |
+          # Ensure KUBECONFIG is set
+          if [ -f "$HOME/.kube/config_dgx" ]; then
+            export KUBECONFIG="$HOME/.kube/config_dgx"
+          fi
+          if [ "${{ github.event.inputs.job }}" = "smoke" ]; then
+            chmod +x scripts/dgx/dgx_smoke_test.sh
+            ./scripts/dgx/dgx_smoke_test.sh
+          else
+            chmod +x tools/benchmarks/deepspeed_dgx_benchmark.sh
+            ./tools/benchmarks/deepspeed_dgx_benchmark.sh
+          fi
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-artifacts
+          path: artifacts || .
+
+      - name: Cleanup kubeconfig
+        if: always()
+        run: |
+          rm -f "$HOME/.kube/config_dgx" || true
+
+      - name: Done
+        run: echo "DGX run completed"
+
diff --git a/docs/dgx/ci_secrets.md b/docs/dgx/ci_secrets.md
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/docs/dgx/ci_secrets.md
@@ -0,0 +1,220 @@
+# CI / Runner Secrets for DGX workflows
+
+This document explains which secrets to add to your GitHub repository (or Organization) to run DGX smoke/benchmark workflows from self‑hosted runners.
+
+Required secrets
+- KUBECONFIG_DGX: the kubeconfig YAML content for the DGX Kubernetes cluster. Store the full kubeconfig (base64 or raw) in this secret. The workflow will write it to the runner's file system prior to running kubectl.
+- DGX_REGISTRY_HOST: registry host for DGX images (e.g., ghcr.io or registry.example.com). Optional if images are public.
+- DGX_REGISTRY_USER: registry username (optional).
+- DGX_REGISTRY_PASS: registry password / token (optional).
+
+Optional secrets
+- REGISTRY (string): <REGISTRY> placeholder used by scripts (e.g., ghcr.io/org). Useful when building & pushing images in CI.
+- DO_SPACES_KEY / DO_SPACES_SECRET etc.: object store keys if you're uploading artifacts to an S3-compatible endpoint.
+
+How to add repository secrets
+1. Go to your GitHub repo -> Settings -> Secrets -> Actions -> New repository secret.  
+2. Add the secrets listed above. For KUBECONFIG_DGX it is easiest to paste the raw kubeconfig YAML.
+
+Security notes
+- Limit repository / org secret access to the runners that need them. Use environment protection rules and `environments` to restrict access for workflows that operate on DGX.
+- Rotate kubeconfig / registry tokens periodically and update the secrets.
+- Prefer short-lived service account tokens for kubeconfig when possible (e.g., use a prow/token issuer pattern or ephemeral kubeconfig).
+
+Using the secrets
+- Workflows in .github/workflows/dgx_run_with_secrets.yml expect KUBECONFIG_DGX and optionally DGX_REGISTRY_* secrets to be present.
+- The scripts/dgx/provision_and_get_kubeconfig.sh helper reminds operators to add the KUBECONFIG_DGX secret after obtaining kubeconfig.
+
+If you need a small helper to base64-encode a kubeconfig before uploading:
+  cat kubeconfig-dgx | base64 | pbcopy
+  (paste into the GitHub secret value input as raw content; the workflow uses the raw kubeconfig)
+
+End of document.
+
