*** Begin Patch
*** Add File: scripts/provision_kms_and_set_secrets_multi.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Provision KMS for multiple clouds (aws|gcp|azure|oci|alibaba) using terraform modules
+# and set GitHub secrets for CI signing.
+#
+# Usage:
+#   ./scripts/provision_kms_and_set_secrets_multi.sh <cloud> <owner/repo>
+# Example:
+#   ./scripts/provision_kms_and_set_secrets_multi.sh aws myorg/aegis
+#
+CLOUD=${1:-}
+GH_REPO=${2:-}
+if [[ -z "$CLOUD" || -z "$GH_REPO" ]]; then
+  echo "Usage: $0 <aws|gcp|azure|oci|alibaba> <owner/repo>"
+  exit 2
+fi
+
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+
+case "$CLOUD" in
+  aws)
+    echo "Provisioning AWS via terraform/aws"
+    (cd terraform/aws && terraform init && terraform apply -auto-approve)
+    OUT=$(cd terraform/aws && terraform output -json)
+    KMS_ARN=$(echo "$OUT" | jq -r '.kms_key_arn.value')
+    echo "Setting GitHub secrets..."
+    gh secret set CLOUD_PROVIDER --repo "$GH_REPO" --body "aws"
+    gh secret set AWS_KMS_KEY_ARN --repo "$GH_REPO" --body "$KMS_ARN"
+    ;;
+  gcp)
+    echo "Provisioning GCP via terraform/gcp"
+    (cd terraform/gcp && terraform init && terraform apply -auto-approve)
+    OUT=$(cd terraform/gcp && terraform output -json)
+    KMS_REF=$(echo "$OUT" | jq -r '.gcp_kms_key_ref.value')
+    SA_EMAIL=$(echo "$OUT" | jq -r '.github_actions_sa_email.value')
+    gh secret set CLOUD_PROVIDER --repo "$GH_REPO" --body "gcp"
+    gh secret set GCP_KMS_KEY_REF --repo "$GH_REPO" --body "$KMS_REF"
+    gh secret set GCP_SA_EMAIL --repo "$GH_REPO" --body "$SA_EMAIL"
+    echo "Create service account key for $SA_EMAIL and add as GCP_SA_KEY_JSON secret."
+    ;;
+  azure)
+    echo "Provisioning Azure via terraform/azure"
+    (cd terraform/azure && terraform init && terraform apply -auto-approve)
+    OUT=$(cd terraform/azure && terraform output -json)
+    KV_KEY_ID=$(echo "$OUT" | jq -r '.azure_key_vault_key_id.value')
+    STORAGE=$(echo "$OUT" | jq -r '.azure_storage_account_name.value')
+    gh secret set CLOUD_PROVIDER --repo "$GH_REPO" --body "azure"
+    gh secret set AZURE_KEY_VAULT_KEY_ID --repo "$GH_REPO" --body "$KV_KEY_ID"
+    gh secret set AZURE_STORAGE_ACCOUNT --repo "$GH_REPO" --body "$STORAGE"
+    ;;
+  oci)
+    echo "Provisioning OCI via terraform/oci (example)"
+    (cd terraform/oci && terraform init && terraform apply -auto-approve)
+    OUT=$(cd terraform/oci && terraform output -json)
+    KV_ID=$(echo "$OUT" | jq -r '.oci_vault_id.value')
+    BUCKET=$(echo "$OUT" | jq -r '.oci_compliance_bucket.value')
+    gh secret set CLOUD_PROVIDER --repo "$GH_REPO" --body "oci"
+    gh secret set OCI_KV_ID --repo "$GH_REPO" --body "$KV_ID"
+    gh secret set OCI_BUCKET --repo "$GH_REPO" --body "$BUCKET"
+    ;;
+  alibaba)
+    echo "Provisioning Alibaba via terraform/alibaba (example)"
+    (cd terraform/alibaba && terraform init && terraform apply -auto-approve)
+    OUT=$(cd terraform/alibaba && terraform output -json)
+    KMS_ID=$(echo "$OUT" | jq -r '.alibaba_kms_key_id.value')
+    BUCKET=$(echo "$OUT" | jq -r '.oss_bucket.value')
+    gh secret set CLOUD_PROVIDER --repo "$GH_REPO" --body "alibaba"
+    gh secret set ALIBABA_KMS_KEY_ID --repo "$GH_REPO" --body "$KMS_ID"
+    gh secret set ALIBABA_OSS_BUCKET --repo "$GH_REPO" --body "$BUCKET"
+    ;;
+  *)
+    echo "Unsupported cloud: $CLOUD"
+    popd >/dev/null
+    exit 2
+    ;;
+esac
+
+echo "Please manually set REKOR_URL, REGISTRY_HOST, REGISTRY_USER, REGISTRY_TOKEN, VAULT_ADDR, and VAULT_TOKEN as GitHub secrets as appropriate."
+popd >/dev/null
+echo "Provisioning step complete. Next: run scripts/run_poc_bundle.sh <cloud> to provision and deploy POC stack."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_poc_bundle.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate POC bundle deployment for training & attestation.
+# Usage:
+#   ./scripts/run_poc_bundle.sh <cloud> <poc-target>
+# poc-target: dgx | gke-tpu | gke-gpu
+#
+CLOUD=${1:-}
+TARGET=${2:-dgx}
+if [[ -z "$CLOUD" ]]; then
+  echo "Usage: $0 <cloud> <dgx|gke-tpu|gke-gpu>"
+  exit 2
+fi
+
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+
+echo "1) Ensure KMS & GitHub secrets are provisioned (see scripts/provision_kms_and_set_secrets_multi.sh)"
+
+echo "2) Deploy supporting infra: Vault, MLflow, Argo, Prometheus, Gatekeeper, JupyterHub"
+./scripts/deploy_production.sh "$CLOUD" || true
+
+echo "3) Deploy attestation service and webhook + Gatekeeper constraints"
+./scripts/deploy_attestation_and_gatekeeper.sh || true
+
+echo "4) Deploy JupyterHub (dev workspaces)"
+./scripts/setup_jupyterhub.sh || true
+
+echo "5) Build & push training image (deepspeed example)"
+docker build -t "${REGISTRY:-ghcr.io/yourorg}/deepspeed-poc:latest" -f docker/deepspeed.Dockerfile . || true
+docker push "${REGISTRY:-ghcr.io/yourorg}/deepspeed-poc:latest" || true
+
+echo "6) Run training validation depending on target"
+case "$TARGET" in
+  dgx)
+    kubectl apply -f k8s/nccl-configmap.yaml || true
+    kubectl apply -f argo/dgx_distributed_deepspeed_validation.yaml || true
+    ;;
+  gke-tpu)
+    kubectl apply -f argo/gke_tpu_poc_workflow.yaml || true
+    ;;
+  gke-gpu)
+    kubectl apply -f argo/deepspeed_zeo_workflow.yaml || true
+    ;;
+  *)
+    echo "Unknown target: $TARGET"
+    ;;
+esac
+
+echo "7) Run attestation smoke tests after CI produces signed image"
+./scripts/attestation_smoke_test.sh || true
+
+echo "POC bundle orchestration complete. Inspect artifacts/ for logs and evidence."
+popd >/dev/null
+
*** End Patch
*** Begin Patch
*** Add File: argo/gke_tpu_poc_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: gke-tpu-train-poc
+  namespace: staging
+spec:
+  entrypoint: tpu-train
+  templates:
+  - name: tpu-train
+    dag:
+      tasks:
+      - name: launch-tpu
+        template: tpu-job
+
+  - name: tpu-job
+    container:
+      image: ghcr.io/yourorg/tf-training:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          echo "Starting lightweight TPU training POC"
+          python train_tpu.py --iterations 100 --ckpt /data/checkpoints
+      resources:
+        requests:
+          cpu: "16"
+          memory: "64Gi"
+    nodeSelector:
+      cloud.google.com/gke-accelerator: "tpu-v4"
+    tolerations:
+    - key: "cloud.google.com/gke-accelerator"
+      operator: "Exists"
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/ci_poc_sign_deploy.yml
+name: POC CI: Build, Sign (multi-cloud/vault/pkcs11) & Deploy to staging
+on:
+  workflow_dispatch:
+    inputs:
+      image_tag:
+        required: false
+        default: latest
+
+jobs:
+  build-sign-deploy:
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+      id-token: write
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build image
+        run: |
+          IMAGE=${{ secrets.REGISTRY_HOST }}/aegis-poc:${{ github.event.inputs.image_tag }}
+          docker build -t "$IMAGE" -f docker/deepspeed.Dockerfile .
+          docker push "$IMAGE"
+          echo "IMAGE=$IMAGE" >> $GITHUB_OUTPUT
+
+      - name: Sign image (multicloud/vault/pkcs11)
+        env:
+          IMAGE: ${{ steps.build-sign-deploy.outputs.IMAGE }}
+        run: |
+          case "${{ secrets.CLOUD_PROVIDER }}" in
+            aws) cosign sign --key "awskms://${{ secrets.AWS_KMS_KEY_ARN }}" "${IMAGE}" ;;
+            gcp) cosign sign --key "gcpkms://${{ secrets.GCP_KMS_KEY_REF }}" "${IMAGE}" ;;
+            azure) cosign sign --key "azurekms://${{ secrets.AZURE_KEY_VAULT_NAME }}/${{ secrets.AZURE_KEY_NAME }}" "${IMAGE}" ;;
+            pkcs11)
+              echo "PKCS11 signing selected; this workflow must run on a self-hosted runner with HSM access"
+              ./scripts/ci_sign_with_pkcs11.sh "${IMAGE}"
+              ;;
+            vault|*)
+              echo "Vault transit signing: call wrapper (requires VAULT_ADDR & VAULT_TOKEN secrets)"
+              ./scripts/cosign_vault_transit_sign_wrapper.sh "sha256:$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE" | awk -F@ '{print $2}')" /tmp/vault_sig.out
+              ;;
+          esac
+
+      - name: Verify Rekor (best-effort)
+        run: |
+          DIG=$(docker inspect --format='{{index .RepoDigests 0}}' "${{ steps.build-sign-deploy.outputs.IMAGE }}" | awk -F@ '{print $2}')
+          curl -sS "${{ secrets.REKOR_URL }}/api/v1/log/entries?hash=${DIG}"
+
+      - name: Deploy annotated manifest (CD)
+        env:
+          IMAGE: ${{ steps.build-sign-deploy.outputs.IMAGE }}
+          KUBECONFIG: ${{ secrets.KUBE_CONFIG_DATA }}
+        run: |
+          # annotate and apply manifest using scripts/cd_annotate_and_deploy.sh
+          ./scripts/cd_annotate_and_deploy.sh k8s/poc_deployment.yaml "$IMAGE"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/poc_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: deepspeed-poc
+  namespace: staging
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: deepspeed-poc
+  template:
+    metadata:
+      labels:
+        app: deepspeed-poc
+    spec:
+      serviceAccountName: training-sa
+      containers:
+      - name: trainer
+        image: ghcr.io/yourorg/deepspeed-poc:latest
+        resources:
+          limits:
+            nvidia.com/gpu: 4
+          requests:
+            memory: "200Gi"
+            cpu: "24"
+        volumeMounts:
+        - name: data
+          mountPath: /data
+      volumes:
+      - name: data
+        persistentVolumeClaim:
+          claimName: deepspeed-data-pvc
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constrainttemplate_attested_annotation_only.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8srequireattestedannotation
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sRequireAttestedAnnotation
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8srequireattestedannotation
+
+        violation[{"msg": msg}] {
+          input.review.object.kind == "Deployment"
+          annotations := input.review.object.metadata.annotations
+          not annotations["aegis.attested"]
+          msg := "Deployment missing aegis.attested annotation - artifacts must be attested by attestation service"
+        }
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_require_attested_annotation.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequireAttestedAnnotation
+metadata:
+  name: require-attested-annotation
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["staging","ops","production"]
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File: services/attestation/attestation_ha.py
+"""
+Attestation service (HA) with Rekor verification and SBOM checks.
+This is an enhanced attestation Flask app that:
+ - Verifies cosign signatures (supports cosign JSON output)
+ - Confirms Rekor entries for the image digest
+ - Returns a compact attest result (JSON) used by webhook
+"""
+import os, json, subprocess, tempfile, requests
+from flask import Flask, request, jsonify
+
+app = Flask("attestation")
+COSIGN = os.environ.get("COSIGN_BIN", "cosign")
+REKOR = os.environ.get("REKOR_URL", "https://rekor.sigstore.dev")
+
+def run(cmd, timeout=60):
+    try:
+        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, timeout=timeout)
+        return True, out.decode()
+    except subprocess.CalledProcessError as e:
+        return False, e.output.decode()
+    except Exception as e:
+        return False, str(e)
+
+def cosign_verify(image):
+    ok, out = run([COSIGN, "verify", "--output", "json", "--rekor-url", REKOR, image], timeout=90)
+    if not ok:
+        return False, {"error": out}
+    try:
+        return True, json.loads(out)
+    except Exception:
+        return True, {"raw": out}
+
+def rekor_has_entry(digest):
+    try:
+        r = requests.get(f"{REKOR}/api/v1/log/entries?hash={digest}", timeout=10)
+        if r.status_code == 200 and r.json():
+            return True, r.json()
+        return False, {}
+    except Exception as e:
+        return False, {"error": str(e)}
+
+@app.route("/verify", methods=["POST"])
+def verify():
+    body = request.get_json() or {}
+    image = body.get("image")
+    if not image:
+        return jsonify({"ok": False, "reason": "no_image"}), 400
+
+    ok, cosign_info = cosign_verify(image)
+    if not ok:
+        return jsonify({"ok": False, "reason": "cosign_failed", "details": cosign_info}), 403
+
+    digest = None
+    if "@" in image:
+        digest = image.split("@",1)[1]
+    if digest:
+        rekor_ok, rekor_info = rekor_has_entry(digest)
+        if not rekor_ok:
+            return jsonify({"ok": False, "reason": "rekor_missing", "details": rekor_info}), 403
+    # succeed
+    return jsonify({"ok": True, "cosign": cosign_info})
+
+@app.route("/health", methods=["GET"])
+def health():
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8443)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/attestation_hardening.md
+# Attestation Hardening Checklist
+
+1) Deploy attestation service as a Deployment with 3 replicas, readiness & liveness probes.
+2) Use cert-manager to issue a certificate and inject CA bundle into ValidatingWebhookConfiguration (scripts/inject_ca_bundle.sh).
+3) Set ValidatingWebhookConfiguration.failurePolicy=Fail and reasonable timeoutSeconds (10â€“15).
+4) Gatekeeper checks for aegis.attested annotation only; attestation service guarantees Rekor presence.
+5) Monitor attestation service: Prometheus alert AttestationServiceDown on absent(up{job="attestation"} == 1).
+6) Add an emergency bypass process in runbooks to temporarily pause gatekeeper constraints during platform maintenance.
+
*** End Patch
*** Begin Patch
*** Add File: argo/deepspeed_zeo_workflow_poc.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: deepspeed-poc
+  namespace: staging
+spec:
+  entrypoint: train
+  templates:
+  - name: train
+    dag:
+      tasks:
+      - name: deepspeed
+        template: deepspeed-poc-task
+
+  - name: deepspeed-poc-task
+    container:
+      image: ${DEEPSPEED_IMAGE:-ghcr.io/yourorg/deepspeed-poc:latest}
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -euxo pipefail
+          export NCCL_DEBUG=INFO
+          export MASTER_ADDR=$(hostname -i)
+          export MASTER_PORT=29500
+          mkdir -p /data/checkpoints
+          deepspeed --num_nodes ${NUM_NODES:-2} --num_gpus ${NUM_GPUS_PER_NODE:-4} train.py --ckpt_path /data/checkpoints --epochs 1
+      resources:
+        requests:
+          cpu: "24"
+          memory: "200Gi"
+        limits:
+          nvidia.com/gpu: 4
+    nodeSelector:
+      aegis/worker-type: dgx
+    volumeMounts:
+      - name: data
+        mountPath: /data
+    volumes:
+      - name: data
+        persistentVolumeClaim:
+          claimName: deepspeed-data-pvc
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_deepspeed_validation.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Run a sequence: submit deepspeed workflow, wait, inject a failure, restart to validate resume
+WORKFLOW_NAME=${1:-deepspeed-validation}
+kubectl apply -f argo/dgx_distributed_deepspeed_validation.yaml
+echo "Submitted deepspeed validation workflow. Wait 60s to allow start"
+sleep 60
+# identify trainer pods and simulate kill
+TRAINER_POD=$(kubectl get pods -n staging -l app=deepspeed-validation -o jsonpath='{.items[0].metadata.name}' || true)
+if [ -n "$TRAINER_POD" ]; then
+  echo "Killing pod $TRAINER_POD to simulate preemption"
+  kubectl delete pod "$TRAINER_POD" -n staging --grace-period=0 --force || true
+fi
+echo "Wait and resubmit resume job"
+sleep 10
+kubectl apply -f argo/deepspeed_zeo_workflow_poc.yaml
+echo "Validation run submitted (resume attempt). Monitor via: argo get --watch <workflow>"
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_tpu_validation.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Submit TPU POC workflow (requires GKE TPU-enabled cluster)
+kubectl apply -f argo/gke_tpu_poc_workflow.yaml
+echo "TPU POC workflow submitted. Monitor with: argo get --watch gke-tpu-train-poc"
+
*** End Patch
*** Begin Patch
*** Add File: helm/argo/values-autoscaler-sample.yaml
+# Sample values for Argo/Scaler integration and spot orchestration (adapter)
+controller:
+  extraArgs:
+    - --concurrent-workflows=50
+  resources:
+    requests:
+      cpu: 500m
+      memory: 1Gi
+workflows:
+  containerRuntimeExecutor: k8sapi
+
+# Autoscaler pattern: create nodeGroup taints/labels and use pod tolerations in job specs (see scripts/spot_autoscaler_helper.sh)
+
*** End Patch
*** Begin Patch
*** Add File: docs/poc_bundle_readme.md
+# Aegis POC Bundle (training + attestation + developer workspaces)
+
+Overview
+- This POC bundle provisions KMS keys (multi-cloud), deploys Vault/attestation/Gatekeeper/Argo/JupyterHub and runs a DeepSpeed or TPU POC training workflow.
+
+Steps
+1) Provision KMS & set GH secrets:
+   ./scripts/provision_kms_and_set_secrets_multi.sh <cloud> <owner/repo>
+2) Deploy core infra & attestation:
+   ./scripts/run_poc_bundle.sh <cloud> <dgx|gke-tpu|gke-gpu>
+3) Trigger CI POC build & sign:
+   Use GitHub Actions workflow "POC CI" (workflow_dispatch).
+4) Verify attestation:
+   ./scripts/attestation_smoke_test.sh
+5) Run training validation:
+   ./scripts/run_deepspeed_validation.sh
+
+Notes
+- Fill secrets: REKOR_URL, REGISTRY_HOST, REGISTRY_USER, REGISTRY_TOKEN, VAULT_ADDR, VAULT_TOKEN, and KUBE_CONFIG_DATA in GitHub repository secrets before running CI.
+- For PKCS#11 HSM signing, use a self-hosted runner with HSM access; set CLOUD_PROVIDER=pkcs11 and provide PKCS11_* env secrets.
+
*** End Patch
*** End Patch
