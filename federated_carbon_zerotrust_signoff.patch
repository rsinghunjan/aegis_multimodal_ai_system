*** Begin Patch
*** Add File: federated/orchestrator.py
+#!/usr/bin/env python3
+"""
+Simple Federated Learning Orchestrator prototype.
+ - dispatches rounds to clients (via SSH/Ansible or k8s Jobs)
+ - collects encrypted model updates (simulated)
+ - performs secure aggregation (simple weighted average here)
+ - logs round & aggregate model to MLflow
+
+This is a prototype for demonstration only. Replace transport/encryption
+with production-grade secure aggregation (e.g., MPC / secure enclave).
+"""
+import argparse
+import json
+import os
+import time
+from pathlib import Path
+import mlflow
+
+CLIENTS_FILE = "federated/clients.json"  # list of client endpoints / ids
+LOCAL_UPDATES_DIR = "/tmp/fed_updates"
+
+def load_clients(path=CLIENTS_FILE):
+    if not os.path.exists(path):
+        return []
+    return json.load(open(path))
+
+def dispatch_round(round_id, clients):
+    # In a real system you'd trigger remote training (k8s job, ssh, or device agent).
+    # Here we simulate by creating placeholder update files per client.
+    os.makedirs(LOCAL_UPDATES_DIR, exist_ok=True)
+    updates=[]
+    for c in clients:
+        fn = f"{LOCAL_UPDATES_DIR}/update_{c['id']}_r{round_id}.json"
+        # Simulate a client update (weights as small vector)
+        update = {"client": c["id"], "weights": [round_id + 0.1, round_id + 0.2], "n": 100}
+        json.dump(update, open(fn, "w"))
+        updates.append(fn)
+    return updates
+
+def aggregate_updates(update_files):
+    # naive weighted average
+    total_n=0
+    acc=None
+    for f in update_files:
+        u=json.load(open(f))
+        n=u.get("n",1)
+        w=u.get("weights",[])
+        if acc is None:
+            acc=[0.0]*len(w)
+        for i,wi in enumerate(w):
+            acc[i]+=wi*n
+        total_n+=n
+    if total_n==0:
+        return []
+    return [x/total_n for x in acc]
+
+def log_round_mlflow(round_id, client_files, aggregate_weights, mlflow_uri=None, experiment="federated_demo"):
+    if mlflow_uri:
+        mlflow.set_tracking_uri(mlflow_uri)
+    mlflow.set_experiment(experiment)
+    with mlflow.start_run(run_name=f"fed_round_{round_id}"):
+        mlflow.log_param("round", round_id)
+        mlflow.log_param("num_clients", len(client_files))
+        for f in client_files:
+            mlflow.log_artifact(f, artifact_path="client_updates")
+        # log aggregate
+        ag_path=f"/tmp/fed_aggregate_r{round_id}.json"
+        json.dump({"weights":aggregate_weights}, open(ag_path,"w"))
+        mlflow.log_artifact(ag_path, artifact_path="aggregate")
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--round", type=int, default=1)
+    p.add_argument("--mlflow-uri", default=os.environ.get("MLFLOW_TRACKING_URI"))
+    args=p.parse_args()
+
+    clients=load_clients()
+    if not clients:
+        print("No clients found in federated/clients.json. Create placeholder clients and re-run.")
+        return
+    print(f"Dispatching round {args.round} to {len(clients)} clients")
+    update_files=dispatch_round(args.round, clients)
+    print("Collected updates:", update_files)
+    agg=aggregate_updates(update_files)
+    print("Aggregate weights:", agg)
+    log_round_mlflow(args.round, update_files, agg, mlflow_uri=args.mlflow_uri)
+    print("Round logged to MLflow")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: federated/client_runner.py
+#!/usr/bin/env python3
+"""
+Client-side federated runner prototype.
+ - Runs a small local "training" step (simulated)
+ - Produces an encrypted update artifact (here simulated as JSON)
+ - Uploads artifact to an orchestrator-accessible location (local fs for prototype)
+"""
+import argparse, json, os, random, time
+
+def run_local_training(client_id, round_id, output_dir="/tmp/fed_updates"):
+    os.makedirs(output_dir, exist_ok=True)
+    # Simulate training: produce small weight vector + count
+    weights=[random.random() + round_id for _ in range(2)]
+    update={"client":client_id,"weights":weights,"n":random.randint(50,150)}
+    fn=f"{output_dir}/update_{client_id}_r{round_id}.json"
+    json.dump(update, open(fn,"w"))
+    print("Wrote local update to",fn)
+    return fn
+
+def main():
+    import sys
+    p=argparse.ArgumentParser()
+    p.add_argument("--client-id", required=True)
+    p.add_argument("--round", type=int, default=1)
+    args=p.parse_args()
+    run_local_training(args.client_id, args.round)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: federated/mlflow_helpers.py
+#!/usr/bin/env python3
+"""
+Helpers to enrich MLflow federated runs with client metadata.
+Call mlflow.log_param/artifact as appropriate from orchestrator/client.
+"""
+import mlflow, os
+
+def with_mlflow(uri=None, experiment="federated_demo"):
+    if uri:
+        mlflow.set_tracking_uri(uri)
+    mlflow.set_experiment(experiment)
+
+def log_client_metadata(client_id, metadata):
+    mlflow.log_param(f"client.{client_id}.meta", str(metadata))
+
*** End Patch
*** Begin Patch
*** Add File: federated/clients.json
+[
+  {"id":"device-a","addr":"ssh://device-a.example.com"},
+  {"id":"device-b","addr":"ssh://device-b.example.com"},
+  {"id":"device-c","addr":"ssh://device-c.example.com"}
+]
+
*** End Patch
*** Begin Patch
*** Add File: federated/README.md
+````markdown
+# Federated Learning Prototype
+
+This prototype demonstrates a minimal federated learning flow:
+
+- federated/orchestrator.py: dispatches rounds, collects updates, aggregates and logs to MLflow
+- federated/client_runner.py: client-side runner that simulates local training and writes update artifacts
+- federated/mlflow_helpers.py: convenience helpers to enrich MLflow runs
+- federated/clients.json: sample client list
+
+How to run (local prototype):
+
+1. Ensure MLflow server is available or set `MLFLOW_TRACKING_URI`:
+   export MLFLOW_TRACKING_URI=http://mlflow:5000
+
+2. Simulate client runs (on each client or locally):
+   python federated/client_runner.py --client-id device-a --round 1
+   python federated/client_runner.py --client-id device-b --round 1
+   python federated/client_runner.py --client-id device-c --round 1
+
+3. Run orchestrator to aggregate & log:
+   python federated/orchestrator.py --round 1 --mlflow-uri $MLFLOW_TRACKING_URI
+
+Notes:
+- This is a demo. Replace local file transport with secure transport (k8s jobs, S3 with encryption, or device agent).
+- For production, integrate secure aggregation (MPC/HE) and authenticated/encrypted channels.
+````
+
*** End Patch
*** Begin Patch
*** Add File: carbon/scheduler_carbon_hook.py
+#!/usr/bin/env python3
+"""
+Carbon-aware scheduling decision hook prototype.
+ - Reads provider carbon metadata from a ConfigMap file or API
+ - Scores candidate providers by carbon intensity + cost + latency constraints
+ - Emits chosen provider as stdout JSON (for scheduler integration)
+"""
+import json, os, argparse
+from datetime import datetime
+
+CONFIG_PATH="/etc/aegis/carbon_config.json"  # mounted ConfigMap
+
+def load_config(path=CONFIG_PATH):
+    if os.path.exists(path):
+        return json.load(open(path))
+    # fallback example config
+    return {
+        "providers":{
+            "braket":{"carbon_g_per_kwh":400,"region":"us-west-2","cost_per_job":1.0},
+            "ibm":{"carbon_g_per_kwh":200,"region":"us-east-1","cost_per_job":1.2}
+        },
+        "weight_carbon":0.6,
+        "weight_cost":0.3,
+        "weight_latency":0.1
+    }
+
+def score_provider(pmeta, weights, latency_est=100):
+    # simple scoring: lower is better
+    carbon = pmeta.get("carbon_g_per_kwh",1000)
+    cost = pmeta.get("cost_per_job",1.0)
+    return weights["weight_carbon"]*carbon + weights["weight_cost"]*cost + weights["weight_latency"]*latency_est
+
+def pick_provider(candidates, cfg):
+    weights={"weight_carbon":cfg.get("weight_carbon",0.6),"weight_cost":cfg.get("weight_cost",0.3),"weight_latency":cfg.get("weight_latency",0.1)}
+    best=None
+    best_score=1e18
+    for p in candidates:
+        meta=cfg["providers"].get(p,{})
+        # latency_est could be dynamic; use fixed for prototype
+        score=score_provider(meta, weights, latency_est=meta.get("latency_ms",100))
+        if score < best_score:
+            best_score=score
+            best=p
+    return best, best_score
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--candidates", nargs="+", required=True)
+    p.add_argument("--config", default=CONFIG_PATH)
+    args=p.parse_args()
+    cfg=load_config(args.config)
+    best,score=pick_provider(args.candidates, cfg)
+    out={"selected":best,"score":score,"timestamp":datetime.utcnow().isoformat()}
+    print(json.dumps(out))
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: carbon/carbon_configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: aegis-carbon-config
+  namespace: aegis
+data:
+  carbon_config.json: |
+    {
+      "providers": {
+        "braket": {"carbon_g_per_kwh": 400, "region":"us-west-2", "cost_per_job": 1.0, "latency_ms": 120},
+        "ibm": {"carbon_g_per_kwh": 200, "region":"us-east-1", "cost_per_job": 1.2, "latency_ms": 80},
+        "quantinuum": {"carbon_g_per_kwh": 250, "region":"eu-west-1","cost_per_job":1.5,"latency_ms":150}
+      },
+      "weight_carbon": 0.6,
+      "weight_cost": 0.3,
+      "weight_latency": 0.1
+    }
+
*** End Patch
*** Begin Patch
*** Add File: carbon/README.md
+````markdown
+# Carbon-aware Scheduling Prototype
+
+Files:
+- carbon/scheduler_carbon_hook.py — selection hook to pick provider based on carbon+cost+latency
+- carbon/carbon_configmap.yaml — sample ConfigMap to mount as /etc/aegis/carbon_config.json in scheduler pods
+
+Integration:
+- Mount the ConfigMap at /etc/aegis/carbon_config.json in your scheduler pod and call the hook:
+  python carbon/scheduler_carbon_hook.py --candidates braket ibm quantinuum
+
+Dashboard:
+- You can build a Grafana panel that reads provider carbon metadata (ingest carbon metadata into Prometheus or Loki).
+
+Notes:
+- Replace static carbon values with a real carbon API (ElectricityMap or provider-supplied metrics).
+````
+
*** End Patch
*** Begin Patch
*** Add File: zero_trust/mtls/sidecar_example.yaml
+apiVersion: v1
+kind: Pod
+metadata:
+  name: example-service-with-sidecar
+  namespace: aegis
+spec:
+  containers:
+    - name: app
+      image: alpine:3.18
+      command: ["sh","-c","while true; do echo serving; sleep 30; done"]
+      ports:
+        - containerPort: 8080
+      volumeMounts:
+        - name: certs
+          mountPath: /etc/tls
+    - name: envoy-sidecar
+      image: envoyproxy/envoy:v1.24.0
+      args: ["-c","/etc/envoy/envoy.yaml"]
+      ports:
+        - containerPort: 10000
+      volumeMounts:
+        - name: envoy-config
+          mountPath: /etc/envoy
+        - name: certs
+          mountPath: /etc/tls
+  volumes:
+    - name: envoy-config
+      configMap:
+        name: aegis-envoy-config
+    - name: certs
+      secret:
+        secretName: aegis-service-certs
+
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: aegis-envoy-config
+  namespace: aegis
+data:
+  envoy.yaml: |
+    static_resources:
+      listeners:
+      - name: listener_0
+        address:
+          socket_address: { address: 0.0.0.0, port_value: 10000 }
+        filter_chains:
+        - filters:
+          - name: envoy.filters.network.http_connection_manager
+            typed_config:
+              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
+              stat_prefix: ingress_http
+              route_config:
+                name: local_route
+                virtual_hosts:
+                - name: backend
+                  domains: ["*"]
+                  routes:
+                  - match: { prefix: "/" }
+                    route: { cluster: app_service }
+              http_filters:
+              - name: envoy.filters.http.router
+      clusters:
+      - name: app_service
+        connect_timeout: 1s
+        type: STATIC
+        lb_policy: ROUND_ROBIN
+        load_assignment:
+          cluster_name: app_service
+          endpoints:
+          - lb_endpoints:
+            - endpoint:
+                address:
+                  socket_address:
+                    address: 127.0.0.1
+                    port_value: 8080
+
*** End Patch
*** Begin Patch
*** Add File: zero_trust/ansible/tpm_attestation_bootstrap.yml
+- name: TPM attestation bootstrap playbook
+  hosts: runners
+  become: true
+  vars:
+    tpm_tool_package: "tpm2-tools"
+  tasks:
+    - name: Ensure TPM tools present
+      apt:
+        name: "{{ tpm_tool_package }}"
+        state: present
+      ignore_errors: yes
+
+    - name: Run TPM quote (example)
+      command: tpm2_quote -c 0x81010001 -l sha256:0x0004 -q -m /tmp/tpm_quote.bin
+      register: quote
+      ignore_errors: yes
+
+    - name: Upload quote to attestation server (operator step)
+      debug:
+        msg: "Upload /tmp/tpm_quote.bin to attestation server and verify measurement"
+
+    - name: Fail if no TPM present
+      fail:
+        msg: "TPM attestation failed or TPM not present"
+      when: quote is failed
+
*** End Patch
*** Begin Patch
*** Add File: zero_trust/README.md
+````markdown
+# Zero‑Trust Hardening Examples
+
+Files:
+- zero_trust/mtls/sidecar_example.yaml — example of running an Envoy sidecar to enforce mTLS and centralize TLS termination
+- zero_trust/ansible/tpm_attestation_bootstrap.yml — playbook to collect a TPM quote from devices and guide attestation
+
+Usage notes:
+- Create Kubernetes TLS secrets (aegis-service-certs) containing key/cert for the service and Envoy to enforce mutual TLS.
+- Replace Envoy config with your service-specific routing and mutual TLS listener config (this example is minimal).
+- Run Ansible playbook against runners inventory to collect TPM quotes and upload to your attestation service for verification.
+````
+
*** End Patch
*** Begin Patch
*** Add File: security/secret_blind_rotate.sh
+#!/usr/bin/env bash
+#
+# Secret blind rotate helper (prototype)
+# - rotates a secret by creating a new secret value and updating target secret stores (GH/GCP/AWS) via admin pipeline
+# - does not print secret values to logs (blind rotate). Callers must provide provider-specific update commands via env.
+set -euo pipefail
+
+echo "Starting blind rotate at $(date -u)"
+ROTATE_ID=$(uuidgen 2>/dev/null || echo "rotate-$(date +%s)")
+echo "rotate id: ${ROTATE_ID}"
+
+# Example placeholder: call out to provider-specific secret update script that accepts new value on stdin
+# In CI you would generate new secret via vault/awskms and call API to set secret; do not echo secrets.
+
+if [ -z "${SECRET_UPDATE_CMD:-}" ]; then
+  echo "SECRET_UPDATE_CMD not set; aborting" >&2
+  exit 2
+fi
+
+# Generate new secret (placeholder -- replace with secure generator)
+NEW_SECRET=$(head -c 32 /dev/urandom | base64)
+
+# Pipe secret into update command without logging it
+echo -n "${NEW_SECRET}" | eval "${SECRET_UPDATE_CMD}" >/dev/null 2>&1
+
+echo "Secret rotate command executed (id=${ROTATE_ID}). Please verify access."
+echo "Rotate complete."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/secret_blind_rotate.yml
+name: Secret Blind Rotate (prototype)
+on:
+  workflow_dispatch:
+jobs:
+  blind-rotate:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup environment
+        run: |
+          python - <<'PY'
+          print("Set up placeholder for SECRET_UPDATE_CMD in repo secrets or workflow_sepcrets.")
+          PY
+
+      - name: Run blind rotate script
+        env:
+          SECRET_UPDATE_CMD: ${{ secrets.SECRET_UPDATE_CMD }} # must be an admin script that consumes secret from stdin
+        run: |
+          chmod +x security/secret_blind_rotate.sh
+          ./security/secret_blind_rotate.sh
+
*** End Patch
*** Begin Patch
*** Add File: signoff/generate_signoff.py
+#!/usr/bin/env python3
+"""
+Generate a simple production sign-off package (Markdown -> PDF if pandoc available).
+Collects evidence pointers (S3/GCS urls, MLflow run ids) and writes a signoff document.
+"""
+import argparse, json, os, subprocess, tempfile
+from datetime import datetime
+
+TEMPLATE = """# Aegis Production Sign-off
+
+Generated: {ts}
+
+Operator: {operator}
+
+Summary:
+{summary}
+
+Evidence:
+{evidence_blocks}
+
+Checklist:
+{checklist}
+
+"""
+
+def mk_md(operator, summary, evidence, checklist):
+    ev=""
+    for k,v in evidence.items():
+        ev += f"- **{k}**: {v}\\n"
+    cl=""
+    for item in checklist:
+        cl += f"- [ ] {item}\\n"
+    return TEMPLATE.format(ts=datetime.utcnow().isoformat(), operator=operator, summary=summary, evidence_blocks=ev, checklist=cl)
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--operator", default=os.environ.get("USER","operator"))
+    p.add_argument("--summary", default="Aegis platform production sign-off.")
+    p.add_argument("--evidence-json", default="")
+    p.add_argument("--out", default="signoff/aegis_signoff.md")
+    args=p.parse_args()
+
+    evidence={}
+    if args.evidence_json and os.path.exists(args.evidence_json):
+        evidence=json.load(open(args.evidence_json))
+    else:
+        # placeholder evidence items
+        evidence={
+            "BAA S3 URL": "s3://my-compliance-bucket/compliance/BAA/braket/signed_baa.pdf",
+            "Cosign Rekor entry": "https://rekor.example/entry/abcd",
+            "CloudHSM audit": "/tmp/cloudhsm_audit.json",
+            "MLflow mitigation run": "mlflow://run/1234"
+        }
+
+    checklist=[
+        "BAA signed and attested",
+        "CloudHSM PKCS11 sign/verify validated",
+        "Jetson pilot validated and CPU/GPU tests passed",
+        "Provider live test aggregates and adapter tuning applied",
+        "Billing reconciliation run and exceptions triaged"
+    ]
+
+    md=mk_md(args.operator, args.summary, evidence, checklist)
+    os.makedirs(os.path.dirname(args.out), exist_ok=True)
+    with open(args.out,"w") as fh:
+        fh.write(md)
+    print("Wrote signoff markdown to", args.out)
+    # try to convert to PDF if pandoc available
+    pdf_out=args.out.replace(".md",".pdf")
+    try:
+        subprocess.run(["pandoc", args.out, "-o", pdf_out], check=True)
+        print("Generated PDF:", pdf_out)
+    except Exception:
+        print("Pandoc not available; skipping PDF generation")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: signoff/README.md
+````markdown
+# Production Sign-off Package
+
+Files:
+- signoff/generate_signoff.py — generate a signoff markdown and attempt to produce a PDF via `pandoc`
+
+Usage:
+- Prepare an evidence JSON mapping keys to artifact URLs, or rely on placeholders:
+  python signoff/generate_signoff.py --operator alice --evidence-json ./evidence.json --out ./signoff/aegis_signoff.md
+
+- If `pandoc` is installed the script will also create `signoff/aegis_signoff.pdf`.
+
+Deliver to auditors:
+- Package `signoff/aegis_signoff.md` and `signoff/aegis_signoff.pdf` (if created) along with evidence artifacts.
+````
+
*** End Patch
*** End Patch
