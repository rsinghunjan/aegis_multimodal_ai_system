*** Begin Patch
*** Add File:dbt/dbt_project.yml
name: aegis_bi
version: '1.0'
config-version: 2

profile: aegis_dw_profile

source-paths: ["models"]
target-path: "target"
clean-targets:
  - "target"
  - "dbt_modules"

*** End Patch
*** Begin Patch
*** Add File:dbt/models/schema.yml
version: 2

models:
  - name: daily_team_rollup
    description: "Daily per-team carbon & cost rollups"
    tests:
      - dbt_utils.expression_is_true:
          expression: "jobs >= 0"
    columns:
      - name: team
        tests:
          - not_null
      - name: day
        tests:
          - not_null

  - name: cost_allocation
    description: "Allocated cost per team"
    columns:
      - name: team
        tests:
          - not_null
      - name: allocated_cost_usd
        tests:
          - not_null

  - name: model_performance
    description: "Per-model serving/training performance metrics (daily)"
    columns:
      - name: model_name
        tests:
          - not_null
      - name: day
        tests:
          - not_null

*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/daily_team_rollup.sql
-- Daily per-team rollup: estimated & measured carbon + basic cost allocation
with core as (
    select
      job_id,
      coalesce(team,'unknown') as team,
      start_ts::timestamp as start_ts,
      estimated_kg,
      measured_emissions_kg,
      coalesce(allocated_cost_usd, 0) as allocated_cost_usd
    from {{ ref('core_job_runs') }}
)
select
  team,
  date_trunc('day', start_ts)::date as day,
  count(*) as jobs,
  sum(coalesce(estimated_kg,0)) as total_estimated_kg,
  sum(coalesce(measured_emissions_kg,0)) as total_measured_kg,
  sum(coalesce(allocated_cost_usd,0)) as allocated_cost_usd,
  avg(coalesce(estimated_kg,0)) as avg_estimated_kg_per_job
from core
group by 1,2

*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/cost_allocation.sql
-- Allocate cloud costs to teams using tags and usage heuristics
with costs as (
    select
      resource_id,
      cost_usd,
      tags->>'team' as team
    from {{ source('raw','raw_costs') }}
    where cost_usd is not null
),
usage as (
    select
      team,
      sum(coalesce(cpu,0) + coalesce(gpu,0) * 4) as compute_score
    from {{ ref('core_job_runs') }}
    group by team
),
team_costs as (
    select
      coalesce(c.team, 'unallocated') as team,
      sum(c.cost_usd) as total_cost
    from costs c
    group by 1
)
select
  tc.team,
  tc.total_cost as allocated_cost_usd,
  coalesce(u.compute_score,0) as compute_score
from team_costs tc
left join usage u on u.team = tc.team

*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/model_performance.sql
-- Per-model performance aggregations (daily)
select
  model_name,
  date_trunc('day', start_ts)::date as day,
  count(*) as requests,
  avg(duration_s) as avg_duration_s,
  approx_percentile(duration_s, 0.95) as p95_duration_s,
  sum(case when status != 'success' then 1 else 0 end) as errors
from {{ ref('core_job_runs') }}
group by 1,2

*** End Patch
*** Begin Patch
*** Add File:dbt/macros/approx_percentile.sql
{% macro approx_percentile(column, q) -%}
  -- Adapter-specific percentile implementation placeholder
  -- For Postgres-compatible warehouses use percentile_cont if supported; for others replace accordingly.
  percentile_cont({{ q }}) within group (order by {{ column }})
{%- endmacro %}

*** End Patch
*** Begin Patch
*** Add File:superset/finance_dashboard.json
{
  "dashboard": {
    "dashboard_title": "Aegis Finance Dashboard",
    "position_json": "{}",
    "slices": [
      {
        "slice_name": "Daily Team Rollup",
        "viz_type": "table",
        "params": {
          "groupby": ["team", "day"],
          "metrics": ["allocated_cost_usd", "total_estimated_kg", "total_measured_kg"]
        },
        "data_source_name": "marts.daily_team_rollup"
      },
      {
        "slice_name": "Top Models by Cost",
        "viz_type": "table",
        "params": {
          "groupby": ["model_name"],
          "metrics": ["sum_allocated_cost_usd"]
        },
        "data_source_name": "marts.model_performance"
      }
    ]
  }
}
*** End Patch
*** Begin Patch
*** Add File:superset/carbon_dashboard.json
{
  "dashboard": {
    "dashboard_title": "Aegis Carbon Dashboard",
    "position_json": "{}",
    "slices": [
      {
        "slice_name": "Team Carbon Trend",
        "viz_type": "line",
        "params": {
          "groupby": ["team"],
          "metric": "total_estimated_kg",
          "time_grain_sqla": "P1D",
          "time_range": "Last 30 days"
        },
        "data_source_name": "marts.daily_team_rollup"
      },
      {
        "slice_name": "Estimator Accuracy Snapshot",
        "viz_type": "table",
        "params": {
          "groupby": ["team", "day"],
          "metrics": ["total_estimated_kg", "total_measured_kg"]
        },
        "data_source_name": "marts.daily_team_rollup"
      }
    ]
  }
}
*** End Patch
*** Begin Patch
*** Add File:airflow/dags/bi_etl_pipeline.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
import subprocess, os
from operational.airflow_idempotent_helper import already_done_marker, mark_done

DEFAULT_ARGS = {
    'owner': 'aegis',
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
}

WAREHOUSE_DSN = os.environ.get("WAREHOUSE_DSN", "postgresql://user:pass@host:5432/aegis_dw")
JOB_KEY = "bi_etl_daily_" + datetime.utcnow().strftime("%Y%m%d")

def run_parquet_pipeline():
    # idempotent via helper; uses s3_parquet_pipeline_v3.py
    job_key = "parquet_pipeline_" + datetime.utcnow().strftime("%Y%m%d")
    if already_done_marker(WAREHOUSE_DSN, job_key):
        print("Parquet pipeline already completed today; skipping.")
        return
    ret = subprocess.run("python3 /opt/ingest/s3_parquet_pipeline_v3.py", shell=True)
    if ret.returncode != 0:
        raise RuntimeError("Parquet pipeline failed")
    mark_done(WAREHOUSE_DSN, job_key)

def generate_manifest():
    subprocess.check_call("python3 /opt/ingest/copy_manifest_generator.py", shell=True)

def run_ge_checks():
    ret = subprocess.run("python3 /opt/data_quality/run_expectations.py", shell=True)
    if ret.returncode != 0:
        raise RuntimeError("Great Expectations validations failed")

with DAG(
    dag_id="aegis_bi_etl",
    default_args=DEFAULT_ARGS,
    schedule_interval="@daily",
    start_date=datetime(2025,1,1),
    catchup=False,
    max_active_runs=1
) as dag:

    parquet = PythonOperator(task_id="parquet_pipeline", python_callable=run_parquet_pipeline)

    manifest = PythonOperator(task_id="generate_manifest", python_callable=generate_manifest)

    bulk_load = KubernetesPodOperator(
        task_id="bulk_load_to_dw",
        name="dw-bulk-loader",
        namespace="aegis",
        image="ghcr.io/yourorg/aegis-dw-loader:latest",
        cmds=["/bin/sh","-c"],
        arguments=["/opt/etl/redshift_loader_wrapper.sh"],
        get_logs=True,
        is_delete_operator_pod=True,
        in_cluster=True
    )

    ge = PythonOperator(task_id="great_expectations", python_callable=run_ge_checks)

    parquet >> manifest >> bulk_load >> ge

*** End Patch
*** Begin Patch
*** Add File:datahub/datahub_ingest_recipe.yml
source:
  type: dbt
  config:
    manifest_path: /opt/bi/target/manifest.json
    catalog_path: /opt/bi/target/catalog.json
    project_name: aegis_bi
transformations: []
sink:
  type: datahub-rest
  config:
    server: ${DATAHUB_SERVER}
    api_key: ${DATAHUB_API_KEY}

*** End Patch
*** Begin Patch
*** Add File:datahub/README_DATAHUB.md
# DataHub ingestion & lineage integration (Aegis)

This short handbook explains how to capture lineage from the dbt project into DataHub and link datasets to evidence artifacts.

Prereqs
- DataHub GMS available and reachable (DATAHUB_SERVER)
- dbt run executed and `target/manifest.json` + `target/catalog.json` present
- DataHub CLI or ingestion container (datahub-ingestion)

Steps
1. Prepare dbt manifest
   - Run `cd dbt && dbt run --profiles-dir ./profiles` to generate manifest & catalog.

2. Run DataHub ingestion
   - Deploy an ingestion job/container that mounts the dbt `target` directory and runs:
     datahub ingest -c /opt/datahub/datahub_ingest_recipe.yml
   - Ensure DATAHUB_SERVER and DATAHUB_API_KEY are set in environment.

3. Evidence linkage (recommendation)
   - Add a column `evidence_s3_key` to key marts (e.g., core_job_runs) in dbt models so each row links to signed evidence in S3.
   - Use dbt metadata or DataHub automations to surface provenance of metrics back to S3.

4. Owners & tags
   - Ensure models have `meta.owner` and `meta.team` in dbt model config to populate DataHub owner fields.

Notes
- Run DataHub ingestion after every dbt run (or as separate Airflow task) to keep lineage current.

*** End Patch
*** Begin Patch
*** Add File:onboarding/ONBOARDING_PLAYBOOK.md
# One‑page Onboarding Playbook for Data Scientists (Aegis)

Welcome. This playbook helps a new data scientist get started with Aegis analytics and model work.

1) Access & Tools
- Request access: GitHub repo, S3 evidence bucket, data warehouse (ask BI team), Superset (SSO).
- Local tools: Python 3.11, dbt-core, awscli, psql (or cloud DW CLI), Jupyter.

2) Getting data
- Raw evidence: S3 prefix s3://<EVIDENCE_BUCKET>/evidence/
- Parquet batches: s3://<EVIDENCE_BUCKET>/parquet/evidence/start_dt=<date>/
- Use Athena/BigQuery or the DW to query dbt marts.

3) Key dbt marts to use
- marts.daily_team_rollup — per-team daily carbon & cost
- marts.cost_allocation — allocated cost per team
- marts.model_performance — per-model latency/error aggregates

4) Example quick queries
- Total carbon by team last 7 days:
  SELECT team, sum(total_estimated_kg) FROM marts.daily_team_rollup WHERE day >= CURRENT_DATE - INTERVAL '7 days' GROUP BY team;
- Model p95 latency last 30 days:
  SELECT model_name, day, p95_duration_s FROM marts.model_performance WHERE day >= CURRENT_DATE - INTERVAL '30 days' ORDER BY p95_duration_s DESC LIMIT 50;

5) Notebooks & experiments
- Use the dbt marts as your canonical sources; snapshot inputs for experiments (store dataset snapshot S3 path in experiment metadata).
- Log runs & artifacts to MLflow (tag with team, model_id, dataset_s3_key).

6) Promotion & governance
- If you develop a new feature/model to be used in production:
  - Register experiment in MLflow.
  - Produce reconciliation reports for sample runs.
  - Request CI validation & manual promotion (the repo's CI gating prevents promotions unless median error <= 10%).

7) Support & contacts
- ML Platform: ml-platform@example.com
- Data Eng: data-eng@example.com
- BI / Dashboards: analytics@example.com

Welcome aboard — if you want, I can generate a starter notebook that queries the marts and demonstrates joining model performance with cost allocation.

*** End Patch
*** End Patch
