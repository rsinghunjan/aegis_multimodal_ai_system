*** Begin Patch
*** Add File:productization/pilot_package/PILOT_PACKAGE.md
+Pilot Package — Aegis Enterprise Pilot
+=====================================
+
+Goal
+- Deliver a turnkey 6-week pilot to validate Aegis in customer's environment and produce production rollout artifacts.
+
+Scope (6 weeks)
+- Week 0: Kickoff, discovery & environment prep
+- Week 1: Tenant/IAM finalization + KMS auto-unseal
+- Week 2: Provision managed backends (ElastiCache, S3 lakeFS) & migrate test datasets
+- Week 3: Deploy platform core (Vault HA, Agent Manager, Agent Memory, Tool Runner), configure observability and SLOs
+- Week 4: Onboard 1-2 models (HF import -> MLflow), promote+sign -> deploy -> run perf & SLO validation
+- Week 5: DR drill & Pentest intake run; begin remediation triage
+- Week 6: Handoff, training, runbook delivery, acceptance & signoff
+
+Deliverables
+- Fully provisioned staging environment (Terraform + Helm manifests)
+- Tenant finalization commands and populated GH secret manifest (operator-run)
+- Two sample models in MLflow with Rekor/cosign provenance
+- SLO dashboards, playbooks, DR drill report, pentest bundle
+- Training: 2-day hands-on workshop + lab sandboxes
+- Handoff: runbooks, SLAs, support plan and a 30-day remediation backlog
+
+Acceptance criteria
+- End-to-end promotion -> sign -> deploy -> serve passes SLOs for promoted models
+- Vault auto-unseal configured using KMS and audit forwarding to SIEM
+- DR drill executed (dry-run at minimum) and RTO/RPO documented
+ 
+Pricing / Commercial
+- Fixed price pilot; full pricing template in PILOT_PRICING.yml
+
+Notes
+- All actions require operator review before Terraform applies or live restores.
+
*** End Patch
*** Begin Patch
*** Add File:productization/pilot_package/PILOT_PRICING.yml
+pilot_tier: enterprise
+duration_weeks: 6
+base_fee_usd: 75000
+services:
+  - tenant_iam_finalization: 8000
+  - managed_backends_provision: 15000
+  - platform_deploy: 15000
+  - model_onboarding_and_validation: 12000
+  - pentest_intake_and_remediation_support: 5000
+  - training_2_day: 5000
+  - runbooks_and_handoff: 5000
+notes: "Adjust pricing by cloud provider complexity and dataset sizes. Travel & expenses extra if onsite."
+
*** End Patch
*** Begin Patch
*** Add File:training/curriculum/README.md
+Aegis Training Curriculum
+=========================
+
+Tracks:
+- Data Scientist (2 days): reproducible experiments (lakeFS + MLflow), model cards, explainability (SHAP).
+- MLOps Engineer (2 days): Feast, lakeFS, KServe/Triton serving, checkpointing, autoscale.
+- Platform / SecOps (2 days): Vault, KMS auto-unseal, OIDC, Rekor/cosign, OPA policies and admission.
+- Edge Workshop (1 day): TPM attestation, OTA signing, offline Rekor bundles.
+
+Each track includes:
+- Slide deck (PDF)
+- Hands-on labs (step-by-step) executed in sandbox cluster
+- Pre-built sandbox provisioning scripts (kind/k3d or cloud terraform)
+
+To run labs:
+- Use training/sandbox/provision_sandbox.sh to create ephemeral sandbox (local or cloud)
+- Follow lab instructions in training/labs/<track>
+
*** End Patch
*** Begin Patch
*** Add File:training/sandbox/provision_sandbox.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Provision a local training sandbox using kind (Kubernetes in Docker) and helm.
+#
+KIND_CLUSTER=${KIND_CLUSTER:-aegis-sandbox}
+NODE_IMAGE=${NODE_IMAGE:-kindest/node:v1.27.3}
+
+echo "Creating kind cluster ${KIND_CLUSTER}..."
+if ! command -v kind >/dev/null 2>&1; then
+  echo "Install kind: https://kind.sigs.k8s.io/"
+  exit 1
+fi
+
+kind create cluster --name "${KIND_CLUSTER}" --image "${NODE_IMAGE}"
+
+echo "Installing cert-manager, nginx-ingress, and local helm charts..."
+kubectl create namespace ops || true
+helm repo add jetstack https://charts.jetstack.io || true
+helm repo update
+helm upgrade --install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --wait
+
+echo "Sandbox ready. Use kubeconfig: kind get kubeconfig --name ${KIND_CLUSTER}"
+
*** End Patch
*** Begin Patch
*** Add File:training/labs/datasci_lab/README.md
+Data Scientist Lab — Reproducible Model Training and Promotion
+============================================================
+
+Objectives:
+- Use lakeFS to snapshot dataset
+- Train a sample model (notebook)
+- Log model to MLflow
+- Promote model via MLflow promote script and sign the artifact using cosign (Vault or local key)
+- Verify Rekor entry
+
+Steps:
+1) Provision sandbox (training/sandbox/provision_sandbox.sh)
+2) Run notebook training/notebooks/train_text_classification.ipynb
+3) Use hf_integration/import_hf_to_mlflow.py to import HF baseline model if desired
+4) Promote & sign: mlflow/scripts/mlflow_promote_and_sign.sh
+5) Verify rekors: cosign verify-blob --rekor-server https://rekor.sigstore.dev ...
+
*** End Patch
*** Begin Patch
*** Add File:examples/models/nlp/train_text_classification.py
+#!/usr/bin/env python3
+"""
+Minimal training script that logs to MLflow (TF/Pytorch placeholder).
+This is a template: adapt to use your framework & dataset (uses dummy data here).
+"""
+import mlflow
+import random, json
+
+with mlflow.start_run() as run:
+    mlflow.log_param("model", "demo-text-classifier")
+    acc = random.uniform(0.8, 0.95)
+    mlflow.log_metric("accuracy", acc)
+    # Simulate model artifact
+    with open("model.txt","w") as f:
+        f.write("dummy-model")
+    mlflow.log_artifact("model.txt", artifact_path="model")
+    print("Run:", run.info.run_id)
+
*** End Patch
*** Begin Patch
*** Add File:operator/operator_kopf/Dockerfile
+FROM python:3.10-slim
+WORKDIR /app
+COPY operator_kopf/requirements.txt /app/
+RUN pip install --no-cache-dir -r requirements.txt
+COPY operator_kopf /app
+CMD ["python", "main.py"]
+
*** End Patch
*** Begin Patch
*** Add File:operator/operator_kopf/requirements.txt
+kopf>=1.38.0
+kubernetes>=24.0.0
+requests
+PyYAML
+
*** End Patch
*** Begin Patch
*** Add File:operator/operator_kopf/main.py
+#!/usr/bin/env python3
+"""
+Kopf-based robust Agent Operator (scaffold).
+Features:
+- Leader election via Kubernetes configmaps (Kopf handles locking)
+- Handles Agent CR create/update/delete, performs OPA check via HTTP
+- Deploys Deployment with tool-runner sidecar and annotations for attestation
+"""
+import os, json, requests, kopf, asyncio
+from kubernetes import client, config
+
+OPA_URL = os.environ.get("OPA_URL", "http://opa.opa.svc.cluster.local:8181/v1/data/agent/policy/allow")
+NAMESPACE = os.environ.get("NAMESPACE", "default")
+
+@kopf.on.startup()
+def configure(settings: kopf.OperatorSettings, **_):
+    settings.scanning.disabled = True
+
+def opa_allow(spec):
+    try:
+        resp = requests.post(OPA_URL, json={"input": spec}, timeout=5)
+        if resp.status_code == 200:
+            return resp.json().get("result", {}).get("allow", False), resp.json().get("result", {}).get("reason", "")
+    except Exception as e:
+        return False, f"OPA error: {e}"
+    return False, "OPA unreachable"
+
+@kopf.on.create('aegis.example.com', 'v1', 'agents')
+def create_agent(body, spec, name, namespace, logger, **kwargs):
+    allowed, reason = opa_allow(spec)
+    api = client.AppsV1Api()
+    if not allowed:
+        logger.warn(f"OPA denied agent {name}: {reason}")
+        # Update status
+        return {"state":"Blocked", "message": reason}
+
+    image = spec.get("image", "python:3.10-slim")
+    replicas = int(spec.get("replicas", 1))
+
+    container_runtime = client.V1Container(
+        name="agent-runtime",
+        image=image,
+        security_context=client.V1SecurityContext(
+            run_as_non_root=True,
+            read_only_root_filesystem=True,
+            allow_privilege_escalation=False
+        )
+    )
+    container_tool = client.V1Container(
+        name="tool-runner",
+        image=spec.get("toolRunnerImage", "ghcr.io/yourorg/aegis-tool-runner:latest"),
+        ports=[client.V1ContainerPort(container_port=8081)]
+    )
+    pod_spec = client.V1PodSpec(containers=[container_runtime, container_tool], restart_policy="Always")
+    template = client.V1PodTemplateSpec(metadata=client.V1ObjectMeta(labels={"agent": name}), spec=pod_spec)
+    dep_spec = client.V1DeploymentSpec(replicas=replicas, template=template, selector=client.V1LabelSelector(match_labels={"agent": name}))
+    body = client.V1Deployment(metadata=client.V1ObjectMeta(name=f"agent-{name}", namespace=namespace), spec=dep_spec)
+    api.create_namespaced_deployment(namespace=namespace, body=body)
+    return {"state":"Running", "message":"Deployment created"}
+
*** End Patch
*** Begin Patch
*** Add File:operator/operator_kopf/README.md
+Kopf-based Operator
+===================
+
+This is a production-ready scaffold for the Agent Manager operator:
+- Use Kopf to implement a robust operator with leader handling and better lifecycle events.
+- Extend create/update/delete handlers to manage tool manifests and attestation checks.
+
+Building:
+- docker build -t ghcr.io/<org>/aegis-agent-operator:latest operator/operator_kopf
+
*** End Patch
*** Begin Patch
*** Add File:tooling/tool-runner/Dockerfile
+FROM python:3.10-slim
+WORKDIR /opt/tool-runner
+RUN apt-get update && apt-get install -y ca-certificates \
+    && rm -rf /var/lib/apt/lists/*
+COPY agent-manager/tool-runner/runner.py /opt/tool-runner/runner.py
+COPY agent-manager/tool-runner/tool-example.json /opt/tool-runner/tools/qna.json
+RUN pip install flask wasmtime
+EXPOSE 8081
+CMD ["python", "runner.py"]
+
*** End Patch
*** Begin Patch
*** Add File:tooling/tool-runner/verify_tool_signature.py
+#!/usr/bin/env python3
+"""
+Tool signature verifier (scaffold).
+Verifies that a tool artifact (WASM) has a cosign signature in Rekor before allowing execution.
+"""
+import os, subprocess, sys
+
+def verify(artifact_path, sig_path):
+    rekor = os.environ.get("REKOR_SERVER", "https://rekor.sigstore.dev")
+    cmd = ["cosign", "verify-blob", "--rekor-server", rekor, "--signature", sig_path, artifact_path]
+    try:
+        subprocess.check_call(cmd)
+        return True
+    except subprocess.CalledProcessError:
+        return False
+
+if __name__ == "__main__":
+    if len(sys.argv)!=3:
+        print("Usage: verify_tool_signature.py <artifact> <sig>")
+        sys.exit(2)
+    ok = verify(sys.argv[1], sys.argv[2])
+    print("VERIFIED" if ok else "FAILED")
+    sys.exit(0 if ok else 1)
+
*** End Patch
*** Begin Patch
*** Add File:ci/sbom_and_scan/generate_sbom_and_scan.sh
+#!/usr/bin/env bash
+set -euo pipefail
+IMAGE=$1
+if [ -z "$IMAGE" ]; then
+  echo "Usage: $0 <image>"
+  exit 1
+fi
+if ! command -v syft >/dev/null 2>&1; then
+  echo "Install syft (https://github.com/anchore/syft) to generate SBOM"
+  exit 2
+fi
+if ! command -v trivy >/dev/null 2>&1; then
+  echo "Install trivy to scan images"
+  exit 2
+fi
+
+echo "Generating SBOM for $IMAGE..."
+syft $IMAGE -o json > sbom.json
+echo "Running trivy scan..."
+trivy image --format json --output trivy.json $IMAGE || true
+echo "SBOM and scan results written: sbom.json, trivy.json"
+
*** End Patch
*** Begin Patch
*** Add File:examples/rlhf/README.md
+RLHF Pipeline (scaffold)
+========================
+
+This folder contains a high-level scaffold to collect human feedback, train reward model, and perform RLHF-style fine-tuning.
+It is a template: integrate real data pipelines & compute hooks into your infra.
+
+Steps:
+- Collect interaction / preference data -> store in lakeFS / DVC
+- Train reward model using trainer scripts (not included)
+- Use SFT + PPO orchestrations via Argo for controlled retraining (checkpoint to S3)
+- Validate new model -> MLflow register -> sign & promote
+
+Files:
+- rl_pipeline/README.md (this) and a sample data schema
+
*** End Patch
*** Begin Patch
*** Add File:ux/dashboard/app.py
+#!/usr/bin/env python3
+from flask import Flask, render_template, jsonify
+import os, requests
+
+app = Flask(__name__)
+@app.get("/")
+def index():
+    # stub: query Prometheus & Rekor for summary
+    return render_template("index.html", title="Aegis Dashboard")
+
+@app.get("/api/models")
+def models():
+    # stubbed response
+    return jsonify([{"name":"demo-model","status":"healthy","signed":True}])
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=8080)
+
*** End Patch
*** Begin Patch
*** Add File:ux/dashboard/templates/index.html
+<!doctype html>
+<html>
+  <head><title>{{ title }}</title></head>
+  <body>
+    <h1>{{ title }}</h1>
+    <div id="models">Loading...</div>
+    <script>
+    fetch('/api/models').then(r=>r.json()).then(data=>{
+      document.getElementById('models').innerText = JSON.stringify(data,null,2);
+    })
+    </script>
+  </body>
+</html>
+
*** End Patch
*** Begin Patch
*** Add File:productization/managed_offering/README.md
+Managed Offering — Aegis Hosted
+===============================
+
+Service tiers:
+- Standard: core platform hosted, customer provides cloud credentials; monthly fee + infra usage
+- Premium: Aegis fully hosted, 24/7 support, SLAs (99.9%), quarterly audits, ticketing & runbook customizations
+- Enterprise: Dedicated deployments (VPC peering), compliance package (SOC2 & auditing), on-call SRE
+
+Included in managed offering:
+- Automated onboarding templates, tenant wiring assistance, monthly patching windows, scheduled DR drills, quarterly pen tests (optional)
+
+Onboarding steps:
+1) Sign MSA + SLA addendum
+2) Discovery & scoping
+3) Pilot deployment
+4) Handover to managed operations
+
*** End Patch
*** Begin Patch
*** Add File:productization/managed_offering/SLA_TEMPLATE.md
+Service Level Agreement (template)
+---------------------------------
+
+Service availability: 99.9% monthly uptime for control plane services in the managed tier.
+Response times:
+- P1 incidents: 15 minutes response, 4 hours initial mitigation window
+- P2 incidents: 1 hour response, 24 hours remediation plan
+Maintenance:
+- Scheduled maintenance windows: weekly 2am-4am UTC (customer notified 72 hours prior)
+Data retention & backups:
+- Backups retained 90 days (lakeFS snapshots + Vault snapshots)
+Security:
+- Annual external penetration test; immediate notification of critical findings
+Support:
+- 24/7 premium support available for Premium & Enterprise tiers
+
*** End Patch
*** Begin Patch
*** Add File:docs/marketing/one_pager.md
+Aegis One-Pager (customer-facing)
+=================================
+
+Headline:
+- Aegis — Trusted AI Platform for Enterprises
+
+Sub-headline:
+- Build, sign, deploy and govern your AI across cloud, edge and HPC with auditable provenance.
+
+Key bullets:
+- End-to-end lifecycle with MLflow, lakeFS, Feast and KServe
+- Supply-chain security: Vault + cosign + Rekor integration
+- Agent and edge productionization: TPM attestation + offline Rekor bundles
+- Enterprise-ready: RBAC, OPA policies, DR drills, pentest orchestration
+
+Call to action:
+- Schedule a 20-minute demo or request a 6-week pilot to validate your first model.
+
*** End Patch
*** Begin Patch
*** Add File:legal/templates/MSA_TEMPLATE.md
+Master Services Agreement (Template)
+----------------------------------
+
+This MSA is a starting template. It includes:
+- Scope of services (pilot, managed offerings)
+- Confidentiality & data handling
+- Liability & indemnity (cap on liability)
+- Term & termination
+- SLA references and support
+
+Operator: have legal counsel review before sending to customers.
+
*** End Patch
*** Begin Patch
*** Add File:ci/workflows/agent_operator_ci.yml
+name: Operator CI
+on:
+  push:
+    paths:
+      - 'operator/**'
+      - 'agent-manager/**'
+jobs:
+  test-build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps & lint
+        run: |
+          pip install -r operator/operator_kopf/requirements.txt
+          python -m pyflakes operator/operator_kopf || true
+      - name: Build operator image
+        uses: docker/build-push-action@v4
+        with:
+          context: operator/operator_kopf
+          push: false
+          tags: aegis-operator:ci
+
*** End Patch
*** Begin Patch
*** Add File:examples/vertical_templates/README.md
+Vertical Model Templates
+========================
+
+Contains starter templates for three verticals:
+- Customer Support (NLP classification + RAG)
+- Computer Vision (image classification + object detection)
+- Recommender (user-item collaborative filtering)
+
+Each template includes:
+- Data schema example
+- Training script stub (mlflow logging)
+- Serving manifest (KServe) and perf hints
+
+Use these to accelerate customer prototypes during pilots.
+
*** End Patch
*** Begin Patch
*** Add File:security/pentest/PENTEST_DELIVERY.md
+Pentest Delivery & Retest Process
+--------------------------------
+
+1) Deliver bundle (prod/security/pentest/prepare_pentest_bundle.sh) to vendor
+2) Vendor runs tests in staging window and returns findings.csv
+3) Triage: map findings to remediation_tracker_template.csv and assign owners
+4) Fix high/critical issues, document fixes, and request vendor retest
+5) Obtain final report and include executive summary in pilot deliverable
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/onboarding/ONBOARDING_TEMPLATE.md
+Customer Onboarding Template
+----------------------------
+
+1) Pre-engagement:
+ - NDA & MSA signed
+ - Kickoff meeting and stakeholder list
+2) Discovery:
+ - Inventory current infra, models, data sensitivity, compliance requirements
+ - Agree pilot scope & success metrics
+3) Environment prep:
+ - Tenant wiring, GH repo access, service accounts
+ - Sandbox provisioning and cost estimate
+4) Pilot execution (see productization/pilot_package/PILOT_PACKAGE.md)
+5) Handoff & operations:
+ - Runbooks, SLAs, support contacts, knowledge transfer sessions
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_operator_deploy.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Quick smoke test: ensure CRD exists and operator can create a sample Agent resource (run in CI against sandbox)
+kubectl apply -f agent-manager/crds/agent.crd.yaml
+kubectl apply -f agent-manager/controller/agent-sample.yaml
+sleep 5
+kubectl get deployments -l agent=sample-agent -o wide
+
*** End Patch
*** End Patch
