*** Begin Patch
*** Add File:jupyterhub/values.yaml
proxy:
  secretToken: "REPLACE_WITH_SECURE_TOKEN"
singleuser:
  image:
    name: jupyter/base-notebook
    tag: "python-3.10.12"
  storage:
    dynamic:
      storageClass: "gp2"
      accessModes: ["ReadWriteOnce"]
      size: 20Gi
hub:
  extraEnv:
    OAUTH2_CALLBACK_URL: "https://<YOUR_DOMAIN>/hub/oauth_callback"
    OIDC_ISSUER: "https://auth.example.com"
    OIDC_CLIENT_ID: "REPLACE_CLIENT_ID"
    OIDC_CLIENT_SECRET: "REPLACE_CLIENT_SECRET"
auth:
  type: "oauthenticator"
  oauthenticator:
    type: "generic"
    login_service: "Aegis SSO"
    client_id: "REPLACE_CLIENT_ID"
    client_secret: "REPLACE_CLIENT_SECRET"
    authorize_url: "https://auth.example.com/authorize"
    token_url: "https://auth.example.com/token"
    userdata_url: "https://auth.example.com/userinfo"
    userdata_method: "GET"
    userdata_params:
      state: ""

singleuser:
  defaultUrl: "/lab"
  extraRepos:
    - repo: "https://github.com/yourorg/aegis-notebooks"
      branch: "main"

rbac:
  enabled: true

resources:
  limits:
    cpu: "4"
    memory: 12Gi
  requests:
    cpu: "1"
    memory: 4Gi
*** End Patch
*** Begin Patch
*** Add File:jupyterhub/README.md
# JupyterHub on Aegis (Helm values)

1. Install Helm chart (zero-to-jupyterhub) and supply these values:
   helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
   helm repo update
   helm upgrade --install jhub jupyterhub/jupyterhub -f jupyterhub/values.yaml --namespace aegis

2. Replace OIDC and secret placeholders before installing.
3. Configure PersistentVolumeClaims and StorageClass to match your cloud.
4. Use RoleBindings to provide per-team namespaces or use JupyterHub multi-namespace delegates.

Security notes:
- Use OIDC integration (Keycloak/Okta) to centralize auth and RBAC.
- Enable network policies to isolate notebook traffic.
*** End Patch
*** Begin Patch
*** Add File:lakefs/values.yaml
image:
  repository: lakefs/lakefs
  tag: "0.101.0"
persistence:
  enabled: true
  storageClass: gp2
  size: 20Gi
service:
  type: ClusterIP
config:
  uri: "http://lakefs.aegis.svc.cluster.local:8000"
  s3:
    endpoint: "https://s3.amazonaws.com"
    bucket: "REPLACE_EVIDENCE_BUCKET"
    access_key: "REPLACE_ACCESS_KEY"
    secret_key: "REPLACE_SECRET_KEY"
  database:
    type: sqlite
*** End Patch
*** Begin Patch
*** Add File:lakefs/snapshot_and_register.py
#!/usr/bin/env python3
"""
Snapshot a dataset in lakeFS and register snapshot id in MLflow run metadata.

Requires:
 - lakefs CLI or server reachable and MLflow tracking URI set in env
"""
import os
import subprocess
import sys
import requests
from mlflow.tracking import MlflowClient

LAKEFS_URL = os.environ.get("LAKEFS_URL", "http://lakefs.aegis.svc.cluster.local:8000")
REPO = os.environ.get("LAKEFS_REPO", "aegis")
BRANCH = os.environ.get("LAKEFS_BRANCH", "main")
DATA_PATH = os.environ.get("DATA_PATH", "s3://REPLACE_EVIDENCE_BUCKET/datasets/sample")
MLFLOW_RUN_ID = os.environ.get("MLFLOW_RUN_ID", "")

def create_branch():
    # Using lakeFS REST API to create a new branch (snapshot)
    url = f"{LAKEFS_URL}/api/v1/repositories/{REPO}/refs/branches"
    payload = {"name": "snapshot-" + os.urandom(4).hex(), "source": BRANCH}
    # Auth: operator must set LAKEFS_AUTH env var
    auth = (os.environ.get("LAKEFS_USER"), os.environ.get("LAKEFS_PASS"))
    r = requests.post(url, json=payload, auth=auth, timeout=10)
    r.raise_for_status()
    return r.json()["name"]

def commit_path(branch):
    # Create commit adding pointer to dataset path (this is a placeholder; adapt per lakeFS usage)
    url = f"{LAKEFS_URL}/api/v1/repositories/{REPO}/refs/commits"
    payload = {"message": "Snapshot dataset", "branch": branch, "operations": [{"type": "COPY", "source": DATA_PATH, "destination": f"lakefs://{REPO}/{branch}/dataset"}]}
    auth = (os.environ.get("LAKEFS_USER"), os.environ.get("LAKEFS_PASS"))
    r = requests.post(url, json=payload, auth=auth, timeout=30)
    r.raise_for_status()
    return r.json().get("commit_id","")

def register_in_mlflow(snapshot_id):
    if not MLFLOW_RUN_ID:
        print("No MLFLOW_RUN_ID provided; skipping registration")
        return
    client = MlflowClient()
    client.set_tag(MLFLOW_RUN_ID, "dataset_snapshot", snapshot_id)
    print("Registered snapshot in MLflow:", snapshot_id)

def main():
    branch = create_branch()
    commit = commit_path(branch)
    snapshot_ref = f"lakefs://{REPO}@{branch}:{commit}"
    print("Snapshot created:", snapshot_ref)
    register_in_mlflow(snapshot_ref)
    return 0

if __name__ == "__main__":
    sys.exit(main())
*** End Patch
*** Begin Patch
*** Add File:feast/README.md
# Feast quickstart (Aegis)

1. Deploy Feast core components (serving / core) in the aegis namespace or use managed Feast.
2. Configure feature repo to use your backing store (S3/BigQuery/Postgres).
3. Sample ingestion job provided in feast/ingest_job.py

Note: operator must set FEAST_REDIS_HOST, FEAST_CORE_URL and credentials before running ingest_job.py
*** End Patch
*** Begin Patch
*** Add File:feast/ingest_job.py
#!/usr/bin/env python3
"""
Simple Feast ingestion example: materialize features from parquet in S3 into Feast (online store).

Requires environment variables:
 - FEAST_CORE_URL
 - FEAST_REDIS_HOST
 - S3_DATA_PATH
"""
import os
from feast import FeatureStore
from feast.infra.offline_stores import file_source
from feast import Entity, Feature, ValueType, FeatureView
from datetime import timedelta
import pandas as pd

S3_DATA_PATH = os.environ.get("S3_DATA_PATH", "s3://REPLACE_EVIDENCE_BUCKET/datasets/features.parquet")
FEAST_CORE_URL = os.environ.get("FEAST_CORE_URL", "http://feast-core.aegis.svc.cluster.local:6566")

# This is a very small illustrative example; adapt to your Feast repo.
def ingest():
    # Example: create a temporary file source from S3 (download then point)
    local = "/tmp/features.parquet"
    os.system(f"aws s3 cp {S3_DATA_PATH} {local}")
    df = pd.read_parquet(local)
    # Operator should implement FeatureStore repo code; here we just demonstrate materialize
    print("Loaded rows:", len(df))
    # In production, use FeatureStore(repo_path=...) and register FeatureViews and run materialize

if __name__ == "__main__":
    ingest()
*** End Patch
*** Begin Patch
*** Add File:argo/workflows/notebook_to_pipeline.yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: notebook-to-pipeline-
spec:
  entrypoint: pipeline
  templates:
    - name: pipeline
      steps:
        - - name: pre-check
            template: pre-check
        - - name: snapshot-dataset
            template: snapshot-dataset
        - - name: run-training
            template: run-training
        - - name: validate-and-register
            template: validate-and-register
    - name: pre-check
      container:
        image: alpine:3.16
        command: [sh, -c]
        args: ["echo 'pre-flight checks'; exit 0"]
    - name: snapshot-dataset
      container:
        image: ghcr.io/yourorg/aegis-tools:latest
        command: [python, /app/lakefs_snapshot.py]
        env:
          - name: LAKEFS_URL
            value: "http://lakefs.aegis.svc.cluster.local:8000"
    - name: run-training
      container:
        image: ghcr.io/yourorg/aegis-train:latest
        command: [sh, -c]
        args: ["python train.py --data /data/dataset --mlflow_run_id $MLFLOW_RUN_ID"]
    - name: validate-and-register
      container:
        image: ghcr.io/yourorg/aegis-tools:latest
        command: [python, /app/validate_and_register.py]
        env:
          - name: MLFLOW_TRACKING_URI
            value: "https://mlflow.aegis.svc.cluster.local:5000"
*** End Patch
*** Begin Patch
*** Add File:k8s/resource-quotas.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ds-team-alpha
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ds-team-alpha-quota
  namespace: ds-team-alpha
spec:
  hard:
    requests.cpu: "40"
    limits.cpu: "80"
    requests.memory: 200Gi
    limits.memory: 400Gi
    requests.storage: 1Ti
---
apiVersion: v1
kind: LimitRange
metadata:
  name: ds-team-alpha-limits
  namespace: ds-team-alpha
spec:
  limits:
    - type: Container
      defaultRequest:
        cpu: "1"
        memory: 4Gi
      default:
        cpu: "4"
        memory: 12Gi
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "High priority for training jobs"
*** End Patch
*** Begin Patch
*** Add File:great_expectations/great_expectations.yml
# Minimal Great Expectations config (simplified)
name: aegis_ge
config_version: 3
datasources:
  datasets_s3:
    class_name: Datasource
    execution_engine:
      class_name: SparkDFExecutionEngine
    data_connectors:
      default_runtime_data_connector_name:
        class_name: RuntimeDataConnector
        batch_identifiers:
          - default_identifier_name
stores:
  expectations_store:
    class_name: ExpectationsStore
  validations_store:
    class_name: ValidationsStore
expectations_store_name: expectations_store
validations_store_name: validations_store
evaluation_parameters: {}
*** End Patch
*** Begin Patch
*** Add File:great_expectations/validate_and_train.sh
#!/usr/bin/env bash
set -euo pipefail
# Example pipeline step: validate dataset using Great Expectations then train if passed
DATA_FILE=${1:-"/data/dataset.csv"}
GE_HOME=${GE_HOME:-"/app/great_expectations"}
if [ ! -f "$DATA_FILE" ]; then
  echo "Data file not found: $DATA_FILE"
  exit 2
fi
# Run GE validation (operator must create suites)
great_expectations checkpoint run my_checkpoint || { echo "Data validation failed"; exit 3; }
# If passed, run training
python train.py --data "$DATA_FILE"
*** End Patch
*** Begin Patch
*** Add File:connectors/snowflake/sample_connector.py
#!/usr/bin/env python3
"""
Sample Snowflake connector (placeholder) — use securely via Vault/ExternalSecrets
"""
import os
import snowflake.connector
import pandas as pd

USER = os.environ.get("SNOWFLAKE_USER")
PASSWORD = os.environ.get("SNOWFLAKE_PASSWORD")
ACCOUNT = os.environ.get("SNOWFLAKE_ACCOUNT")
WAREHOUSE = os.environ.get("SNOWFLAKE_WAREHOUSE")
DATABASE = os.environ.get("SNOWFLAKE_DATABASE")
SCHEMA = os.environ.get("SNOWFLAKE_SCHEMA")

def query(sql, out="/tmp/out.parquet"):
    ctx = snowflake.connector.connect(
        user=USER, password=PASSWORD, account=ACCOUNT, warehouse=WAREHOUSE, database=DATABASE, schema=SCHEMA
    )
    cur = ctx.cursor()
    cur.execute(sql)
    df = cur.fetch_pandas_all()
    df.to_parquet(out)
    print("Saved", out)
    ctx.close()

if __name__ == "__main__":
    query("SELECT * FROM analytics.sample LIMIT 1000")
*** End Patch
*** Begin Patch
*** Add File:connectors/bigquery/sample_connector.py
#!/usr/bin/env python3
"""
Sample BigQuery connector (placeholder)
"""
from google.cloud import bigquery
import os

def query(sql, out="/tmp/bq_out.parquet"):
    client = bigquery.Client()
    df = client.query(sql).to_dataframe()
    df.to_parquet(out)
    print("Saved", out)

if __name__ == "__main__":
    query("SELECT * FROM `project.dataset.table` LIMIT 1000")
*** End Patch
*** Begin Patch
*** Add File:connectors/kafka/sample_consumer.py
#!/usr/bin/env python3
"""
Example Kafka consumer for streaming ingestion
"""
from confluent_kafka import Consumer
import json
import os

KAFKA_BOOTSTRAP = os.environ.get("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC = os.environ.get("KAFKA_TOPIC", "aegis-events")

conf = {'bootstrap.servers': KAFKA_BOOTSTRAP, 'group.id': 'aegis-ingest', 'auto.offset.reset': 'earliest'}
c = Consumer(conf)
c.subscribe([TOPIC])

try:
    while True:
        msg = c.poll(1.0)
        if msg is None:
            continue
        if msg.error():
            print("Consumer error: {}".format(msg.error()))
            continue
        record = json.loads(msg.value().decode('utf-8'))
        print("Received:", record)
except KeyboardInterrupt:
    pass
finally:
    c.close()
*** End Patch
*** Begin Patch
*** Add File:reproducibility/Dockerfile.template
FROM python:3.10-slim
WORKDIR /app
COPY . /app
RUN pip install --no-cache-dir -r requirements.txt
# Use CONDA env if required; operator may adapt
CMD ["python","train.py"]
*** End Patch
*** Begin Patch
*** Add File:reproducibility/conda_env.yml
name: aegis-experiment
channels:
  - defaults
dependencies:
  - python=3.10
  - pip
  - pip:
    - mlflow
    - pandas
    - scikit-learn
*** End Patch
*** Begin Patch
*** Add File:reproducibility/register_env_mlflow.py
#!/usr/bin/env python3
"""
Capture environment fingerprint and register as MLflow tags for reproducibility.
Tags:
 - conda_env_hash
 - image_uri
"""
import os
import hashlib
from mlflow.tracking import MlflowClient

MLFLOW_RUN_ID = os.environ.get("MLFLOW_RUN_ID", "")
IMAGE_URI = os.environ.get("IMAGE_URI", "")

def hash_file(path):
    h = hashlib.sha256()
    with open(path,'rb') as f:
        h.update(f.read())
    return h.hexdigest()

def main():
    if not MLFLOW_RUN_ID:
        print("No run id; exit")
        return
    client = MlflowClient()
    env_hash = ""
    if os.path.exists("conda_env.yml"):
        env_hash = hash_file("conda_env.yml")
    client.set_tag(MLFLOW_RUN_ID, "conda_env_hash", env_hash)
    if IMAGE_URI:
        client.set_tag(MLFLOW_RUN_ID, "image_uri", IMAGE_URI)
    print("Registered env fingerprint to MLflow")

if __name__ == "__main__":
    main()
*** End Patch
*** Begin Patch
*** Add File:cost/chargeback/chargeback_report.py
#!/usr/bin/env python3
"""
Generate per-run cost/chargeback report by combining MLflow run metrics, carbon evidence and node pricing.

This is a heuristic script — operators must adapt pricing sources and tenant mapping.
"""
import os
import json
from mlflow.tracking import MlflowClient

MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI")
client = MlflowClient(tracking_uri=MLFLOW_URI) if MLFLOW_URI else MlflowClient()

def estimate_cost_from_metrics(run):
    # Placeholder: use resource/time metrics from run (e.g., gpu_time_hours and cpu_time_hours)
    metrics = {m.key: m.value for m in run.data.metrics}
    gpu_hours = float(metrics.get("gpu_hours", 0.0))
    cpu_hours = float(metrics.get("cpu_hours", 0.0))
    # sample pricing
    gpu_price = float(os.environ.get("GPU_PRICE_PER_HOUR", "2.5"))
    cpu_price = float(os.environ.get("CPU_PRICE_PER_HOUR", "0.05"))
    return gpu_hours * gpu_price + cpu_hours * cpu_price

def attach_cost_tag(run_id, cost):
    client.set_tag(run_id, "estimated_cost_usd", str(cost))

def run_for_experiment(experiment_id):
    runs = client.search_runs([experiment_id], filter_string="", max_results=1000)
    for r in runs:
        cost = estimate_cost_from_metrics(r)
        attach_cost_tag(r.info.run_id, cost)
        print("Run", r.info.run_id, "cost:", cost)

if __name__ == "__main__":
    import sys
    exp = sys.argv[1] if len(sys.argv)>1 else "0"
    run_for_experiment(exp)
*** End Patch
*** Begin Patch
*** Add File:runbooks/data_science_platform/README.md
# Data Science Platform Extensions — Runbook & How to use

This patch adds:
- JupyterHub Helm values for interactive notebooks
- lakeFS snapshot + MLflow registration helper
- Feast ingestion example
- Argo Workflows example pipeline
- Kubernetes ResourceQuota, LimitRange and PriorityClass for multi-tenant resource control
- Great Expectations validation hook and simple validation/training script
- Connectors for Snowflake, BigQuery and Kafka (placeholders)
- Reproducibility templates (Dockerfile, conda env) and MLflow env registration
- Cost/chargeback example script
- Runbook with high-level steps

Recommended quickstart:
1. Fill placeholders (REPLACE_*) and secrets, install JupyterHub with provided values.
2. Deploy lakeFS and configure S3/backing store; run lakefs/snapshot_and_register.py from CI or pipeline step to snapshot datasets and tag MLflow runs.
3. Deploy Feast or use managed Feast for online features; run feast/ingest_job.py to populate features.
4. Use Argo Workflows to orchestrate notebook→snapshot→train→validate flows (argo/workflows/notebook_to_pipeline.yaml).
5. Apply resource quotas per-team (k8s/resource-quotas.yaml) to enforce compute limits.
6. Add Great Expectations suites and wire validate_and_train.sh into pipelines to block training on failed data checks.
7. Wire connectors securely (ExternalSecrets/Vault) for Snowflake/BigQuery/Kafka.
8. Use reproducibility/register_env_mlflow.py at training end to tag environment fingerprint in MLflow.
9. Run cost/chargeback/chargeback_report.py periodically to estimate costs per experiment and feed billing systems.

Operator notes:
- All scripts and manifests must be adjusted to your infra (domains, ARNs, bucket names, DB endpoints).
- Ensure OIDC and network policies secure notebook traffic.
- Use ExternalSecrets to inject credentials, never store secrets inline.
*** End Patch
*** End Patch
