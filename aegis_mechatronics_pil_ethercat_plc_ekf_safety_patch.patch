*** Begin Patch
*** Add File:pil/pil_job_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: pil-job-
+  namespace: aegis
+spec:
+  entrypoint: pil-run
+  templates:
+    - name: pil-run
+      steps:
+        - - name: fetch-controller
+            template: fetch-controller
+        - - name: run-pil
+            template: run-pil
+            arguments:
+              parameters:
+                - name: controller_path
+                  value: "{{steps.fetch-controller.outputs.parameters.controller}}"
+                - name: trace_path
+                  value: "{{steps.fetch-controller.outputs.parameters.trace}}"
+        - - name: collect-report
+            template: collect-report
+
+    - name: fetch-controller
+      outputs:
+        parameters:
+          - name: controller
+            valueFrom:
+              path: /tmp/controller_path.txt
+          - name: trace
+            valueFrom:
+              path: /tmp/trace_path.txt
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            # Operator: replace these with real artifact locations
+            CTRL="/tmp/controller"
+            TRACE="/tmp/sim_trace.json"
+            echo "$CTRL" > /tmp/controller_path.txt
+            echo "$TRACE" > /tmp/trace_path.txt
+            # If MODEL_ARTIFACT_BUCKET configured, try download
+            if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+              aws s3 cp s3://${MODEL_ARTIFACT_BUCKET}/pil/controller_binary "$CTRL" || true
+              aws s3 cp s3://${MODEL_ARTIFACT_BUCKET}/pil/sample_sim_trace.json "$TRACE" || true
+            fi
+            ls -l "$CTRL" "$TRACE" || true
+
+    - name: run-pil
+      inputs:
+        parameters:
+          - name: controller_path
+          - name: trace_path
+      container:
+        image: registry.example.com/aegis/pil-runner:latest
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            echo "Running PIL harness: controller={{inputs.parameters.controller_path}}, trace={{inputs.parameters.trace_path}}"
+            python3 pil/run_pil_harness.py --controller "{{inputs.parameters.controller_path}}" --trace "{{inputs.parameters.trace_path}}" --out /tmp/pil_report.json || true
+            cat /tmp/pil_report.json
+      volumeMounts:
+        - name: controller-volume
+          mountPath: /opt/controller
+
+    - name: collect-report
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            if [ -f /tmp/pil_report.json ]; then
+              echo "Uploading PIL report to evidence bucket if configured"
+              if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+                aws s3 cp /tmp/pil_report.json s3://${EVIDENCE_BUCKET}/pil/pil_report_$(date -u +%s).json || true
+              fi
+            else
+              echo "No PIL report generated"
+            fi
+
+  volumes:
+    - name: controller-volume
+      emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:pil/run_pil_harness.py
+#!/usr/bin/env python3
+"""
+PIL harness: runs a controller binary in a container/VM and feeds simulated sensor data from a trace.
+Measures timing, missed deadlines and basic stability metrics and writes a JSON report.
+"""
+import argparse, json, subprocess, time, os, tempfile
+from statistics import mean
+
+def run_controller_process(controller_path):
+    # Run controller as subprocess; controller must accept sensor input on stdin or read from a socket/file (operator adapts)
+    proc = subprocess.Popen([controller_path], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    return proc
+
+def feed_trace_and_collect(proc, trace_path, step_interval_s=0.01):
+    trace = json.load(open(trace_path))
+    steps = trace.get("steps", [])
+    timings = []
+    violations = []
+    out_samples = []
+    for i, s in enumerate(steps):
+        t0 = time.time()
+        # provide sensor JSON line on stdin (controller must read/parse accordingly)
+        try:
+            send = json.dumps(s).encode() + b"\n"
+            proc.stdin.write(send)
+            proc.stdin.flush()
+        except Exception:
+            violations.append({"type":"stdin_write_failed","index":i})
+        # Optionally read stdout for controller outputs (non-blocking not implemented in stub)
+        time.sleep(step_interval_s)
+        dt = time.time() - t0
+        timings.append(dt)
+        # crude missed deadline detection
+        if dt > step_interval_s * 1.5:
+            violations.append({"type":"missed_deadline","index":i,"latency_s":dt})
+        # collect sample of stdout
+        try:
+            out = proc.stdout.readline().decode().strip()
+            if out:
+                out_samples.append(out)
+        except Exception:
+            pass
+    return timings, violations, out_samples
+
+def generate_report(timings, violations, out_samples, out_path):
+    report = {
+        "timings_summary": {"count": len(timings), "mean_s": mean(timings) if timings else None, "p95_s": sorted(timings)[int(len(timings)*0.95)] if timings else None},
+        "violations": violations,
+        "sample_outputs": out_samples[:20]
+    }
+    with open(out_path, "w") as f:
+        json.dump(report, f, indent=2)
+    print("Wrote PIL report to", out_path)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--controller", required=True)
+    p.add_argument("--trace", required=True)
+    p.add_argument("--out", default="/tmp/pil_report.json")
+    p.add_argument("--step-interval", type=float, default=0.01)
+    args = p.parse_args()
+    if not os.path.exists(args.controller):
+        print("Controller not found at", args.controller)
+        # try to mark as failure but continue to generate a stub report
+    proc = run_controller_process(args.controller)
+    timings, violations, out_samples = feed_trace_and_collect(proc, args.trace, args.step_interval)
+    try:
+        proc.terminate()
+    except Exception:
+        pass
+    generate_report(timings, violations, out_samples, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ethercat/soem_adapter.py
+#!/usr/bin/env python3
+"""
+EtherCAT adapter scaffold using python-soem (pysoem).
+This is a best-effort stub: operator must install and configure SOEM and run privileged.
+It maps trace steps into PDO writes. Requires knowledge of slave PDO layout.
+"""
+import argparse, json, time, os
+
+def connect_and_map(iface):
+    try:
+        import pysoem
+    except Exception as e:
+        raise RuntimeError("pysoem (SOEM python bindings) not installed: " + str(e))
+    master = pysoem.SlaveConfig()
+    # The real code must call master.open(iface) and configure slaves
+    print("SOEM adapter stub: would open interface", iface)
+    return None
+
+def replay_ethercat(trace, iface="eth0", dry_run=True, inter_delay_ms=1):
+    print("Loading trace", trace)
+    obj = json.load(open(trace))
+    steps = obj.get("steps", [])
+    print(f"Loaded {len(steps)} steps")
+    if dry_run:
+        for s in steps[:10]:
+            print("DRY-PDO:", s.get("pdo_map", {}))
+        print("Dry-run complete")
+        return
+    # TODO: implement mapping to PDOs using pysoem
+    raise RuntimeError("EtherCAT hardware replay not implemented. Replace with pysoem mapping logic.")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--iface", default="eth0")
+    p.add_argument("--dry-run", action="store_true")
+    args = p.parse_args()
+    replay_ethercat(args.trace, iface=args.iface, dry_run=args.dry_run)
+
*** End Patch
*** Begin Patch
*** Add File:argo/ethercat/ethercat_adapter_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: ethercat-adapter-
+  namespace: aegis
+spec:
+  entrypoint: ethercat-adapter
+templates:
+  - name: ethercat-adapter
+    steps:
+      - - name: fetch-trace
+          template: fetch-trace
+      - - name: run-adapter
+          template: run-adapter
+
+  - name: fetch-trace
+    outputs:
+      parameters:
+        - name: trace
+          valueFrom:
+            path: /tmp/trace_path.txt
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          TRACE="/tmp/ethercat_trace.json"
+          if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+            aws s3 cp s3://${MODEL_ARTIFACT_BUCKET}/hil/ethercat_trace.json "$TRACE" || true
+          else
+            cp ethercat/sample_trace_ethercat.json "$TRACE" || true
+          fi
+          echo "$TRACE" > /tmp/trace_path.txt
+
+  - name: run-adapter
+    inputs:
+      parameters:
+        - name: trace
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          pip install pysoem || true
+          python3 ethercat/soem_adapter.py --trace "{{inputs.parameters.trace}}" --iface ${ETHERCAT_IFACE:-eth0} --dry-run ${HIL_DRY_RUN:-true}
+
*** End Patch
*** Begin Patch
*** Add File:plc/opcua_bridge.py
+#!/usr/bin/env python3
+"""
+OPC-UA bridge: connects to an OPC-UA server (e.g., Kepware) and polls configured node ids,
+publishes telemetry to a local file, S3, or Kafka (operator config).
+"""
+import argparse, time, json, os
+try:
+    from opcua import Client
+except Exception:
+    Client = None
+
+def run_bridge(endpoint, node_ids, interval=1.0, out="/tmp/opcua_poll.json"):
+    if Client is None:
+        raise RuntimeError("python-opcua is not installed in this container. pip install opcua")
+    client = Client(endpoint)
+    client.connect()
+    print("Connected to OPC-UA", endpoint)
+    nodes = [client.get_node(nid) for nid in node_ids]
+    while True:
+        rec = {"ts": int(time.time()*1000), "values": {}}
+        for nid, node in zip(node_ids, nodes):
+            try:
+                val = node.get_value()
+            except Exception as e:
+                val = {"error": str(e)}
+            rec["values"][nid] = val
+        # write locally
+        with open(out, "a") as f:
+            f.write(json.dumps(rec) + "\n")
+        # optional upload
+        if os.environ.get("MODEL_ARTIFACT_BUCKET"):
+            try:
+                import boto3
+                s3 = boto3.client("s3")
+                key = f"plc/opcua_poll_{int(time.time())}.json"
+                s3.put_object(Bucket=os.environ["MODEL_ARTIFACT_BUCKET"], Key=key, Body=json.dumps(rec).encode())
+            except Exception:
+                pass
+        time.sleep(interval)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--endpoint", required=True)
+    p.add_argument("--nodes", required=True, help="comma separated node ids")
+    p.add_argument("--interval", type=float, default=1.0)
+    p.add_argument("--out", default="/tmp/opcua_poll.json")
+    args = p.parse_args()
+    node_ids = [n.strip() for n in args.nodes.split(",")]
+    run_bridge(args.endpoint, node_ids, args.interval, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/plc/opcua_bridge_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: opcua-bridge-
+  namespace: aegis
+spec:
+  entrypoint: opcua-bridge
+templates:
+  - name: opcua-bridge
+    steps:
+      - - name: run-bridge
+          template: run-bridge
+
+  - name: run-bridge
+    container:
+      image: registry.example.com/aegis/opcua-bridge:latest
+      command: [sh, -c]
+      args:
+        - |
+          set -euo pipefail
+          python3 plc/opcua_bridge.py --endpoint "${OPCUA_ENDPOINT:-opc.tcp://kepware:49320}" --nodes "${OPCUA_NODE_IDS:-ns=2;s=Channel1.Device1.Tag1,ns=2;s=Channel1.Device1.Tag2}" --interval 1.0 --out /tmp/opcua_out.json
+
*** End Patch
*** Begin Patch
*** Add File:sensors/ekf_ukf/ekf_ukf_service.py
+#!/usr/bin/env python3
+"""
+Simple EKF/UKF service scaffold using filterpy.
+Accepts JSON sensor messages (stdin or file) and outputs state estimates to stdout or file.
+"""
+import argparse, json, math, os
+try:
+    from filterpy.kalman import ExtendedKalmanFilter, UnscentedKalmanFilter, MerweScaledSigmaPoints
+except Exception:
+    ExtendedKalmanFilter = None
+
+def fx(x, dt, u):
+    # example motion model (constant velocity)
+    px, vx = x[0], x[1]
+    px_new = px + vx * dt
+    return [px_new, vx]
+
+def hx(x):
+    # measurement model: measure position
+    return [x[0]]
+
+def run_ekf(trace_file, out="/tmp/estimates.json"):
+    if ExtendedKalmanFilter is None:
+        raise RuntimeError("filterpy not installed. pip install filterpy")
+    data = [json.loads(l) for l in open(trace_file)]
+    # build a trivial EKF for 1D position+vel
+    from filterpy.kalman import ExtendedKalmanFilter
+    ekf = ExtendedKalmanFilter(dim_x=2, dim_z=1)
+    ekf.x = [0., 0.]
+    ekf.P *= 10.
+    ekf.R = 1.0
+    ekf.Q = 0.01
+    estimates = []
+    last_t = None
+    for rec in data:
+        t = rec.get("ts", None)
+        z = rec.get("z", None)
+        if last_t is None:
+            dt = 0.01
+        else:
+            dt = max(1e-6, (t - last_t)/1000.0)
+        last_t = t
+        # predict with simple constant velocity linear model
+        F = [[1.0, dt],[0,1.0]]
+        ekf.F = F
+        ekf.predict()
+        if z is not None:
+            ekf.update([z])
+        estimates.append({"ts": t, "x": ekf.x.tolist()})
+    with open(out, "w") as f:
+        json.dump(estimates, f, indent=2)
+    print("Wrote estimates to", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--out", default="/tmp/est.json")
+    args = p.parse_args()
+    run_ekf(args.trace, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/sensors/ekf_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: ekf-job
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: ekf
+          image: registry.example.com/aegis/ekf-service:latest
+          command: [sh, -c]
+          args:
+            - |
+              python3 sensors/ekf_ukf/ekf_ukf_service.py --trace /tmp/sensor_log.json --out /tmp/estimates.json || true
+      restartPolicy: Never
+  backoffLimit: 1
+
*** End Patch
*** Begin Patch
*** Add File:safety/expanded_safety_oracles.py
+#!/usr/bin/env python3
+"""
+Expanded safety oracles:
+ - computes rise time, overshoot, settling time for step responses (per channel)
+ - retains previous basic checks (missed deadlines, saturation)
+"""
+import argparse, json, math
+
+def compute_step_metrics(signal, ts_s, setpoint=None):
+    # signal: list of samples; ts_s: list of timestamps in seconds
+    # setpoint optional; if None, assume final value = last sample
+    if not signal:
+        return {}
+    final = setpoint if setpoint is not None else signal[-1]
+    start = signal[0]
+    # rise time 10->90%
+    low = start + 0.1*(final-start)
+    high = start + 0.9*(final-start)
+    t_low = next((t for v,t in zip(signal, ts_s) if v >= low), None)
+    t_high = next((t for v,t in zip(signal, ts_s) if v >= high), None)
+    rise_time = t_high - t_low if t_low is not None and t_high is not None else None
+    # overshoot
+    max_v = max(signal)
+    overshoot = (max_v - final)/abs(final) if final != 0 else None
+    # settling time: time after which signal remains within 2% of final
+    settle_idx = None
+    for i,v in enumerate(signal[::-1]):
+        if abs(v - final) > 0.02*abs(final if final!=0 else 1):
+            settle_idx = len(signal) - i
+            break
+    settling_time = ts_s[settle_idx] - ts_s[0] if settle_idx is not None and settle_idx < len(ts_s) else None
+    return {"rise_time_s": rise_time, "overshoot": overshoot, "settling_time_s": settling_time}
+
+def run_checks(trace, out="/tmp/expanded_safety_report.json"):
+    data = json.load(open(trace))
+    steps = data.get("steps", [])
+    timestamps = [(s.get("timestamp_ns",0))/1e9 for s in steps]
+    # assume one channel "setpoint" and "actuator" exist in steps
+    setpoints = [s.get("setpoint",0) for s in steps]
+    actuator = [s.get("actuators", {}).get("joint_0", 0) for s in steps]
+    metrics = compute_step_metrics(actuator, timestamps, setpoint=setpoints[-1] if setpoints else None)
+    report = {"metrics": metrics, "basic_checks": []}
+    # reuse basic checks
+    # missed deadlines approx
+    last_ts = None
+    for i, t in enumerate(timestamps):
+        if last_ts:
+            delta = t - last_ts
+            if delta > 0.05:
+                report["basic_checks"].append({"type":"missed_deadline","index": i, "delta_s": delta})
+        last_ts = t
+    json.dump(report, open(out, "w"), indent=2)
+    print("Wrote expanded safety report to", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--trace", required=True)
+    p.add_argument("--out", default="/tmp/expanded_safety_report.json")
+    args = p.parse_args()
+    run_checks(args.trace, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/safety/expanded_safety_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: expanded-safety-
+  namespace: aegis
+spec:
+  entrypoint: expanded-safety
+templates:
+  - name: expanded-safety
+    steps:
+      - - name: fetch-trace
+          template: fetch-trace
+      - - name: run-expanded-safety
+          template: run-expanded-safety
+          arguments:
+            parameters:
+              - name: trace
+                value: "{{steps.fetch-trace.outputs.parameters.trace}}"
+      - - name: upload-report
+          template: upload-report
+
+  - name: fetch-trace
+    outputs:
+      parameters:
+        - name: trace
+          valueFrom:
+            path: /tmp/trace_path.txt
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          TRACE="/tmp/trace.json"
+          if [ -n "${MODEL_ARTIFACT_BUCKET:-}" ]; then
+            aws s3 cp s3://${MODEL_ARTIFACT_BUCKET}/hil/latest_trace.json "$TRACE" || true
+          else
+            cp hil/replay/sample_trace_can.json "$TRACE" || true
+          fi
+          echo "$TRACE" > /tmp/trace_path.txt
+
+  - name: run-expanded-safety
+    inputs:
+      parameters:
+        - name: trace
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          python3 safety/expanded_safety_oracles.py --trace "{{inputs.parameters.trace}}" --out /tmp/expanded_safety_report.json || true
+          cat /tmp/expanded_safety_report.json
+
+  - name: upload-report
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          if [ -f /tmp/expanded_safety_report.json ] && [ -n "${EVIDENCE_BUCKET:-}" ]; then
+            aws s3 cp /tmp/expanded_safety_report.json s3://${EVIDENCE_BUCKET}/safety/expanded_report_$(date -u +%s).json || true
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:docs/PIL_ETHERCAT_PLC_EKF_SAFETY_RUNBOOK.md
+# PIL, EtherCAT Adapter, PLC Bridge, EKF/UKF & Expanded Safety Tests — Runbook
+
+Scope
+- This runbook explains how to run the newly scaffolded artifacts safely in staging and how to adapt them for production HIL/PIL tests.
+
+Prereqs
+- K8s cluster with Argo Workflows in namespace `aegis`.  
+- SocketCAN available on HIL host for CAN testing.  
+- (Optional) EtherCAT master stack (SOEM) available on HIL host for production EtherCAT replay.  
+- OPC-UA endpoint (e.g., Kepware) credentials for PLC bridge.  
+- filterpy installed in EKF container environment.  
+- Safety: independent hardware kill switch and safety PLC must be active during all hardware runs.
+
+Files & What they do
+- pil/pil_job_workflow.yaml — Argo workflow to run a PIL harness.  
+- pil/run_pil_harness.py — harness that launches controller binary and feeds simulated sensors.  
+- hil/replay/can_replayer.py — SocketCAN replayer (dry-run safe).  
+- ethercat/soem_adapter.py — EtherCAT adapter scaffold (dry-run).  
+- plc/opcua_bridge.py — OPC-UA bridge to poll PLC tags and store to object store.  
+- sensors/ekf_ukf/ekf_ukf_service.py — EKF service to produce state estimates.  
+- safety/expanded_safety_oracles.py — computes rise-time/overshoot/settling plus basic checks.  
+- Argo workflows: argo/ethercat/ethercat_adapter_workflow.yaml, argo/plc/opcua_bridge_workflow.yaml, argo/sensors/ekf_job.yaml, argo/safety/expanded_safety_workflow.yaml
+
+Safe staging steps (dry-run)
+1. Dry-run PIL (no hardware):
+   - Ensure sample trace exists: `hil/replay/sample_trace_can.json`
+   - Submit: `argo submit pil/pil_job_workflow.yaml -n aegis --watch`
+   - Inspect PIL report in Argo logs and /tmp/pil_report.json printed
+
+2. Dry-run CAN replay:
+   - Submit: `argo submit hil/can_eth_hil_replay_workflow.yaml -n aegis --watch`
+   - Confirm it prints sample frames and safety report, but does not send to hardware (HIL_DRY_RUN default)
+
+3. OPC-UA bridge (staging):
+   - Set `OPCUA_ENDPOINT` and `OPCUA_NODE_IDS` as workflow env or Secrets.
+   - Submit: `argo submit argo/plc/opcua_bridge_workflow.yaml -n aegis --watch`
+   - Check /tmp/opcua_out.json in pod logs or S3 if configured
+
+4. EKF batch run:
+   - Upload a simple sensor log to model bucket or mount into job.
+   - Run: `kubectl apply -f argo/sensors/ekf_job.yaml -n aegis`
+   - Inspect /tmp/estimates.json in job logs
+
+5. Expanded safety checks:
+   - Submit: `argo submit argo/safety/expanded_safety_workflow.yaml -n aegis --watch`
+   - Review expanded safety report in logs and EVIDENCE_BUCKET if configured
+
+Hardware execution (CAUTION)
+- Confirm hardware kill switch and safety PLC are operational and tested before any hardware replay.  
+- On HIL host configure SocketCAN (example):
+  - sudo ip link add can0 type vcan
+  - sudo ip link set up can0
+- To send to actual CAN interface, set HIL_DRY_RUN=false and mount host device into Argo pod (requires privileged container and nodeSelector). Modify the workflow to mount /dev and add securityContext.
+- EtherCAT: only run when SOEM or vendor SDK is installed and configured; use test rigs with actuators disabled or limited power.
+- PLC: ensure read-only access for initial tests; do not write to PLCs without operator review.
+
+Adapting for your environment
+- Controller binary IO: adapt `pil/run_pil_harness.py` to match your controller's IPC (stdin, TCP socket, or shared memory).  
+- EtherCAT PDO mapping: update `ethercat/soem_adapter.py` with your slave IDs and PDO layout and test in dry-run first.  
+- OPC-UA node IDs: use fully-qualified NodeIds for your Kepware tags.  
+- EKF model: replace trivial motion model with your robot's motion/observation model. Use UKF for high nonlinearity.
+
+Acceptance criteria (example)
+- PIL: mean step latency < control_period * 0.3 and zero missed_deadlines in N=1000 cycles.  
+- CAN/EtherCAT HIL: zero safety_invariant_violations across test set.  
+- EKF: state RMSE below required threshold on validation trace.  
+- Safety: expanded safety metrics within bounds (rise_time, overshoot, settling_time).
+
+Next steps I can help with
+- Generate a privileged Argo pod template that mounts host SocketCAN (/dev) for hardware CAN testing (requires cluster admin).  
+- Implement SOEM-based EtherCAT mapping for your specific slave PDO layout (provide vendor slave IDs & PDO map).  
+- Build a production OPC-UA bridge image with TLS and authentication support.  
+- Replace EKF stub with a UKF for your robot and integrate continuous estimator service that publishes to Kafka/Feast.
+
*** End Patch
*** End Patch
