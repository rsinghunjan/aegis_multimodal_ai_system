diff --git a/k8s/manifests/dgx/gpu-operator-values.yaml b/k8s/manifests/dgx/gpu-operator-values.yaml
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/k8s/manifests/dgx/gpu-operator-values.yaml
@@ -0,0 +1,120 @@
+# Values for installing NVIDIA GPU Operator (Helm) tailored for DGX Cloud
+# Usage:
+#   helm repo add nvidia https://nvidia.github.io/gpu-operator
+#   helm repo update
+#   helm install --namespace gpu-operator --create-namespace gpu-operator nvidia/gpu-operator -f k8s/manifests/dgx/gpu-operator-values.yaml
+
+driver:
+  enabled: true
+  # On DGX Cloud the driver may be provided by the platform; set to false if DGX images include drivers.
+  # set to false if you prefer to use system driver managed by the DGX Cloud team.
+  autoInstall: true
+
+toolkit:
+  enabled: true
+  installMode: "daemonset"
+
+driver:
+  persistenceMode: "legacy"
+
+devicePlugin:
+  enabled: true
+  resources:
+    limits: {}
+    requests: {}
+
+dcgmExporter:
+  enabled: true
+  # tune scraping interval if desired
+  scrapeInterval: "15s"
+
+nodeStatusExporter:
+  enabled: true
+
+operator:
+  watchNamespace: ""
+
+migrate:
+  enabled: false
+
+# Optional: specify image registry override if you host NVIDIA images in your private registry
+image:
+  registry: "<REGISTRY>"
+  repository: ""
+  tag: ""
+
+# Use node labels expected on DGX nodes (we recommend adding node.kubernetes.io/dgx=true)
+nodeSelector:
+  "node.kubernetes.io/dgx": "true"
+
+tolerations: []
+
+resources: {}
+
+rbac:
+  create: true
+
+securityContext:
+  enabled: true
diff --git a/k8s/manifests/dgx/dcgm-exporter-daemonset.yaml b/k8s/manifests/dgx/dcgm-exporter-daemonset.yaml
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/k8s/manifests/dgx/dcgm-exporter-daemonset.yaml
@@ -0,0 +1,86 @@
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+  labels:
+    app: dcgm-exporter
+spec:
+  selector:
+    matchLabels:
+      app: dcgm-exporter
+  template:
+    metadata:
+      labels:
+        app: dcgm-exporter
+    spec:
+      nodeSelector:
+        "node.kubernetes.io/dgx": "true"
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      containers:
+        - name: dcgm-exporter
+          image: nvidia/dcgm-exporter:2.3.1-2.10.0-ubuntu20.04
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+          env:
+            - name: DCGM_EXPORTER_TELEMETRY
+              value: "true"
+          volumeMounts:
+            - name: nvidia-smi
+              mountPath: /var/run/nvidia/driver
+              readOnly: true
+      volumes:
+        - name: nvidia-smi
+          hostPath:
+            path: /var/run/nvidia/driver
+            type: DirectoryOrCreate
+---
+# ServiceMonitor for Prometheus Operator (optional)
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: dcgm-exporter
+  namespace: monitoring
+spec:
+  selector:
+    matchLabels:
+      app: dcgm-exporter
+  endpoints:
+    - port: metrics
+      interval: 15s
+      path: /metrics
+  namespaceSelector:
+    matchNames:
+      - monitoring
diff --git a/k8s/manifests/dgx/deepspeed-dgx-job.yaml b/k8s/manifests/dgx/deepspeed-dgx-job.yaml
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/k8s/manifests/dgx/deepspeed-dgx-job.yaml
@@ -0,0 +1,154 @@
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: deepspeed-dgx-smoke
+  namespace: aegis-ml
+  labels:
+    app: deepspeed-dgx
+spec:
+  template:
+    metadata:
+      labels:
+        app: deepspeed-dgx
+    spec:
+      restartPolicy: Never
+      nodeSelector:
+        "node.kubernetes.io/dgx": "true"
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      containers:
+        - name: deepspeed-runner
+          image: <REGISTRY>/aegis-deepspeed:latest
+          imagePullPolicy: IfNotPresent
+          command: ["/bin/bash", "-c"]
+          args:
+            - |
+              set -euo pipefail
+              echo "NCCL tunables:"
+              env | grep NCCL || true
+              echo "Running small DeepSpeed smoke training..."
+              export NCCL_DEBUG=INFO
+              export NCCL_IB_DISABLE=0
+              export NCCL_SOCKET_IFNAME=eth0
+              export NCCL_IB_HCA=mlx5_0
+              export NCCL_NET_GDR_LEVEL=2
+              export TORCH_DISTRIBUTED_DEBUG=INFO
+              # Example deepspeed one-node multi-gpu (adjust model script & args)
+              deepspeed --num_gpus 8 examples/deepspeed_smoke/train.py --epochs 1 --batch-size 8 || { rc=$?; echo "Deepspeed failed: $rc"; exit $rc; }
+          resources:
+            limits:
+              nvidia.com/gpu: 8
+            requests:
+              cpu: "8"
+              memory: "32Gi"
+      # Mount a PVC for checkpoints if available; fallback to emptyDir in dev/staging
+      volumes:
+        - name: checkpoints
+          emptyDir: {}
+  backoffLimit: 1
+
+# Notes:
+# - Set `node.kubernetes.io/dgx=true` label on DGX nodes (or adjust selector).
+# - The image <REGISTRY>/aegis-deepspeed:latest should contain DeepSpeed, matching PyTorch and CUDA/NCCL versions for DGX.
+# - Adjust env values for your DGX topology (socket interface, IB device name).
diff --git a/scripts/dgx/nccl_tuning.sh b/scripts/dgx/nccl_tuning.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/nccl_tuning.sh
@@ -0,0 +1,160 @@
+#!/usr/bin/env bash
+#
+# Run NCCL tests and collect a short report for DGX topology validation.
+# Intended to be run on a DGX node (container with nccl-tests installed or via bastion with ssh).
+#
+# Usage:
+#   ./scripts/dgx/nccl_tuning.sh --nodes 1 --gpus-per-node 8
+
+set -euo pipefail
+
+NODES=1
+GPUS_PER_NODE=8
+OUT_DIR="${OUT_DIR:-./artifacts/nccl}"
+NCCL_TEST_REPO="${NCCL_TEST_REPO:-https://github.com/NVIDIA/nccl-tests.git}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --gpus-per-node) GPUS_PER_NODE="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+mkdir -p "$OUT_DIR"
+
+echo "Cloning nccl-tests..."
+tmpdir=$(mktemp -d)
+git clone --depth 1 "$NCCL_TEST_REPO" "$tmpdir/nccl-tests"
+pushd "$tmpdir/nccl-tests"
+
+echo "Building nccl-tests (requires CUDA & make)..."
+make MPI=0 -j || true
+
+BIN="./build/all_reduce_perf"
+if [[ ! -f "$BIN" ]]; then
+  echo "nccl-tests build failed or binary missing; listing build dir:"
+  ls -lah build || true
+fi
+
+echo "Running all_reduce_perf benchmark..."
+./build/all_reduce_perf -b 8 -e 512M -f 2 -g $GPUS_PER_NODE > "$OUT_DIR/all_reduce_perf.txt" 2>&1 || true
+
+echo "Running p2p tests..."
+./build/p2pBandwidthLatency -g $GPUS_PER_NODE > "$OUT_DIR/p2p.txt" 2>&1 || true
+
+echo "Collecting dmesg and nvidia-smi..."
+nvidia-smi -q > "$OUT_DIR/nvidia-smi.txt" || true
+dmesg | tail -n 200 > "$OUT_DIR/dmesg_tail.txt" || true
+
+echo "NCCL tuning artifacts saved to $OUT_DIR"
+popd
+rm -rf "$tmpdir"
+
+echo "Tip: adjust NCCL_SOCKET_IFNAME / NCCL_IB_HCA values in deepspeed-dgx-job.yaml based on `ip -o link` / `ibv_devices` output on your DGX nodes."
+exit 0
diff --git a/scripts/dgx/dgx_smoke_test.sh b/scripts/dgx/dgx_smoke_test.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/dgx/dgx_smoke_test.sh
@@ -0,0 +1,140 @@
+#!/usr/bin/env bash
+#
+# Orchestrate a quick DGX smoke test:
+#  - ensure GPU operator is installed
+#  - apply DCGM exporter
+#  - run deepspeed job (k8s Job) and tail logs
+#
+set -euo pipefail
+
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+JOB_MANIFEST="k8s/manifests/dgx/deepspeed-dgx-job.yaml"
+DCGM_MANIFEST="k8s/manifests/dgx/dcgm-exporter-daemonset.yaml"
+GPU_OPERATOR_VALUES="k8s/manifests/dgx/gpu-operator-values.yaml"
+
+echo "Ensure kubectl is configured and points to the DGX cluster..."
+kubectl version --short || true
+
+echo "Apply DCGM exporter (monitoring namespace)..."
+kubectl apply -f "$DCGM_MANIFEST" || true
+
+echo "Apply GPU Operator via Helm (values file must be created/edited as needed)..."
+if ! helm ls -n gpu-operator | grep gpu-operator >/dev/null 2>&1; then
+  helm repo add nvidia https://nvidia.github.io/gpu-operator || true
+  helm repo update || true
+  helm install --namespace gpu-operator --create-namespace gpu-operator nvidia/gpu-operator -f "$GPU_OPERATOR_VALUES" || true
+else
+  echo "GPU operator already installed"
+fi
+
+echo "Deploying deepspeed smoke job into namespace $NAMESPACE..."
+kubectl create ns "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f - || true
+kubectl apply -f "$JOB_MANIFEST"
+
+JOB_NAME=$(yq e '.metadata.name' "$JOB_MANIFEST" 2>/dev/null || echo "deepspeed-dgx-smoke")
+echo "Waiting for job pods..."
+kubectl wait --for=condition=ready pod -l app=deepspeed-dgx -n "$NAMESPACE" --timeout=300s || true
+
+POD=$(kubectl -n "$NAMESPACE" get pod -l app=deepspeed-dgx -o jsonpath='{.items[0].metadata.name}')
+echo "Tailing logs of pod $POD"
+kubectl -n "$NAMESPACE" logs -f "$POD" || true
+
+echo "Fetching job status..."
+kubectl -n "$NAMESPACE" get job -l app=deepspeed-dgx -o wide || true
+
+echo "Smoke test complete; collect artifacts (nccl tuning script recommended)"
+exit 0
diff --git a/tools/benchmarks/deepspeed_dgx_benchmark.sh b/tools/benchmarks/deepspeed_dgx_benchmark.sh
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/tools/benchmarks/deepspeed_dgx_benchmark.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Simple scheduler-aware DeepSpeed benchmark wrapper for DGX clusters.
+# Run from a machine with kubectl configured for the DGX k8s cluster.
+#
+set -euo pipefail
+
+MODEL=${MODEL:-"gpt2-small"}
+NUM_NODES=${NUM_NODES:-1}
+GPUS_PER_NODE=${GPUS_PER_NODE:-8}
+JOB_NAME="deepspeed-dgx-benchmark-$(date +%s)"
+NAMESPACE=${NAMESPACE:-aegis-ml}
+IMAGE=${IMAGE:-"<REGISTRY>/aegis-deepspeed:latest"}
+
+cat <<EOF > /tmp/deepspeed_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: ${JOB_NAME}
+  namespace: ${NAMESPACE}
+spec:
+  template:
+    metadata:
+      labels:
+        app: deepspeed-dgx-benchmark
+    spec:
+      restartPolicy: Never
+      nodeSelector:
+        "node.kubernetes.io/dgx": "true"
+      containers:
+        - name: runner
+          image: ${IMAGE}
+          command: ["/bin/bash", "-c"]
+          args:
+            - |
+              set -euo pipefail
+              export NCCL_DEBUG=INFO
+              export NCCL_IB_DISABLE=0
+              export NCCL_SOCKET_IFNAME=eth0
+              export NCCL_IB_HCA=mlx5_0
+              deepspeed --num_gpus ${GPUS_PER_NODE} examples/deepspeed_benchmark/run_benchmark.py --model ${MODEL} --steps 50
+          resources:
+            limits:
+              nvidia.com/gpu: ${GPUS_PER_NODE}
+            requests:
+              cpu: "16"
+              memory: "64Gi"
+      backoffLimit: 1
+EOF
+
+kubectl apply -f /tmp/deepspeed_job.yaml
+
+echo "Waiting for job to start..."
+kubectl -n ${NAMESPACE} wait --for=condition=ready pod -l app=deepspeed-dgx-benchmark --timeout=600s || true
+POD=$(kubectl -n ${NAMESPACE} get pod -l app=deepspeed-dgx-benchmark -o jsonpath='{.items[0].metadata.name}')
+kubectl -n ${NAMESPACE} logs -f "$POD"
+
+echo "Benchmark finished; collect logs to ./artifacts/${JOB_NAME}.log"
+mkdir -p ./artifacts
+kubectl -n ${NAMESPACE} logs "$POD" > ./artifacts/${JOB_NAME}.log || true
+echo "Saved logs to ./artifacts/${JOB_NAME}.log"
+exit 0
diff --git a/.github/workflows/dgx_benchmark.yml b/.github/workflows/dgx_benchmark.yml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/.github/workflows/dgx_benchmark.yml
@@ -0,0 +1,170 @@
+name: DGX Benchmark & Smoke (staging)
+
+on:
+  workflow_dispatch:
+    inputs:
+      job:
+        description: "Which job to run (smoke|benchmark)"
+        required: true
+        default: "smoke"
+      model:
+        description: "Model for benchmark (gpt2-small etc.)"
+        required: false
+        default: "gpt2-small"
+      num_nodes:
+        description: "Number of DGX nodes to use (1,2,4)"
+        required: false
+        default: "1"
+
+jobs:
+  dgx-smoke:
+    runs-on: self-hosted
+    if: runner.labels contains 'dgx'  # ensure this runs on runners able to access DGX cluster
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install yq kubernetes boto3 || true
+
+      - name: Run DGX smoke or benchmark
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_DGX }}
+          REGISTRY: ${{ secrets.REGISTRY }}
+        run: |
+          if [ "${{ github.event.inputs.job }}" = "smoke" ]; then
+            chmod +x scripts/dgx/dgx_smoke_test.sh
+            ./scripts/dgx/dgx_smoke_test.sh
+          else
+            chmod +x tools/benchmarks/deepspeed_dgx_benchmark.sh
+            ./tools/benchmarks/deepspeed_dgx_benchmark.sh --model "${{ github.event.inputs.model }}" --num_nodes "${{ github.event.inputs.num_nodes }}"
+          fi
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: dgx-artifacts
+          path: artifacts || .
+
+      - name: Done
+        run: echo "DGX workflow finished"
diff --git a/docs/dgx/README.md b/docs/dgx/README.md
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/docs/dgx/README.md
@@ -0,0 +1,240 @@
+# NVIDIA DGX Cloud Support for Aegis — Quickstart & Notes
+
+This folder provides a starting point to integrate NVIDIA DGX Cloud with Aegis. It includes:
+
+- Helm values for NVIDIA GPU Operator (k8s/manifests/dgx/gpu-operator-values.yaml)  
+- DCGM exporter daemonset for GPU metrics (k8s/manifests/dgx/dcgm-exporter-daemonset.yaml)  
+- Example DeepSpeed k8s Job tuned for DGX (k8s/manifests/dgx/deepspeed-dgx-job.yaml)  
+- NCCL tuning and smoke scripts (scripts/dgx/*)  
+- Benchmark wrapper and CI workflow (.github/workflows/dgx_benchmark.yml)
+
+Prerequisites
+- DGX Cloud account and cluster with admin access. Ensure you have kubeconfig for the DGX k8s cluster or scheduler access (Slurm) as appropriate.  
+- Confirm CUDA, cuDNN and NCCL versions you intend to use and pin your training images accordingly.  
+- Ensure network & storage: RDMA enabled, NVLink/NVSwitch configured, and fast shared storage (Lustre, NVMe or S3) is available for checkpoints.
+
+Quick steps
+1. Label DGX nodes:
+   kubectl label nodes <dgx-node-name> node.kubernetes.io/dgx=true
+
+2. Install NVIDIA GPU Operator (recommended):
+   helm repo add nvidia https://nvidia.github.io/gpu-operator
+   helm repo update
+   helm install --namespace gpu-operator --create-namespace gpu-operator nvidia/gpu-operator -f k8s/manifests/dgx/gpu-operator-values.yaml
+
+3. Deploy DCGM exporter:
+   kubectl apply -f k8s/manifests/dgx/dcgm-exporter-daemonset.yaml
+
+4. Build/push your deepspeed image (matching CUDA/NCCL):
+   docker build -t <REGISTRY>/aegis-deepspeed:latest -f Dockerfile.deepspeed .
+   docker push <REGISTRY>/aegis-deepspeed:latest
+
+5. Run the smoke job:
+   kubectl apply -f k8s/manifests/dgx/deepspeed-dgx-job.yaml
+   kubectl -n aegis-ml logs -l app=deepspeed-dgx -f
+
+Validation & tuning
+- Run scripts/dgx/nccl_tuning.sh on a DGX node or in a privileged container to gather nccl-tests outputs and `nvidia-smi` state.  
+- Adjust NCCL env vars in deepspeed-dgx-job.yaml:
+  - NCCL_SOCKET_IFNAME (e.g., eth0 or bond0)
+  - NCCL_IB_HCA (IB device, e.g., mlx5_0)
+- Use tools/benchmarks/deepspeed_dgx_benchmark.sh to run larger benchmarks and store artifacts for tuning.
+
+Operational recommendations
+- Use NVIDIA GPU Operator to manage drivers, toolkit and device plugin. Disable driver install in the operator if DGX images already include the correct drivers.  
+- Use DCGM exporter metrics to feed Prometheus/Grafana dashboards for GPU utilization, NVLink throughput and ECC errors.  
+- Integrate the DGX profile into your chaos/rollback matrix and nightly validation CI to build confidence before enabling larger auto‑approval scopes.
+
+Security & governance notes
+- Ensure Aegis control plane endpoints (OPA, token issuer, Auditor UI) are reachable from the DGX cluster or deploy local proxies.  
+- Enforce mTLS and RBAC between control plane services and DGX nodes where possible.  
+- Add OPA budget rules specifically for DGX node types to prevent accidental large-scale runs without approval.
+
+Next steps / optional artifacts to add
+- A Slurm scheduler adapter for non‑kubernetes DGX deployments.  
+- Signed, pinned base images for DGX with CUDA/cuDNN/NCCL matrix.  
+- Automated NCCL tuning calibrator that updates recommended env vars per topology.
+
+If you want, I can generate:
+- The GPU Operator Helm command + values file prefilled for your REGISTRY (provide registry URL), or  
+- A Slurm adapter scaffold to submit DeepSpeed jobs to DGX Slurm scheduler, or  
+- A base Dockerfile.deepspeed tuned for H100/A100 (provide desired CUDA/NCCL versions).
+
+End of DGX quickstart.
+
