*** Begin Patch
*** Add File: monitoring/prometheus_rules_aegis_kpis_full.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-kpi-rules-full
+  namespace: monitoring
+spec:
+  groups:
+    - name: aegis.kpi.rules
+      rules:
+        - alert: OperatorHighFailureRate
+          expr: |
+            (increase(aegis_operator_actions_total{phase="failed"}[1h]) /
+             max(1, increase(aegis_operator_actions_total[1h]))) > 0.05
+          for: 30m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Operator failure rate > 5% in the last hour"
+            description: "High operator failure rate. Check operator logs, suggested-action diffs and recent run artifacts."
+
+        - alert: OperatorLowRollbackSuccess
+          expr: |
+            (sum(rate(aegis_operator_rollback_total{result="success"}[1h])) /
+             max(1, sum(rate(aegis_operator_rollback_total[1h])))) < 0.99
+          for: 30m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Rollback success rate < 99%"
+            description: "Rollback reliability is below threshold. Investigate rollback jobs and snapshot integrity."
+
+        - alert: LLMGatewayP95TooHigh
+          expr: |
+            histogram_quantile(0.95, sum(rate(aegis_llm_request_latency_seconds_bucket[5m])) by (le)) > 0.5
+          for: 10m
+          labels:
+            severity: critical
+          annotations:
+            summary: "LLM gateway p95 latency > 0.5s"
+            description: "The 95th percentile latency for LLM gateway exceeded the target. Consider scaling or changing upstream."
+
+        - alert: LLMGatewayErrorRateHigh
+          expr: |
+            (sum(rate(aegis_llm_requests_total{status!~"2.."}[5m])) /
+             sum(rate(aegis_llm_requests_total[5m]))) > 0.01
+          for: 10m
+          labels:
+            severity: critical
+          annotations:
+            summary: "LLM gateway error rate > 1%"
+            description: "High error rate from the LLM gateway. Inspect gateway and upstream logs."
+
+        - alert: QuotaExceededSpike
+          expr: |
+            increase(aegis_llm_requests_total{status="quota_exceeded"}[5m]) > 0
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Quota exceeded events detected"
+            description: "One or more requests triggered quota enforcement in the last 5 minutes."
+
+        - alert: RAGCoverageLow
+          expr: |
+            rag_index_coverage_ratio < 0.75
+          for: 30m
+          labels:
+            severity: warning
+          annotations:
+            summary: "RAG index coverage below 75%"
+            description: "RAG index coverage below target; check index freshness or ingestion pipelines."
+
+        - alert: AuditUploadFailure
+          expr: |
+            increase(aegis_audit_upload_failed_total[1h]) > 0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Audit upload failures detected"
+            description: "Audit artifact upload failures observed. Verify storage credentials and network access."
+
+        - alert: GatekeeperDenialsSpike
+          expr: |
+            increase(aegis_gatekeeper_admission_denials_total[5m]) > 5
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Spike in Gatekeeper admission denials"
+            description: "Multiple admission denials observed. May indicate constraint regression or mass misconfiguration."
+
+        - alert: PrometheusTargetDown
+          expr: |
+            absent(up{job=~"pushgateway|prometheus"})
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Prometheus scrape target down"
+            description: "A critical monitoring service may be unreachable. Check Prometheus and Pushgateway connectivity."
+
*** End Patch
*** Begin Patch
*** Add File: grafana/aegis_kpi_dashboard_full.json
+{
+  "annotations": { "list": [] },
+  "editable": true,
+  "gnetId": null,
+  "graphTooltip": 0,
+  "id": null,
+  "links": [],
+  "panels": [
+    {
+      "id": 1,
+      "title": "LLM Gateway p95 (5m)",
+      "type": "graph",
+      "gridPos": { "x": 0, "y": 0, "w": 12, "h": 8 },
+      "targets": [
+        {
+          "expr": "histogram_quantile(0.95, sum(rate(aegis_llm_request_latency_seconds_bucket[5m])) by (le))",
+          "legendFormat": "p95",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 2,
+      "title": "LLM Gateway Error Rate (5m)",
+      "type": "graph",
+      "gridPos": { "x": 12, "y": 0, "w": 12, "h": 8 },
+      "targets": [
+        {
+          "expr": "sum(rate(aegis_llm_requests_total{status!~\"2..\"}[5m])) / sum(rate(aegis_llm_requests_total[5m]))",
+          "legendFormat": "error_rate",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 3,
+      "title": "Operator Failure Rate (1h)",
+      "type": "graph",
+      "gridPos": { "x": 0, "y": 8, "w": 12, "h": 6 },
+      "targets": [
+        {
+          "expr": "increase(aegis_operator_actions_total{phase=\"failed\"}[1h]) / max(1, increase(aegis_operator_actions_total[1h]))",
+          "legendFormat": "operator_fail_rate",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 4,
+      "title": "Rollback Success Rate (1h)",
+      "type": "graph",
+      "gridPos": { "x": 12, "y": 8, "w": 12, "h": 6 },
+      "targets": [
+        {
+          "expr": "sum(rate(aegis_operator_rollback_total{result=\"success\"}[1h])) / max(1, sum(rate(aegis_operator_rollback_total[1h])))",
+          "legendFormat": "rollback_success",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 5,
+      "title": "RAG Coverage Ratio",
+      "type": "stat",
+      "gridPos": { "x": 0, "y": 14, "w": 8, "h": 4 },
+      "targets": [
+        {
+          "expr": "rag_index_coverage_ratio",
+          "legendFormat": "rag_coverage",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 6,
+      "title": "Quota Exceeded Rate (5m)",
+      "type": "graph",
+      "gridPos": { "x": 8, "y": 14, "w": 8, "h": 4 },
+      "targets": [
+        {
+          "expr": "sum(rate(aegis_llm_requests_total{status=\"quota_exceeded\"}[5m]))",
+          "legendFormat": "quota_exceeded",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 7,
+      "title": "Gatekeeper Denials (5m)",
+      "type": "graph",
+      "gridPos": { "x": 16, "y": 14, "w": 8, "h": 4 },
+      "targets": [
+        {
+          "expr": "increase(aegis_gatekeeper_admission_denials_total[5m])",
+          "legendFormat": "denials",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    },
+    {
+      "id": 8,
+      "title": "Audit Upload Failures (1h)",
+      "type": "graph",
+      "gridPos": { "x": 0, "y": 18, "w": 24, "h": 4 },
+      "targets": [
+        {
+          "expr": "increase(aegis_audit_upload_failed_total[1h])",
+          "legendFormat": "audit_failures",
+          "refId": "A"
+        }
+      ],
+      "datasource": "${DS_PROMETHEUS}"
+    }
+  ],
+  "schemaVersion": 27,
+  "style": "dark",
+  "tags": ["aegis", "kpi", "slo"],
+  "templating": { "list": [] },
+  "time": { "from": "now-1h", "to": "now" },
+  "title": "Aegis KPI & SLO Dashboard",
+  "uid": "aegis-kpi-dashboard-full",
+  "version": 1
+}
+
*** End Patch
*** Begin Patch
*** Add File: scripts/operator_metrics_exporter.py
+#!/usr/bin/env python3
+"""
+operator_metrics_exporter.py
+
+Small CLI helper to push operator E2E run metrics to a Pushgateway instance.
+Usage examples:
+  # push a single run increment (successful execution + successful rollback)
+  python scripts/operator_metrics_exporter.py \
+    --pushgateway http://pushgateway.monitoring.svc:9091 \
+    --job aegis-operator-e2e \
+    --instance runner-1 \
+    --inc total_runs=1 executed_success=1 rollback_success=1
+
+  # push multiple counters from a JSON file:
+  python scripts/operator_metrics_exporter.py --pushgateway ... --job ... --instance ... --file /tmp/run.json
+
+Notes:
+- This script issues an HTTP PUT to the Pushgateway /metrics/job/<job>/instance/<instance> endpoint.
+- Prometheus should be configured to scrape the Pushgateway.
+"""
+
+import argparse
+import json
+import requests
+import sys
+from urllib.parse import urljoin
+
+def build_payload(counters):
+    """
+    Build text payload in Prometheus exposition format.
+    Example:
+      aegis_operator_e2e_total_runs 1
+      aegis_operator_e2e_executed_success 1
+    """
+    lines = []
+    for k, v in counters.items():
+        # sanitize metric names (replace invalid chars)
+        metric = k.replace('-', '_').replace('.', '_')
+        lines.append(f"{metric} {v}")
+    return "\n".join(lines) + "\n"
+
+def push_metrics(pushgateway, job, instance, counters):
+    # Pushgateway expects path /metrics/job/<job>/instance/<instance>
+    base = pushgateway.rstrip("/")
+    path = f"/metrics/job/{job}/instance/{instance}"
+    url = base + path
+    payload = build_payload(counters)
+    headers = {"Content-Type": "text/plain; charset=utf-8"}
+    resp = requests.put(url, data=payload.encode("utf-8"), headers=headers, timeout=15)
+    resp.raise_for_status()
+    return resp
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--pushgateway", required=True, help="Pushgateway base URL (http://pushgateway:9091)")
+    p.add_argument("--job", required=True, help="Pushgateway job name (e.g. aegis-operator-e2e)")
+    p.add_argument("--instance", required=True, help="Instance id/runner name")
+    p.add_argument("--inc", nargs="*", help="Counters to increment as key=value", default=[])
+    p.add_argument("--file", help="JSON file with counters map {metric: value}")
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    counters = {}
+    if args.file:
+        with open(args.file) as f:
+            counters.update(json.load(f))
+    for kv in args.inc:
+        if "=" not in kv:
+            print("Invalid inc argument:", kv, file=sys.stderr)
+            sys.exit(2)
+        k, v = kv.split("=", 1)
+        try:
+            v = float(v) if "." in v else int(v)
+        except Exception:
+            v = kv.split("=", 1)[1]
+        counters[k] = v
+
+    if not counters:
+        print("No counters provided", file=sys.stderr)
+        sys.exit(2)
+
+    resp = push_metrics(args.pushgateway, args.job, args.instance, counters)
+    print("Pushed metrics to", resp.url, "status", resp.status_code)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus_pushgateway_scrape.yaml
+# Example Prometheus scrape config snippet to scrape Pushgateway.
+# Merge this into Prometheus additionalScrapeConfigs or your Prometheus helm values.
+scrape_configs:
+  - job_name: 'pushgateway'
+    metrics_path: '/metrics'
+    static_configs:
+      - targets:
+          - 'pushgateway.monitoring.svc:9091'   # replace with your pushgateway service address
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/fully_ai_readiness_gate_auto.yml
+name: Fully AI Readiness Gate (Auto-evaluate)
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: read
+
+jobs:
+  readiness-gate-auto:
+    runs-on: ubuntu-latest
+    timeout-minutes: 120
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python 3.10
+        uses: actions/setup-python@v4
+        with:
+          python-version: "3.10"
+
+      - name: Install tools
+        run: |
+          sudo apt-get update -y
+          sudo apt-get install -y jq
+          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
+          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
+      - name: Configure kubeconfig
+        env:
+          KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
+        run: |
+          if [ -z "${KUBE_CONFIG_DATA:-}" ]; then
+            echo "KUBE_CONFIG_DATA not set; aborting."
+            exit 2
+          fi
+          echo "$KUBE_CONFIG_DATA" | base64 --decode > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+          kubectl cluster-info || true
+
+      - name: Install Python deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install -r requirements.txt || true
+          pip install requests prometheus-api-client || true
+
+      - name: Run readiness gate steps (milvus / llm / gatekeeper / operator)
+        env:
+          MILVUS_VALUES_BASE64: ${{ secrets.MILVUS_VALUES_BASE64 }}
+          LLM_GATEWAY_URL: ${{ secrets.LLM_GATEWAY_URL }}
+          LLM_GATEWAY_API_KEY: ${{ secrets.LLM_GATEWAY_API_KEY }}
+          MILVUS_HOST: ${{ secrets.MILVUS_HOST }}
+          MILVUS_PORT: ${{ secrets.MILVUS_PORT }}
+        run: |
+          set -euo pipefail
+          echo "Milvus deploy & verify (best-effort)"
+          if [ -x scripts/deploy_milvus_verify.sh ]; then
+            ./scripts/deploy_milvus_verify.sh || echo "Milvus deploy/verify returned non-zero (check logs)"
+          else
+            echo "deploy_milvus_verify.sh not present; skipping"
+          fi
+
+          echo "Milvus stress test (if host provided)"
+          if [ -n "${MILVUS_HOST:-}" ] && [ -n "${MILVUS_PORT:-}" ]; then
+            python scripts/milvus_stress_test.py --host "${MILVUS_HOST}" --port "${MILVUS_PORT}" --collection aegis_docs --inserts 200 --queries 100 --concurrency 4 || echo "Milvus stress test reported issues"
+          fi
+
+          echo "LLM gateway load test (if URL provided)"
+          if [ -n "${LLM_GATEWAY_URL:-}" ]; then
+            python scripts/llm_load_test.py --url "${LLM_GATEWAY_URL}" --key "${LLM_GATEWAY_API_KEY:-}" --concurrency 8 --requests 100 || echo "LLM load test reported issues"
+          fi
+
+          echo "Apply Gatekeeper constraints (staging)"
+          if kubectl get pods -n gatekeeper >/dev/null 2>&1; then
+            kubectl apply -f k8s/gatekeeper_constrainttemplate_suggestedaction_v2.yaml || true
+            kubectl apply -f k8s/constraint_require_approval_and_owner_v2.yaml || true
+          else
+            echo "Gatekeeper not installed in cluster (namespace 'gatekeeper')"
+          fi
+
+          echo "Run operator E2E repeats (collect metrics via exporter if PUSHGATEWAY_URL provided)"
+          if [ -x scripts/operator_e2e_repeat.py ]; then
+            # run a small number of runs for the workflow; operator_e2e_repeat.py supports env E2E_RUNS
+            E2E_RUNS=${E2E_RUNS:-10}
+            E2E_RUNS=$E2E_RUNS python scripts/operator_e2e_repeat.py || echo "operator_e2e_repeat returned non-zero"
+          fi
+
+      - name: Collect KPIs from Prometheus
+        env:
+          PROM_URL: ${{ secrets.PROM_URL }}
+          PROM_TOKEN: ${{ secrets.PROM_TOKEN }}
+        run: |
+          if [ -z "${PROM_URL:-}" ]; then
+            echo "PROM_URL not set; skipping KPI collection"
+            exit 2
+          fi
+          python scripts/collect_kpis.py --minutes 60 --out /tmp/kpi_snapshot.json
+          cat /tmp/kpi_snapshot.json
+
+      - name: Evaluate KPIs against SLOs (auto-fail on breach)
+        run: |
+          python scripts/evaluate_kpis.py --input /tmp/kpi_snapshot.json --slo monitoring/slo_values.yaml
+        shell: bash
+
+      - name: Upload readiness artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: readiness-artifacts
+          path: |
+            /tmp/kpi_snapshot.json
+            monitoring/prometheus_rules_aegis_kpis_full.yaml
+            grafana/aegis_kpi_dashboard_full.json
+
*** End Patch
*** Begin Patch
*** Add File: docs/SPRINT_BACKLOG_2WKS.md
+```markdown
+# Aegis 2‑Week Sprint Backlog — Production Proof & Fully-AI Final Mile
+
+Sprint goal
+- Prove Milvus HA + RAG backups, validate operator E2E rollback reliability, and establish KPI baseline required for enabling controlled automation.
+
+Sprint duration
+- 2 weeks (14 days). Break into two 1-week milestones.
+
+Owners
+- SRE/Infra: Milvus, Prometheus, Pushgateway, Gatekeeper install help
+- Platform: Operator E2E, exporter, readiness gate
+- ML Eng: Upstream model + SLO tuning
+- Security: Identity migration, audit retention
+
+Milestone 1 (Days 1–7) — Stabilize core systems
+- Task A: Milvus HA deploy & backup verification (SRE)
+  - Acceptance:
+    - Helm release deployed in staging
+    - TLS secret present (aegis-milvus-tls)
+    - Backup CronJob can run a one-off job and backup object appears in audit bucket
+  - Checklist:
+    - kubectl -n aegis get pods (milvus) all Ready
+    - run `./scripts/deploy_milvus_verify.sh` success
+    - verify backup object in GCS/S3
+
+- Task B: Gatekeeper policy enforce & CI (Security)
+  - Acceptance:
+    - Gatekeeper installed in staging
+    - Policy tests pass in `Gatekeeper Policy CI`
+  - Checklist:
+    - kubectl -n gatekeeper get pods Ready
+    - `kubectl apply` of test manifests behaves as expected (allowed/denied)
+
+- Task C: Operator E2E run & metrics exporter (Platform)
+  - Acceptance:
+    - operator_e2e_repeat runs N >= 100 in staging with captured per-run outcomes
+    - per-run counters pushed to Pushgateway
+  - Checklist:
+    - Pushgateway receives `aegis_operator_e2e_total_runs`, `...executed_success`, `...rollback_success`
+    - Grafana panel shows counts
+
+Milestone 2 (Days 8–14) — SLOs, audit, identity & gating
+- Task D: Deploy upstream model behind gateway & SLO tuning (ML Eng)
+  - Acceptance:
+    - Representative upstream model deployed
+    - p95 and error rate meet configured SLOs under synthetic load
+  - Checklist:
+    - Run `python scripts/llm_load_test.py` and confirm p95 <= target
+
+- Task E: Audit immutability & verification (Security)
+  - Acceptance:
+    - AUTO_UPLOAD_AUDIT enabled in staging
+    - programmatic verification (verify_audit_retention.py) returns OK
+  - Checklist:
+    - gsutil ls / aws s3 ls show audit objects
+    - retention/object-lock configured and verified
+
+- Task F: KPI collection + gating & signoff (Product / SRE)
+  - Acceptance:
+    - collect_kpis.py run daily; evaluate_kpis.py pass for N consecutive days (N=3 recommended)
+    - Signoff document created and approvals collected
+  - Checklist:
+    - Add signoff issue with attach `/tmp/kpi_snapshot.json`
+    - SRE & Security approvals present in issue comments
+
+PR templates, Issue templates, Checklists
+- Use `.github/PULL_REQUEST_TEMPLATE.md` for PRs that change operator or policy code.
+- Use `.github/ISSUE_TEMPLATE/production-proof.md` to file issues when a gate fails.
+
+Daily cadence
+- Daily readiness gate run (Actions) → triage failures → capture artifacts → assign fix owners
+- Platform owner aggregates metrics and reports daily to SRE/ML/Security
+
+Definition of Done for sprint
+- Readiness gate completes without SLO breaches for one full run
+- Operator E2E rollback success ≥ 99% across test runs
+- Milvus backups produced and verified programmatically
+- Gatekeeper policies enforced in staging and policy CI passes
+- KPI reporter configured and scheduled to run daily
+``` 
+
*** End Patch
*** Begin Patch
*** Add File: .github/PULL_REQUEST_TEMPLATE.md
+### Summary
+
+Provide a short description of the change and why it's needed.
+
+### Related Issues / PRs
+
+- Closes: #<issue>
+
+### Changes
+
+- List of notable changes
+
+### Test Plan
+
+- How was this change tested? Include commands and environment.
+
+### Checklist (required for operator / Gatekeeper / infra changes)
+
+- [ ] I ran the Gatekeeper policy CI locally or in staging and confirmed expected allow/deny behavior
+- [ ] I ran operator_e2e_repeat (or relevant E2E) and attached results
+- [ ] I validated no long-lived keys were added (secret scanner)
+- [ ] I updated monitoring (PrometheusRule/Grafana) if metrics changed
+- [ ] Change includes roll-back plan / runbook notes if applicable
+
*** End Patch
*** Begin Patch
*** Add File: .github/ISSUE_TEMPLATE/production-proof.md
+name: Production Proof - Readiness Failure
+about: File when the Fully AI readiness gate fails or shows SLO breaches
+title: "[Readiness Gate] - {short summary}"
+labels: ["readiness", "investigate"]
+assignees: []
+
+body:
+  - type: markdown
+    attributes:
+      value: |
+        Please include the readiness run artifacts and a short summary of the failing gate.
+  - type: input
+    id: failing_step
+    attributes:
+      label: Failing step
+      description: e.g., Milvus backup verification, LLM p95, Gatekeeper policy denial
+  - type: textarea
+    id: logs
+    attributes:
+      label: Relevant logs / summary
+      description: Paste important log excerpts or attach the readiness artifacts.
+  - type: input
+    id: owner
+    attributes:
+      label: Owner
+      description: Person/team assigned to investigate
+
*** End Patch
*** Begin Patch
*** Add File: docs/ACCEPTANCE_CRITERIA.md
+Aegis Fully-AI Final Mile — Acceptance Criteria (short)
+
+1. Milvus
+ - HA cluster running in staging & prod (pods Ready)
+ - TLS and auth secrets present & validated
+ - Backups produced and verified programmatically
+
+2. Gateway & Upstream
+ - Upstream model runs behind gateway and meets p95 & error SLO under representative load
+ - No direct upstream provider keys exist in repo/workflows
+
+3. Gatekeeper
+ - Policies deny unsafe SuggestedAction creations/updates in staging
+ - Gatekeeper policy CI passes for policy changes
+
+4. Operator
+ - operator_e2e_repeat run N>=100 in staging
+ - Rollback success >= 99% for runs
+ - Operator FP rate <= defined threshold (e.g., 1%)
+
+5. Audit & Identity
+ - Audit artifacts auto-uploaded and retention verified
+ - All critical KSAs migrated to Workload Identity/IRSA or Vault
+
+6. Observability & KPIs
+ - PrometheusRules & Grafana dashboard present
+ - KPI reporter scheduled and passing for N consecutive days
+ - SRE & Security signoff recorded
+
*** End Patch
*** Begin Patch
*** Add File: README_SPRINT.md
+Sprint playbook (quick commands)
+
+1) Run readiness gate:
+   gh workflow run fully_ai_readiness_gate_auto.yml --ref main
+
+2) Run operator E2E locally and push metrics to Pushgateway:
+   python scripts/operator_e2e_repeat.py
+   # or, if instrumented to call exporter per-run:
+   python scripts/operator_metrics_exporter.py --pushgateway http://pushgateway.monitoring.svc:9091 --job aegis-operator-e2e --instance runner-1 --inc aegis_operator_e2e_total_runs=10 aegis_operator_e2e_executed_success=9
+
+3) Collect KPIs and evaluate:
+   PROM_URL=<prom_url> PROM_TOKEN=<token> python scripts/collect_kpis.py --minutes 60 --out /tmp/kpi.json
+   python scripts/evaluate_kpis.py --input /tmp/kpi.json --slo monitoring/slo_values.yaml
+
+4) Apply Gatekeeper policies in staging:
+   kubectl apply -f k8s/gatekeeper_constrainttemplate_suggestedaction_v2.yaml
+   kubectl apply -f k8s/constraint_require_approval_and_owner_v2.yaml
+
+Notes:
+- The exporter requires a reachable Pushgateway and Prometheus scrape configuration (provided `monitoring/prometheus_pushgateway_scrape.yaml` as a sample).
+- The enhanced readiness gate workflow will fail if evaluate_kpis.py returns non-zero; ensure Prometheus credentials (PROM_URL, PROM_TOKEN) are set in repository or environment secrets.
+- Adjust SLO thresholds in `monitoring/slo_values.yaml` if you need different targets.
+
*** End Patch
