*** Begin Patch
*** Add File: mlops/helm/mlflow/values-production.yaml
+# MLflow Helm values (production guidance)
+#
+# Use with a maintained MLflow Helm chart or adapt to your packaging.
+#
+replicaCount: 2
+service:
+  type: ClusterIP
+  port: 5000
+
+ingress:
+  enabled: true
+  annotations:
+    kubernetes.io/ingress.class: "nginx"
+  hosts:
+    - host: mlflow.example.com
+      paths: ["/"]
+
+env:
+  # Use a managed Postgres/Cloud SQL endpoint for production.
+  SQLALCHEMY_DATABASE_URI: "postgresql+psycopg2://mlflow_user:MLFLOW_DB_PASSWORD@postgres-mlflow.aegis.svc.cluster.local:5432/mlflow"
+  DEFAULT_ARTIFACT_ROOT: "s3://${MLFLOW_ARTIFACT_BUCKET}/mlflow-artifacts"
+
+resources:
+  requests:
+    cpu: "500m"
+    memory: "1Gi"
+  limits:
+    cpu: "1"
+    memory: "2Gi"
+
+readinessProbe:
+  httpGet:
+    path: /api/2.0/preview/mlflow/experiments/list
+    port: 5000
+  initialDelaySeconds: 10
+  periodSeconds: 10
+
+livenessProbe:
+  httpGet:
+    path: /health
+    port: 5000
+  initialDelaySeconds: 30
+  periodSeconds: 30
+
+securityContext:
+  runAsUser: 1000
+  runAsGroup: 1000
+  fsGroup: 1000
+
*** End File: mlops/helm/mlflow/values-production.yaml
*** Begin Patch
*** Add File: mlops/k8s/postgres/postgres-statefulset.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-ml
+
+---
+apiVersion: v1
+kind: Secret
+metadata:
+  name: postgres-credentials
+  namespace: aegis-ml
+type: Opaque
+stringData:
+  POSTGRES_DB: mlflow
+  POSTGRES_USER: mlflow_user
+  POSTGRES_PASSWORD: "{{ REPLACE_WITH_STRONG_PASSWORD }}"
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: postgres-mlflow
+  namespace: aegis-ml
+spec:
+  serviceName: postgres-mlflow
+  replicas: 1
+  selector:
+    matchLabels:
+      app: postgres-mlflow
+  template:
+    metadata:
+      labels:
+        app: postgres-mlflow
+    spec:
+      containers:
+      - name: postgres
+        image: postgres:14
+        env:
+        - name: POSTGRES_DB
+          valueFrom:
+            secretKeyRef:
+              name: postgres-credentials
+              key: POSTGRES_DB
+        - name: POSTGRES_USER
+          valueFrom:
+            secretKeyRef:
+              name: postgres-credentials
+              key: POSTGRES_USER
+        - name: POSTGRES_PASSWORD
+          valueFrom:
+            secretKeyRef:
+              name: postgres-credentials
+              key: POSTGRES_PASSWORD
+        ports:
+        - containerPort: 5432
+        volumeMounts:
+        - name: pgdata
+          mountPath: /var/lib/postgresql/data
+        resources:
+          requests:
+            cpu: "500m"
+            memory: "1Gi"
+          limits:
+            cpu: "1"
+            memory: "2Gi"
+  volumeClaimTemplates:
+  - metadata:
+      name: pgdata
+    spec:
+      accessModes: [ "ReadWriteOnce" ]
+      resources:
+        requests:
+          storage: 50Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: postgres-mlflow
+  namespace: aegis-ml
+spec:
+  type: ClusterIP
+  selector:
+    app: postgres-mlflow
+  ports:
+  - port: 5432
+    targetPort: 5432
+
*** End File: mlops/k8s/postgres/postgres-statefulset.yaml
*** Begin Patch
*** Add File: mlops/k8s/postgres/pg_backup_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: mlflow-postgres-backup
+  namespace: aegis-ml
+spec:
+  schedule: "0 2 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-backup-sa
+          containers:
+          - name: pgdump
+            image: amazon/aws-cli:2.13.2
+            env:
+            - name: PGHOST
+              value: "postgres-mlflow.aegis-ml.svc.cluster.local"
+            - name: PGUSER
+              valueFrom:
+                secretKeyRef:
+                  name: postgres-credentials
+                  key: POSTGRES_USER
+            - name: PGPASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: postgres-credentials
+                  key: POSTGRES_PASSWORD
+            - name: BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-cloud-creds
+                  key: MLFLOW_ARTIFACT_BUCKET
+            - name: AWS_REGION
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-cloud-creds
+                  key: AWS_REGION
+            command:
+            - sh
+            - -c
+            - |
+              set -e
+              FILENAME=mlflow-db-$(date -u +%Y%m%dT%H%M%SZ).sql.gz
+              pg_dump -h $PGHOST -U $PGUSER -d mlflow | gzip > /tmp/$FILENAME
+              aws s3 cp /tmp/$FILENAME s3://$BUCKET/backups/mlflow-db/$FILENAME --sse aws:kms || (echo "upload failed" && exit 1)
+          restartPolicy: OnFailure
+
*** End File: mlops/k8s/postgres/pg_backup_cronjob.yaml
*** Begin Patch
*** Add File: mlops/lakefs/deployment-notes.md
+LakeFS / Dataset versioning notes
+--------------------------------
+- Production recommendation: deploy LakeFS (Helm chart) backed by your CMK-protected object store.
+- Key config:
+  - S3 gateway endpoint (MLFLOW_ARTIFACT_BUCKET) as LakeFS storage namespace
+  - Database for LakeFS metadata (Postgres)
+- Quick steps:
+  1) helm repo add treeverse https://charts.treeverse.io
+  2) helm install lakefs treeverse/lakefs -f values.yaml
+  3) Configure LakeFS branches and mount points; update Argo pipeline to call `lakefs commit` or use `dvc` with LakeFS remote.
+
+If you cannot install LakeFS immediately, use DVC with your S3 bucket and ensure each Argo run records the DVC commit id as provenance.
+
*** End File: mlops/lakefs/deployment-notes.md
*** Begin Patch
*** Add File: mlops/argo/train_pipeline_prod.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-ml-train-prod-
+spec:
+  entrypoint: train-validate-register
+  serviceAccountName: aegis-ml-operator
+  templates:
+  - name: train-validate-register
+    steps:
+    - - name: snapshot-data
+        template: snapshot-data
+    - - name: train
+        template: train
+    - - name: validate
+        template: validate
+    - - name: register
+        template: register
+
+  - name: snapshot-data
+    retryStrategy:
+      limit: 3
+      backoff:
+        duration: "10s"
+        factor: 2
+        maxDuration: "5m"
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args: ["python3 mlops/dvc/snapshot_data.py --out /tmp/dataset_snapshot && echo SNAPSHOT_ID=$(cat /tmp/dataset_snapshot/manifest.txt) > /tmp/snapshot_meta"]
+      resources:
+        requests:
+          cpu: "500m"
+          memory: "1Gi"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: train
+    retryStrategy:
+      limit: 2
+      backoff:
+        duration: "30s"
+        factor: 2
+        maxDuration: "10m"
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args: ["python3 mlops/scripts/train_and_register.py --mode train-only | tee /tmp/run_id && export RUN_ID=$(tail -n1 /tmp/run_id)"]
+      resources:
+        requests:
+          cpu: "4"
+          memory: "16Gi"
+          nvidia.com/gpu: 1
+        limits:
+          cpu: "8"
+          memory: "32Gi"
+      nodeSelector:
+        aegis/gpu: "true"
+      tolerations:
+      - key: "nvidia.com/gpu"
+        operator: "Exists"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: validate
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args: ["python3 mlops/scripts/validate_model.py --run-id ${RUN_ID} --min-accuracy 0.80"]
+      resources:
+        requests:
+          cpu: "500m"
+          memory: "1Gi"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  - name: register
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args: ["python3 mlops/scripts/train_and_register.py --mode register --run-id ${RUN_ID}"]
+      resources:
+        requests:
+          cpu: "200m"
+          memory: "512Mi"
+      volumeMounts:
+      - name: work
+        mountPath: /tmp
+
+  volumes:
+  - name: work
+    emptyDir: {}
+
+  podMetadata:
+    annotations:
+      sidecar.istio.io/inject: "false"
+
*** End File: mlops/argo/train_pipeline_prod.yaml
*** Begin Patch
*** Add File: mlops/k8s/resource-quota-and-limits.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: tenant-{{TENANT}}
+
+---
+apiVersion: v1
+kind: ResourceQuota
+metadata:
+  name: compute-quota
+  namespace: tenant-{{TENANT}}
+spec:
+  hard:
+    requests.cpu: "40"
+    requests.memory: 200Gi
+    limits.cpu: "80"
+    limits.memory: 400Gi
+    persistentvolumeclaims: "10"
+
+---
+apiVersion: v1
+kind: LimitRange
+metadata:
+  name: default-limits
+  namespace: tenant-{{TENANT}}
+spec:
+  limits:
+  - type: Container
+    default:
+      cpu: "1"
+      memory: "2Gi"
+    defaultRequest:
+      cpu: "500m"
+      memory: "1Gi"
+
*** End File: mlops/k8s/resource-quota-and-limits.yaml
*** Begin Patch
*** Add File: mlops/k8s/namespace-rbac-tenant.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: tenant-{{TENANT}}
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  namespace: tenant-{{TENANT}}
+  name: tenant-operator
+rules:
+- apiGroups: [""]
+  resources: ["pods","services","configmaps","secrets","persistentvolumeclaims"]
+  verbs: ["get","list","create","update","patch","delete"]
+- apiGroups: ["batch"]
+  resources: ["jobs","cronjobs"]
+  verbs: ["get","list","create","delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: tenant-operator-binding
+  namespace: tenant-{{TENANT}}
+subjects:
+- kind: User
+  name: "{{TENANT_ADMIN_USER}}"
+  apiGroup: rbac.authorization.k8s.io
+roleRef:
+  kind: Role
+  name: tenant-operator
+  apiGroup: rbac.authorization.k8s.io
+
*** End File: mlops/k8s/namespace-rbac-tenant.yaml
*** Begin Patch
*** Add File: .github/workflows/model_ci.yml
+name: Model CI — PR Validation & SBOM
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  build-and-scan:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Build trainer image
+        run: |
+          IMAGE=ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:pr-${{ github.event.number }}
+          docker build -t $IMAGE -f mlops/docker/Dockerfile .
+          echo "IMAGE=$IMAGE" >> $GITHUB_OUTPUT
+
+      - name: Generate SBOM & scan (Syft + Trivy)
+        run: |
+          if ! command -v syft >/dev/null 2>&1; then
+            curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+          fi
+          syft $IMAGE -o json > sbom.json
+          curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+          trivy image --exit-code 1 --severity HIGH,CRITICAL $IMAGE || (echo "Vulnerabilities found" && exit 1)
+        env:
+          IMAGE: ${{ steps.build-and-scan.outputs.IMAGE }}
+
+  run-validation:
+    needs: build-and-scan
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run unit & model-tests
+        run: |
+          pip install -r mlops/requirements.txt
+          pytest tests/unit || (echo "Unit tests failed" && exit 1)
+          # Optionally run a small train & validate locally (fast smoke)
+          python3 mlops/scripts/train_and_register.py --mode train-only || (echo "Smoke training failed" && exit 1)
+
*** End File: .github/workflows/model_ci.yml
*** Begin Patch
*** Add File: .github/workflows/promotion_canary.yml
+name: Model Promotion — Canary Deployment
+
+on:
+  workflow_dispatch:
+    inputs:
+      model_uri:
+        description: "MLflow model URI (models:/name/version)"
+        required: true
+      canary_percent:
+        description: "Initial traffic percent for canary"
+        required: false
+        default: "10"
+
+permissions:
+  contents: read
+  id-token: write
+
+jobs:
+  promote:
+    runs-on: ubuntu-latest
+    environment: production
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Authenticate to cluster (OIDC or KUBECONFIG secret)
+        run: |
+          echo "Ensure KUBECONFIG is available via repo secret or OIDC-based kubectl executor."
+
+      - name: Render KServe InferenceService
+        run: |
+          MODEL_URI="${{ github.event.inputs.model_uri }}"
+          CANARY="${{ github.event.inputs.canary_percent }}"
+          sed "s|{{MODEL_URI}}|${MODEL_URI}|g; s|{{CANARY_PERCENT}}|${CANARY}|g" mlops/k8s/kserve_canary_template.yaml > /tmp/kserve-canary.yaml
+          kubectl apply -f /tmp/kserve-canary.yaml -n aegis-ml
+
+      - name: Apply Istio VirtualService for traffic split (example)
+        run: |
+          # Assumes Istio and ingress configured for KServe predictor service
+          sed "s|{{CANARY_PERCENT}}|${CANARY}|g" mlops/k8s/istio_virtualservice_canary.yaml | kubectl apply -f - -n aegis-ml
+
+      - name: Notify
+        run: |
+          echo "Canary deployed for ${MODEL_URI} with ${CANARY}% traffic. Monitor metrics and rollback if needed."
+
*** End File: .github/workflows/promotion_canary.yml
*** Begin Patch
*** Add File: mlops/k8s/kserve_canary_template.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-ml-canary
+  namespace: aegis-ml
+spec:
+  predictor:
+    serviceAccountName: aegis-model-sa
+    model:
+      modelFormat:
+        name: "mlflow"
+      storageUri: "{{MODEL_URI}}"
+    minReplicas: 1
+    maxReplicas: 5
+    resources:
+      limits:
+        cpu: "2"
+        memory: "4Gi"
+  autoscaler:
+    minReplicas: 1
+    maxReplicas: 5
+
+# Note: KServe autoscaling can use HPA/KEDA. Configure horizontal autoscaler in your cluster.
+
*** End File: mlops/k8s/kserve_canary_template.yaml
*** Begin Patch
*** Add File: mlops/k8s/istio_virtualservice_canary.yaml
+apiVersion: networking.istio.io/v1alpha3
+kind: VirtualService
+metadata:
+  name: aegis-ml-canary-vs
+spec:
+  hosts:
+  - "aegis-ml-canary.aegis-ml.svc.cluster.local"
+  http:
+  - route:
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: stable
+      weight: "{{100 - CANARY_PERCENT}}"
+    - destination:
+        host: "aegis-ml-canary-predictor-default.aegis-ml.svc.cluster.local"
+        subset: canary
+      weight: "{{CANARY_PERCENT}}"
+
+# You must define DestinationRule subsets 'stable' and 'canary' for the predictor service.
+
*** End File: mlops/k8s/istio_virtualservice_canary.yaml
*** Begin Patch
*** Add File: monitoring/prometheus/rules/mlflow_and_model_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-model-alerts
+  namespace: monitoring
+spec:
+  groups:
+  - name: model.rules
+    rules:
+    - alert: ModelDriftDetected
+      expr: |
+        # Example: compare recent feature mean to baseline metric (instrumentation required)
+        abs(ml_feature_mean_diff) > 0.1
+      for: 10m
+      labels:
+        severity: warning
+      annotations:
+        summary: "Model feature drift detected ({{ $labels.feature }})"
+        description: "Feature {{ $labels.feature }} mean has drifted more than 10% over baseline."
+
+    - alert: MLflowDBDown
+      expr: up{job="postgres-mlflow"} == 0
+      for: 5m
+      labels:
+        severity: critical
+      annotations:
+        summary: "MLflow DB appears down"
+        description: "The Postgres instance backing MLflow is unavailable. Check postgres-mlflow pod and storage."
+
*** End File: monitoring/prometheus/rules/mlflow_and_model_alerts.yaml
*** Begin Patch
*** Add File: monitoring/grafana/dashboards/aegis_model_dashboard.json
+{
+  "dashboard": {
+    "id": null,
+    "title": "Aegis Model Performance",
+    "panels": [
+      {
+        "type": "graph",
+        "title": "Model Accuracy (recent runs)",
+        "targets": [
+          {
+            "expr": "mlflow_run_metric_accuracy",
+            "legendFormat": "{{ run_id }}",
+            "refId": "A"
+          }
+        ],
+        "gridPos": { "h": 8, "w": 24, "x": 0, "y": 0 }
+      },
+      {
+        "type": "graph",
+        "title": "Inference Latency (P50/P95)",
+        "targets": [
+          { "expr": "histogram_quantile(0.5, rate(model_inference_latency_bucket[5m]))", "refId": "A" },
+          { "expr": "histogram_quantile(0.95, rate(model_inference_latency_bucket[5m]))", "refId": "B" }
+        ],
+        "gridPos": { "h": 8, "w": 24, "x": 0, "y": 8 }
+      }
+    ],
+    "schemaVersion": 16,
+    "version": 0
+  }
+}
+
*** End File: monitoring/grafana/dashboards/aegis_model_dashboard.json
*** Begin Patch
*** Add File: scripts/backup_mlflow_artifacts.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Backup MLflow artifacts (copy artifact prefix to a backup bucket)
+#
+BUCKET="${MLFLOW_ARTIFACT_BUCKET:-}"
+BACKUP_BUCKET="${MLFLOW_BACKUP_BUCKET:-}"
+PREFIX="${1:-mlflow-artifacts}"
+REGION="${AWS_REGION:-us-west-2}"
+
+if [ -z "$BUCKET" ] || [ -z "$BACKUP_BUCKET" ]; then
+  echo "Set MLFLOW_ARTIFACT_BUCKET and MLFLOW_BACKUP_BUCKET env vars"
+  exit 2
+fi
+
+echo "Starting artifact copy s3://$BUCKET/$PREFIX -> s3://$BACKUP_BUCKET/$PREFIX-$(date -u +%Y%m%dT%H%M%SZ)"
+aws s3 sync "s3://$BUCKET/$PREFIX" "s3://$BACKUP_BUCKET/$PREFIX-$(date -u +%Y%m%dT%H%M%SZ)" --region "$REGION" --sse aws:kms
+echo "Artifact backup complete"
+
*** End File: scripts/backup_mlflow_artifacts.sh
*** Begin Patch
*** Add File: scripts/restore_mlflow_metadata.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Restore MLflow Postgres metadata from a gzipped pg_dump file
+#
+FILE="$1"
+if [ -z "$FILE" ]; then
+  echo "Usage: $0 <s3://bucket/path/file.sql.gz> (ensure PG env vars set)"
+  exit 2
+fi
+
+TMP="/tmp/mlflow-restore.sql.gz"
+echo "Downloading $FILE to $TMP"
+aws s3 cp "$FILE" "$TMP"
+gunzip -c "$TMP" | psql "$PGDATABASE" -h "$PGHOST" -U "$PGUSER"
+echo "Restore complete"
+
*** End File: scripts/restore_mlflow_metadata.sh
*** Begin Patch
*** Add File: scripts/generate_sbom_and_scan.sh
+#!/usr/bin/env bash
+set -euo pipefail
+IMAGE="$1"
+if [ -z "$IMAGE" ]; then
+  echo "Usage: $0 <image>"
+  exit 2
+fi
+
+if ! command -v syft >/dev/null 2>&1; then
+  curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
+fi
+syft "$IMAGE" -o json > sbom.json
+
+if ! command -v trivy >/dev/null 2>&1; then
+  curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
+fi
+trivy image --format json --output trivy-report.json --severity HIGH,CRITICAL "$IMAGE" || true
+
+echo "SBOM: sbom.json"
+echo "Trivy report: trivy-report.json"
+jq '.Results[] | select(.Vulnerabilities != null and .Vulnerabilities | length > 0)' trivy-report.json || true
+
*** End File: scripts/generate_sbom_and_scan.sh
*** Begin Patch
*** Add File: scripts/provenance_tagging.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Attaches provenance metadata to an MLflow run: git sha, dvc commit, image tag, dataset snapshot id
+#
+RUN_ID="$1"
+GIT_SHA="${GIT_SHA:-$(git rev-parse --short HEAD 2>/dev/null || echo '')}"
+DVC_COMMIT="${DVC_COMMIT:-$(git rev-parse --short HEAD 2>/dev/null || echo '')}"
+IMAGE_TAG="${IMAGE_TAG:-}"
+SNAPSHOT_ID="${SNAPSHOT_ID:-}"
+
+if [ -z "$RUN_ID" ]; then
+  echo "Usage: $0 <mlflow-run-id>"
+  exit 2
+fi
+
+python3 - <<PY
+import mlflow, os
+client = mlflow.tracking.MlflowClient()
+run_id = "${RUN_ID}"
+tags = {
+  "git.sha": "${GIT_SHA}",
+  "dvc.commit": "${DVC_COMMIT}",
+  "image.tag": "${IMAGE_TAG}",
+  "dataset.snapshot": "${SNAPSHOT_ID}"
+}
+for k,v in tags.items():
+    if v:
+        client.set_tag(run_id, k, v)
+print("Provenance tags applied to", run_id)
+PY
+
*** End File: scripts/provenance_tagging.sh
*** Begin Patch
*** Add File: docs/production_hardening.md
+# Production Hardening & Checklist for Aegis MLOps
+
+This document summarizes required actions and the new artifacts added by this patch.
+
+1) MLflow
+- Deploy MLflow via Helm and point SQLALCHEMY_DATABASE_URI to a managed Postgres (RDS/Cloud SQL) — do NOT use sqlite.
+- Ensure Postgres has backups enabled and that the pg_dump CronJob is adapted to your cloud provider (this patch includes a CronJob for S3 upload).
+- Configure ingress with TLS and front MLflow with an authentication proxy (e.g., oauth2-proxy or API gateway) to enforce RBAC and SSO.
+
+2) Dataset versioning
+- Deploy LakeFS or use DVC with your S3 bucket. Ensure Argo pipeline calls `lakefs commit` or `dvc push` and records the commit id into MLflow run tags (scripts/provenance_tagging.sh).
+
+3) Training orchestration
+- Use the Argo workflow with GPU nodeSelector and tolerations. Create a GPU nodepool with label `aegis/gpu=true`.
+- Configure ResourceQuota and LimitRange per tenant namespace to control cost and enforce quotas.
+
+4) Model CI & SBOM
+- Model CI (`.github/workflows/model_ci.yml`) runs SBOM generation (syft) and vulnerability scan (Trivy) on PRs. High/critical findings must block PR merge.
+
+5) Promotion & canary
+- Promotion workflow uses KServe and Istio VirtualService for traffic split. Implement DestinationRule subsets and monitor performance during canary.
+
+6) Monitoring & drift detection
+- The PrometheusRule alerts for drift (example). You must instrument inference code to publish feature stats (Prometheus metrics) and compute baseline metrics.
+- Import Grafana dashboard JSON into your Grafana instance.
+
+7) RBAC & multi-tenant isolation
+- Use per-tenant namespaces, Role/RoleBinding, ResourceQuota and LimitRange to isolate tenants and control cost.
+- Front MLflow with an auth proxy and map users to tenant roles (use SSO/Keycloak or cloud IAM with Dex).
+
+8) Backups & DR
+- Postgres CronJob backs up metadata to a KMS-encrypted S3 backup bucket. Also run periodic artifact backups (scripts/backup_mlflow_artifacts.sh).
+- Test restores monthly and include steps in your DR runbook.
+
+9) Reproducibility & provenance
+- Set MLflow run tags for git SHA, DVC/LakeFS commit, trainer image tag and dataset snapshot id (scripts/provenance_tagging.sh).
+- Ensure CI writes these values into the training Argo workflow environment so pipelines record provenance.
+
+10) Next actions
+- Replace placeholder secrets and configure Vault OIDC trust for GitHub Actions and Argo workloads.
+- Wire ingress auth and ensure MLflow only accessible via authenticated routes.
+- Implement automated Argo wait & artifact download in CI and add an automated promotion gate that validates metrics before applying canary.
+
*** End File: docs/production_hardening.md
*** End Patch
