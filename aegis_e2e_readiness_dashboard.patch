*** Begin Patch
*** Add File:argo/full_staging_e2e_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: full-staging-e2e-
+  namespace: aegis
+spec:
+  entrypoint: full-e2e
+  templates:
+    - name: full-e2e
+      steps:
+        - - name: run-world-model-pipeline
+            template: run-argo-workflow
+            arguments:
+              parameters:
+                - name: workflow
+                  value: "argo/world_model_pipeline.yaml"
+        - - name: feast-slo-validate
+            template: run-argo-workflow
+            arguments:
+              parameters:
+                - name: workflow
+                  value: "argo/feast_slo_validation_workflow.yaml"
+        - - name: deepspeed-validate
+            template: run-deepspeed-mpi
+        - - name: inference-load-test
+            template: run-argo-workflow
+            arguments:
+              parameters:
+                - name: workflow
+                  value: "argo/inference_load_test.yaml"
+        - - name: hil-replay
+            template: run-argo-workflow
+            arguments:
+              parameters:
+                - name: workflow
+                  value: "hil/hil_replay_workflow.yaml"
+        - - name: erasure-dryrun
+            template: run-argo-workflow
+            arguments:
+              parameters:
+                - name: workflow
+                  value: "argo/erasure_discovery_and_erase.yaml"
+        - - name: assemble-evidence
+            template: assemble-evidence
+
+    - name: run-argo-workflow
+      inputs:
+        parameters:
+          - name: workflow
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            WFILE="{{inputs.parameters.workflow}}"
+            echo "Attempting to submit workflow: $WFILE"
+            if command -v argo >/dev/null 2>&1; then
+              argo submit "$WFILE" -n aegis --watch || true
+            else
+              echo "argo CLI not found in container. Attempting kubectl apply -f $WFILE"
+              if command -v kubectl >/dev/null 2>&1; then
+                kubectl -n aegis apply -f "$WFILE" || true
+                echo "Applied $WFILE (workflow CR created). Monitor via Argo UI or kubectl get wf -n aegis"
+              else
+                echo "Neither argo nor kubectl found. Please run: argo submit $WFILE -n aegis --watch from a host with KUBECONFIG"
+                exit 0
+              fi
+            fi
+
+    - name: run-deepspeed-mpi
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            echo "Launching deepspeed MPIJob (requires MPI Operator and GPU nodes)"
+            kubectl -n aegis apply -f deepspeed/mpi/deepspeed_mpi_job.yaml || true
+            echo "MPIJob applied. Monitor with: kubectl get mpijob -n aegis && kubectl get pods -n aegis -l job-name=deepspeed-multi-node-test"
+
+    - name: assemble-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            set -euo pipefail
+            echo "Assembling evidence bundle..."
+            pip install boto3 || true
+            python3 compliance/assemble_audit_bundle.py || true
+            # Try enhanced assembler if available
+            python3 evidence/enhanced_assemble_audit_bundle.py || true
+            echo "Evidence assembly attempted. Check /tmp and EVIDENCE_BUCKET (if configured)."
+
*** End Patch
*** Begin Patch
*** Add File:scripts/readiness/execute_readiness_checklist.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Execute the readiness checklist for production readiness:
+# - Feast/Redis SLO runs
+# - DeepSpeed multi-node validation
+# - HSM rotation dry-run & validation
+# - HIL mapping smoke test
+# - Assemble evidence and upload
+#
+NAMESPACE="${NAMESPACE:-aegis}"
+MODEL_BUCKET="${MODEL_ARTIFACT_BUCKET:-}"
+EVIDENCE_BUCKET="${EVIDENCE_BUCKET:-}"
+OUTDIR="${OUTDIR:-/tmp/aegis_readiness}"
+mkdir -p "$OUTDIR"
+
+report="$OUTDIR/readiness_report.json"
+echo "{}" > "$report"
+
+function update_report() {
+  local k=$1; local v=$2
+  python3 - <<PY
+import json,sys
+p='$report'
+data=json.load(open(p))
+data['$k'] = $v
+json.dump(data, open(p,'w'), indent=2)
+print("Updated report: $k")
+PY
+}
+
+echo "1) Run Feast SLO validation via Argo (if argo CLI available)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/feast_slo_validation_workflow.yaml -n "$NAMESPACE" --watch || update_report "feast_slo" '"failed"'
+  update_report "feast_slo" '"submitted"'
+else
+  echo "argo CLI not found. Please run argo submit argo/feast_slo_validation_workflow.yaml -n $NAMESPACE --watch manually"
+  update_report "feast_slo" '"manual"'
+fi
+
+echo "2) Launch Redis/Feast bench (Argo benchmark)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/benchmarks/feast_redis_benchmark.yaml -n "$NAMESPACE" --watch || true
+  update_report "feast_redis_bench" '"launched"'
+else
+  echo "Please run argo/benchmarks/feast_redis_benchmark.yaml manually"
+  update_report "feast_redis_bench" '"manual"'
+fi
+
+echo "3) HSM vendor rotation dry-run and validation"
+bash ops/hsm/vendor_rotation_runner.sh --dry-run > "$OUTDIR/hsm_rotation_dryrun.log" 2>&1 || true
+python3 ops/hsm/vendor_validation_suite.py --runs 10 > "$OUTDIR/hsm_validation.json" 2>&1 || true
+update_report "hsm" '{"dryrun_log": "hsm_rotation_dryrun.log", "validation": "hsm_validation.json"}'
+
+echo "4) Launch DeepSpeed multi-node validation (MPIJob)"
+kubectl -n "$NAMESPACE" apply -f deepspeed/mpi/deepspeed_mpi_job.yaml || true
+update_report "deepspeed_mpi" '"applied"'
+
+echo "5) HIL mapping smoke test (local safe trace)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit hil/hil_replay_workflow.yaml -n "$NAMESPACE" --watch || true
+  update_report "hil_replay" '"submitted"'
+else
+  echo "Please run hil/hil_replay_workflow.yaml manually"
+  update_report "hil_replay" '"manual"'
+fi
+
+echo "6) Run inference load test"
+if command -v argo >/dev/null 2>&1; then
+  argo submit argo/inference_load_test.yaml -n "$NAMESPACE" --watch || true
+  update_report "inference_load_test" '"submitted"'
+else
+  echo "Please run argo/inference_load_test.yaml manually"
+  update_report "inference_load_test" '"manual"'
+fi
+
+echo "7) Assemble evidence bundle"
+python3 compliance/assemble_audit_bundle.py || true
+python3 evidence/enhanced_assemble_audit_bundle.py || true
+if [ -n "$EVIDENCE_BUCKET" ]; then
+  echo "Uploading readiness artifacts to s3://$EVIDENCE_BUCKET/readiness/"
+  aws s3 cp "$OUTDIR" "s3://$EVIDENCE_BUCKET/readiness/" --recursive || true
+  update_report "evidence_upload" '"attempted"'
+else
+  update_report "evidence_upload" '"skipped_no_bucket"'
+fi
+
+echo "Readiness execution complete. Report: $report"
+cat "$report"
+
*** End Patch
*** Begin Patch
*** Add File:grafana/dashboards/pipeline_status_dashboard.json
+{
+  "title": "Aegis Pipeline Status & Acceptance",
+  "panels": [
+    {
+      "type": "row",
+      "title": "Argo Workflow Status"
+    },
+    {
+      "type": "stat",
+      "title": "Active Workflows",
+      "targets": [
+        {
+          "expr": "count(argo_workflow_info)",
+          "legendFormat": "active_workflows"
+        }
+      ],
+      "gridPos": {"x":0,"y":1,"w":6,"h":3}
+    },
+    {
+      "type": "table",
+      "title": "Key Pipeline Phases",
+      "targets": [
+        {
+          "expr": "argo_workflow_status_phase",
+          "legendFormat": "{{workflow}}"
+        }
+      ],
+      "gridPos": {"x":6,"y":1,"w":18,"h":6}
+    },
+    {
+      "type": "row",
+      "title": "SLOs & Benchmarks"
+    },
+    {
+      "type": "graph",
+      "title": "Feast online lookup P95",
+      "targets": [
+        {
+          "expr": "feast_online_lookup_p95",
+          "legendFormat": "feast_p95"
+        }
+      ],
+      "gridPos": {"x":0,"y":7,"w":12,"h":6}
+    },
+    {
+      "type": "graph",
+      "title": "Control loop latency p95 (HIL)",
+      "targets": [
+        {
+          "expr": "histogram_quantile(0.95, sum(rate(control_loop_duration_seconds_bucket[5m])) by (le))",
+          "legendFormat": "control_p95"
+        }
+      ],
+      "gridPos": {"x":12,"y":7,"w":12,"h":6}
+    },
+    {
+      "type": "row",
+      "title": "Acceptance Criteria Summary"
+    },
+    {
+      "type": "text",
+      "title": "Acceptance Criteria",
+      "options": {
+        "content": "- Feast online lookup P95 < 50ms\n- DeepSpeed multi-node throughput within expected bounds\n- HSM signing success >= 99%\n- HIL safety violations == 0 in N tests\n- Evidence bundle uploaded to EVIDENCE_BUCKET"
+      },
+      "gridPos": {"x":0,"y":13,"w":24,"h":4}
+    }
+  ],
+  "schemaVersion": 16,
+  "version": 1
+}
+
*** End Patch
*** Begin Patch
*** Add File:docs/pipeline_status_dashboard.md
+# Pipeline Status Dashboard (Aegis)
+
+This dashboard shows the high-level status of Aegis pipelines, SLOs and acceptance criteria used by SRE and Legal.
+
+Metrics & Panels
+- Active Workflows: count of Argo workflows (uses argo_workflow_info).  
+- Key Pipeline Phases: table showing argo_workflow_status_phase grouped by workflow names.  
+- Feast online lookup P95: time series of feast_online_lookup_p95 (target: < 50 ms).  
+- Control loop latency p95: HIL latency via histogram_quantile for control_loop_duration_seconds_bucket (target: less than control period).  
+- Acceptance Criteria: static checklist summarizing required pass/fail items.
+
+How to use
+1. Import grafana/dashboards/pipeline_status_dashboard.json into Grafana (Dashboards â†’ Import).  
+2. Ensure Prometheus is scraping Argo metrics and Feast/Feast-Redis exporter; configure Prometheus adapter if using custom metrics.  
+3. Wire MLflow / Pushgateway as needed; the dashboard augments rather than replaces those systems.
+
+Per-pipeline status flags (suggested)
+- world_model_pipeline: phase (Running/Failed/Succeeded), last_run_timestamp, evidence_bundle_present (bool).  
+- feast_slo_validation: p95_ms (numeric), last_run_status (pass/fail).  
+- deepspeed_mpi: last_run_throughput, checkpoint_upload (s3 path).  
+- hil_replay: last_report (s3 path), safety_violations (count).
+
+Acceptance criteria mapping
+- Map these fields into Grafana using Prometheus exporters or pushgateway metrics during workflow runs so the dashboard can be used for automated gating and handoff to legal.
+
*** End Patch
*** End Patch
