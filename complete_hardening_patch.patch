*** Begin Patch
*** Add File: .github/workflows/oidc_conversion_checks.yml
+name: OIDC Conversion Checks
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+permissions:
+  contents: read
+
+jobs:
+  check-workflows:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Run OIDC / static-secret workflow scan
+        run: |
+          chmod +x scripts/find_oidc_issues.sh
+          ./scripts/find_oidc_issues.sh
+
+      - name: Fail if non-OIDC workflows detected
+        if: ${{ steps.find_oidc_issues.outcome == 'failure' || always() }}
+        run: |
+          echo "If the previous step detected non-OIDC workflows or inline secrets, please convert them to OIDC/federated auth and avoid static keys."
+          # The find_oidc_issues.sh script will exit non-zero when it finds issues.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/find_oidc_issues.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# find_oidc_issues.sh
+# Scans .github/workflows for usage of static cloud secrets or missing OIDC usage patterns.
+# Exits with non-zero if issues are found.
+#
+WORKFLOW_DIR=".github/workflows"
+ISSUES=0
+
+echo "Scanning GitHub Actions workflows for OIDC usage and static secret patterns..."
+for f in ${WORKFLOW_DIR}/*.yml ${WORKFLOW_DIR}/*.yaml; do
+  [ -f "$f" ] || continue
+  echo "Checking $f"
+  # Detect explicit environment variables that look like static keys
+  if grep -E "AWS_ACCESS_KEY_ID|AWS_SECRET_ACCESS_KEY|OCI_PRIVATE_KEY|OCI_FINGERPRINT|AZURE_CLIENT_SECRET|GCP_SERVICE_ACCOUNT" "$f" >/dev/null 2>&1; then
+    echo "ERROR: $f references classic secret identifiers or static secret names. Convert to OIDC/federated auth or Vault."
+    ISSUES=1
+  fi
+
+  # Detect AWS OIDC usage (role-to-assume)
+  if grep -q "role-to-assume" "$f" || grep -q "google-github-actions/auth" "$f" || grep -q "azure/login" "$f"; then
+    echo "OK: $f appears to use OIDC/federated patterns."
+  else
+    # Not necessarily an error (workflow may be unrelated) — warn if workflow runs on push to main or uses secrets extensively
+    if grep -q "on: push" "$f" || grep -q "on: workflow_dispatch" "$f" || grep -q "secrets\." "$f"; then
+      echo "WARN: $f may not be using OIDC/federated auth. Review and convert to OIDC if it performs cloud operations."
+      ISSUES=1
+    fi
+  fi
+done
+
+if [ "$ISSUES" -ne 0 ]; then
+  echo "OIDC conversion scan found issues. Please address the reported workflows."
+  exit 2
+fi
+
+echo "No obvious OIDC/workflow issues detected."
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/vault_short_lived_example.yml
+name: Vault Short-lived Token Example
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: read
+
+jobs:
+  vault-login:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Login to Vault (GitHub OIDC + Vault AppRole or OIDC method)
+        uses: hashicorp/vault-action@v2
+        with:
+          url: ${{ secrets.VAULT_ADDR }}
+          method: github_oidc
+          role: ${{ secrets.VAULT_GITHUB_ROLE }}
+        env:
+          VAULT_TOKEN: ${{ steps.vault_login.outputs.client_token || '' }}
+
+      - name: Fetch short-lived credentials/secret
+        run: |
+          # Example: read a secret and export as env var (avoid echoing secret values)
+          SECRET_JSON=$(vault kv get -format=json secret/aegis/backup || true)
+          echo "Fetched secret metadata (not value): $(echo "$SECRET_JSON" | jq -r '.data.meta' 2>/dev/null || echo "none")"
+
+      - name: Use secret (example)
+        run: |
+          echo "Run job steps that use short-lived Vault-issued credentials here."
+
+# Notes:
+# - This workflow demonstrates using HashiCorp Vault with GitHub OIDC or AppRole to fetch short-lived secrets.
+# - Replace with your Vault auth method and secret paths. Do not store long-lived cloud keys in repo secrets.
+
*** End Patch
*** Begin Patch
*** Add File: scripts/gh_remove_repo_secret.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# gh_remove_repo_secret.sh
+# Removes GitHub repository secrets matching a pattern via gh CLI (interactive / best-effort).
+#
+# Usage:
+#   GH_TOKEN must be set (or gh auth login performed). Example:
+#   ./scripts/gh_remove_repo_secret.sh --pattern 'AWS_SECRET_ACCESS_KEY' --confirm
+#
+PATTERN=""
+CONFIRM="no"
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --pattern) PATTERN="$2"; shift 2;;
+    --confirm) CONFIRM="yes"; shift;;
+    -h|--help) sed -n '1,200p' "$0"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [ -z "$PATTERN" ]; then
+  echo "Usage: $0 --pattern '<regex or literal>' [--confirm]"
+  exit 2
+fi
+
+if ! command -v gh >/dev/null 2>&1; then
+  echo "gh CLI required: https://cli.github.com/"
+  exit 1
+fi
+
+echo "Listing repository secrets..."
+SECRETS=$(gh secret list --limit 1000 --visibility all -R "${GITHUB_REPOSITORY:-$(git config --get remote.origin.url | sed -n 's|.*github.com[:/]\([^/]*\/[^/]*\).git|\1|p')}") || true
+echo "$SECRETS" | awk '{print $1}' | grep -E "$PATTERN" || true
+
+if [ "$CONFIRM" != "yes" ]; then
+  echo "Run again with --confirm to delete the listed secrets."
+  exit 0
+fi
+
+echo "Deleting secrets matching pattern..."
+echo "$SECRETS" | awk '{print $1}' | grep -E "$PATTERN" | while read -r secret; do
+  echo "Deleting secret: $secret"
+  gh secret remove "$secret" || echo "Failed to remove $secret (check permissions)"
+done
+
+echo "Removal complete. Rotate any cloud credentials that were removed."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/crosscloud_rotation_orchestrator.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# crosscloud_rotation_orchestrator.sh
+# Orchestrates a KMS key rotation across clouds in staging:
+# 1) perform pre-rotation backup
+# 2) call cloud-specific rotation script
+# 3) kick a post-rotation backup and verify restore
+# 4) produce an artifacts bundle
+#
+# Usage:
+#   ./scripts/crosscloud_rotation_orchestrator.sh --cloud aws --staging-namespace aegis --backup-cronjob aegis-milvus-backup --out /tmp/artifacts
+#
+CLOUD=""
+STAGING_NS=""
+BACKUP_CRONJOB=""
+OUT_DIR="/tmp/aegis-rotation-artifacts-$(date -u +%Y%m%dT%H%M%SZ)"
+RUN_DRILL="yes"
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --cloud) CLOUD="$2"; shift 2;;
+    --staging-namespace) STAGING_NS="$2"; shift 2;;
+    --backup-cronjob) BACKUP_CRONJOB="$2"; shift 2;;
+    --out) OUT_DIR="$2"; shift 2;;
+    --run-drill) RUN_DRILL="$2"; shift 2;;
+    -h|--help) sed -n '1,240p' "$0"; exit 0;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [ -z "$CLOUD" ] || [ -z "$STAGING_NS" ] || [ -z "$BACKUP_CRONJOB" ]; then
+  echo "Missing required args. See --help."
+  exit 2
+fi
+
+mkdir -p "$OUT_DIR"
+
+echo "[orchestrator] Triggering pre-rotation backup job in namespace $STAGING_NS (from CronJob $BACKUP_CRONJOB)"
+kubectl -n "$STAGING_NS" create job --from=cronjob/"$BACKUP_CRONJOB" "pre-rotation-$(date -u +%s)" || true
+JOB_NAME=$(kubectl -n "$STAGING_NS" get jobs -o name | grep pre-rotation | tail -n1 | sed 's|job/||')
+kubectl -n "$STAGING_NS" wait --for=condition=complete job/"$JOB_NAME" --timeout=900s || (kubectl -n "$STAGING_NS" logs job/"$JOB_NAME" --all-containers > "$OUT_DIR/pre-rotation-job.log" || true; echo "Pre-rotation job failed"; exit 1)
+kubectl -n "$STAGING_NS" logs job/"$JOB_NAME" --all-containers > "$OUT_DIR/pre-rotation-job.log" || true
+
+case "$CLOUD" in
+  aws)
+    echo "[orchestrator] Running AWS rotation script"
+    chmod +x scripts/rotate_kms_aws.sh
+    ./scripts/rotate_kms_aws.sh --alias "alias/aegis-backup-$(date +%Y%m%d%H%M%S)" --role-arn "${AWS_IRSA_ROLE_ARN:-}" --run-drill no --bucket "${AWS_BACKUP_BUCKET:-}" || true
+    ;;
+  azure)
+    echo "[orchestrator] Azure rotation placeholder: ensure azure key vault rotation script exists and is callable"
+    # Implement azure rotate script: scripts/rotate_kms_azure.sh
+    ;;
+  gcp)
+    echo "[orchestrator] GCP rotation placeholder: implement scripts/rotate_kms_gcp.sh"
+    ;;
+  oci)
+    echo "[orchestrator] OCI rotation placeholder: implement scripts/oci_provision_vault_key.sh or rotate script"
+    ;;
+  *)
+    echo "Unsupported cloud: $CLOUD"
+    exit 2
+    ;;
+esac
+
+if [ "$RUN_DRILL" = "yes" ]; then
+  echo "[orchestrator] Triggering post-rotation backup and verification"
+  kubectl -n "$STAGING_NS" create job --from=cronjob/"$BACKUP_CRONJOB" "post-rotation-$(date -u +%s)" || true
+  POST_JOB=$(kubectl -n "$STAGING_NS" get jobs -o name | grep post-rotation | tail -n1 | sed 's|job/||')
+  kubectl -n "$STAGING_NS" wait --for=condition=complete job/"$POST_JOB" --timeout=900s || (kubectl -n "$STAGING_NS" logs job/"$POST_JOB" --all-containers > "$OUT_DIR/post-rotation-job.log" || true; echo "Post-rotation job failed"; exit 1)
+  kubectl -n "$STAGING_NS" logs job/"$POST_JOB" --all-containers > "$OUT_DIR/post-rotation-job.log" || true
+  # Run per-cloud verify scripts (best-effort)
+  case "$CLOUD" in
+    aws)
+      chmod +x scripts/aws_backup_restore_drill.sh scripts/aws_verify_restore_integrity.sh
+      ./scripts/aws_backup_restore_drill.sh --namespace "$STAGING_NS" --backup-job "$BACKUP_CRONJOB" --s3-bucket "${AWS_BACKUP_BUCKET:-}" || true
+      ;;
+    oci)
+      chmod +x scripts/oci_backup_restore_drill.sh scripts/oci_verify_objects.sh
+      ./scripts/oci_backup_restore_drill.sh --namespace "$STAGING_NS" --cronjob "${OCI_BACKUP_CRONJOB:-}" --oci-namespace "${OCI_NAMESPACE:-}" --bucket "${OCI_BUCKET:-}" --compartment "${OCI_COMPARTMENT:-}" || true
+      ;;
+    *)
+      echo "No verify script configured for cloud: $CLOUD"
+      ;;
+  esac
+fi
+
+echo "[orchestrator] Bundling artifacts"
+ART_TAR="${OUT_DIR}/aegis-rotation-${CLOUD}-$(date -u +%Y%m%dT%H%M%SZ).tar.gz"
+tar -czf "$ART_TAR" -C "$OUT_DIR" . || true
+sha256sum "$ART_TAR" > "${ART_TAR}.sha256"
+echo "Artifacts bundled at $ART_TAR"
+
+echo "Orchestration complete. Review $OUT_DIR for logs and verification output before retiring old keys."
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/retention_tests_runner.yml
+name: Retention Tests Runner
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 6 * * 1'  # weekly on Monday at 06:00 UTC
+
+permissions:
+  contents: read
+
+jobs:
+  retention-tests:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Run AWS retention test (if secret present)
+        if: ${{ secrets.AWS_BACKUP_BUCKET }}
+        run: |
+          chmod +x scripts/aws_retention_test.sh
+          ./scripts/aws_retention_test.sh --bucket "${{ secrets.AWS_BACKUP_BUCKET }}" --region "${{ secrets.AWS_REGION }}"
+
+      - name: Run Azure retention test (if secrets present)
+        if: ${{ secrets.AZ_STORAGE_ACCOUNT && secrets.AZ_BACKUP_CONTAINER }}
+        run: |
+          chmod +x scripts/azure_retention_test.sh
+          ./scripts/azure_retention_test.sh --storage-account "${{ secrets.AZ_STORAGE_ACCOUNT }}" --container "${{ secrets.AZ_BACKUP_CONTAINER }}" --resource-group "${{ secrets.AZ_RESOURCE_GROUP }}"
+
+      - name: Run OCI retention test (if secrets present)
+        if: ${{ secrets.OCI_BUCKET && secrets.OCI_NAMESPACE }}
+        run: |
+          chmod +x scripts/oci_retention_test.sh
+          ./scripts/oci_retention_test.sh --bucket "${{ secrets.OCI_BUCKET }}" --namespace "${{ secrets.OCI_NAMESPACE }}" --compartment "${{ secrets.OCI_COMPARTMENT }}"
+
+      - name: Upload retention artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: retention-test-artifacts
+          path: artifacts || true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/setup_azure_diagnostics.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# setup_azure_diagnostics.sh
+# Enables diagnostic settings for a storage account or Key Vault and sends logs to a storage account or event hub for SIEM ingestion.
+#
+# Usage:
+#   ./scripts/setup_azure_diagnostics.sh --resource-id /subscriptions/.../resourceGroups/.../providers/Microsoft.Storage/storageAccounts/mystorage --dest-storage-account mystorage-diagnostics
+#
+RESOURCE_ID=""
+DEST_STORAGE_ACCOUNT=""
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --resource-id) RESOURCE_ID="$2"; shift 2;;
+    --dest-storage-account) DEST_STORAGE_ACCOUNT="$2"; shift 2;;
+    -h|--help) sed -n '1,200p' "$0"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$RESOURCE_ID" ] || [ -z "$DEST_STORAGE_ACCOUNT" ]; then
+  echo "Missing required args"
+  exit 2
+fi
+
+echo "Enabling diagnostic settings for $RESOURCE_ID to storage account $DEST_STORAGE_ACCOUNT"
+az monitor diagnostic-settings create --resource "$RESOURCE_ID" --name "aegis-diagnostics" --storage-account "${DEST_STORAGE_ACCOUNT}" --metrics '[{"category":"AllMetrics","enabled":true}]' --logs '[{"category":"AuditEvent","enabled":true},{"category":"Requests","enabled":true}]' || true
+
+echo "Diagnostic settings applied (check azure portal to confirm)."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/setup_oci_logging.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# setup_oci_logging.sh
+# Placeholder helper to instruct on enabling OCI logging and forwarding to a SIEM or Object Storage.
+#
+# Usage:
+#   ./scripts/setup_oci_logging.sh --compartment <compartment_ocid>
+#
+COMPARTMENT=""
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --compartment) COMPARTMENT="$2"; shift 2;;
+    -h|--help) sed -n '1,200p' "$0"; exit 0;;
+    *) echo "Unknown arg: $1"; exit 2;;
+  esac
+done
+
+if [ -z "$COMPARTMENT" ]; then
+  echo "Usage: $0 --compartment <compartment_ocid>"
+  exit 2
+fi
+
+echo "To centralize OCI logs, configure Service Connector Hub or Logging service to forward logs to Object Storage or Streaming (Event Hub)."
+echo "This script is a placeholder. Use OCI Console or 'oci logging' and 'oci service-connector' commands to wire logs."
+
*** End Patch
*** Begin Patch
*** Add File: docs/enable_branch_protection_and_codeowners.md
+# Enforce Branch Protections & CODEOWNERS for Security Approvals
+
+This document describes the steps repo admins must perform to finish the remaining enforcement tasks (cannot be applied purely as code).
+
+Steps:
+1. Add CODEOWNERS entries for infra/iam and infra/oci (if not already present). Example:
+
+   /infra/iam/ @your-org/security-team
+   /infra/oci/ @your-org/security-team
+
+2. Configure Branch Protection for main (Settings → Branches → Add rule)
+   - Protect matching branches: main
+   - Require pull request reviews before merging
+   - Require status checks to pass before merging: add "CI Security Checks (pre-merge)" and "OIDC Conversion Checks" and any other required jobs
+   - Require review from Code Owners (enable)
+   - Restrict who can push to matching branches
+
+3. Setup Environments for dangerous ops (Settings → Environments)
+   - Create environment 'dangerous-ops' and require reviewers for that environment
+   - Use environment protection in workflows that perform IAM/KMS or resource-creation ops
+
+4. Communicate to the team: converting workflows to OIDC may require tenant/admin steps in cloud consoles (create federated credentials, workload identity providers or IAM trust relationships).
+
+If you want, I can prepare the CODEOWNERS file and a PR that adds these docs and example settings; repo admins must enable the protections in the GitHub UI.
+
*** End Patch
*** End Patch
