*** Begin Patch
*** Add File: vault/policies/hsm-operator-policy.hcl
+# Vault policy granting read access to LLM keys and cosign public keys for aegis operator role
+path "secret/data/aegis/llm/*" {
+  capabilities = ["read"]
+}
+
+path "secret/data/aegis/cosign/*" {
+  capabilities = ["read"]
+}
+
+# allow operator service accounts to create rekor entries if rekor is configured with tokens in Vault
+path "secret/data/aegis/rekor/*" {
+  capabilities = ["read"]
+}
+
+# Minimal kubernetes auth login capability (used by SA -> Vault role)
+path "auth/kubernetes/login" {
+  capabilities = ["create","read"]
+}
+
*** End Patch
*** Begin Patch
*** Add File: vault/scripts/configure_vault_hsm.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Usage:
+# VAULT_ADDR=https://vault.example VAULT_TOKEN=... ./configure_vault_hsm.sh
+
+: "${VAULT_ADDR:?Must set VAULT_ADDR}"
+: "${VAULT_TOKEN:?Must set VAULT_TOKEN}"
+
+echo "Configuring Vault policies and Kubernetes role for Aegis operator signing..."
+export VAULT_ADDR
+export VAULT_TOKEN
+
+vault login "$VAULT_TOKEN" >/dev/null
+
+# write policy
+vault policy write aegis-hsm-policy vault/policies/hsm-operator-policy.hcl || true
+
+# enable Kubernetes auth (idempotent)
+if ! vault auth list -format=json | jq -r 'keys[]' | grep -q '^kubernetes'; then
+  vault auth enable kubernetes
+fi
+
+# Create role for aegis operator service account
+cat > /tmp/aegis-k8s-role.json <<EOF
+{
+  "bound_service_account_names": ["operator-signing-sa"],
+  "bound_service_account_namespaces": ["aegis"],
+  "policies": ["aegis-hsm-policy"],
+  "ttl": "24h"
+}
+EOF
+
+vault write auth/kubernetes/role/aegis-operator-role @/tmp/aegis-k8s-role.json
+
+echo "Vault configured. IMPORTANT: ensure only operator nodes can access operator-signing service (NetworkPolicy)."
+
*** End Patch
*** Begin Patch
*** Add File: k8s/network/operator-signing-networkpolicy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: allow-operator-to-signing
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: operator-signing
+  policyTypes:
+    - Ingress
+  ingress:
+    - from:
+        # allow incoming only from pods in namespaces labeled aegis-role=operator
+        - namespaceSelector:
+            matchLabels:
+              aegis-role: operator
+      ports:
+        - protocol: TCP
+          port: 8110
+
+---
+# Default deny egress from CI namespace to operator-signing endpoint:
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: deny-ci-to-operator-signing
+  namespace: ci
+spec:
+  podSelector: {}
+  policyTypes:
+    - Egress
+  egress:
+    # allow egress to general internet (HTTP/HTTPS) but not port 8110 to aegis namespace
+    - to:
+        - ipBlock:
+            cidr: 0.0.0.0/0
+      ports:
+        - protocol: TCP
+          port: 80
+        - protocol: TCP
+          port: 443
+    # Note: many CNI plugins require combination with default deny to achieve explicit blocking.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/rbac/operator-signing-rbac.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: operator-signing-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: operator-signing-role
+  namespace: aegis
+rules:
+- apiGroups: [""]
+  resources: ["pods","secrets","services"]
+  verbs: ["get","list"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: operator-signing-binding
+  namespace: aegis
+subjects:
+- kind: ServiceAccount
+  name: operator-signing-sa
+  namespace: aegis
+roleRef:
+  kind: Role
+  name: operator-signing-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File: agents/kata-runtime-setup.md
+# Kata runtime setup (operator guidance)
+
+1. Install Kata runtime on a set of dedicated sandbox nodes (provider-specific).
+   - Label nodes: kubectl label node <node> accelerator=kata
+2. Install RuntimeClass: see agents/runtimeclass-kata.yaml (already applied)
+3. Use Gatekeeper constraint (see agents/sandbox_admission_constraint.yaml) to enforce kata-runtime usage in aegis-sandbox namespace.
+4. Ensure the kata-runtime nodes are isolated and have limited network/elevated privileges.
+
+This document assumes operator will run the Kata installation steps on host OS and configure CRI properly.
+
*** End Patch
*** Begin Patch
*** Add File: agents/sandbox-operator-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-sandbox-operator
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-sandbox-operator
+  template:
+    metadata:
+      labels:
+        app: aegis-sandbox-operator
+    spec:
+      serviceAccountName: aegis-sandbox-operator-sa
+      containers:
+        - name: sandbox-operator
+          image: aegis/sandbox-operator:latest
+          env:
+            - name: REDIS_URL
+              value: "redis://redis:6379/6"
+          args: ["--watch-namespace=aegis-sandbox"]
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-sandbox-operator-sa
+  namespace: aegis
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: sandbox-operator-role
+  namespace: aegis
+rules:
+- apiGroups: [""]
+  resources: ["pods","pods/log","events"]
+  verbs: ["create","get","list","watch","delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: sandbox-operator-binding
+  namespace: aegis
+subjects:
+- kind: ServiceAccount
+  name: aegis-sandbox-operator-sa
+roleRef:
+  kind: Role
+  name: sandbox-operator-role
+  apiGroup: rbac.authorization.k8s.io
+
*** End Patch
*** Begin Patch
*** Add File: agents/sandbox_isolation_test.sh
+#!/usr/bin/env bash
+# Simple isolation test:
+# - launches a pod in aegis-sandbox using kata-runtime that attempts host escape operations
+# - returns non-zero if any dangerous host artifacts are visible (e.g., /host-root)
+
+: "${KUBECONFIG:=}"
+set -euo pipefail
+NS=aegis-sandbox
+POD_NAME=test-escape-$(date +%s)
+
+cat > /tmp/pod.json <<EOF
+{
+  "apiVersion":"v1",
+  "kind":"Pod",
+  "metadata":{"name":"${POD_NAME}"},
+  "spec":{
+    "runtimeClassName":"kata-runtime",
+    "containers":[
+      {"name":"escape","image":"alpine:3.18","command":["/bin/sh","-c","ls /host-root || true; sleep 2; echo done"]}
+    ],
+    "restartPolicy":"Never"
+  }
+}
+EOF
+
+kubectl apply -f /tmp/pod.json -n ${NS}
+kubectl wait --for=condition=Ready pod/${POD_NAME} -n ${NS} --timeout=20s || true
+LOGS=$(kubectl logs ${POD_NAME} -n ${NS} || true)
+echo "Pod logs:"
+echo "${LOGS}"
+kubectl delete pod ${POD_NAME} -n ${NS} --wait=true || true
+
+if echo "${LOGS}" | grep -q "/host-root"; then
+  echo "Isolation FAILED: host root visible in sandbox pod"
+  exit 2
+fi
+echo "Isolation test passed"
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: rl/accelerate_train.py
+#!/usr/bin/env python3
+"""
+Accelerate-based distributed supervised fine-tuning example.
+ - uses HuggingFace Trainer + accelerate for multi-GPU / multi-node runs
+ - integrates with checkpoint_manager for robust upload & MLflow logging
+
+This script focuses on a small sample pipeline to validate distributed runs.
+Operator should replace dataset and model with production datasets and model.
+"""
+import os, argparse, json
+from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
+from datasets import Dataset
+from rl.checkpoint_manager import upload_checkpoint
+import mlflow
+
+def make_dataset():
+    samples = [{"text":"Q: 2+2? A: 4"},{"text":"Q: FR capital? A: Paris"}]
+    return Dataset.from_list(samples)
+
+def train(output_dir="/tmp/accel_out", model_name="distilgpt2", epochs=1, batch_size=2):
+    tokenizer = AutoTokenizer.from_pretrained(model_name)
+    model = AutoModelForCausalLM.from_pretrained(model_name)
+    ds = make_dataset()
+    def tok(ex):
+        return tokenizer(ex["text"], truncation=True, padding="max_length", max_length=64)
+    ds = ds.map(tok, batched=True)
+    ds.set_format(type="torch", columns=["input_ids","attention_mask"])
+    args = TrainingArguments(
+        output_dir=output_dir,
+        per_device_train_batch_size=batch_size,
+        num_train_epochs=epochs,
+        logging_steps=10,
+        save_strategy="epoch"
+    )
+    trainer = Trainer(model=model, args=args, train_dataset=ds)
+    with mlflow.start_run():
+        trainer.train()
+        trainer.save_model(output_dir)
+    # upload checkpoint
+    ckpt_file = os.path.join(output_dir, "pytorch_model.bin")
+    meta = {"model": model_name, "epochs": epochs}
+    rec = upload_checkpoint(ckpt_file, f"rlhf/accel/{os.path.basename(ckpt_file)}", metadata=meta)
+    print("Uploaded checkpoint:", rec)
+
+if __name__=="__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--out", default="/tmp/accel_out")
+    p.add_argument("--model", default="distilgpt2")
+    p.add_argument("--epochs", type=int, default=1)
+    args = p.parse_args()
+    train(output_dir=args.out, model_name=args.model, epochs=args.epochs)
+
*** End Patch
*** Begin Patch
*** Add File: k8s/rlhf/distributed-train-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: rlhf-distributed-train
+  namespace: aegis-ml
+spec:
+  template:
+    spec:
+      restartPolicy: Never
+      containers:
+        - name: trainer
+          image: aegis/rlhf:latest
+          command: ["accelerate", "launch", "/opt/rl/accelerate_train.py", "--out", "/tmp/accel_out", "--model", "distilgpt2", "--epochs", "1"]
+          env:
+            - name: MLFLOW_TRACKING_URI
+              valueFrom:
+                secretKeyRef:
+                  name: mlflow-secrets
+                  key: tracking-uri
+            - name: ARTIFACT_S3_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: artifact-bucket
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              memory: "24Gi"
+              cpu: "4"
+            requests:
+              nvidia.com/gpu: 1
+              memory: "12Gi"
+              cpu: "2"
+  backoffLimit: 1
+
+---
+# NOTE: For multi-node, use a launcher/controller such as kubeflow MPIJob or run multiple Jobs with networking configured.
+
*** End Patch
*** Begin Patch
*** Add File: safety/adversarial_pipeline.yml
+name: Adversarial Hallucination Gate (CI)
+on:
+  workflow_dispatch:
+  repository_dispatch:
+    types: [model-prepromotion]
+
+jobs:
+  run-adversarial:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install deps
+        run: python -m pip install --upgrade pip && pip install requests scikit-learn
+      - name: Run adversarial suite
+        env:
+          LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }}
+          ADV_MANIFEST: tests/adversarial_prompts.jsonl
+          COMPLIANCE_BUCKET: ${{ secrets.COMPLIANCE_BUCKET }}
+        run: |
+          python safety/adversarial_harness.py || (echo "Adversarial tests failed" && exit 2)
+      - name: Fail PR if hallucination rate exceeded
+        if: failure()
+        run: |
+          echo "Hallucination rate exceeded threshold; blocking promotion"
+          exit 2
+
*** End Patch
*** Begin Patch
*** Add File: ci/auto_canary_manager_improved.py
+#!/usr/bin/env python3
+"""
+Improved Auto-Canary Manager:
+ - queries ES for hallucination and cost metrics (placeholder ES queries)
+ - promotes or rolls back via model registry API
+ - emits events to compliance bucket and operator webhook
+"""
+import os, requests, json, time
+from datetime import datetime, timedelta
+
+ES_HOST = os.environ.get("ES_HOST")
+MODEL_REGISTRY_API = os.environ.get("MODEL_REGISTRY_API")
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+def query_hallu_rate(model_id, window_mins=30):
+    # Placeholder: implement proper ES query
+    # Return float between 0..1
+    return 0.01
+
+def query_cost_per_hour(model_id):
+    # Placeholder cost aggregator
+    return 1.2
+
+def act(model_id):
+    hallu = query_hallu_rate(model_id)
+    cost = query_cost_per_hour(model_id)
+    out = {"model": model_id, "hallu": hallu, "cost": cost, "ts": datetime.utcnow().isoformat()}
+    if hallu < float(os.environ.get("CANARY_HALLU_THRESH","0.02")) and cost <= float(os.environ.get("CANARY_COST_THRESH","10.0")):
+        # promote
+        requests.post(f"{MODEL_REGISTRY_API}/models/promote", json={"model": model_id}, timeout=10)
+        out["action"] = "promote"
+    else:
+        requests.post(f"{MODEL_REGISTRY_API}/models/rollback", json={"model": model_id}, timeout=10)
+        out["action"] = "rollback"
+    # notify operator
+    if OPERATOR_WEBHOOK:
+        try:
+            requests.post(OPERATOR_WEBHOOK, json=out, timeout=5)
+        except Exception:
+            pass
+    # persist to compliance bucket via S3 if configured (operator can wire)
+    print("Action:", out)
+    return out
+
+if __name__=="__main__":
+    import sys
+    if len(sys.argv)<2:
+        print("usage: auto_canary_manager_improved.py <model_id>")
+        raise SystemExit(2)
+    print(act(sys.argv[1]))
+
*** End Patch
*** Begin Patch
*** Add File: observability/opensearch/deploy_opensearch_single.yaml
+# Minimal single-node OpenSearch StatefulSet for audit indexing (for staging only)
+apiVersion: v1
+kind: Service
+metadata:
+  name: opensearch
+  namespace: observability
+spec:
+  selector:
+    app: opensearch
+  ports:
+    - port: 9200
+      name: http
+
+---
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: opensearch
+  namespace: observability
+spec:
+  serviceName: "opensearch"
+  replicas: 1
+  selector:
+    matchLabels:
+      app: opensearch
+  template:
+    metadata:
+      labels:
+        app: opensearch
+    spec:
+      containers:
+        - name: opensearch
+          image: opensearchproject/opensearch:2.3.0
+          env:
+            - name: discovery.type
+              value: "single-node"
+            - name: OPENSEARCH_JAVA_OPTS
+              value: "-Xms1g -Xmx1g"
+          ports:
+            - containerPort: 9200
+              name: http
+            - containerPort: 9300
+              name: transport
+          volumeMounts:
+            - name: data
+              mountPath: /usr/share/opensearch/data
+  volumeClaimTemplates:
+    - metadata:
+        name: data
+      spec:
+        accessModes: ["ReadWriteOnce"]
+        resources:
+          requests:
+            storage: 20Gi
+
+---
+# Namespace
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: observability
+
*** End Patch
*** Begin Patch
*** Add File: observability/es_role_setup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+: "${ES_URL:?Please set ES_URL like http://opensearch.observability.svc:9200}"
+: "${ES_ADMIN_USER:?}"
+: "${ES_ADMIN_PASS:?}"
+
+echo "Creating index template and role for aegis auditors..."
+curl -s -u "${ES_ADMIN_USER}:${ES_ADMIN_PASS}" -XPUT "${ES_URL}/_security/role/aegis_auditor" -H 'Content-Type: application/json' -d '{
+  "cluster": ["cluster:monitor/main"],
+  "indices": [
+    {
+      "names": ["aegis-audit*"],
+      "privileges": ["read","view_index_metadata"]
+    }
+  ]
+}'
+
+echo "Create user aegis-auditor (change password)"
+curl -s -u "${ES_ADMIN_USER}:${ES_ADMIN_PASS}" -XPUT "${ES_URL}/_security/user/aegis-auditor" -H 'Content-Type: application/json' -d '{
+  "password": "CHANGE_ME",
+  "roles": ["aegis_auditor"],
+  "full_name": "Aegis Auditor"
+}'
+
+echo "Index template created and auditor user created. Operator must change password and store in Vault."
+
*** End Patch
*** Begin Patch
*** Add File: billing/alerts_monitor.py
+#!/usr/bin/env python3
+"""
+Billing anomaly monitor:
+ - reads invoices or token counters from Redis
+ - triggers an alert to OPERATOR_NOTIFY_WEBHOOK if sudden spike or cost anomaly is detected
+"""
+import os, time, json
+import redis, requests
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/3")
+redis_client = redis.from_url(REDIS_URL)
+WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+SPIKE_FACTOR = float(os.environ.get("BILL_SPIKE_FACTOR", "3.0"))
+
+def check_spike():
+    # simple comparison: compare last_period tokens to median of previous 24 periods
+    key = "billing:history"
+    history = [int(x) for x in redis_client.lrange(key, 0, -1)] or []
+    if not history or len(history) < 2:
+        return
+    last = history[-1]
+    med = sorted(history[:-1])[len(history[:-1])//2]
+    if med > 0 and last > med * SPIKE_FACTOR:
+        msg = {"alert":"billing_spike", "last": last, "median": med, "ts": int(time.time())}
+        if WEBHOOK:
+            try:
+                requests.post(WEBHOOK, json=msg, timeout=3)
+            except Exception:
+                pass
+        print("Billing spike detected", msg)
+
+if __name__=="__main__":
+    while True:
+        check_spike()
+        time.sleep(int(os.environ.get("BILL_CHECK_SEC", "300")))
+
*** End Patch
*** Begin Patch
*** Add File: compliance/pen_test_remediation_workflow.md
+# Pen Test Remediation Workflow (operators)
+
+1. When a pen-test finding is opened, add it to compliance/pen_findings.json with: id, title, severity, created_at, remediation_days.
+2. The remediation_tracker.py will notify operators within 7 days of SLA breach.
+3. Operators must assign an owner and update the finding with status/progress; upon close, record evidence (PR, fix changelog, verification script).
+4. For critical/high findings, schedule an immediate mitigation playbook run and add to weekly compliance review.
+
+Store closure evidence in COMPLIANCE_BUCKET and reference it in the issue record.
+
*** End Patch
*** End Patch
