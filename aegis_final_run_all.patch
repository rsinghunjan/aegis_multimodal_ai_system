*** Begin Patch
*** Add File: .github/workflows/run_full_staging_proof.yml
+name: Run Full Staging Proof (orchestrator)
+
+on:
+  workflow_dispatch:
+    inputs:
+      e2e_runs:
+        description: "Total operator E2E runs (for operator proof)"
+        required: false
+        default: '500'
+      kpi_days:
+        description: "Number of KPI samples / days for baseline (N)"
+        required: false
+        default: '3'
+
+permissions:
+  contents: read
+  issues: write
+
+jobs:
+  full-staging-proof:
+    runs-on: ubuntu-latest
+    timeout-minutes: 1440
+    env:
+      KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
+      PROM_URL: ${{ secrets.PROM_URL }}
+      PROM_TOKEN: ${{ secrets.PROM_TOKEN }}
+      PUSHGATEWAY_URL: ${{ secrets.PUSHGATEWAY_URL }}
+      LLM_GATEWAY_URL: ${{ secrets.LLM_GATEWAY_URL }}
+      LLM_GATEWAY_API_KEY: ${{ secrets.LLM_GATEWAY_API_KEY }}
+      GCS_AUDIT_BUCKET: ${{ secrets.GCS_AUDIT_BUCKET }}
+      S3_AUDIT_BUCKET: ${{ secrets.S3_AUDIT_BUCKET }}
+      CLOUD_PROVIDER: ${{ secrets.CLOUD_PROVIDER }}
+      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Prepare runtime
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests prometheus-api-client || true
+          chmod +x scripts/*.sh || true
+          chmod +x scripts/*.py || true
+
+      - name: Run orchestrator script
+        id: run_all
+        run: |
+          ./scripts/run_all_checks.sh --e2e-runs "${{ github.event.inputs.e2e_runs }}" --kpi-days "${{ github.event.inputs.kpi_days }}" --artifact-dir "/tmp/aegis-full-$(date +%s)"
+        timeout-minutes: 1200
+
+      - name: Upload artifacts
+        if: always()
+        uses: actions/upload-artifact@v4
+        with:
+          name: aegis-full-run-artifacts
+          path: /tmp/aegis-full-*
+
+      - name: Create signoff issue (if succeeded)
+        if: success()
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          GITHUB_REPOSITORY: ${{ github.repository }}
+        run: |
+          python3 scripts/create_prefilled_signoff_from_latest_artifacts.py || true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_all_checks.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# run_all_checks.sh
+# Orchestrates the "prove it" run in staging:
+# 1) Run readiness gate (full validation orchestration)
+# 2) Operator proof at scale (batch E2E runs & Pushgateway metrics)
+# 3) Deploy representative upstream and run load tests + SLO suggestion
+# 4) Install Milvus HA, run backup/restore and failover tests
+# 5) Migrate secrets/KSAs guidance (Vault/Workload Identity)
+# 6) Enable audit upload and verify retention
+# 7) Collect KPI baseline for N samples and evaluate
+# 8) Produce artifacts and exit non-zero on critical failures
+#
+# Dependencies (expected in env / secrets):
+# - KUBE_CONFIG_DATA (base64 kubeconfig)
+# - PROM_URL, PROM_TOKEN
+# - PUSHGATEWAY_URL (optional)
+# - LLM_GATEWAY_URL, LLM_GATEWAY_API_KEY (for load tests)
+# - CLOUD_PROVIDER (gcp|aws) and corresponding bucket envs
+# - GITHUB_TOKEN for signoff creation
+#
+usage() {
+  cat <<EOF
+Usage: $0 [--e2e-runs N] [--kpi-days N] [--artifact-dir DIR]
+
+Defaults:
+  --e2e-runs: 500
+  --kpi-days: 3
+  --artifact-dir: /tmp/aegis-full-<ts>
+EOF
+}
+
+E2E_RUNS=500
+KPI_DAYS=3
+ARTIFACT_DIR="/tmp/aegis-full-$(date +%s)"
+
+while [ $# -gt 0 ]; do
+  case "$1" in
+    --e2e-runs) E2E_RUNS="$2"; shift 2;;
+    --kpi-days) KPI_DAYS="$2"; shift 2;;
+    --artifact-dir) ARTIFACT_DIR="$2"; shift 2;;
+    -h|--help) usage; exit 0;;
+    *) echo "Unknown arg: $1"; usage; exit 2;;
+  esac
+done
+
+mkdir -p "$ARTIFACT_DIR"
+echo "Artifacts and logs will be collected under: $ARTIFACT_DIR"
+
+# Helper to decode kubeconfig if provided
+if [ -n "${KUBE_CONFIG_DATA:-}" ]; then
+  KUBECONFIG_FILE="/tmp/kubeconfig_aegis_full"
+  echo "$KUBE_CONFIG_DATA" | base64 --decode > "$KUBECONFIG_FILE"
+  export KUBECONFIG="$KUBECONFIG_FILE"
+  echo "Wrote kubeconfig to $KUBECONFIG_FILE"
+fi
+
+log_and_run() {
+  local label="$1"; shift
+  local out="$ARTIFACT_DIR/${label}.log"
+  echo "=== START: $label ===" | tee "$out"
+  if "$@" >> "$out" 2>&1; then
+    echo "=== OK: $label ===" | tee -a "$out"
+    return 0
+  else
+    echo "=== FAILED: $label (see $out) ===" | tee -a "$out"
+    return 1
+  fi
+}
+
+########################################################################
+# 1) Readiness gate (full validation)
+########################################################################
+log_and_run readiness_gate ./scripts/run_full_validation.sh --minutes 60 --e2e-runs 50 --artifact-dir "$ARTIFACT_DIR/readiness" || {
+  echo "Readiness gate reported failures. Writing analysis and exiting with non-zero."
+  ./scripts/analyze_readiness_artifacts.py --artifact "$ARTIFACT_DIR/readiness.tgz" > "$ARTIFACT_DIR/readiness_analysis.txt" 2>&1 || true
+  cat "$ARTIFACT_DIR/readiness_analysis.txt" || true
+  # Allow continuation: user may want to proceed with other tests, but record failure.
+  echo "Readiness gate failed; continuing with further tests for debugging." | tee -a "$ARTIFACT_DIR/readiness_analysis.txt"
+}
+
+########################################################################
+# 2) Operator proof at scale (batches)
+########################################################################
+log_and_run operator_proof python3 scripts/operator_proof_runner.py --total "$E2E_RUNS" --batch 50 --pushgateway "${PUSHGATEWAY_URL:-}" --tmpdir "$ARTIFACT_DIR/operator" || {
+  echo "Operator proof failed thresholds or encountered errors; see logs."
+  exit 2
+}
+
+########################################################################
+# 3) Deploy representative upstream and run LLM load tests
+########################################################################
+if [ -n "${LLM_GATEWAY_URL:-}" ]; then
+  log_and_run deploy_upstream ./scripts/deploy_sample_upstream.sh || {
+    echo "Upstream deploy failed; continuing but marking for triage."
+  }
+
+  log_and_run llm_load_test python3 scripts/llm_load_test.py --url "${LLM_GATEWAY_URL}" --key "${LLM_GATEWAY_API_KEY:-}" --requests 1000 --concurrency 16 || {
+    echo "LLM load test reported issues. Capturing suggestion via slo_tuning."
+  }
+  python3 scripts/slo_tuning.py --minutes 1440 --out "$ARTIFACT_DIR/suggested_slos.json" || true
+else
+  echo "LLM_GATEWAY_URL not set; skipping upstream deploy/load tests" | tee -a "$ARTIFACT_DIR/llm_skip.txt"
+fi
+
+########################################################################
+# 4) Milvus HA, backups & failover tests
+########################################################################
+log_and_run milvus_install ./scripts/milvus_ha_install.sh || {
+  echo "Milvus HA install step encountered errors; continue to next steps."
+}
+log_and_run milvus_backup ./scripts/milvus_backup_restore_verify.sh || {
+  echo "Milvus backup verify failed or skipped."
+}
+log_and_run milvus_failover ./scripts/milvus_failover_test.sh || {
+  echo "Milvus failover test failed or skipped."
+}
+
+########################################################################
+# 5) Identity & secrets migration guidance (dry-run / print commands)
+########################################################################
+log_and_run identity_migration ./scripts/identity_migration_automate.sh --dry-run --gcp || true
+log_and_run vault_migrate ./scripts/migrate_to_vault.sh || true
+
+########################################################################
+# 6) Enable AUTO_UPLOAD_AUDIT and verify
+########################################################################
+log_and_run enable_audit ./scripts/enable_audit_and_verify.sh || {
+  echo "Audit enable/verify encountered issues; continue."
+}
+
+########################################################################
+# 7) Collect KPI baseline for N days (samples) and evaluate
+########################################################################
+SLEEP_SECONDS=5
+mkdir -p "$ARTIFACT_DIR/kpi"
+for i in $(seq 1 "$KPI_DAYS"); do
+  echo "Collecting KPI snapshot $i/$KPI_DAYS"
+  python3 scripts/collect_kpis.py --minutes 60 --out "$ARTIFACT_DIR/kpi/kpi_snapshot_${i}.json" || true
+  # Sleep between samples; in CI we keep short, but in real usage use 24h between samples.
+  sleep "$SLEEP_SECONDS"
+done
+
+python3 scripts/evaluate_kpis.py --input "$ARTIFACT_DIR/kpi" --slo monitoring/slo_values.yaml || {
+  echo "KPI evaluation failed; aborting promotion and signoff."
+  exit 2
+}
+
+########################################################################
+# 8) Final: create prefilled signoff issue
+########################################################################
+python3 scripts/create_prefilled_signoff_from_latest_artifacts.py || {
+  echo "Signoff creation failed (best-effort); you can create manually using scripts/render_signoff_prefilled.py"
+}
+
+echo "All orchestrated steps completed. Artifacts: $ARTIFACT_DIR"
+exit 0
+
*** End Patch
