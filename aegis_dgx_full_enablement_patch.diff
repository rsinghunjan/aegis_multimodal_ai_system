diff --git a/scripts/dgx/provision_and_store_kubeconfig.sh b/scripts/dgx/provision_and_store_kubeconfig.sh
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/scripts/dgx/provision_and_store_kubeconfig.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Guided helper to record DGX cluster provisioning steps and optionally
+# push kubeconfig into GitHub Actions secrets (requires gh CLI + permissions).
+#
+# Usage:
+#   ./scripts/dgx/provision_and_store_kubeconfig.sh --kubeconfig ./kubeconfig-dgx --repo org/repo --secret-name KUBECONFIG_DGX
+
+set -euo pipefail
+
+KUBECONFIG_PATH=""
+REPO=""
+SECRET_NAME="${SECRET_NAME:-KUBECONFIG_DGX}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --kubeconfig) KUBECONFIG_PATH="$2"; shift 2;;
+    --repo) REPO="$2"; shift 2;;
+    --secret-name) SECRET_NAME="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$KUBECONFIG_PATH" || ! -f "$KUBECONFIG_PATH" ]]; then
+  echo "Provide existing kubeconfig file via --kubeconfig <path>"
+  exit 2
+fi
+
+echo "Step 1: Provision DGX cluster (manual or vendor portal)"
+echo "- Work with NVIDIA DGX Cloud onboarding to provision your cluster."
+echo "- Choose k8s (recommended) or scheduler-only (Slurm)."
+echo "- Ensure network connectivity from CI/self-hosted runners to the cluster control plane."
+echo
+read -p "Press ENTER once DGX provisioning is complete..."
+
+echo "Step 2: Validate provided kubeconfig file"
+kubectl --kubeconfig="$KUBECONFIG_PATH" version --short || { echo "kubeconfig failed; please verify"; exit 3; }
+kubectl --kubeconfig="$KUBECONFIG_PATH" get nodes -o wide || true
+
+echo "Step 3: Store kubeconfig locally in project (safe file perms)"
+OUT="./kubeconfigs/kubeconfig-dgx"
+mkdir -p "$(dirname "$OUT")"
+cp "$KUBECONFIG_PATH" "$OUT"
+chmod 0600 "$OUT"
+echo "Saved kubeconfig -> $OUT"
+
+if [[ -n "$REPO" && -n "$(command -v gh 2>/dev/null)" ]]; then
+  echo "Attempting to store kubeconfig into GitHub Actions secret $SECRET_NAME for repository $REPO"
+  echo "This requires the GH CLI and that you are authenticated and have admin:repo scope."
+  base64_kc=$(base64 -w 0 "$OUT")
+  read -p "Proceed to write secret to repo $REPO? [y/N] " confirm
+  if [[ "$confirm" =~ ^[Yy]$ ]]; then
+    echo "$base64_kc" | gh secret set "$SECRET_NAME" --repo "$REPO" --body-file - --visibility all
+    echo "Secret $SECRET_NAME written to $REPO"
+  else
+    echo "Skipping GitHub secret write. You can add KUBECONFIG_DGX manually in repo settings."
+  fi
+else
+  echo "To add KUBECONFIG_DGX to GitHub, run:"
+  echo "  gh secret set KUBECONFIG_DGX --repo <owner/repo> --body-file $OUT"
+fi
+
+echo "Reminder: Prefer ephemeral / short-lived kubeconfigs where possible."
+echo "Done."
+exit 0
+
diff --git a/scripts/dgx/label_nodes_or_update_selector.sh b/scripts/dgx/label_nodes_or_update_selector.sh
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/scripts/dgx/label_nodes_or_update_selector.sh
@@ -0,0 +1,200 @@
+#!/usr/bin/env bash
+#
+# Label DGX nodes, or if you prefer, update manifests to match an existing nodeSelector.
+#
+# Usage examples:
+#  ./scripts/dgx/label_nodes_or_update_selector.sh --nodes node-1,node-2 --label node.kubernetes.io/dgx=true
+#  ./scripts/dgx/label_nodes_or_update_selector.sh --update-manifests --selector "node.kubernetes.io/role=dgx"
+
+set -euo pipefail
+
+NODES=""
+LABEL="node.kubernetes.io/dgx=true"
+UPDATE_MANIFESTS="false"
+MANIFESTS_DIR="${MANIFESTS_DIR:-k8s/manifests}"
+SELECTOR=""
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --nodes) NODES="$2"; shift 2;;
+    --label) LABEL="$2"; shift 2;;
+    --update-manifests) UPDATE_MANIFESTS="true"; shift 1;;
+    --selector) SELECTOR="$2"; shift 2;;
+    --manifests-dir) MANIFESTS_DIR="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ "$UPDATE_MANIFESTS" == "true" ]]; then
+  if [[ -z "$SELECTOR" ]]; then
+    echo "--selector required when using --update-manifests"
+    exit 2
+  fi
+  echo "Updating nodeSelector in YAML manifests under $MANIFESTS_DIR to use: $SELECTOR"
+  # naive YAML replacement: replace the nodeSelector block with the provided selector key:value
+  find "$MANIFESTS_DIR" -type f -name "*.yaml" -print0 | while IFS= read -r -d '' f; do
+    if grep -q "nodeSelector" "$f"; then
+      echo "Patching $f"
+      # replace the whole nodeSelector block (simple heuristic)
+      awk -v sel="$SELECTOR" '
+        BEGIN { key=substr(sel,1,index(sel,"=")-1); val=substr(sel,index(sel,"=")+1) }
+        { if ($0 ~ /nodeSelector:/) { print "      nodeSelector:"; print "        \"" key "\": \"" val "\""; skip=1; next } }
+        { if (skip==1) { if ($0 ~ /^[[:space:]]/ && $0 !~ /^[[:space:]]+- /) next; else skip=0 } print $0 }
+      ' "$f" > "$f.tmp" && mv "$f.tmp" "$f"
+    fi
+  done
+  echo "Manifest nodeSelector update complete."
+  exit 0
+fi
+
+if [[ -z "$NODES" ]]; then
+  echo "--nodes is required when not updating manifests"
+  exit 2
+fi
+
+IFS=',' read -r -a node_array <<< "$NODES"
+for n in "${node_array[@]}"; do
+  if kubectl get node "$n" >/dev/null 2>&1; then
+    echo "Labeling node $n with $LABEL"
+    kubectl label node "$n" "$LABEL" --overwrite || true
+  else
+    echo "Node $n not found; skipping"
+  fi
+done
+
+echo "Node labeling complete."
+exit 0
+
diff --git a/scripts/dgx/publish_sign_register_image.sh b/scripts/dgx/publish_sign_register_image.sh
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/scripts/dgx/publish_sign_register_image.sh
@@ -0,0 +1,320 @@
+#!/usr/bin/env bash
+#
+# Build, SBOM, scan, sign and register validated DGX images and append a canonical entry to docs/dgx/IMAGE_MATRIX.md.
+#
+# Usage:
+#  REGISTRY=ghcr.io/org TARGET=h100 IMAGE_TAG=aegis-deepspeed:h100-cuda12.1-pytorch2.2 \
+#   ./scripts/dgx/publish_sign_register_image.sh
+
+set -euo pipefail
+
+REGISTRY="${REGISTRY:-<REGISTRY>}"
+TARGET="${TARGET:-h100}"   # h100 or a100
+IMAGE_TAG="${IMAGE_TAG:-aegis-deepspeed:${TARGET}-validated}"
+BUILD_DIR="${BUILD_DIR:-.}"
+IMAGE_FULL="${REGISTRY}/${IMAGE_TAG}"
+SBOM_DIR="${SBOM_DIR:-./artifacts/sbom}"
+IMAGE_MATRIX_FILE="${IMAGE_MATRIX_FILE:-docs/dgx/IMAGE_MATRIX.md}"
+
+if [[ "$REGISTRY" == "<REGISTRY>" ]]; then
+  echo "Please set REGISTRY env var (e.g., REGISTRY=ghcr.io/org)"; exit 2
+fi
+
+case "$TARGET" in
+  h100) DOCKERFILE="docker/Dockerfile.deepspeed.h100";;
+  a100) DOCKERFILE="docker/Dockerfile.deepspeed.a100";;
+  *) echo "Unknown TARGET $TARGET"; exit 3;;
+esac
+
+if [[ ! -f "$DOCKERFILE" ]]; then
+  echo "Dockerfile missing: $DOCKERFILE"; exit 4
+fi
+
+echo "Building image $IMAGE_FULL"
+docker build -t "$IMAGE_FULL" -f "$DOCKERFILE" "$BUILD_DIR"
+
+mkdir -p "$SBOM_DIR" ./artifacts
+SBOM_FILE="${SBOM_DIR}/${IMAGE_TAG//[:\/]/_}-sbom.json"
+if command -v syft >/dev/null 2>&1; then
+  syft "$IMAGE_FULL" -o json > "$SBOM_FILE"
+  echo "SBOM generated: $SBOM_FILE"
+else
+  echo "syft not installed; skipping SBOM generation"
+fi
+
+if command -v trivy >/dev/null 2>&1; then
+  echo "Scanning image with Trivy (CRITICAL/HIGH cause non-zero exit)"
+  trivy image --severity CRITICAL,HIGH --no-progress "$IMAGE_FULL" > "./artifacts/${IMAGE_TAG//[:\/]/_}-trivy.txt" || { echo "Trivy found critical/high issues; aborting publish"; exit 5; }
+else
+  echo "trivy not found; skipping vulnerability scan"
+fi
+
+echo "Pushing image to registry"
+docker push "$IMAGE_FULL"
+
+DIGEST="unknown"
+if command -v skopeo >/dev/null 2>&1; then
+  skopeo inspect "docker://${IMAGE_FULL}" --raw > ./artifacts/inspect_raw.json || true
+  DIGEST=$(jq -r '.Digest // empty' ./artifacts/inspect_raw.json || echo "unknown")
+else
+  DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE_FULL" 2>/dev/null || echo "unknown")
+fi
+echo "Image digest: $DIGEST" > ./artifacts/${IMAGE_TAG//[:\/]/_}-digest.txt
+
+echo "Signing image with cosign"
+if command -v cosign >/dev/null 2>&1; then
+  if [[ -n "${COSIGN_KEY:-}" ]]; then
+    cosign sign --key "$COSIGN_KEY" "$IMAGE_FULL"
+  else
+    cosign sign --oidc "$IMAGE_FULL" || echo "keyless cosign sign failed or not supported in runner"
+  fi
+else
+  echo "cosign not available; skipping sign (recommended to sign images)"
+fi
+
+echo "Registering validated image in $IMAGE_MATRIX_FILE"
+timestamp="$(date -u +%Y-%m-%d)"
+cat >> "$IMAGE_MATRIX_FILE" <<EOF
+- image: ${IMAGE_FULL}
+  digest: ${DIGEST}
+  target: ${TARGET}
+  validated_on: ${timestamp}
+  sbom: ${SBOM_FILE}
+  signed: $(if command -v cosign >/dev/null 2>&1; then echo "true"; else echo "false"; fi)
+  notes: "Built and validated via scripts/dgx/publish_sign_register_image.sh"
+EOF
+
+echo "Registered image entry appended to $IMAGE_MATRIX_FILE"
+echo "Done. Keep the image digest/pinned tag in your production manifests."
+exit 0
+
diff --git a/scripts/dgx/run_nccl_tune_apply_and_update_job.sh b/scripts/dgx/run_nccl_tune_apply_and_update_job.sh
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/scripts/dgx/run_nccl_tune_apply_and_update_job.sh
@@ -0,0 +1,240 @@
+#!/usr/bin/env bash
+#
+# Run NCCL tuning on DGX, generate suggested envs and apply them to ConfigMap,
+# then patch deepspeed job manifest to use envFrom dgx-nccl-config.
+#
+# Usage:
+#  ./scripts/dgx/run_nccl_tune_apply_and_update_job.sh --out ./artifacts/nccl --job-manifest k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml
+
+set -euo pipefail
+
+OUT_DIR="${OUT_DIR:-./artifacts/nccl}"
+JOB_MANIFEST="${JOB_MANIFEST:-k8s/manifests/dgx/deepspeed-dgx-job-with-configmap.yaml}"
+
+mkdir -p "$OUT_DIR"
+
+echo "Running nccl_tuning.sh..."
+if [[ -x "./scripts/dgx/nccl_tuning.sh" ]]; then
+  ./scripts/dgx/nccl_tuning.sh --out "$OUT_DIR" --gpus-per-node 8 || true
+else
+  echo "nccl_tuning.sh not found; cannot run tuning." ; exit 2
+fi
+
+ENV_SUGGEST="$OUT_DIR/env_suggest.sh"
+if [[ -x "./scripts/dgx/generate_nccl_env_suggest.sh" ]]; then
+  ./scripts/dgx/generate_nccl_env_suggest.sh --nccl-output "$OUT_DIR/all_reduce_perf.txt" --out "$ENV_SUGGEST" || true
+else
+  echo "generate_nccl_env_suggest.sh not found; skipping generation of env suggestions"
+fi
+
+if [[ -f "$ENV_SUGGEST" ]]; then
+  echo "Applying suggested NCCL envs to ConfigMap"
+  chmod +x ./scripts/dgx/apply_nccl_config.sh
+  ./scripts/dgx/apply_nccl_config.sh --env-file "$ENV_SUGGEST" --namespace aegis-ml || true
+else
+  echo "No env_suggest produced; skipping ConfigMap apply"
+fi
+
+if [[ -f "$JOB_MANIFEST" ]]; then
+  echo "Patching job manifest $JOB_MANIFEST to reference dgx-nccl-config (envFrom)"
+  # If the manifest already references envFrom, skip. Otherwise make sure the job includes envFrom reference.
+  if grep -q "envFrom:" "$JOB_MANIFEST"; then
+    echo "Job manifest already references envFrom; verify it points to dgx-nccl-config"
+  else
+    # Insert envFrom under container spec; a simple sed-based insertion after 'env:' placeholder if present
+    awk '
+      BEGIN { inserted=0 }
+      {
+        print $0
+        if ($0 ~ /env:/ && inserted==0) {
+          print "          envFrom:"
+          print "            - configMapRef:"
+          print "                name: dgx-nccl-config"
+          inserted=1
+        }
+      }' "$JOB_MANIFEST" > "$JOB_MANIFEST.tmp" && mv "$JOB_MANIFEST.tmp" "$JOB_MANIFEST"
+    echo "Updated $JOB_MANIFEST"
+  fi
+else
+  echo "Job manifest not found: $JOB_MANIFEST; skipping patch"
+fi
+
+echo "Done. Please review the ConfigMap and patched job manifest prior to running jobs."
+exit 0
+
diff --git a/k8s/manifests/dgx/checkpoint-offloader-cronjob.yaml b/k8s/manifests/dgx/checkpoint-offloader-cronjob.yaml
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/k8s/manifests/dgx/checkpoint-offloader-cronjob.yaml
@@ -0,0 +1,220 @@
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: dgx-checkpoint-offloader
+  namespace: aegis-ml
+spec:
+  schedule: "0 */6 * * *"  # every 6 hours
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          restartPolicy: OnFailure
+          containers:
+            - name: offloader
+              image: amazon/aws-cli:latest
+              command:
+                - /bin/sh
+                - -c
+                - |
+                  set -euo pipefail
+                  # sync PVC contents to S3 (requires AWS envvars in the CronJob or a IRSA/service account)
+                  SRC_DIR=/checkpoints
+                  DEST=s3://your-bucket/dgx-checkpoints/\$(date -u +%Y%m%d%H%M%S)/
+                  if [ -d "\$SRC_DIR" ]; then
+                    aws s3 sync "\$SRC_DIR" "\$DEST" --quiet || true
+                  else
+                    echo "No checkpoints dir mounted"
+                  fi
+              env:
+                - name: AWS_REGION
+                  value: "us-west-2"
+              volumeMounts:
+                - name: checkpoints
+                  mountPath: /checkpoints
+          volumes:
+            - name: checkpoints
+              persistentVolumeClaim:
+                claimName: dgx-checkpoints-pvc
+
+# Notes:
+# - Configure RBAC/service account and secrets for aws credentials (or use IRSA/GSA)
+# - Adjust DEST S3 bucket and region to your environment.
+
diff --git a/scripts/dgx/create_offloader_cronjob.sh b/scripts/dgx/create_offloader_cronjob.sh
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/scripts/dgx/create_offloader_cronjob.sh
@@ -0,0 +1,160 @@
+#!/usr/bin/env bash
+#
+# Create the checkpoint offloader cronjob and configure required secrets.
+#
+# Usage:
+#   ./scripts/dgx/create_offloader_cronjob.sh --s3-bucket my-bucket --namespace aegis-ml
+
+set -euo pipefail
+
+S3_BUCKET=""
+NAMESPACE="${NAMESPACE:-aegis-ml}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --s3-bucket) S3_BUCKET="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$S3_BUCKET" ]]; then
+  echo "--s3-bucket is required"
+  exit 2
+fi
+
+echo "Ensure you have a secret with AWS credentials named aws-credentials in namespace $NAMESPACE"
+echo "Create secret example:"
+echo "  kubectl create secret generic aws-credentials --from-literal=AWS_ACCESS_KEY_ID=... --from-literal=AWS_SECRET_ACCESS_KEY=... -n $NAMESPACE"
+
+kubectl apply -f k8s/manifests/dgx/checkpoint-offloader-cronjob.yaml
+echo "CronJob applied. Configure proper AWS creds / service account for production use."
+exit 0
+
diff --git a/.github/workflows/dgx_release_and_validate.yml b/.github/workflows/dgx_release_and_validate.yml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/.github/workflows/dgx_release_and_validate.yml
@@ -0,0 +1,260 @@
+name: DGX Release -> Build, Sign, Validate & Promote
+
+on:
+  workflow_dispatch:
+    inputs:
+      target:
+        description: "Target hardware profile (h100|a100)"
+        required: true
+        default: "h100"
+      image_tag:
+        description: "Image tag to produce (e.g., aegis-deepspeed:h100-cuda12.1-pytorch2.2)"
+        required: true
+        default: "aegis-deepspeed:h100-cuda12.1-pytorch2.2"
+
+jobs:
+  build-and-publish:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Set up tools
+        run: |
+          sudo apt-get update
+          sudo apt-get install -y jq docker.io
+          pip install --user yq
+          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin || true
+          curl -sSfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin || true
+          curl -sSfL https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64 -o /usr/local/bin/cosign || true
+          chmod +x /usr/local/bin/cosign || true
+
+      - name: Build/Sign/Register image
+        env:
+          REGISTRY: ${{ secrets.DGX_IMAGE_REGISTRY }}
+          TARGET: ${{ github.event.inputs.target }}
+          IMAGE_TAG: ${{ github.event.inputs.image_tag }}
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }}
+        run: |
+          chmod +x scripts/dgx/publish_sign_register_image.sh
+          ./scripts/dgx/publish_sign_register_image.sh
+
+      - name: Trigger production validation workflow
+        env:
+          IMAGE: ${{ github.event.inputs.image_tag }}
+        run: |
+          echo "Triggering dgx_production_validation workflow via repository_dispatch"
+          # Requires repo-level token (GITHUB_TOKEN may have limited scopes). Use gh CLI or REST API with PAT in secrets.PROD_VALIDATOR_TOKEN
+          if command -v gh >/dev/null 2>&1 && [ -n "${{ secrets.PROD_VALIDATOR_TOKEN }}" ]; then
+            echo "${{ secrets.PROD_VALIDATOR_TOKEN }}" | gh auth login --with-token
+            gh workflow run dgx_production_validation.yml -f image="${IMAGE}"
+            echo "Triggered validation workflow (dgx_production_validation.yml)."
+          else
+            echo "Please trigger dgx_production_validation.yml manually with image=${IMAGE}"
+          fi
+
+  wait-for-validation:
+    needs: build-and-publish
+    runs-on: ubuntu-latest
+    steps:
+      - name: Wait (placeholder) and instruct user
+        run: |
+          echo "This job waits for the production validation run to complete."
+          echo "In a fully integrated setup, this step would poll the validation workflow run and verify artifacts."
+          echo "After validation succeeds, run scripts/dgx/promote_image_if_validated.sh to finalize promotion."
+
+  promotion:
+    needs: wait-for-validation
+    runs-on: ubuntu-latest
+    steps:
+      - name: Promote image if validated
+        run: |
+          echo "Promotion step: run manual or scripted promotion after validation artifacts are confirmed."
+          echo "See scripts/dgx/promote_image_if_validated.sh for automated checks against docs/dgx/IMAGE_MATRIX.md"
+
+# Note:
+# - This workflow builds & registers the image and triggers a separate production validation workflow.
+# - For security, production validation should run on self-hosted DGX runner and post back artifacts; only after success should images be promoted to production manifests.
+
diff --git a/scripts/dgx/promote_image_if_validated.sh b/scripts/dgx/promote_image_if_validated.sh
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/scripts/dgx/promote_image_if_validated.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Promote an image to production by verifying it has:
+#  - a signed cosign signature (or keyless verification)
+#  - an SBOM entry present in docs/dgx/IMAGE_MATRIX.md
+#  - a successful validation artifact (presence check in artifacts path)
+#
+# Usage:
+#  ./scripts/dgx/promote_image_if_validated.sh --image ghcr.io/org/aegis-deepspeed:h100-... --artifact-path ./artifacts/dgx_prod_validate
+
+set -euo pipefail
+
+IMAGE=""
+ARTIFACT_PATH=""
+IMAGE_MATRIX_FILE="${IMAGE_MATRIX_FILE:-docs/dgx/IMAGE_MATRIX.md}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --image) IMAGE="$2"; shift 2;;
+    --artifact-path) ARTIFACT_PATH="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$IMAGE" || -z "$ARTIFACT_PATH" ]]; then
+  echo "--image and --artifact-path are required"
+  exit 2
+fi
+
+echo "Verifying cosign signature for $IMAGE"
+if command -v cosign >/dev/null 2>&1; then
+  if cosign verify "$IMAGE" > /dev/null 2>&1; then
+    echo "cosign verification passed"
+  else
+    echo "cosign verify failed; aborting promotion" ; exit 3
+  fi
+else
+  echo "cosign not available; cannot verify signature; aborting" ; exit 4
+fi
+
+echo "Checking IMAGE_MATRIX for SBOM/digest entry"
+if ! grep -q "$IMAGE" "$IMAGE_MATRIX_FILE"; then
+  echo "Image not recorded in $IMAGE_MATRIX_FILE; aborting promotion" ; exit 5
+fi
+
+echo "Checking for validation artifacts under $ARTIFACT_PATH"
+if [[ ! -d "$ARTIFACT_PATH" || -z "$(ls -A "$ARTIFACT_PATH")" ]]; then
+  echo "No validation artifacts found in $ARTIFACT_PATH; aborting" ; exit 6
+fi
+
+echo "All checks passed. Promotion may proceed."
+echo "Production manifests should reference the image digest from $IMAGE_MATRIX_FILE (not just the tag)."
+exit 0
+
diff --git a/scripts/dgx/cordon_and_evacuate.sh b/scripts/dgx/cordon_and_evacuate.sh
new file mode 100644
index 0000000..9999999
--- /dev/null
+++ b/scripts/dgx/cordon_and_evacuate.sh
@@ -0,0 +1,220 @@
+#!/usr/bin/env bash
+#
+# Script to cordon a DGX node and drain/evacuate running pods safely.
+#
+# Usage:
+#   ./scripts/dgx/cordon_and_evacuate.sh --node <node-name> [--namespace aegis-ml] [--grace-period 120]
+
+set -euo pipefail
+
+NODE=""
+NAMESPACE="${NAMESPACE:-}"
+GRACE="${GRACE:-120}"
+FORCE_DRAIN="${FORCE_DRAIN:-false}"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --node) NODE="$2"; shift 2;;
+    --namespace) NAMESPACE="$2"; shift 2;;
+    --grace-period) GRACE="$2"; shift 2;;
+    --force) FORCE_DRAIN="true"; shift 1;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$NODE" ]]; then
+  echo "--node is required"; exit 2
+fi
+
+echo "Cordoning node $NODE"
+kubectl cordon "$NODE"
+
+echo "Draining node $NODE"
+drain_cmd=(kubectl drain "$NODE" --ignore-daemonsets --delete-local-data --grace-period="$GRACE")
+if [[ -n "$NAMESPACE" ]]; then
+  drain_cmd+=(--namespace "$NAMESPACE")
+fi
+if [[ "$FORCE_DRAIN" == "true" ]]; then
+  drain_cmd+=(--force)
+fi
+
+echo "Running: ${drain_cmd[*]}"
+"${drain_cmd[@]}" || echo "Drain command returned non-zero; review pods and retry with --force if appropriate"
+
+echo "Node cordon & drain initiated. Check pods with: kubectl get pods -o wide --all-namespaces"
+exit 0
+
diff --git a/k8s/manifests/monitoring/alertmanager-route-dgx.yaml b/k8s/manifests/monitoring/alertmanager-route-dgx.yaml
new file mode 100644
index 0000000..aaaaaaaa
--- /dev/null
+++ b/k8s/manifests/monitoring/alertmanager-route-dgx.yaml
@@ -0,0 +1,220 @@
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: alertmanager-dgx-route
+  namespace: monitoring
+data:
+  alertmanager.yml: |
+    route:
+      receiver: 'pagerduty'
+      group_by: ['alertname', 'severity']
+      group_wait: 10s
+      group_interval: 5m
+      repeat_interval: 1h
+      routes:
+        - match:
+            severity: critical
+            alertname: DGX_GPU_ECC_Error
+          receiver: 'pagerduty-critical'
+        - match:
+            alertname: DGX_NCCL_Timeouts
+          receiver: 'pagerduty-critical'
+        - match:
+            alertname: DGX_NVLINK_Errors
+          receiver: 'pagerduty'
+    receivers:
+      - name: 'pagerduty'
+        pagerduty_configs:
+          - service_key: '{{ .Values.pagerduty.key }}'
+      - name: 'pagerduty-critical'
+        pagerduty_configs:
+          - service_key: '{{ .Values.pagerduty.critical_key }}'
+
+# Note:
+# - Customize receiver configs per your Alertmanager setup (PagerDuty, OpsGenie, Slack).
+# - Apply this ConfigMap as part of your Alertmanager deployment config or use the provider-specific CR.
+
diff --git a/docs/dgx/PRODUCTION_PROMOTION.md b/docs/dgx/PRODUCTION_PROMOTION.md
new file mode 100644
index 0000000..bbbbbbbb
--- /dev/null
+++ b/docs/dgx/PRODUCTION_PROMOTION.md
@@ -0,0 +1,240 @@
+# DGX Production Promotion Policy (how images become "production")
+
+This document describes the required checks and artifacts before an image may be promoted for production DeepSpeed runs on DGX.
+
+Required artifacts:
+- cosign signature verification (key-based or keyless)  
+- SBOM present in CI artifacts (syft output)  
+- Vulnerability scan (Trivy) shows no HIGH/CRITICAL findings or has documented mitigating notes  
+- Validation artifacts from nightlies or explicit production validation runs (logs, NCCL tuning outputs, job run logs)  
+- Entry recorded in docs/dgx/IMAGE_MATRIX.md including image digest
+
+Promotion flow:
+1. Build & publish image; sign and produce SBOM (scripts/dgx/publish_sign_register_image.sh).  
+2. Trigger production validation workflow (.github/workflows/dgx_production_validation.yml) which runs on a self-hosted DGX runner, collects artifacts and writes them to artifacts/ path.  
+3. After validation completes and artifacts appear, run scripts/dgx/promote_image_if_validated.sh with --image and --artifact-path to enforce checks and finalize promotion.  
+4. Production manifests MUST reference the image digest recorded in IMAGE_MATRIX.md (not loose tags).
+
+Security & process:
+- Only authorized platform engineers may run the release workflow. Use GitHub Environments and required reviewers to gate runs that touch secret KUBECONFIG_DGX and COSIGN_KEY.  
+- Rotate COSIGN_KEY and kubeconfigs periodically. Use keyless sign where practicable with valid OIDC.
+
+Failure handling:
+- If validation fails, update IMAGE_MATRIX.md with failure notes and do not promote. Investigate NCCL/driver/storage issues via runbooks and iterate.
+
diff --git a/docs/dgx/CI_and_SECRET_SETUP.md b/docs/dgx/CI_and_SECRET_SETUP.md
new file mode 100644
index 0000000..cccccccc
--- /dev/null
+++ b/docs/dgx/CI_and_SECRET_SETUP.md
@@ -0,0 +1,240 @@
+# Setup CI secrets and self-hosted runners for DGX validation pipelines
+
+Required secrets (Repository or Organization level)
+- KUBECONFIG_DGX: the raw kubeconfig YAML (or base64-encoded content) for DGX cluster. Prefer short-lived tokens where possible.  
+- DGX_IMAGE_REGISTRY: container registry host string (e.g., ghcr.io/org)  
+- DGX_REGISTRY_USER, DGX_REGISTRY_PASS: optional registry credentials (if required)  
+- COSIGN_KEY: cosign private key (if using key-based signing). Consider using environment-protected secrets.  
+- PROD_VALIDATOR_TOKEN: PAT or token used to trigger production validation workflow remotely (if needed)
+
+Self-hosted runners
+- Provision self-hosted runners with label 'dgx' on a secure host that can access the DGX control plane (or run the runner on a secure bastion).  
+- Limit runner usage to the DGX workflows via GitHub Environments and required reviewers.
+
+Permissions & best practices
+- Use GitHub Environments to require at least one approver for release workflows that use COSIGN_KEY or KUBECONFIG_DGX.  
+- Use least-privilege service accounts for kubeconfig (only allow necessary operations).  
+- Rotate tokens and audit usage regularly.
+
+Example: how to add KUBECONFIG_DGX secret
+1. Go to repo -> Settings -> Secrets -> Actions -> New repository secret.  
+2. Name: KUBECONFIG_DGX. Paste raw kubeconfig YAML (or base64 content).  
+3. Protect with environment rules or repository access controls.
+
+End of doc.
+
diff --git a/docs/runbooks/dgx_sre_playbooks.md b/docs/runbooks/dgx_sre_playbooks.md
new file mode 100644
index 0000000..dddddddd
--- /dev/null
+++ b/docs/runbooks/dgx_sre_playbooks.md
@@ -0,0 +1,240 @@
+# DGX SRE Playbooks (cordon/evacuate, offload checkpoints, escalate)
+
+Playbook: ECC Error (DGX_GPU_ECC_Error)
+1. Acknowledge Alert and gather node name(s) from alert payload.  
+2. Run:
+   ./scripts/dgx/cordon_and_evacuate.sh --node <node> --grace-period 120
+3. Collect diagnostics:
+   kubectl -n monitoring logs -l app=dcgm-exporter --tail=200
+   kubectl -n aegis-ml exec <pod> -- nvidia-smi -q > /tmp/nvidia-smi-q.txt
+4. Open vendor hardware ticket and mark node for replacement.
+
+Playbook: NCCL timeouts or errors
+1. Run nccl tests:
+   ./scripts/dgx/nccl_tuning.sh --nodes 1 --gpus-per-node 8 --out ./artifacts/nccl
+2. Generate env suggestions:
+   ./scripts/dgx/generate_nccl_env_suggest.sh --nccl-output ./artifacts/nccl/all_reduce_perf.txt --out ./artifacts/nccl/env_suggest.sh
+3. Apply suggested env:
+   ./scripts/dgx/apply_nccl_config.sh --env-file ./artifacts/nccl/env_suggest.sh
+4. Re-run smoke tests and multi-node scaling tests.
+
+Playbook: Checkpoint PVC near capacity
+1. Offload checkpoints to S3:
+   kubectl create -n aegis-ml secret generic aws-credentials --from-literal=AWS_ACCESS_KEY_ID=... --from-literal=AWS_SECRET_ACCESS_KEY=...
+2. Ensure checkpoint-offloader CronJob exists:
+   ./scripts/dgx/create_offloader_cronjob.sh --s3-bucket my-bucket
+3. Clean up or increase PVC size if local storage exhausted.
+
+Contact: platform-team@example.com (on-call rotation)
+
