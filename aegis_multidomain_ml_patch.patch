*** Begin Patch
*** Add File:ml/nlp/rag/train_finetune.py
+#!/usr/bin/env python3
+"""
+Simple Hugging Face fine-tune script (text2text) for a RAG-style reader model.
+This is a minimal example using transformers Trainer for demo purposes.
+"""
+import os
+from datasets import load_dataset
+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
+
+MODEL_NAME = os.environ.get("BASE_MODEL", "t5-small")
+OUT_DIR = os.environ.get("OUT_DIR", "out/model")
+
+def main():
+    ds = load_dataset("squad", split="train[:1%]")  # tiny sample for demo
+    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
+    def preprocess(ex):
+        inputs = ["question: " + q + " context: " + c for q,c in zip(ex["question"], ex["context"])]
+        model_in = tokenizer(inputs, truncation=True, padding="max_length", max_length=256)
+        labels = tokenizer(ex["answers"]["text"], truncation=True, padding="max_length", max_length=64)
+        model_in["labels"] = labels["input_ids"]
+        return model_in
+    ds = ds.map(preprocess, batched=True, remove_columns=ds.column_names)
+    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
+    args = Seq2SeqTrainingArguments(output_dir=OUT_DIR, per_device_train_batch_size=4, num_train_epochs=1, logging_steps=10)
+    trainer = Seq2SeqTrainer(model=model, args=args, train_dataset=ds)
+    trainer.train()
+    trainer.save_model(OUT_DIR)
+    print("Saved model to", OUT_DIR)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/nlp/rag/argo_train_rag.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rag-train-
+  namespace: aegis
+spec:
+  entrypoint: train-rag
+  templates:
+    - name: train-rag
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install datasets transformers[sentencepiece] accelerate; python3 ml/nlp/rag/train_finetune.py
+
*** End Patch
*** Begin Patch
*** Add File:ml/nlp/rag/serve_fastapi/app.py
+#!/usr/bin/env python3
+"""
+Minimal FastAPI reader service that loads fine-tuned HF model and answers queries.
+It demonstrates how to wire RAG reader into Aegis serving.
+"""
+import os
+from fastapi import FastAPI
+from pydantic import BaseModel
+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
+
+MODEL_DIR = os.environ.get("MODEL_DIR","out/model")
+
+app = FastAPI(title="Aegis RAG Reader")
+tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
+model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)
+
+class Query(BaseModel):
+    question: str
+    context: str = ""
+
+@app.post("/answer")
+def answer(q: Query):
+    inputs = tokenizer("question: " + q.question + " context: " + q.context, return_tensors="pt", truncation=True, padding=True)
+    out = model.generate(**inputs, max_length=128)
+    ans = tokenizer.decode(out[0], skip_special_tokens=True)
+    return {"answer": ans}
+
+@app.get("/health")
+def health():
+    return {"status":"ok"}
+
*** End Patch
*** Begin Patch
*** Add File:ml/nlp/rag/langchain_rag_example.py
+"""
+Minimal LangChain-style RAG orchestration using HF embeddings + Milvus retrieval (conceptual).
+This is a small runner showing how to stitch retriever + reader.
+"""
+from sentence_transformers import SentenceTransformer
+import requests, os
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST","milvus.milvus.svc")
+EMBED_MODEL = os.environ.get("EMBED_MODEL","sentence-transformers/all-MiniLM-L6-v2")
+READER_URL = os.environ.get("READER_URL","http://rag-reader.aegis.svc:8080/answer")
+
+def embed(texts):
+    m = SentenceTransformer(EMBED_MODEL)
+    return m.encode(texts).tolist()
+
+def retrieve(query_embedding):
+    # placeholder: call Milvus or vector DB to retrieve docs
+    return [{"id":"doc1","text":"This is a short context about Aegis."}]
+
+def run(query):
+    emb = embed([query])[0]
+    docs = retrieve(emb)
+    context = " ".join(d["text"] for d in docs)
+    r = requests.post(READER_URL, json={"question": query, "context": context}, timeout=10)
+    return r.json()
+
+if __name__=="__main__":
+    print(run("What is Aegis?"))
+
*** End Patch
*** Begin Patch
*** Add File:ml/cv/train_detectron2/sample_train.py
+#!/usr/bin/env python3
+"""
+Minimal Detectron2 training stub for object detection (demo-level).
+Real training requires proper dataset registration and GPU nodes.
+"""
+import os
+def main():
+    print("This is a stub training script for CV models (Detectron2/YOLO).")
+    print("In production, register COCO-like dataset, configure train.yaml, and run detectron2 training.")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/cv/argo_train_cv.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: cv-train-
+  namespace: aegis
+spec:
+  entrypoint: train-cv
+  templates:
+    - name: train-cv
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu116/torch1.13/index.html || true; python3 ml/cv/train_detectron2/sample_train.py
+
*** End Patch
*** Begin Patch
*** Add File:ml/cv/serving/triton/model_config.md
+# Example: Triton model config should be generated for your CV model artifacts.
+# This README is a placeholder. For production, convert your model to ONNX/TorchScript and create the correct config.pbtxt.
+
*** End Patch
*** Begin Patch
*** Add File:ml/patterns/stream/consumer.py
+#!/usr/bin/env python3
+"""
+Stream consumer that writes telemetry to MongoDB and triggers anomaly detection training periodically.
+This is a simple Kafka consumer stub.
+"""
+import os, json, time
+from pymongo import MongoClient
+
+MONGO = os.environ.get("MONGO_URI","mongodb://localhost:27017")
+
+def main():
+    client = MongoClient(MONGO)
+    db = client['aegis']
+    # In production, subscribe to Kafka and insert telemetry docs.
+    for i in range(10):
+        db.telemetry.insert_one({"value": i, "ts": time.time()})
+    print("Inserted sample telemetry")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/patterns/autoencoder/train_ae.py
+#!/usr/bin/env python3
+"""
+Minimal autoencoder training script for anomaly detection on time series / telemetry.
+This is a conceptual stub; replace with real model/architectures.
+"""
+import os, numpy as np
+def main():
+    X = np.random.randn(100, 10)
+    # trivial "training"
+    mean = X.mean(axis=0)
+    np.save("out/ae_mean.npy", mean)
+    print("Saved model artifact out/ae_mean.npy")
+
+if __name__=="__main__":
+    os.makedirs("out", exist_ok=True)
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:infra/label-studio/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: label-studio
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: label-studio
+  template:
+    metadata:
+      labels:
+        app: label-studio
+    spec:
+      containers:
+        - name: label-studio
+          image: heartexlabs/label-studio:1.7.0
+          ports:
+            - containerPort: 8080
+          env:
+            - name: LABEL_STUDIO_USER_EMAIL
+              value: "admin@example.com"
+            - name: LABEL_STUDIO_USER_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-config
+                  key: LABEL_STUDIO_PASSWORD
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: label-studio
+  namespace: aegis
+spec:
+  selector:
+    app: label-studio
+  ports:
+    - port: 8080
+      targetPort: 8080
+
*** End Patch
*** Begin Patch
*** Add File:infra/milvus/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: milvus
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: milvus
+  template:
+    metadata:
+      labels:
+        app: milvus
+    spec:
+      containers:
+        - name: milvus
+          image: milvusdb/milvus:v2.2.9
+          ports:
+            - containerPort: 19530
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: milvus
+  namespace: aegis
+spec:
+  selector:
+    app: milvus
+  ports:
+    - port: 19530
+      targetPort: 19530
+
*** End Patch
*** Begin Patch
*** Add File:infra/feast/README.md
+# Lightweight Feast integration notes
+
+This folder is a placeholder describing how to integrate Feast as a feature store.
+- Deploy a Redis or BigQuery backend for Feast
+- Register feature tables produced from preprocessing jobs
+- Connect Feast to your model training Argo workflows to read feature sets by entity_id and timestamp
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/drift/drift_detector.py
+#!/usr/bin/env python3
+"""
+Simple drift detector that computes distributional change on embeddings or features stored in MongoDB.
+Writes drift events to MongoDB 'drift' collection for KPI exporter to pick up.
+"""
+import os, time
+from pymongo import MongoClient
+import numpy as np
+
+MONGO = os.environ.get("MONGO_URI","mongodb://localhost:27017")
+
+def main():
+    client = MongoClient(MONGO)
+    db = client['aegis']
+    # placeholder: compute drift using simple statistic differences
+    features = list(db.features.find().limit(100))
+    if len(features) < 10:
+        print("Not enough features to compute drift")
+        return
+    vals = [f.get("vec",[0])[0] for f in features if f.get("vec")]
+    if not vals:
+        return
+    mean = float(sum(vals)/len(vals))
+    # simple threshold drift
+    baseline = db.meta.find_one({"name":"drift_baseline"}) or {"value": mean}
+    if abs(mean - baseline["value"]) > 0.5:
+        db.drift.insert_one({"flagged":True,"value":mean,"baseline":baseline["value"],"ts":time.time()})
+        print("Drift detected")
+    else:
+        print("No drift")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/drift/argo_drift_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: drift-check-
+  namespace: aegis
+spec:
+  entrypoint: drift-check
+  templates:
+    - name: drift-check
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pymongo numpy || true; python3 monitoring/drift/drift_detector.py
+
*** End Patch
*** Begin Patch
*** Add File:dashboards/grafana/domain_metrics.json
+{
+  "dashboard": {
+    "id": null,
+    "title": "Aegis Domain Metrics",
+    "panels": [
+      {"type":"stat","title":"NLP: RAG queries / min","targets":[{"expr":"aegis_rag_queries_total"}],"gridPos":{"x":0,"y":0,"w":6,"h":3}},
+      {"type":"stat","title":"CV: mAP (placeholder)","targets":[{"expr":"aegis_cv_map"}],"gridPos":{"x":6,"y":0,"w":6,"h":3}},
+      {"type":"stat","title":"Pattern: Anomalies (24h)","targets":[{"expr":"aegis_anomalies_24h"}],"gridPos":{"x":12,"y":0,"w":6,"h":3}},
+      {"type":"stat","title":"Drift rate","targets":[{"expr":"aegis_model_drift_rate"}],"gridPos":{"x":18,"y":0,"w":6,"h":3}}
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:demo/compact_e2e/nlp_rag_demo/README.md
+Compact E2E Demo: RAG NLP pipeline using HF hosted inference and Milvus for retrieval (local/demo).
+
+Steps:
+1. Start Milvus (infra/milvus/deployment.yaml) and Label Studio (infra/label-studio/deployment.yaml) if desired.
+2. Run demo ingestion to populate a few documents and embeddings (demo script below).
+3. Run the reader service: python3 ml/nlp/rag/serve_fastapi/app.py
+4. Run the small LangChain runner: python3 ml/nlp/rag/langchain_rag_example.py
+5. Use scripts/evidence/attach_and_sign.py to bundle manifest and evidence, and upload to EVIDENCE_BUCKET.
+
+This demo demonstrates lineage & evidence for an NLP RAG pipeline.
+
*** End Patch
*** Begin Patch
*** Add File:demo/compact_e2e/demo_ingest.py
+#!/usr/bin/env python3
+"""
+Populate simple documents and embeddings into MongoDB and Milvus stub for demo.
+"""
+import os, json
+from pymongo import MongoClient
+
+MONGO = os.environ.get("MONGO_URI", "mongodb://localhost:27017")
+
+def main():
+    client = MongoClient(MONGO)
+    db = client['aegis']
+    docs = [
+        {"id":"d1","text":"Aegis is a secure auditable AI platform."},
+        {"id":"d2","text":"Aegis supports NLP, CV and pattern recognition demos."}
+    ]
+    db.documents.delete_many({})
+    db.documents.insert_many(docs)
+    print("Inserted docs into MongoDB")
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/costs/token_budget_integration.md
+# Token Budget Integration (how to wire)
+
+1. Deploy the token-budget service (ops/token_budget/service.py) to aegis namespace.
+2. Configure LangGraph / RAG runner to call POST /check and POST /consume before HF API calls.
+3. Configure quotas per team in MongoDB via /quota endpoint.
+4. Alert when consumption approaches budget; integrate with billing/cost dashboards.
+
*** End Patch
*** Begin Patch
*** Add File:security/llm/redaction_util.py
+#!/usr/bin/env python3
+"""
+Simple PII redaction utilities for model traces. This is a conservative stub; replace with robust redaction rules.
+"""
+import re
+
+EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")
+PHONE_RE = re.compile(r"\+?\d[\d\-\s]{7,}\d")
+
+def redact_text(s):
+    s = EMAIL_RE.sub("[REDACTED_EMAIL]", s)
+    s = PHONE_RE.sub("[REDACTED_PHONE]", s)
+    if len(s) > 500:
+        return s[:300] + "...[REDACTED]"
+    return s
+
+if __name__=="__main__":
+    print(redact_text("Contact me at alice@example.com or +1 555-123-4567. Very long text..."*50))
+
*** End Patch
*** Begin Patch
*** Add File:compliance/adversarial_tests/README.md
+# Adversarial Testing for LLM/CV
+
+Use ml/sim/adversarial_generator.py (from previous patch) and feed adversarial prompts into digital twin and LangGraph test workflows.
+Record all results and signed evidence.
+
*** End Patch
*** Begin Patch
*** Add File:docs/ML_DOMAINS_OVERVIEW.md
+# Adding NLP, CV & Pattern Recognition to Aegis
+
+This document summarizes the artifacts included in this patch and how they map to Aegis platform components:
+- NLP / RAG: training script (HF), Argo training workflow, FastAPI reader service, LangChain-style runner example, compact demo.
+- CV: Detectron2 training stub and Argo workflow, Triton placeholder for serving.
+- Pattern recognition: telemetry consumer, autoencoder training stub, Argo drift workflow & detector.
+- Infra: Label Studio, Milvus (vector DB) manifests, Feast notes.
+- Monitoring & governance: drift detector, Grafana panels, token budget integration, PII redaction utils, compliance/adversarial testing notes.
+
+Next steps to productionize:
+1. Replace stubs with real training scripts, dataset registrations, and GPU-backed Argo runners.
+2. Wire Milvus (or managed vector DB) and Feast for feature retrieval.
+3. Add SBOM/SCA generation & attach_and_sign integration to all training & promotion workflows (use scripts/evidence/attach_and_sign.py).
+4. Wire Vault for HF keys and use token-budget service to enforce HF usage.
+5. Run digital twin & adversarial campaigns and register signed evidence for all runs.
+
*** End Patch
*** End Patch
