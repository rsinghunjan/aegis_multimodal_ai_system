*** Begin Patch
*** Add File: mlops/lakefs/helm-values-production.yaml
+---
+# LakeFS Helm values (production example)
+service:
+  type: ClusterIP
+
+storage:
+  type: s3
+  s3:
+    region: "{{AWS_REGION}}"
+    bucket: "{{MLFLOW_ARTIFACT_BUCKET}}"
+    endpoint: "{{MLFLOW_S3_ENDPOINT}}"
+
+server:
+  replicas: 2
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "1"
+      memory: "2Gi"
+
+database:
+  postgres:
+    host: postgres-mlflow.aegis-ml.svc.cluster.local
+    user: lakefs
+    password: "{{LAKEFS_DB_PASSWORD}}"
+    dbname: lakefs
+
+auth:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/lakefs_commit_and_tag.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# lakefs_commit_and_tag.sh
+# 1) Creates a lakeFS commit for the dataset path
+# 2) Writes the commit id to stdout and to /tmp/snapshot_id (for Argo)
+# 3) Optionally tags the running MLflow run with the snapshot id
+#
+# Required env:
+# - LAKEFS_BRANCH, LAKEFS_REPO, LAKEFS_PATH, LAKEFS_API_URL, LAKEFS_ACCESS_KEY, LAKEFS_SECRET_KEY
+# - If tagging MLflow: MLFLOW_TRACKING_URI and MLFLOW_RUN_ID
+
+LAKEFS_API="${LAKEFS_API_URL:-http://lakefs:8000/api/v1}"
+REPO="${LAKEFS_REPO:-aegis}"
+BRANCH="${LAKEFS_BRANCH:-main}"
+PATH_TO_ADD="${LAKEFS_PATH:-/data/train}"
+COMMIT_MSG="${LAKEFS_COMMIT_MESSAGE:-Aegis dataset snapshot}"
+
+if ! command -v curl >/dev/null 2>&1; then
+  echo "curl required"
+  exit 1
+fi
+
+echo "Creating LakeFS commit for ${REPO}:${BRANCH}${PATH_TO_ADD}"
+# Create a commit that records current prefix state (this is a simplified example)
+RESP=$(curl -s -u "${LAKEFS_ACCESS_KEY}:${LAKEFS_SECRET_KEY}" -X POST \
+  -H "Content-Type: application/json" \
+  -d "{\"message\":\"${COMMIT_MSG}\",\"branch\":\"${BRANCH}\"}" \
+  "${LAKEFS_API}/repositories/${REPO}/commits")
+
+COMMIT_ID=$(echo "$RESP" | jq -r '.commit_id // empty')
+if [ -z "$COMMIT_ID" ]; then
+  echo "Failed to create commit: $RESP"
+  exit 2
+fi
+
+echo "$COMMIT_ID" | tee /tmp/snapshot_id
+echo "LakeFS commit created: $COMMIT_ID"
+
+if [ -n "${MLFLOW_RUN_ID:-}" ] && command -v python3 >/dev/null 2>&1; then
+  echo "Tagging MLflow run ${MLFLOW_RUN_ID} with lakefs.commit=${COMMIT_ID}"
+  python3 - <<PY
+import os, mlflow
+client = mlflow.tracking.MlflowClient()
+run_id = os.environ.get("MLFLOW_RUN_ID")
+if run_id:
+    client.set_tag(run_id, "lakefs.commit", "$COMMIT_ID")
+    print("Tagged run", run_id)
+else:
+    print("No MLFLOW_RUN_ID provided")
+PY
+fi
+
*** End Patch
*** Begin Patch
*** Add File: mlops/argo/train_pipeline_prod_with_lakefs.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-ml-train-prod-
+spec:
+  entrypoint: train-validate-register
+  serviceAccountName: aegis-ml-operator
+  templates:
+  - name: train-validate-register
+    outputs:
+      parameters:
+      - name: snapshot_id
+      - name: run_id
+    steps:
+    - - name: snapshot-data
+        template: snapshot-data
+    - - name: train
+        template: train
+    - - name: validate
+        template: validate
+    - - name: register
+        template: register
+
+  - name: snapshot-data
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          ./scripts/lakefs_commit_and_tag.sh
+          # lakefs_commit_and_tag.sh writes snapshot id to /tmp/snapshot_id
+          cat /tmp/snapshot_id
+    outputs:
+      parameters:
+      - name: snapshot_id
+        valueFrom:
+          path: /tmp/snapshot_id
+
+  - name: train
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID=$(python3 scripts/train_and_register.py --mode train-only | tail -n1)
+          echo "$RUN_ID" > /tmp/run_id
+          echo "TRAIN_RUN_ID=$RUN_ID"
+    outputs:
+      parameters:
+      - name: run_id
+        valueFrom:
+          path: /tmp/run_id
+    resources:
+      requests:
+        cpu: "4"
+        memory: "16Gi"
+        nvidia.com/gpu: 1
+      limits:
+        cpu: "8"
+        memory: "32Gi"
+    nodeSelector:
+      aegis/gpu: "true"
+    tolerations:
+    - key: "nvidia.com/gpu"
+      operator: "Exists"
+
+  - name: validate
+    inputs:
+      parameters:
+      - name: run_id
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID="{{inputs.parameters.run_id}}"
+          python3 mlops/scripts/validate_model.py --run-id $RUN_ID --min-accuracy 0.80
+
+  - name: register
+    inputs:
+      parameters:
+      - name: run_id
+    container:
+      image: ghcr.io/${{ github.repository_owner }}/aegis-ml-trainer:latest
+      command: ["/bin/sh", "-c"]
+      args:
+        - |
+          set -e
+          RUN_ID="{{inputs.parameters.run_id}}"
+          python3 scripts/train_and_register.py --mode register --run-id $RUN_ID
+
+  volumes:
+  - name: work
+    emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/argo_wait_and_promote_enhanced.yml
+name: Argo Wait, Validate & Auto-Promote
+
+on:
+  workflow_dispatch:
+    inputs:
+      workflow_prefix:
+        description: "Argoworkflow name prefix"
+        required: true
+      kubeconfig_secret:
+        description: "Name of secret containing base64 KUBECONFIG"
+        required: true
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  wait-validate-promote:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: 3.10
+
+      - name: Restore kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG_BASE64 }}" | base64 --decode > /tmp/kubeconfig
+          export KUBECONFIG=/tmp/kubeconfig
+          kubectl config view --minify
+        env:
+          KUBECONFIG_BASE64: ${{ secrets.KUBECONFIG_BASE64 }}
+
+      - name: Install Argo CLI
+        run: |
+          curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo-workflows/releases/latest/download/argo-linux-amd64
+          chmod +x /usr/local/bin/argo
+
+      - name: Wait for workflow and fetch outputs
+        id: fetch
+        run: |
+          set -e
+          PREFIX="${{ github.event.inputs.workflow_prefix }}"
+          NAMESPACE="${{ secrets.K8S_NAMESPACE }}"
+          # Find latest workflow with prefix
+          WF=$(kubectl -n "$NAMESPACE" get wf -o name | grep "$PREFIX" | tail -n1 | sed 's|workflow.argoproj.io/||')
+          if [ -z "$WF" ]; then
+            echo "No workflow found matching prefix $PREFIX"
+            exit 1
+          fi
+          echo "Found workflow: $WF"
+          argo wait -n "$NAMESPACE" "$WF" --timeout 2h || (echo "workflow failed or timed out"; exit 1)
+          argo get -n "$NAMESPACE" "$WF" -o json > /tmp/argo_${WF}.json
+          SNAPSHOT_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="snapshot_id") .value' /tmp/argo_${WF}.json || echo "")
+          RUN_ID=$(jq -r '.status.outputs.parameters[]? | select(.name=="run_id") .value' /tmp/argo_${WF}.json || echo "")
+          echo "snapshot_id=$SNAPSHOT_ID" >> $GITHUB_OUTPUT
+          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
+
+      - name: Validate run (fetch MLflow artifacts and run validation)
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+          RUN_ID: ${{ steps.fetch.outputs.run_id }}
+        run: |
+          set -e
+          echo "Running validation for MLflow run $RUN_ID"
+          pip install -r mlops/requirements.txt
+          python3 mlops/scripts/validate_model.py --run-id "$RUN_ID" --min-accuracy 0.80
+
+      - name: Create promotion PR on success
+        if: success()
+        env:
+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        run: |
+          RUN_ID=${{ steps.fetch.outputs.run_id }}
+          gh pr create --title "Promote ML model run ${RUN_ID} to staging/canary" --body "Automated promotion PR for run ${RUN_ID}; snapshot ${ { steps.fetch.outputs.snapshot_id } }" || true
+
+      - name: Upload argo JSON
+        uses: actions/upload-artifact@v4
+        with:
+          name: argo-workflow-json
+          path: /tmp/argo_*.json
+
+# Note:
+# - Requires repo secrets: KUBECONFIG_BASE64, K8S_NAMESPACE, MLFLOW_TRACKING_URI, and GITHUB_TOKEN (repo token).
+# - Argo workflow must expose outputs parameters named 'run_id' and 'snapshot_id' as shown in train_pipeline_prod_with_lakefs.yaml
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/argo_fetch_and_validate.py
+#!/usr/bin/env python3
+"""
+argo_fetch_and_validate.py
+- Connects to Argo server (via KUBECONFIG), fetches workflow outputs and artifacts,
+  extracts MLflow run_id, runs validation, and optionally creates a promotion PR.
+"""
+import os
+import sys
+import json
+import subprocess
+import shlex
+
+def run(cmd):
+    print("RUN:", cmd)
+    return subprocess.check_output(cmd, shell=True).decode()
+
+def main():
+    if len(sys.argv) < 2:
+        print("Usage: argo_fetch_and_validate.py <workflow-name>")
+        sys.exit(2)
+    wf = sys.argv[1]
+    namespace = os.environ.get("ARGO_NAMESPACE", "aegis-ml")
+    out_json = f"/tmp/argo_{wf}.json"
+    run(f"argo get -n {namespace} {wf} -o json > {out_json}")
+    data = json.load(open(out_json))
+    params = data.get("status", {}).get("outputs", {}).get("parameters", [])
+    run_id = None
+    snapshot = None
+    for p in params:
+        if p.get("name") == "run_id":
+            run_id = p.get("value")
+        if p.get("name") == "snapshot_id":
+            snapshot = p.get("value")
+    if not run_id:
+        print("No run_id found in workflow outputs; exiting")
+        sys.exit(1)
+    print("Found run_id:", run_id)
+    # Run validation
+    rc = os.system(f"python3 mlops/scripts/validate_model.py --run-id {run_id} --min-accuracy 0.80")
+    if rc != 0:
+        print("Validation failed for run", run_id)
+        sys.exit(2)
+    print("Validation passed for run", run_id)
+    # Create promotion PR
+    title = f"Promote model run {run_id} to staging/canary"
+    body = f"Automated promotion PR for run {run_id}; snapshot {snapshot}"
+    try:
+        run(f'gh pr create --title "{title}" --body "{body}"')
+    except Exception as e:
+        print("Failed to create PR:", e)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: mlops/scripts/drift_alert_runner.py
+#!/usr/bin/env python3
+"""
+Drift detector runner:
+- Queries Prometheus for per-feature mean metrics
+- Compares with baseline stored in a JSON file (baseline.json)
+- Emits an alert (prints / webhooks) when drift exceeds threshold
+"""
+import os
+import sys
+import requests
+import json
+from math import fabs
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus.monitoring.svc.cluster.local")
+BASELINE_FILE = os.environ.get("DRIFT_BASELINE_FILE", "/etc/aegis/baseline.json")
+DRIFT_THRESHOLD = float(os.environ.get("DRIFT_THRESHOLD", "0.1"))
+SLACK_WEBHOOK = os.environ.get("DRIFT_SLACK_WEBHOOK", "")
+
+def query_prometheus(expr):
+    resp = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr})
+    resp.raise_for_status()
+    data = resp.json()
+    if data["status"] != "success":
+        return []
+    return data["data"]["result"]
+
+def load_baseline():
+    if not os.path.exists(BASELINE_FILE):
+        print("Baseline file not found:", BASELINE_FILE)
+        return {}
+    return json.load(open(BASELINE_FILE))
+
+def send_alert(msg):
+    print("ALERT:", msg)
+    if SLACK_WEBHOOK:
+        requests.post(SLACK_WEBHOOK, json={"text": msg})
+
+def main():
+    baseline = load_baseline()
+    # Example metric name: model_feature_mean{feature="f0"}
+    for feature, base_stats in baseline.get("features", {}).items():
+        expr = f'model_feature_mean{{feature="{feature}"}}'
+        results = query_prometheus(expr)
+        if not results:
+            continue
+        current = float(results[0]["value"][1])
+        base_mean = float(base_stats.get("mean", 0))
+        diff = abs(current - base_mean)
+        if base_mean > 0:
+            rel = diff / base_mean
+        else:
+            rel = diff
+        if rel > DRIFT_THRESHOLD:
+            send_alert(f"Feature {feature} drift detected: baseline={base_mean}, current={current}, rel={rel:.2f}")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/rules/production_drift_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-production-drift-alerts
+  namespace: monitoring
+spec:
+  groups:
+  - name: aegis-drift.rules
+    rules:
+    - alert: FeatureDriftHigh
+      expr: |
+        # Example: if absolute mean diff metric exceeds threshold; requires instrumentation to export ml_feature_mean_diff
+        avg_over_time(ml_feature_mean_diff[5m]) > 0.1
+      for: 10m
+      labels:
+        severity: warning
+      annotations:
+        summary: "High per-feature drift detected"
+        description: "Feature mean difference is high; check model inputs and data pipeline."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/drill_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# drill_restore_drill.sh
+# Automated DR drill:
+# 1) Find latest pg_dump in backup bucket
+# 2) Restore into an isolated namespace (aegis-ml-dr)
+# 3) Restore artifact backup to a test artifact bucket and point MLflow at that bucket
+# 4) Start MLflow in test namespace and run smoke validation
+#
+BACKUP_BUCKET="${MLFLOW_BACKUP_BUCKET:-}"
+REGION="${AWS_REGION:-us-west-2}"
+TEST_NS="aegis-ml-dr"
+
+if [ -z "$BACKUP_BUCKET" ]; then
+  echo "Set MLFLOW_BACKUP_BUCKET env var"
+  exit 2
+fi
+
+echo "Creating test namespace $TEST_NS"
+kubectl create ns "$TEST_NS" || true
+
+echo "Listing backups in s3://$BACKUP_BUCKET/backups/mlflow-db/"
+LATEST=$(aws s3 ls "s3://$BACKUP_BUCKET/backups/mlflow-db/" --region "$REGION" | sort | tail -n1 | awk '{print $4}')
+if [ -z "$LATEST" ]; then
+  echo "No backups found"
+  exit 1
+fi
+
+S3PATH="s3://$BACKUP_BUCKET/backups/mlflow-db/$LATEST"
+echo "Latest backup: $S3PATH"
+
+echo "Restoring Postgres to test cluster - using scripts/restore_mlflow_metadata.sh"
+./scripts/restore_mlflow_metadata.sh "$S3PATH"
+
+echo "Prepare test MLflow deployment pointing to restored DB and test artifacts (manual step: create secret aegis-mlflow-db-dr and aegis-cloud-creds-dr)"
+echo "Apply mlflow deployment in $TEST_NS and ensure it uses restored DB and artifact root."
+
+echo "Run smoke validation: attempt to load a model from restored registry and run inference"
+echo "This DR drill requires manual verification and signoff. Collect artifacts and record restore time."
+
*** End Patch
*** Begin Patch
*** Add File: mlops/k8s/oauth2-proxy-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: oauth2-proxy-config
+  namespace: aegis-ml
+data:
+  oauth2-proxy.cfg: |
+    provider = "oidc"
+    upstreams = ["http://mlflow:5000/"]
+    cookie-secret = "<use k8s secret>"
+    cookie-secure = true
+    cookie-same-site = "lax"
+    set-authorization-header = true
+    skip-provider-button = true
+
*** End Patch
*** Begin Patch
*** Add File: docs/finalize_lakefs_and_dr.md
+# Finalize LakeFS, Argo integration and DR drill
+
+Steps to finalize LakeFS:
+1. Install LakeFS using mlops/lakefs/values-production.yaml (helm install).
+2. Create LakeFS user/keys and store LAKEFS_ACCESS_KEY/LAKEFS_SECRET_KEY in k8s secret.
+3. Ensure Argo workflow (mlops/argo/train_pipeline_prod_with_lakefs.yaml) is applied and the snapshot step can reach LakeFS API.
+4. Confirm that lakefs_commit_and_tag.sh produces /tmp/snapshot_id and that the Argo outputs capture it.
+
+Argo CI wait & promote:
+- Use .github/workflows/argo_wait_and_promote_enhanced.yml to wait for a workflow, fetch run_id and snapshot_id, run validation, and create a promotion PR automatically.
+
+DR drill (how to run):
+1. Ensure backups exist in MLFLOW_BACKUP_BUCKET.
+2. Run scripts/drill_restore_drill.sh to restore metadata and artifacts into an isolated namespace.
+3. Start a temporary MLflow pointing at restored DB and artifacts. Run mlops/scripts/validate_model.py against a restored run to verify restore success.
+
+Drift detection:
+- Deploy mlops/scripts/drift_alert_runner.py as a CronJob or an external lambda that queries Prometheus.
+- Configure baseline file at /etc/aegis/baseline.json with feature means and set DRIFT_SLACK_WEBHOOK for alerts.
+
*** End Patch
*** End Patch
