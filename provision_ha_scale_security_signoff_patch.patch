*** Begin Patch
*** Add File: terraform/aws/infra_full.tf
+// Terraform: Full infra for AWS production-ready Aegis
+// - KMS key for cosign
+// - IAM role for GitHub OIDC
+// - RDS Postgres (Multi-AZ)
+// - ElastiCache Redis (replication)
+// - S3 compliance bucket (versioned, encryption)
+//
+// Usage:
+//  - create terraform.tfvars with aws_region, github_org, github_repo, oidc_provider_arn, oidc_provider_url, db_password, db_username
+//  - terraform init && terraform apply -auto-approve
+
+provider "aws" {
+  region = var.aws_region
+}
+
+variable "aws_region" { type = string }
+variable "github_org" { type = string }
+variable "github_repo" { type = string }
+variable "oidc_provider_arn" { type = string }
+variable "oidc_provider_url" { type = string }
+variable "db_username" { type = string }
+variable "db_password" { type = string }
+
+data "aws_iam_policy_document" "github_oidc_assume" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(var.oidc_provider_url, "https://", "")}:sub"
+      values   = ["repo:${var.github_org}/${var.github_repo}:ref:refs/heads/main"]
+    }
+  }
+}
+
+resource "aws_iam_role" "github_oidc_role" {
+  name               = "aegis-github-oidc-role"
+  assume_role_policy = data.aws_iam_policy_document.github_oidc_assume.json
+}
+
+resource "aws_kms_key" "cosign_key" {
+  description             = "KMS key for cosign signing"
+  deletion_window_in_days = 30
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/aegis-cosign"
+  target_key_id = aws_kms_key.cosign_key.key_id
+}
+
+resource "aws_iam_role_policy" "allow_kms_sign" {
+  name = "allow-kms-sign"
+  role = aws_iam_role.github_oidc_role.id
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Statement = [
+      {
+        Action = [
+          "kms:Sign",
+          "kms:GetPublicKey",
+          "kms:DescribeKey"
+        ]
+        Effect   = "Allow"
+        Resource = aws_kms_key.cosign_key.arn
+      }
+    ]
+  })
+}
+
+resource "aws_s3_bucket" "compliance" {
+  bucket = "aegis-compliance-${random_id.bucket_id.hex}"
+  acl    = "private"
+  versioning { enabled = true }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm = "aws:kms"
+        kms_master_key_id = aws_kms_key.cosign_key.arn
+      }
+    }
+  }
+}
+
+resource "random_id" "bucket_id" {
+  byte_length = 4
+}
+
+resource "aws_db_subnet_group" "db_subnet" {
+  name       = "aegis-db-subnet"
+  subnet_ids = data.aws_subnets.default.ids
+}
+
+data "aws_subnets" "default" {
+  filter { name = "tag:Environment", values = ["default","kubernetes"] }
+}
+
+resource "aws_db_instance" "postgres" {
+  identifier              = "aegis-postgres"
+  engine                  = "postgres"
+  engine_version          = "15.3"
+  instance_class          = "db.m6g.large"
+  allocated_storage       = 100
+  username                = var.db_username
+  password                = var.db_password
+  multi_az                = true
+  skip_final_snapshot     = true
+  publicly_accessible     = false
+  vpc_security_group_ids  = []
+  db_subnet_group_name    = aws_db_subnet_group.db_subnet.name
+}
+
+resource "aws_elasticache_subnet_group" "redis_subnet" {
+  name       = "aegis-redis-subnet"
+  subnet_ids = data.aws_subnets.default.ids
+}
+
+resource "aws_elasticache_replication_group" "redis" {
+  replication_group_id          = "aegis-redis"
+  replication_group_description = "Aegis Redis cluster"
+  node_type                     = "cache.m6g.large"
+  number_cache_clusters         = 3
+  automatic_failover_enabled    = true
+  subnet_group_name             = aws_elasticache_subnet_group.redis_subnet.name
+}
+
+output "kms_key_arn" { value = aws_kms_key.cosign_key.arn }
+output "github_oidc_role_arn" { value = aws_iam_role.github_oidc_role.arn }
+output "s3_compliance_bucket" { value = aws_s3_bucket.compliance.bucket }
+output "rds_endpoint" { value = aws_db_instance.postgres.endpoint }
+output "redis_endpoint" { value = aws_elasticache_replication_group.redis.primary_endpoint_address }
+
*** End Patch
*** Begin Patch
*** Add File: terraform/gcp/infra_full.tf
+// Terraform: Full infra for GCP production-ready Aegis
+// - KMS key ring & key for cosign
+// - Service account for CI signing
+// - Cloud SQL Postgres (regional HA)
+// - Memorystore Redis (regional)
+// - GCS compliance bucket
+//
+provider "google" {
+  project = var.gcp_project
+  region  = var.gcp_region
+}
+
+variable "gcp_project" { type = string }
+variable "gcp_region" { type = string }
+variable "db_user" { type = string }
+variable "db_password" { type = string }
+
+resource "google_storage_bucket" "compliance" {
+  name     = "aegis-compliance-${random_id.bucket_id.hex}"
+  location = var.gcp_region
+  versioning { enabled = true }
+}
+
+resource "random_id" "bucket_id" { byte_length = 4 }
+
+resource "google_kms_key_ring" "cosign_ring" {
+  name     = "aegis-cosign-ring"
+  location = var.gcp_region
+}
+
+resource "google_kms_crypto_key" "cosign_key" {
+  name     = "aegis-cosign-key"
+  key_ring = google_kms_key_ring.cosign_ring.id
+}
+
+resource "google_service_account" "github_actions" {
+  account_id   = "aegis-github-actions"
+  display_name = "Aegis GitHub Actions"
+}
+
+resource "google_kms_crypto_key_iam_member" "binding" {
+  crypto_key_id = google_kms_crypto_key.cosign_key.id
+  role          = "roles/cloudkms.signerVerifier"
+  member        = "serviceAccount:${google_service_account.github_actions.email}"
+}
+
+resource "google_sql_database_instance" "postgres" {
+  name             = "aegis-postgres"
+  database_version = "POSTGRES_15"
+  region           = var.gcp_region
+  settings {
+    tier = "db-custom-2-8192"
+    availability_type = "REGIONAL"
+    backup_configuration { enabled = true }
+  }
+}
+
+resource "google_sql_user" "postgres_user" {
+  name     = var.db_user
+  instance = google_sql_database_instance.postgres.name
+  password = var.db_password
+}
+
+resource "google_redis_instance" "redis" {
+  name           = "aegis-redis"
+  tier           = "STANDARD_HA"
+  memory_size_gb = 4
+  region         = var.gcp_region
+  redis_version  = "6.x"
+}
+
+output "gcp_kms_key_ref" {
+  value = "gcpkms://projects/${var.gcp_project}/locations/${var.gcp_region}/keyRings/${google_kms_key_ring.cosign_ring.name}/cryptoKeys/${google_kms_crypto_key.cosign_key.name}"
+}
+output "github_actions_sa_email" { value = google_service_account.github_actions.email }
+output "gcs_compliance_bucket" { value = google_storage_bucket.compliance.url }
+output "cloudsql_instance_name" { value = google_sql_database_instance.postgres.name }
+output "memorystore_addr" { value = google_redis_instance.redis.host }
+
*** End Patch
*** Begin Patch
*** Add File: scripts/deploy_infra.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper: provision infra for AWS or GCP via terraform and set GH secrets
+#
+if [ "$#" -lt 1 ]; then
+  echo "Usage: $0 <aws|gcp>"
+  exit 2
+fi
+CLOUD=$1
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+if [ "$CLOUD" = "aws" ]; then
+  (cd terraform/aws && terraform init && terraform apply -auto-approve)
+  echo "Retrieve outputs: terraform output -json"
+  (cd terraform/aws && terraform output -json) > /tmp/aws_out.json
+  KMS_ARN=$(jq -r '.kms_key_arn.value' /tmp/aws_out.json)
+  BUCKET=$(jq -r '.s3_compliance_bucket.value' /tmp/aws_out.json)
+  echo "Setting GitHub secrets (ensure GH_REPO env set)"
+  ./scripts/set_github_secrets_aws.sh "$KMS_ARN" "${REKOR_URL:-https://rekor.sigstore.dev}" "${REGISTRY_HOST:-}" "${REGISTRY_USER:-}" "${REGISTRY_TOKEN:-}" "$BUCKET"
+elif [ "$CLOUD" = "gcp" ]; then
+  (cd terraform/gcp && terraform init && terraform apply -auto-approve)
+  (cd terraform/gcp && terraform output -json) > /tmp/gcp_out.json
+  KMS_REF=$(jq -r '.gcp_kms_key_ref.value' /tmp/gcp_out.json)
+  SA_EMAIL=$(jq -r '.github_actions_sa_email.value' /tmp/gcp_out.json)
+  echo "GCP outputs: KMS=$KMS_REF SA=$SA_EMAIL"
+  echo "Please create GitHub secret GCP_KMS_KEY_REF and upload service account JSON for CI if required."
+else
+  echo "Unsupported cloud: $CLOUD"
+  exit 2
+fi
+popd >/dev/null
+echo "Infra provisioning complete. Follow runbooks to bootstrap Vault and deploy attestation."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_scale_validations.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Orchestrate scale validations: Milvus index build & scale insert, distributed training (Argo), and inference load test (Locust)
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+
+echo "1) Milvus scale insert & index rebuild"
+python3 scripts/milvus_scale_test.py || echo "Milvus test failed (check cluster)"
+
+echo "2) Trigger Argo distributed TPU training workflow (requires argo CLI and context configured)"
+if command -v argo >/dev/null 2>&1; then
+  argo submit -n staging argo/workflows/distributed_tpu_realdata.yaml --watch || echo "Argo distributed workflow failed or timed out"
+else
+  echo "argo CLI not installed - skipping distributed TPU run"
+fi
+
+echo "3) Run Locust inference load test (headless)"
+python3 -m pip install locust >/dev/null 2>&1 || true
+locust -f services/loadtest/locustfile.py --headless -u ${LOCUST_USERS:-200} -r ${LOCUST_SPAWN_RATE:-20} --run-time ${LOCUST_DURATION:-2m} --host ${INFERENCE_HOST:-http://model-serving.staging.svc.cluster.local} --csv=artifacts/locust || true
+
+echo "Collect Prometheus metrics snapshot (if kubectl and cluster accessible)"
+kubectl get svc prometheus -n monitoring >/dev/null 2>&1 && kubectl port-forward svc/prometheus -n monitoring 9090:9090 >/dev/null 2>&1 & sleep 2 || true
+if curl -sS http://localhost:9090/-/ready >/dev/null 2>&1; then
+  curl -sS "http://localhost:9090/api/v1/query?query=histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le))" > artifacts/prom_p95.json || true
+fi
+
+echo "Scale validations completed. Check artifacts/"
+popd >/dev/null
+
*** End Patch
*** Begin Patch
*** Add File: scripts/run_security_validations.sh
+#!/usr/bin/env bash
+set -euo pipefail
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+pushd "$ROOT" >/dev/null
+mkdir -p artifacts/security
+
+echo "1) Attestation smoke test (unsigned reject / signed accept)"
+./scripts/attestation_smoke_test.sh || { echo "Attestation smoke test failed"; exit 1; }
+cp /tmp/signed_deploy.yaml artifacts/security/ || true
+
+echo "2) Agent sandbox pen test"
+bash tests/agent_sandbox_pen_test.sh || echo "Agent sandbox test reported issues - inspect logs"
+
+echo "3) Run pen-tests suite"
+bash tests/pen_tests/attestation_and_sandbox_suite.sh || echo "Pen-tests suite had failures - capture logs"
+
+echo "4) Collect logs from critical namespaces (attestation, agent-exec, staging)"
+for ns in ops agent-exec staging; do
+  mkdir -p artifacts/security/logs/$ns
+  kubectl get pods -n $ns -o name | while read -r p; do
+    name=$(echo $p | sed 's#pod/##')
+    kubectl logs -n $ns "$name" --all-containers > artifacts/security/logs/$ns/$name.log || true
+  done
+done
+
+echo "Security validations completed; artifacts in artifacts/security"
+popd >/dev/null
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/nightly_validations.yml
+name: Nightly: Scale & Security Validations
+on:
+  schedule:
+    - cron: '0 4 * * *' # daily at 04:00 UTC
+  workflow_dispatch:
+
+jobs:
+  validations:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: python -m pip install -r requirements.txt || true
+      - name: Run scale validations (if cluster access available)
+        env:
+          INFERENCE_HOST: ${{ secrets.INFERENCE_HOST }}
+        run: |
+          ./scripts/run_scale_validations.sh
+      - name: Run security validations
+        run: |
+          ./scripts/run_security_validations.sh || true
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: aegis-validations
+          path: artifacts
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/signoff_template.md
+# Production Sign-Off Template — Aegis Platform
+
+Project: Aegis AI Platform
+Environment: staging / production
+Date: YYYY-MM-DD
+
+Owners:
+- Platform engineering: ___________________
+- Security: ___________________
+- SRE: ___________________
+- Product: ___________________
+
+Checklist (mark PASS/FAIL and attach evidence link):
+1) KMS & CI signing
+   - Terraform: KMS key created and IAM role for GitHub OIDC provisioned — PASS/FAIL
+   - CI run produced cosign signature and Rekor entry — PASS/FAIL
+   - Evidence: link to Rekor entry / CI run: __________________
+
+2) Vault & Secrets
+   - Vault deployed HA + KV v2 enabled — PASS/FAIL
+   - All secrets migrated; apps using Vault agent or K8s auth — PASS/FAIL
+   - Evidence: policy list / vault audit logs: __________________
+
+3) HA Infra
+   - Postgres (Multi-AZ or managed) deployed + backups validated — PASS/FAIL
+   - Redis cluster deployed + failover tested — PASS/FAIL
+   - Milvus deployed HA + index restore tested — PASS/FAIL
+   - Evidence: backup restore logs / screenshots: __________________
+
+4) Attestation & Gatekeeper
+   - cert-manager & attestation webhook deployed and CA injected — PASS/FAIL
+   - Gatekeeper constraint rejects unsigned, accepts signed artifacts — PASS/FAIL
+   - Evidence: attestation_smoke_test output link: __________________
+
+5) Scale tests
+   - Distributed training run completed and checkpoints stored — PASS/FAIL
+   - Milvus insert/search throughput achieved target — PASS/FAIL
+   - Inference Locust p95/p99 within SLO — PASS/FAIL
+   - Evidence: artifacts/ (links): __________________
+
+6) Security & Pen tests
+   - Agent sandbox pen-tests executed; findings remediated — PASS/FAIL
+   - Red-team or external pen-test completed — PASS/FAIL
+   - Evidence: pen-test report link: __________________
+
+7) Runbooks & DR
+   - Runbooks updated and exercised (incident + restore) — PASS/FAIL
+   - On-call rotation and escalation confirmed — PASS/FAIL
+   - Evidence: runbook sign-off: __________________
+
+Signatures:
+- Platform eng lead: (name & signature) ___________________
+- SRE lead: ___________________
+- Security lead: ___________________
+- Product owner: ___________________
+
+Deployment window: approved / deny: ___________________
+
*** End Patch
*** Begin Patch
*** Add File: scripts/collect_evidence.sh
+#!/usr/bin/env bash
+set -euo pipefail
+ROOT="$(cd "$(dirname "$0")/.." && pwd)"
+OUT="${1:-evidence_$(date +%F).tgz}"
+pushd "$ROOT" >/dev/null
+mkdir -p artifacts/evidence
+cp artifacts/* artifacts/evidence/ -r || true
+kubectl get pods -A -o wide > artifacts/evidence/pods_all.txt || true
+kubectl get events -A > artifacts/evidence/events_all.txt || true
+for ns in ops staging agent-exec monitoring; do
+  mkdir -p artifacts/evidence/logs/$ns
+  kubectl get pods -n $ns -o name | while read -r p; do
+    name=$(echo $p | sed 's#pod/##')
+    kubectl logs -n $ns "$name" --all-containers > artifacts/evidence/logs/$ns/$name.log || true
+  done
+done
+tar -czf "$OUT" -C artifacts evidence || true
+echo "Evidence packaged: $OUT"
+popd >/dev/null
+
*** End Patch
*** End Patch
