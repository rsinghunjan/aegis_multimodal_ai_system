*** Begin Patch
*** Add File:ops/populate_and_verify_secrets_irsa.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Populate GitHub secrets from Terraform output (cosign KMS, Rekor, evidence bucket),
+# verify AWS SecretsManager entries used by ExternalSecrets, check ExternalSecrets sync,
+# and verify IRSA annotations on critical ServiceAccounts.
+#
+# Usage:
+#   export GITHUB_TOKEN=... AWS_PROFILE=...  # or env AWS creds
+#   ./ops/populate_and_verify_secrets_irsa.sh /tmp/aegis_tf_output.json [kubecontext]
+#
+TF_OUT=${1:-/tmp/aegis_tf_output.json}
+KUBECTX=${2:-}
+REPO=${GITHUB_REPOSITORY:-owner/repo}
+
+if [ ! -f "$TF_OUT" ]; then
+  echo "Terraform output JSON required at $TF_OUT" >&2
+  exit 2
+fi
+
+: "${GITHUB_TOKEN:?GITHUB_TOKEN must be set (repo:write & secrets perms)}"
+
+COSIGN_KMS_ARN=$(jq -r '.cosign_kms_arn.value // .cosign_kms_arn' "$TF_OUT")
+EVIDENCE_BUCKET=$(jq -r '.evidence_bucket.value // .evidence_bucket' "$TF_OUT")
+REKOR_URL=$(jq -r '.rekor_url.value // .rekor_url' "$TF_OUT")
+DB_SECRET_ARN=$(jq -r '.db_secret_arn.value // .db_secret_arn' "$TF_OUT" 2>/dev/null || echo "")
+
+echo ">> Setting GitHub secrets (COSIGN_KMS_ARN, REKOR_URL, EVIDENCE_BUCKET) for repo $REPO"
+if command -v gh >/dev/null 2>&1; then
+  gh secret set COSIGN_KMS_ARN --repo "$REPO" --body "$COSIGN_KMS_ARN"
+  gh secret set REKOR_URL --repo "$REPO" --body "$REKOR_URL"
+  gh secret set EVIDENCE_BUCKET --repo "$REPO" --body "$EVIDENCE_BUCKET"
+else
+  echo "gh CLI not found. Please set GitHub secrets manually." >&2
+fi
+
+echo ">> Verifying AWS SecretsManager entries referenced by ExternalSecrets (if any)"
+if [ -n "$DB_SECRET_ARN" ]; then
+  if aws secretsmanager get-secret-value --secret-id "$DB_SECRET_ARN" >/dev/null 2>&1; then
+    echo "Found DB secret in SecretsManager: $DB_SECRET_ARN"
+  else
+    echo "WARNING: DB secret $DB_SECRET_ARN not found in SecretsManager" >&2
+  fi
+else
+  echo "No DB secret ARN found in Terraform output; verify ExternalSecrets mappings manually."
+fi
+
+KUBECTL="kubectl"
+if [ -n "$KUBECTX" ]; then
+  KUBECTL="kubectl --context $KUBECTX"
+fi
+
+echo ">> Checking ExternalSecrets-synced k8s Secrets in namespaces 'security' and 'aegis'"
+for ns in security aegis; do
+  for sfx in aegis-cosign aegis-rekor aegis-db-creds; do
+    echo -n "Checking secret $ns/$sfx ... "
+    if $KUBECTL -n "$ns" get secret "$sfx" >/dev/null 2>&1; then
+      echo "OK"
+    else
+      echo "MISSING"
+      echo "  - ExternalSecret may not have synced $sfx in $ns. Check external-secrets controller logs and ExternalSecret CRD."
+    fi
+  done
+done
+
+echo ">> Verifying IRSA annotations for critical ServiceAccounts"
+declare -A SA_ROLE_MAP
+SA_ROLE_MAP[cosign-signer]=arn:aws:iam::REPLACE_ACCOUNT:role/aegis-cosign-irsa-role
+SA_ROLE_MAP[mlflow-agent]=arn:aws:iam::REPLACE_ACCOUNT:role/aegis-mlflow-irsa-role
+SA_ROLE_MAP[spark]=arn:aws:iam::REPLACE_ACCOUNT:role/aegis-spark-irsa-role
+
+for sa in "${!SA_ROLE_MAP[@]}"; do
+  ns=$( [ "$sa" = "cosign-signer" ] && echo security || echo aegis )
+  echo -n "Checking SA $ns/$sa annotation ... "
+  ann=$($KUBECTL -n "$ns" get sa "$sa" -o jsonpath='{.metadata.annotations}' 2>/dev/null || echo "")
+  if echo "$ann" | grep -q 'eks.amazonaws.com/role-arn'; then
+    echo "OK"
+  else
+    echo "MISSING"
+    echo "  - Annotate: kubectl -n $ns annotate sa $sa eks.amazonaws.com/role-arn=${SA_ROLE_MAP[$sa]} --overwrite"
+  fi
+done
+
+echo ">> All checks run. Fix warnings above and re-run this script. To automatically annotate SAs, run k8s/irsa/annotate_serviceaccounts.sh (operator must provide role ARNs)."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/build_and_push_all_images.yml
+name: Build and Push All Images
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: write
+  id-token: write
+
+env:
+  IMAGE_TAG: latest
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v2
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to GHCR
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build & push images
+        run: |
+          ORG=${{ secrets.GHCR_ORG }}
+          TAG=${IMAGE_TAG}
+          docker build -t $ORG/aegis-spark:$TAG -f docker/spark/Dockerfile docker
+          docker push $ORG/aegis-spark:$TAG
+          docker build -t $ORG/aegis-flink:$TAG -f docker/flink/Dockerfile docker
+          docker push $ORG/aegis-flink:$TAG
+          docker build -t $ORG/aegis-train:$TAG -f train/Dockerfile .
+          docker push $ORG/aegis-train:$TAG
+          docker build -t $ORG/aegis-serve:$TAG -f serve/Dockerfile .
+          docker push $ORG/aegis-serve:$TAG
+          docker build -t $ORG/mcpx-logger:$TAG -f k8s/mcpx/Dockerfile .
+          docker push $ORG/mcpx-logger:$TAG
+      - name: Install cosign
+        uses: sigstore/cosign-installer@v2
+      - name: Cosign sign images
+        env:
+          COSIGN_KMS_ARN: ${{ secrets.COSIGN_KMS_ARN }}
+        run: |
+          ORG=${{ secrets.GHCR_ORG }}
+          TAG=${IMAGE_TAG}
+          cosign sign --key $COSIGN_KMS_ARN $ORG/aegis-spark:$TAG
+          cosign sign --key $COSIGN_KMS_ARN $ORG/aegis-flink:$TAG
+          cosign sign --key $COSIGN_KMS_ARN $ORG/aegis-train:$TAG
+          cosign sign --key $COSIGN_KMS_ARN $ORG/aegis-serve:$TAG
+          cosign sign --key $COSIGN_KMS_ARN $ORG/mcpx-logger:$TAG
+      - name: Verify signatures (Rekor)
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          ORG=${{ secrets.GHCR_ORG }}
+          TAG=${IMAGE_TAG}
+          cosign verify --rekor-server $REKOR_URL $ORG/aegis-spark:$TAG
+          cosign verify --rekor-server $REKOR_URL $ORG/aegis-flink:$TAG
+          cosign verify --rekor-server $REKOR_URL $ORG/aegis-train:$TAG
+          cosign verify --rekor-server $REKOR_URL $ORG/aegis-serve:$TAG
+          cosign verify --rekor-server $REKOR_URL $ORG/mcpx-logger:$TAG
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/deploy_gate.yml
+name: Deploy Gate (wait for images)
+
+on:
+  workflow_run:
+    workflows: ["Build and Push All Images"]
+    types: [completed]
+
+jobs:
+  gated-deploy:
+    if: ${{ github.event.workflow_run.conclusion == 'success' }}
+    runs-on: ubuntu-latest
+    steps:
+      - name: Notify operator
+        run: echo "Images built & signed. Operator can now deploy manifests wired to GHCR images."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/overlays/staging/kustomization.yaml
+apiVersion: kustomize.config.k8s.io/v1beta1
+kind: Kustomization
+resources:
+  - ../../base
+images:
+  - name: ghcr.io/yourorg/aegis-spark
+    newName: $GHCR_ORG/aegis-spark
+    newTag: latest
+  - name: ghcr.io/yourorg/aegis-flink
+    newName: $GHCR_ORG/aegis-flink
+    newTag: latest
+  - name: ghcr.io/yourorg/aegis-train
+    newName: $GHCR_ORG/aegis-train
+    newTag: latest
+  - name: ghcr.io/yourorg/aegis-serve
+    newName: $GHCR_ORG/aegis-serve
+    newTag: latest
+configMapGenerator:
+  - name: aegis-config
+    literals:
+      - EVIDENCE_BUCKET=REPLACE_EVIDENCE_BUCKET
+      - MLFLOW_TRACKING_URI=REPLACE_MLFLOW_URI
+
*** End Patch
*** Begin Patch
*** Add File:k8s/ha/production_values.yaml
+# Tuned HA & sizing values (examples) â€” review & adapt to workload and cluster node sizes
+postgres:
+  persistence:
+    storageClass: gp3
+    size: 500Gi
+  resources:
+    requests:
+      cpu: "2000m"
+      memory: "4Gi"
+    limits:
+      cpu: "4000m"
+      memory: "12Gi"
+  replicaCount: 2
+
+milvus:
+  persistence:
+    storageClass: gp3
+    size: 800Gi
+  resources:
+    requests:
+      cpu: "2000m"
+      memory: "8Gi"
+    limits:
+      cpu: "8000m"
+      memory: "32Gi"
+  replicaCount: 3
+
+redis:
+  persistence:
+    size: 50Gi
+  resources:
+    requests:
+      cpu: "500m"
+      memory: "1Gi"
+    limits:
+      cpu: "2000m"
+      memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/dr/full_dr_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Full DR drill wrapper:
+# - runs Postgres restore drill (scripts/dr/extended_restore_and_rto.sh)
+# - validates Milvus PVC mount & simple index restore (if applicable)
+# - produces a small report JSON
+#
+TF_OUT=${1:-/tmp/aegis_tf_output.json}
+DUMP_KEY=${2:-backups/postgres/latest.sql.gz}
+REPORT_OUT=${3:-/tmp/dr_report.json}
+
+START=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
+echo "Starting DR drill at $START"
+
+./scripts/dr/extended_restore_and_rto.sh "$(jq -r '.evidence_bucket.value // .evidence_bucket' $TF_OUT)" "$DUMP_KEY" aegis-dr-test
+POSTRESTORE_END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
+
+# Optionally validate Milvus (if deployed): check pod status & simple health endpoint
+MILVUS_OK="unknown"
+if kubectl -n aegis get statefulset milvus >/dev/null 2>&1; then
+  kubectl -n aegis rollout status sts/milvus --timeout=300s || true
+  if kubectl -n aegis get pods -l app=milvus -o jsonpath='{.items[*].status.phase}' | grep -q Running; then
+    MILVUS_OK="ok"
+  else
+    MILVUS_OK="not_ready"
+  fi
+else
+  MILVUS_OK="not_deployed"
+fi
+
+END=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
+
+cat > "$REPORT_OUT" <<EOF
+{
+  "start": "$START",
+  "postrestore_completed": "$POSTRESTORE_END",
+  "milvus_status": "$MILVUS_OK",
+  "notes": "Review kube events and restore pod logs for details."
+}
+EOF
+
+echo "DR drill report written to $REPORT_OUT"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/mcpx/mcpx-auth-networkpolicy.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: mcpx-logger-policy
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: mcpx-logger
+  policyTypes:
+    - Ingress
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: aegis
+      ports:
+        - protocol: TCP
+          port: 8081
+
+---
+# Optional egress restriction: allow mcpx to call S3 via NAT / VPC endpoint only
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: mcpx-egress
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app: mcpx-logger
+  policyTypes:
+    - Egress
+  egress:
+    - to:
+        - ipBlock:
+            cidr: 0.0.0.0/0
+      ports:
+        - protocol: TCP
+          port: 443
+
*** End Patch
*** Begin Patch
*** Add File:ops/apply_baseline_and_prometheus.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Wrapper: generate baseline from a sample, upload to evidence bucket, write tuned prometheus rules,
+# and apply Alertmanager config mapping (operator must supply PagerDuty key as secret)
+#
+SAMPLE_S3=${1:-""}
+OUT_RULES=${2:-monitoring/tuned_prometheus_rules.yaml}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-}
+
+if [ -z "$SAMPLE_S3" ]; then
+  echo "Usage: $0 s3://bucket/path/features.parquet [out_rules.yaml]" >&2
+  exit 2
+fi
+
+echo "Generating tuned Prometheus rules from sample $SAMPLE_S3"
+python3 ops/generate_baseline_and_tune_prometheus.py --sample "$SAMPLE_S3" --out "$OUT_RULES"
+
+echo "Uploading baseline and rules to evidence bucket and applying to cluster"
+if [ -z "$EVIDENCE_BUCKET" ]; then
+  echo "EVIDENCE_BUCKET not set. Export it or run with env." >&2
+  exit 3
+fi
+
+aws s3 cp "$OUT_RULES" "s3://${EVIDENCE_BUCKET}/monitoring/$(basename $OUT_RULES)"
+
+echo "Applying tuned rules to Prometheus operator (monitoring namespace)"
+kubectl -n monitoring apply -f "$OUT_RULES" || echo "Ensure Prometheus operator is installed and CRDs available"
+
+echo "Baseline & rules applied. Review Prometheus UI and tune thresholds as needed."
+
*** End Patch
*** Begin Patch
*** Add File:testing/parallel_inference_runner.py
+#!/usr/bin/env python3
+"""
+Parallel inference runner for smoke/load testing KServe endpoints.
+Usage:
+  python testing/parallel_inference_runner.py --endpoint https://kserve.aegis.example.com/models/aegis-transfer-sentiment:predict --concurrency 50 --requests 1000
+"""
+import argparse, requests, time, concurrent.futures
+
+def single_request(url, payload):
+    try:
+        r = requests.post(url, json=payload, timeout=30)
+        return r.status_code, r.text[:200]
+    except Exception as e:
+        return 0, str(e)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--endpoint", required=True)
+    p.add_argument("--concurrency", type=int, default=10)
+    p.add_argument("--requests", type=int, default=100)
+    args = p.parse_args()
+    payload = {"instances": ["test input"]}  # replace with realistic sample
+    start = time.time()
+    with concurrent.futures.ThreadPoolExecutor(max_workers=args.concurrency) as ex:
+        futures = [ex.submit(single_request, args.endpoint, payload) for _ in range(args.requests)]
+        results = [f.result() for f in concurrent.futures.as_completed(futures)]
+    end = time.time()
+    success = sum(1 for s,t in results if s in (200,201,202))
+    print(f"Requests: {args.requests}, Success: {success}, Duration: {end-start:.2f}s")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:pipelines/replace_demo_fallbacks_check.yml
+name: Block Demo/Synthetic Fallbacks
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run demo fallback scanner
+        run: |
+          chmod +x pipelines/enforce_prod_reads.sh
+          ./pipelines/enforce_prod_reads.sh
+
*** End Patch
*** Begin Patch
*** Add File:docs/advanced_productionization_roadmap.md
+# Advanced Productionization Roadmap
+
+This document outlines next steps and design considerations for:
+- Federated secure aggregation
+- Distributed RL orchestration
+- Large LLM serving at scale
+
+Federated secure aggregation
+- Prototype stack: Flower + a secure aggregation library (MPC) + client certificate enrollment.
+- infra needs: dedicated cert-issuance service, client management DB, per-client quotas, secure aggregation gateway.
+- audit: log per-round metadata to EVIDENCE_BUCKET, sign final aggregate, Rekor entry.
+
+Distributed RL orchestration
+- Prototype: RayCluster + RLlib on GPU nodepools; gang scheduling & checkpointing to S3.
+- Safety: shadow evaluation & policy canaries; human approval gates for policy promotion.
+- Observability: episodic metrics, reward distributions, safety violation alerts.
+
+Large LLM serving
+- Use Triton / FastAPI + adaptive batching + model sharding (DeepSpeed inference or FasterTransformer).
+- Cost controls: rate-limiting, batch windows, pre-warming, autoscaling with PDBs for GPU eviction.
+- Safety: toxicity & hallucination detection, human-in-loop review for high-risk queries.
+
+Each of these tracks requires dedicated design, security review and capacity planning. Use this doc as a starting checklist for project proposals.
+
*** End Patch
*** End Patch
