*** Begin Patch
*** Add File: k8s/llm/llm-server-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: llm-server
+  namespace: aegis
+  labels:
+    app: llm-server
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: llm-server
+  template:
+    metadata:
+      labels:
+        app: llm-server
+    spec:
+      containers:
+        - name: llm-server
+          # Example image: replace with your self-hosted LLM server (e.g., text-generation-inference, vLLM)
+          image: ghcr.io/huggingface/text-generation-inference:latest
+          ports:
+            - containerPort: 8080
+          env:
+            - name: MODEL_ID
+              value: "local-multimodal-model"
+            - name: LOG_LEVEL
+              value: "info"
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              memory: "48Gi"
+              cpu: "8"
+            requests:
+              nvidia.com/gpu: 1
+              memory: "20Gi"
+              cpu: "2"
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 8080
+            initialDelaySeconds: 10
+            periodSeconds: 10
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+      nodeSelector:
+        accelerator: "gpu"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/llm/llm-gateway-hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: llm-gateway-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: llm-gateway
+  minReplicas: 2
+  maxReplicas: 20
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
+---
+# KEDA ScaledObject example to scale on Redis queue length (optional)
+apiVersion: keda.sh/v1alpha1
+kind: ScaledObject
+metadata:
+  name: llm-gateway-redis-scaledobject
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    name: llm-gateway
+  minReplicaCount: 2
+  maxReplicaCount: 20
+  triggers:
+    - type: redis
+      metadata:
+        address: redis.aegis.svc:6379
+        listName: "llm:requests"
+        listLength: "50"
+
*** End Patch
*** Begin Patch
*** Add File: llm/fine_tune_rlhf.py
+#!/usr/bin/env python3
+"""
+Skeleton: fine-tuning / RLHF pipeline for LLMs.
+ - This is a scaffold that demonstrates steps: data prep, supervised fine-tune, reward model training, PPO loop (RLHF)
+ - Operator must provision GPU nodes, MLflow, and storage (S3) and install required libraries (transformers, accelerate, trl, datasets).
+ - Designed to be executed on a GPU node or training cluster.
+"""
+import os, json, time
+from pathlib import Path
+
+# NOTE: This file is a high-level orchestration example. Fill specific training scripts per model infra.
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI")
+ARTIFACT_S3 = os.environ.get("ARTIFACT_S3", "s3://aegis-ml/models")
+
+def prepare_supervised_data(src_manifest, out_dir):
+    # src_manifest: JSONL of {"prompt":..., "completion":...}
+    os.makedirs(out_dir, exist_ok=True)
+    # In practice convert to dataset format used by training (e.g., HuggingFace dataset)
+    print("Prepare supervised data from", src_manifest, "->", out_dir)
+
+def run_supervised_finetune(config_path):
+    # call training entrypoint (accelerate/transformers)
+    print("Run supervised fine-tune with config", config_path)
+    # Example placeholder:
+    # os.system("accelerate launch train_supervised.py --config " + config_path)
+
+def train_reward_model(train_data, out_dir):
+    print("Train reward model on", train_data, "->", out_dir)
+    # placeholder
+
+def run_rlhf_loop(policy_model, reward_model, config):
+    print("Start PPO loop with policy", policy_model, "reward", reward_model)
+    # placeholder - call trl PPO loop
+
+def log_artifact_to_mlflow(path, metadata=None):
+    if not MLFLOW_URI:
+        print("MLflow not configured; skipping artifact log")
+        return
+    import mlflow
+    mlflow.set_tracking_uri(MLFLOW_URI)
+    with mlflow.start_run():
+        mlflow.log_artifact(path)
+        if metadata:
+            for k,v in (metadata or {}).items():
+                mlflow.log_param(k, v)
+
+def main():
+    # Example orchestration
+    workdir = "/tmp/llm_finetune"
+    os.makedirs(workdir, exist_ok=True)
+    # Steps (operator replaces with real paths/configs)
+    prepare_supervised_data("/data/supervised.jsonl", os.path.join(workdir, "supervised"))
+    run_supervised_finetune("/configs/supervised_config.yaml")
+    train_reward_model(os.path.join(workdir, "supervised"), os.path.join(workdir, "reward"))
+    run_rlhf_loop("policy:latest", "reward:latest", {"ppo_steps":1000})
+    log_artifact_to_mlflow(os.path.join(workdir, "policy_checkpoint.pt"), {"type":"rlhf"})
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: ci/rlhf_training_job.yml
+name: RLHF Training Job (GPU)
+on:
+  workflow_dispatch:
+    inputs:
+      config:
+        required: true
+
+jobs:
+  rlhf:
+    runs-on: self-hosted
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup python
+        run: python -m pip install --upgrade pip && pip install -r requirements-rlhf.txt
+      - name: Run RLHF pipeline (operator must ensure GPU labeller/runner)
+        run: |
+          python llm/fine_tune_rlhf.py --config "${{ github.event.inputs.config }}"
+
*** End Patch
*** Begin Patch
*** Add File: multimodal/ingest_preprocess.py
+#!/usr/bin/env python3
+"""
+Multimodal ingestion & preprocessing:
+ - Ingest assets (images/audio/video) from local or remote sources
+ - Produce manifest files and upload preprocessed artifacts to S3
+ - Operator must provide AWS credentials and S3_BUCKET env
+"""
+import os, json, hashlib, subprocess
+from pathlib import Path
+import boto3
+
+S3_BUCKET = os.environ.get("S3_BUCKET")
+OUT_PREFIX = os.environ.get("MULTIMODAL_PREFIX", "multimodal/")
+
+def checksum(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as fh:
+        for chunk in iter(lambda: fh.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def preprocess_image(src, dst):
+    # example using imagemagick to resize and normalize
+    Path(dst).parent.mkdir(parents=True, exist_ok=True)
+    cmd = ["convert", src, "-resize", "1024x1024>", "-strip", dst]
+    subprocess.check_call(cmd)
+
+def upload_to_s3(local, key):
+    s3 = boto3.client("s3")
+    s3.upload_file(local, S3_BUCKET, key)
+
+def ingest_files(src_dir, manifest_out="/tmp/multimodal_manifest.jsonl"):
+    manifest = []
+    for p in Path(src_dir).rglob("*"):
+        if p.is_file() and p.suffix.lower() in [".jpg",".jpeg",".png",".mp4",".wav"]:
+            target = f"/tmp/pre/{p.stem}{p.suffix}"
+            if p.suffix.lower() in [".jpg",".jpeg",".png"]:
+                preprocess_image(str(p), target)
+            else:
+                # pass-through for audio/video in this example
+                target = str(p)
+            csum = checksum(target)
+            s3_key = f"{OUT_PREFIX}{p.name}"
+            upload_to_s3(target, s3_key)
+            manifest.append({"path": s3_key, "checksum": csum, "orig": str(p)})
+    with open(manifest_out,"w") as fh:
+        for m in manifest:
+            fh.write(json.dumps(m) + "\n")
+    print("Wrote manifest", manifest_out)
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--src", required=True)
+    args = p.parse_args()
+    ingest_files(args.src)
+
*** End Patch
*** Begin Patch
*** Add File: multimodal/kserve_multimodal_inference.yaml
+apiVersion: "serving.kubeflow.org/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-multimodal"
+  namespace: "aegis-ml"
+spec:
+  predictor:
+    custom:
+      container:
+        image: aegis/multimodal-server:latest
+        command:
+          - /usr/bin/multimodal-server
+        env:
+          - name: MODEL_URI
+            value: "s3://models/multimodal/latest"
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: "24Gi"
+            cpu: "4"
+          requests:
+            nvidia.com/gpu: 1
+            memory: "12Gi"
+            cpu: "2"
+
+# Note: real multimodal serving may require custom logic to accept image+text inputs. The image shows a custom container approach.
+
*** End Patch
*** Begin Patch
*** Add File: agents/langchain_agent_service.py
+#!/usr/bin/env python3
+"""
+Agent orchestration service integrating LangChain-style agents with:
+ - state management (Redis)
+ - approval flow (calls Approval API)
+ - tool adapters (scheduler, ledger, rekor, hsm)
+ - long-running conversational state storage
+
+This is a scaffold: operators should integrate with a real LangChain instance and secure tool adapters.
+"""
+import os, json, time
+import redis
+import requests
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/2")
+redis_client = redis.from_url(REDIS_URL)
+APPROVAL_API = os.environ.get("APPROVAL_API", "http://approval-api.aegis.svc:8207")
+
+app = FastAPI(title="Aegis Agent Orchestrator")
+
+class AgentRequest(BaseModel):
+    tenant: str
+    prompt: str
+    tools_plan: dict  # planned tool actions (list)
+    require_approval: bool = True
+
+def persist_state(session_id, state):
+    redis_client.set(f"agent:state:{session_id}", json.dumps(state), ex=3600)
+
+def enqueue_for_approval(plan):
+    r = requests.post(f"{APPROVAL_API}/submit", json={"plan": plan})
+    r.raise_for_status()
+    return r.json()["id"]
+
+@app.post("/run")
+def run_agent(req: AgentRequest):
+    # Create a session
+    ts = int(time.time())
+    session_id = f"{req.tenant}-{ts}"
+    state = {"tenant": req.tenant, "prompt": req.prompt, "plan": req.tools_plan, "status": "pending"}
+    persist_state(session_id, state)
+    # If requires approval, send to approval API
+    if req.require_approval:
+        approval_id = enqueue_for_approval({"session": session_id, "plan": req.tools_plan})
+        state["status"] = "pending_approval"
+        state["approval_id"] = approval_id
+        persist_state(session_id, state)
+        return {"ok": True, "session_id": session_id, "approval_id": approval_id}
+    # else execute directly (unsafe without approval)
+    # For execution use agents/agent_orchestrator.orchestrate via HTTP call
+    orchestrator = os.environ.get("ORCHESTRATOR_API", "http://agent-orchestrator.aegis.svc:8210")
+    r = requests.post(f"{orchestrator}/execute", json={"session": session_id, "plan": req.tools_plan})
+    if r.status_code != 200:
+        raise HTTPException(status_code=500, detail="orchestration failed")
+    state["status"] = "executing"
+    persist_state(session_id, state)
+    return {"ok": True, "session_id": session_id}
+
+@app.get("/state/{session_id}")
+def get_state(session_id: str):
+    v = redis_client.get(f"agent:state:{session_id}")
+    if not v:
+        raise HTTPException(status_code=404)
+    return json.loads(v)
+
*** End Patch
*** Begin Patch
*** Add File: agents/rollback_registry.py
+"""
+Registry of compensating actions for tool-driven side effects.
+ - Tools register a rollback handler for specific action IDs
+ - Orchestrator can call rollback(action_id) to attempt compensation
+"""
+import os, json
+ROLLBACK_DB = os.environ.get("ROLLBACK_DB", "/tmp/rollback_registry.json")
+
+def _load():
+    if os.path.exists(ROLLBACK_DB):
+        return json.load(open(ROLLBACK_DB))
+    return {}
+
+def _save(d):
+    open(ROLLBACK_DB,"w").write(json.dumps(d, indent=2))
+
+def register(action_id, rollback_cmd):
+    d = _load()
+    d[action_id] = {"rollback_cmd": rollback_cmd, "ts": int(time.time())}
+    _save(d)
+
+def rollback(action_id):
+    d = _load()
+    rec = d.get(action_id)
+    if not rec:
+        return {"ok": False, "error": "not_found"}
+    # naive execution of rollback command (operator must implement secure executor)
+    try:
+        import subprocess
+        subprocess.check_call(rec["rollback_cmd"], shell=True)
+        return {"ok": True}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
*** End Patch
*** Begin Patch
*** Add File: safety/hallucination_pipeline.py
+#!/usr/bin/env python3
+"""
+Hallucination monitoring pipeline:
+ - A detector component using a small supervised model or embedding-based similarity to a factstore
+ - A continuous monitor that consumes LLM audit logs and flags high-risk outputs
+ - Pushes alerts to AlertRouter / operator webhook and writes audit markers
+"""
+import os, json, time
+from typing import List
+from observability.audit_indexer import write_record
+try:
+    from sentence_transformers import SentenceTransformer, util
+except Exception:
+    SentenceTransformer = None
+
+MODEL_NAME = os.environ.get("HALLU_EMBED_MODEL", "all-MiniLM-L6-v2")
+FACTSTORE = os.environ.get("FACTSTORE_FILE", "/data/factstore.jsonl")  # simple list of trusted facts
+THRESHOLD = float(os.environ.get("HALLU_SIM_THRESHOLD", "0.65"))
+OPERATOR_WEBHOOK = os.environ.get("OPERATOR_NOTIFY_WEBHOOK")
+
+class HalluMonitor:
+    def __init__(self):
+        self.model = SentenceTransformer(MODEL_NAME) if SentenceTransformer else None
+        self.facts = []
+        if os.path.exists(FACTSTORE):
+            with open(FACTSTORE) as fh:
+                for l in fh:
+                    try:
+                        self.facts.append(json.loads(l)["text"])
+                    except Exception:
+                        pass
+        if self.model and self.facts:
+            self.fact_emb = self.model.encode(self.facts, convert_to_tensor=True)
+        else:
+            self.fact_emb = None
+
+    def score(self, text: str) -> float:
+        if not self.model or not self.fact_emb:
+            # fallback heuristic
+            return 0.0
+        emb = self.model.encode(text, convert_to_tensor=True)
+        sim_scores = util.cos_sim(emb, self.fact_emb)
+        best = float(sim_scores.max())
+        return best
+
+    def analyze_and_alert(self, audit_record: dict):
+        text = audit_record.get("record", {}).get("prompt","") + " " + json.dumps(audit_record.get("record", {}).get("response",""))
+        score = self.score(text)
+        out = {"score": score, "ts": int(time.time()), "record": audit_record}
+        write_record("hallu_check", out)
+        if score < THRESHOLD and OPERATOR_WEBHOOK:
+            try:
+                import requests
+                requests.post(OPERATOR_WEBHOOK, json={"alert":"hallucination_suspected","score":score,"record": audit_record}, timeout=3)
+            except Exception:
+                pass
+
+if __name__=="__main__":
+    m = HalluMonitor()
+    # read recent audit records from /tmp/aegis_audit (or S3) and run analyze
+    import glob
+    files = glob.glob("/tmp/aegis_audit/*.json")
+    for f in files:
+        r = json.load(open(f))
+        m.analyze_and_alert(r)
+
*** End Patch
*** Begin Patch
*** Add File: billing/token_billing_worker.py
+#!/usr/bin/env python3
+"""
+Per-tenant token billing worker:
+ - Reads token counters from Prometheus pushgateway or Redis counters and produces periodic invoices
+ - Writes invoice JSON to S3 / COMPLIANCE_BUCKET for billing pipeline
+"""
+import os, time, json
+import boto3
+import redis
+
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/3")
+redis_client = redis.from_url(REDIS_URL)
+COMPLIANCE_BUCKET = os.environ.get("COMPLIANCE_BUCKET")
+PRICE_PER_1K_TOKENS = float(os.environ.get("PRICE_PER_1K_TOK", "0.5"))
+
+def collect_and_invoice():
+    # example: token counters stored as redis hash "llm:tokens"
+    data = redis_client.hgetall("llm:tokens")
+    invoices = []
+    for tenant, tokens_b in data.items():
+        tokens = int(tokens_b)
+        cost = (tokens / 1000.0) * PRICE_PER_1K_TOKENS
+        invoice = {"tenant": tenant.decode(), "tokens": tokens, "cost": cost, "ts": int(time.time())}
+        invoices.append(invoice)
+    # reset counters
+    redis_client.delete("llm:tokens")
+    # persist invoices
+    out = f"/tmp/invoice_{int(time.time())}.json"
+    open(out,"w").write(json.dumps(invoices, indent=2))
+    if COMPLIANCE_BUCKET:
+        s3 = boto3.client("s3")
+        key = f"billing/invoices/{os.path.basename(out)}"
+        s3.upload_file(out, COMPLIANCE_BUCKET, key)
+    print("Wrote invoices", out)
+
+if __name__=="__main__":
+    while True:
+        collect_and_invoice()
+        time.sleep(int(os.environ.get("BILL_PERIOD_SEC","3600")))
+
*** End Patch
*** Begin Patch
*** Add File: observability/es_indexer.py
+#!/usr/bin/env python3
+"""
+Optional Elasticsearch indexer for audit/provenance events.
+ - reads JSON audit events and forwards them to ES/Opensearch
+ - fallback: writes to local audit dir
+"""
+import os, json
+from datetime import datetime
+try:
+    from elasticsearch import Elasticsearch
+except Exception:
+    Elasticsearch = None
+
+ES_HOST = os.environ.get("ES_HOST")
+ES_USER = os.environ.get("ES_USER")
+ES_PASS = os.environ.get("ES_PASS")
+OUT_DIR = os.environ.get("AUDIT_DIR", "/tmp/aegis_audit")
+os.makedirs(OUT_DIR, exist_ok=True)
+
+def index(kind, record):
+    rec = {"kind": kind, "ts": datetime.utcnow().isoformat(), "record": record}
+    if Elasticsearch and ES_HOST:
+        es = Elasticsearch([ES_HOST], http_auth=(ES_USER, ES_PASS) if ES_USER else None)
+        try:
+            es.index(index="aegis-audit", body=rec)
+            return True
+        except Exception as e:
+            print("es index failed", e)
+    # fallback: local write
+    path = os.path.join(OUT_DIR, f"audit_{int(time.time())}.json")
+    open(path,"w").write(json.dumps(rec, indent=2))
+    return False
+
*** End Patch
*** Begin Patch
*** Add File: security/vault_csi_secretprovider.yaml
+apiVersion: secrets-store.csi.x-k8s.io/v1
+kind: SecretProviderClass
+metadata:
+  name: aegis-llm-keys
+  namespace: aegis
+spec:
+  provider: vault
+  parameters:
+    vaultAddress: "https://vault.internal"
+    roleName: "aegis-operator-role"
+    objects: |
+      - objectName: "openai_apikey"
+        secretPath: "secret/data/aegis/llm/openai"
+        secretKey: "apikey"
+      - objectName: "cosign_pubkey"
+        secretPath: "secret/data/aegis/cosign/pubkey"
+        secretKey: "pubkey"
+
+# Operators must configure Vault policies and roles to restrict access to secrets and ensure pods mount secrets with appropriate ServiceAccounts.
+
*** End Patch
*** Begin Patch
*** Add File: ui/approval_ui.py
+#!/usr/bin/env python3
+"""
+Simple operator approval UI (Flask) that lists pending plans from Approval API and allows approval/rejection.
+This is a lightweight operator tool for initial rollouts. Replace with a real React UI in production.
+"""
+import os, requests
+from flask import Flask, render_template_string, redirect, url_for, request
+
+APPROVAL_API = os.environ.get("APPROVAL_API", "http://approval-api.aegis.svc:8207")
+
+app = Flask("approval-ui")
+
+TEMPLATE = """
+<html><head><title>Aegis Approvals</title></head>
+<body>
+<h1>Pending Approvals</h1>
+{% for p in pending %}
+  <div style="border:1px solid #ccc;padding:8px;margin:8px">
+    <pre>{{ p|tojson(indent=2) }}</pre>
+    <form method="post" action="/approve">
+      <input type="hidden" name="id" value="{{ p.id }}" />
+      <input type="text" name="operator" placeholder="operator" />
+      <button type="submit">Approve</button>
+    </form>
+    <form method="post" action="/reject">
+      <input type="hidden" name="id" value="{{ p.id }}" />
+      <input type="text" name="operator" placeholder="operator" />
+      <input type="text" name="note" placeholder="note" />
+      <button type="submit">Reject</button>
+    </form>
+  </div>
+{% endfor %}
+</body></html>
+"""
+
+@app.route("/")
+def index():
+    r = requests.get(APPROVAL_API + "/pending")
+    pending = r.json() if r.ok else []
+    return render_template_string(TEMPLATE, pending=pending)
+
+@app.route("/approve", methods=["POST"])
+def approve():
+    pid = request.form["id"]
+    operator = request.form.get("operator","operator")
+    requests.post(f"{APPROVAL_API}/approve/{pid}", params={"operator": operator})
+    return redirect(url_for("index"))
+
+@app.route("/reject", methods=["POST"])
+def reject():
+    pid = request.form["id"]
+    operator = request.form.get("operator","operator")
+    note = request.form.get("note","")
+    requests.post(f"{APPROVAL_API}/reject/{pid}", params={"operator": operator, "note": note})
+    return redirect(url_for("index"))
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8300")))
+
*** End Patch
*** Begin Patch
*** Add File: ci/canary_multimodal_rollout.yml
+name: Canary Multimodal Rollout & Adversarial Tests
+on:
+  workflow_dispatch:
+    inputs:
+      model_s3:
+        required: true
+      region:
+        required: true
+      adversarial_suite:
+        required: false
+
+jobs:
+  canary:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Register canary model
+        env:
+          MODEL_REGISTRY_API: ${{ secrets.MODEL_REGISTRY_API }}
+        run: |
+          curl -XPOST -H "Content-Type: application/json" -d "{\"s3_path\":\"${{ github.event.inputs.model_s3 }}\",\"region\":\"${{ github.event.inputs.region }}\"}" $MODEL_REGISTRY_API/models/register
+      - name: Deploy canary to kserve (staging)
+        run: |
+          # operator: apply kserve manifest configured for the canary model
+          echo "apply canary inference manifest (operator)"
+      - name: Run adversarial suite
+        run: |
+          if [ -n "${{ github.event.inputs.adversarial_suite }}" ]; then
+            echo "Run adversarial suite: ${{ github.event.inputs.adversarial_suite }}"
+            # Placeholder for running adversarial prompts / image tests
+          else
+            echo "No adversarial suite specified, using default smoke tests"
+          fi
+      - name: Monitor metrics & decide
+        run: |
+          # Placeholder: monitor hallucination metrics and cost; call model_registry to promote or rollback
+          echo "Monitor and decide promote/rollback (operator implementation required)"
+
*** End Patch
