*** Begin Patch
*** Add File:quantum/crypto/hybrid_signer.py
+#!/usr/bin/env python3
+"""
+Hybrid signer for Aegis artifacts.
+
+Creates:
+ - Classical signature using cosign (requires cosign in PATH)
+ - Post-quantum (PQ) signature using python-oqs (if available) or via HSM integration hook
+ - Optional Rekor submission of combined metadata (via rekor_client)
+
+Design goals:
+ - Crypto agility: support multiple PQ algorithms and pluggable HSM provider
+ - Produce a single JSON metadata file containing both signatures and verification info
+ - Provide CLI-friendly functions used by MLflow logger or CI
+
+Usage:
+  python3 hybrid_signer.py sign --artifact /path/to/file --outdir ./sigs --cosign-key /path/to/key
+  python3 hybrid_signer.py verify --artifact /path/to/file --sigdir ./sigs
+  python3 hybrid_signer.py submit-rekor --artifact /path/to/file --sigdir ./sigs --rekor-server https://rekor.sigstore.dev
+
+Notes:
+ - If python-oqs is not available, the script emits a placeholder PQ signature and prints a warning.
+ - Replace pq_sign_hsm() implementation to use vendor HSM/PKCS#11 when available.
+"""
+import argparse
+import base64
+import hashlib
+import json
+import os
+import subprocess
+import sys
+import time
+
+try:
+    import oqs
+    OQS_AVAILABLE = True
+except Exception:
+    OQS_AVAILABLE = False
+
+def sha256_hex(path):
+    h = hashlib.sha256()
+    with open(path,"rb") as f:
+        for chunk in iter(lambda: f.read(8192), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def cosign_sign_blob(artifact_path, out_sig_path=None, key=None):
+    cmd = ["cosign", "sign-blob"]
+    if key:
+        cmd += ["--key", key]
+    if out_sig_path:
+        cmd += ["--output-signature", out_sig_path]
+    cmd += [artifact_path]
+    subprocess.check_call(cmd)
+    return out_sig_path
+
+def pq_sign_local(artifact_path, algo="Dilithium2"):
+    if not OQS_AVAILABLE:
+        return {"algorithm": algo, "signature_b64": None, "note": "python-oqs not installed; install or use HSM"}
+    # Using OQS Signature API
+    with oqs.Signature(algo) as signer:
+        with open(artifact_path, "rb") as f:
+            data = f.read()
+        sig = signer.sign(data)
+        # public key extraction depends on liboqs bindings; sometimes signer.generate_keypair() used
+    return {"algorithm": algo, "signature_b64": base64.b64encode(sig).decode("ascii")}
+
+def pq_sign_hsm_placeholder(artifact_path, algo="Dilithium2"):
+    # Placeholder: integrate with HSM / PKCS11 here for PQ signing.
+    # Example: call vendor CLI or pkcs11-tool wrapper to sign digest.
+    digest = sha256_hex(artifact_path)
+    return {"algorithm": algo, "signature_b64": None, "note": "HSM signing not configured; sign externally and populate metadata"}
+
+def build_metadata(artifact_path, classical_sig_path=None, pq_meta=None):
+    meta = {
+        "artifact": os.path.abspath(artifact_path),
+        "sha256": sha256_hex(artifact_path),
+        "timestamp": int(time.time()),
+        "classical": None,
+        "pq": None
+    }
+    if classical_sig_path and os.path.exists(classical_sig_path):
+        with open(classical_sig_path,"rb") as f:
+            meta["classical"] = {"tool":"cosign","signature_b64": base64.b64encode(f.read()).decode("ascii")}
+    if pq_meta:
+        meta["pq"] = pq_meta
+    return meta
+
+def sign_artifact(args):
+    artifact = args.artifact
+    outdir = os.path.abspath(args.outdir)
+    os.makedirs(outdir, exist_ok=True)
+    classical_sig = os.path.join(outdir, "classical.sig")
+    print("Creating classical signature (cosign)...")
+    try:
+        cosign_sign_blob(artifact, out_sig_path=classical_sig, key=args.cosign_key)
+        print("Classical signature written to:", classical_sig)
+    except Exception as e:
+        print("Classical signing failed:", e)
+        # proceed but mark error in metadata
+    print("Creating PQ signature...")
+    if args.hsm:
+        pq_meta = pq_sign_hsm_placeholder(artifact)
+    else:
+        pq_meta = pq_sign_local(artifact, algo=args.pq_alg)
+    meta = build_metadata(artifact, classical_sig_path=classical_sig, pq_meta=pq_meta)
+    meta_path = os.path.join(outdir, "hybrid-signature.json")
+    with open(meta_path,"w") as f:
+        json.dump(meta, f, indent=2)
+    print("Hybrid signature metadata written to:", meta_path)
+    if args.rekor and args.rekor_server:
+        try:
+            from quantum.crypto.rekor_client import submit_rekor_entry
+            submit_rekor_entry(meta_path, args.rekor_server)
+        except Exception as e:
+            print("Rekor submission failed:", e)
+
+def verify_artifact(args):
+    sigdir = args.sigdir
+    meta_path = os.path.join(sigdir, "hybrid-signature.json")
+    if not os.path.exists(meta_path):
+        print("hybrid-signature.json not found in", sigdir)
+        sys.exit(2)
+    meta = json.load(open(meta_path))
+    artifact = args.artifact
+    print("Verifying artifact digest...")
+    if sha256_hex(artifact) != meta.get("sha256"):
+        print("SHA256 mismatch! Artifact may be different from metadata.")
+        sys.exit(3)
+    classical_sig_file = os.path.join(sigdir, "classical.sig")
+    if os.path.exists(classical_sig_file):
+        try:
+            subprocess.check_call(["cosign","verify-blob","--signature", classical_sig_file, artifact])
+            print("Classical signature verified via cosign.")
+        except subprocess.CalledProcessError as e:
+            print("Classical signature verification failed:", e)
+    else:
+        print("No classical signature file; check hybrid metadata.")
+    pq = meta.get("pq", {})
+    if pq.get("signature_b64"):
+        if not OQS_AVAILABLE:
+            print("PQ signature present but local verification not available (python-oqs missing).")
+        else:
+            print("PQ verification stub: implement per chosen PQ signer/HSM and public key storage.")
+    else:
+        print("No PQ signature available in metadata.")
+
+def main():
+    p = argparse.ArgumentParser()
+    sub = p.add_subparsers(dest="cmd")
+    sign = sub.add_parser("sign")
+    sign.add_argument("--artifact", required=True)
+    sign.add_argument("--outdir", required=True)
+    sign.add_argument("--cosign-key", default=None)
+    sign.add_argument("--pq-alg", default="Dilithium2")
+    sign.add_argument("--hsm", action="store_true", help="Use HSM-based PQ signer (requires vendor integration)")
+    sign.add_argument("--rekor", action="store_true", help="Submit hybrid metadata to Rekor")
+    sign.add_argument("--rekor-server", default="https://rekor.sigstore.dev")
+    verify = sub.add_parser("verify")
+    verify.add_argument("--artifact", required=True)
+    verify.add_argument("--sigdir", required=True)
+    args = p.parse_args()
+    if args.cmd == "sign":
+        sign_artifact(args)
+    elif args.cmd == "verify":
+        verify_artifact(args)
+    else:
+        p.print_help()
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/crypto/rekor_client.py
+#!/usr/bin/env python3
+"""
+Minimal Rekor client integration to submit hybrid metadata as a Rekor "entry".
+This is intentionally small and uses Rekor's REST API.
+For production use, use the official Rekor client or integrate via cosign workflows.
+"""
+import json, requests, os, sys
+
+def submit_rekor_entry(hybrid_meta_path, rekor_server="https://rekor.sigstore.dev"):
+    meta = json.load(open(hybrid_meta_path))
+    payload = {
+        "apiVersion": "0.0.1",
+        "kind": "hashedrekord",
+        "spec": {
+            "data": {
+                "hash": {
+                    "algorithm": "sha256",
+                    "value": meta["sha256"]
+                }
+            },
+            "signature": {
+                # store classical signature as base64 string if present
+                "content": meta.get("classical", {}).get("signature_b64", "") 
+            },
+            "publicKey": {
+                "content": meta.get("pq", {}).get("public_key", "") or ""
+            },
+            "extraData": {
+                "hybrid_metadata": meta
+            }
+        }
+    }
+    url = rekor_server.rstrip("/") + "/api/v1/log"
+    # Rekor API is more complex; this is a simplified placeholder that demonstrates intent.
+    # Use official Rekor client or cosign integration in production.
+    r = requests.post(url, json=payload, timeout=30)
+    if r.status_code not in (200,201):
+        raise RuntimeError(f"Rekor submission failed: {r.status_code} {r.text}")
+    print("Submitted Rekor entry (server response):", r.json())
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("Usage: rekor_client.py <hybrid_meta.json> [rekor-server]")
+        sys.exit(1)
+    submit_rekor_entry(sys.argv[1], sys.argv[2] if len(sys.argv)>2 else "https://rekor.sigstore.dev")
+
*** End Patch
*** Begin Patch
*** Add File:quantum/vault_pq/README.md
+Post-Quantum Key Management (Vault + HSM) — Guidance
+===================================================
+
+Goal: prepare Aegis key management for PQ readiness while preserving current signing flows.
+
+Approach summary:
+1) Hybrid signing: produce both classical (cosign) and PQ signatures today; store both signatures in provenance metadata (Rekor + MLflow).
+2) Prepare Vault and HSM integration: create Terraform placeholders to provision HSM or KMS that will host PQ keys when available.
+3) Use Vault KV as a verified public-key store for PQ verification keys until HSM-integrated PQ signing is available.
+4) Maintain rotation & key metadata: "valid_from", "algorithm", "status", "replaced_by".
+
+Operational steps (short):
+- Create a restricted Vault KV path for PQ verification keys: secret/pq-keys/<customer>
+- Ensure only the signing service (or HSM integration) can write PQ private keys; verification keys are readable by auditors and verification services.
+- Add runbook for rotating PQ keys and for backfilling PQ metadata into Rekor entries.
+
+See quantum/vault_pq/terraform_pq_key_example.tf for a Terraform snippet to store PQ public key placeholder.
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/braket_adapter.py
+"""
+AWS Braket adapter skeleton.
+
+Requires AWS credentials & boto3 + amazon-braket-sdk (if using SDK).
+This adapter is a skeleton to submit tasks to AWS Braket; production code should handle credentials, regions and job management robustly.
+"""
+import uuid, json, os, time
+import boto3
+
+from .adapter import QuantumBackendAdapter
+
+class BraketAdapter(QuantumBackendAdapter):
+    def __init__(self, region_name=None):
+        self.region = region_name or os.environ.get("AWS_REGION", "us-east-1")
+        self.client = boto3.client("braket", region_name=self.region)
+        self.jobs = {}
+
+    def submit_job(self, job_spec: dict) -> str:
+        # job_spec: { "circuit_s3_uri": "s3://...", "device": "arn:aws:braket:..." , "shots": 1000 }
+        job_id = str(uuid.uuid4())
+        # For demonstration we put a placeholder; production should call create_job or run_task APIs depending on service
+        # Example SDK flow omitted; instead record request and return job_id
+        self.jobs[job_id] = {"status":"SUBMITTED","spec":job_spec, "submitted_at": time.time()}
+        # TODO: implement create_quantum_task / create_job with proper S3 input/output handling
+        return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        return self.jobs.get(job_id, {"status":"UNKNOWN"})
+
+    def cancel_job(self, job_id: str) -> bool:
+        # Implement cancellation via Braket API if supported
+        if job_id in self.jobs:
+            self.jobs[job_id]["status"] = "CANCELLED"
+            return True
+        return False
+
+    def list_backends(self) -> list:
+        # Query Braket for available devices (skeleton)
+        try:
+            resp = self.client.search_devices()
+            devices = [{"id": d["deviceArn"], "type": d["deviceType"]} for d in resp.get("devices", [])]
+            return devices
+        except Exception:
+            return []
+
*** End Patch
*** Begin Patch
*** Add File:quantum/qbackend/azure_adapter.py
+"""
+Azure Quantum adapter skeleton.
+Requires azure-quantum SDK and subscription credentials.
+This file is a placeholder demonstrating where to integrate Azure Quantum provider.
+"""
+import uuid, os, time
+
+from .adapter import QuantumBackendAdapter
+
+class AzureAdapter(QuantumBackendAdapter):
+    def __init__(self, workspace=None):
+        self.workspace = workspace or os.environ.get("AZURE_QUANTUM_WORKSPACE")
+        self.jobs = {}
+
+    def submit_job(self, job_spec: dict) -> str:
+        job_id = str(uuid.uuid4())
+        # TODO: create a job via azure.quantum SDK
+        self.jobs[job_id] = {"status":"SUBMITTED","spec":job_spec,"submitted_at":time.time()}
+        return job_id
+
+    def get_result(self, job_id: str) -> dict:
+        return self.jobs.get(job_id, {"status":"UNKNOWN"})
+
+    def cancel_job(self, job_id: str) -> bool:
+        if job_id in self.jobs:
+            self.jobs[job_id]["status"] = "CANCELLED"
+            return True
+        return False
+
+    def list_backends(self) -> list:
+        # TODO: return backends available in workspace
+        return []
+
*** End Patch
*** Begin Patch
*** Add File:quantum/mlflow_logger/quantum_logger.py
+"""
+Quantum experiment logger integrated with MLflow and hybrid signing.
+
+Responsibilities:
+ - Log circuit (QASM), backend metadata and result counts to MLflow
+ - Optionally store raw shots to lakeFS/S3 and record S3 path
+ - Produce combined metadata file and call hybrid_signer to sign (classical + PQ)
+ - Optionally submit the hybrid metadata to Rekor (via rekor_client)
+
+Usage: call log_quantum_experiment(...) from compute pipelines or job broker worker after obtaining result.
+"""
+import os, json, time, subprocess
+import mlflow
+from typing import Dict
+
+from quantum.crypto.hybrid_signer import sha256_hex
+
+def upload_to_s3(local_path: str, bucket: str, prefix: str):
+    key = f"{prefix}/{os.path.basename(local_path)}"
+    dest = f"s3://{bucket}/{key}"
+    subprocess.check_call(["aws","s3","cp", local_path, dest])
+    return dest
+
+def log_quantum_experiment(run_name: str, circuit_qasm: str, backend_info: Dict, shots_counts: Dict, raw_shots_file: str=None, s3_bucket: str=None, s3_prefix: str=None, cosign_key: str=None, rekor_server: str=None, use_hsm_pq: bool=False):
+    mlflow.set_experiment("quantum-experiments")
+    with mlflow.start_run(run_name=run_name) as run:
+        mlflow.log_param("backend_id", backend_info.get("id"))
+        mlflow.log_param("backend_type", backend_info.get("type"))
+        mlflow.log_param("circuit_sha256", sha256_hex_from_str(circuit_qasm))
+        # persist circuit
+        circuit_file = f"/tmp/{run.info.run_id}_circuit.qasm"
+        with open(circuit_file,"w") as f:
+            f.write(circuit_qasm)
+        mlflow.log_artifact(circuit_file, artifact_path="circuit")
+        # results artifact
+        counts_file = f"/tmp/{run.info.run_id}_counts.json"
+        with open(counts_file,"w") as f:
+            json.dump(shots_counts, f)
+        mlflow.log_artifact(counts_file, artifact_path="results")
+        # raw shots -> optional external storage
+        s3_path = None
+        if raw_shots_file and s3_bucket:
+            s3_path = upload_to_s3(raw_shots_file, s3_bucket, s3_prefix or "quantum/raw")
+            mlflow.log_param("raw_shots_s3", s3_path)
+        # create metadata for signing
+        meta = {
+            "run_id": run.info.run_id,
+            "timestamp": int(time.time()),
+            "backend": backend_info,
+            "counts": shots_counts,
+            "raw_shots_s3": s3_path
+        }
+        meta_file = f"/tmp/{run.info.run_id}_meta.json"
+        with open(meta_file,"w") as f:
+            json.dump(meta, f, indent=2)
+        mlflow.log_artifact(meta_file, artifact_path="meta")
+        # sign metadata with hybrid signer
+        sigdir = os.path.join("/tmp", f"{run.info.run_id}_sigs")
+        os.makedirs(sigdir, exist_ok=True)
+        signer_cmd = ["python3", os.path.join(os.path.dirname(__file__),"../crypto/hybrid_signer.py"), "sign", "--artifact", meta_file, "--outdir", sigdir]
+        if cosign_key:
+            signer_cmd += ["--cosign-key", cosign_key]
+        if use_hsm_pq:
+            signer_cmd += ["--hsm"]
+        if rekor_server:
+            signer_cmd += ["--rekor", "--rekor-server", rekor_server]
+        subprocess.check_call(signer_cmd)
+        # attach signature metadata artifact
+        mlflow.log_artifact(os.path.join(sigdir,"hybrid-signature.json"), artifact_path="meta/sigs")
+        return run.info.run_id
+
+def sha256_hex_from_str(s: str):
+    import hashlib
+    h = hashlib.sha256()
+    h.update(s.encode("utf-8"))
+    return h.hexdigest()
+
*** End Patch
*** Begin Patch
*** Add File:quantum/opapolicies/quantum_admission.rego
+package aegis.admission.quanta
+
+# Admission policy to require hybrid signature metadata for any promoted quantum experiment artifact.
+default allow = false
+
+has_hybrid_signatures[true] {
+    meta := input.request.object.metadata.annotations["aegis.hybrid-signature"]
+    meta != ""
+}
+
+allow {
+    # If resource references quantum experiments (label or annotation), require signature
+    input.request.kind.kind == "ModelPromotion"
+    input.request.object.metadata.labels["artifact-type"] == "quantum-experiment"
+    has_hybrid_signatures[_]
+}
+
+deny[msg] {
+    input.request.kind.kind == "ModelPromotion"
+    input.request.object.metadata.labels["artifact-type"] == "quantum-experiment"
+    not has_hybrid_signatures[_]
+    msg = "Quantum experiment artifacts must include hybrid signature metadata (classical + PQ)."
+}
+
*** End Patch
*** Begin Patch
*** Add File:quantum/argo/quantum_job_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-quantum-job-
+spec:
+  entrypoint: quantum-pipeline
+  templates:
+  - name: quantum-pipeline
+    steps:
+      - - name: submit-quantum-job
+          template: submit-job
+        - name: wait-and-fetch
+          template: fetch-result
+
+  - name: submit-job
+    container:
+      image: aegis/quantum-job-broker:latest
+      command: ["/bin/sh","-c"]
+      args: ["curl -s -X POST http://aegis-quantum-broker/submit -H 'Content-Type: application/json' -d '{\"circuit_qasm\":\"{{workflow.parameters.circuit}}\",\"shots\":1024}' > /tmp/jobid && cat /tmp/jobid"]
+
+  - name: fetch-result
+    container:
+      image: aegis/quantum-job-broker:latest
+      command: ["/bin/sh","-c"]
+      args: ["JOBID=$(cat /tmp/jobid | jq -r .job_id); sleep 5; curl -s http://aegis-quantum-broker/status/$JOBID | jq ."]
+
+  ttlStrategy:
+    secondsAfterCompletion: 86400
+  arguments:
+    parameters:
+      - name: circuit
+        value: "placeholder-circuit"
+
*** End Patch
*** Begin Patch
*** Add File:quantum/ci/quantum_smoke_ci.yml
+name: Quantum Smoke CI
+on:
+  workflow_dispatch:
+jobs:
+  smoke-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.10'
+      - name: Install deps
+        run: |
+          pip install -r quantum/requirements.txt || true
+      - name: Run smoke demo
+        run: |
+          python3 quantum/tests/smoke_run.sh
+
*** End Patch
*** Begin Patch
*** Add File:quantum/tests/smoke_run.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "Running extended quantum smoke run..."
+python3 -c "from quantum.notebooks.demo_quantum_pipeline import main; main()"
+echo "Running job broker smoke..."
+python3 -c "from quantum.job_broker.app import health as h; print('Broker health:', h())"
+echo "Hybrid signer smoke (create sample file and sign)..."
+echo "hello quantum" > /tmp/aegis_quantum_demo.bin
+python3 quantum/crypto/hybrid_signer.py sign --artifact /tmp/aegis_quantum_demo.bin --outdir /tmp/aegis_qsigs || true
+echo "Smoke run completed. Check /tmp/aegis_qsigs for hybrid-signature.json"
+
*** End Patch
*** Begin Patch
*** Add File:quantum/README_QUANTUM_READY.md
+Aegis Quantum-Ready Patch — Overview & How to use
+================================================
+
+This patch equips Aegis with two primary capabilities:
+ 1) Cryptographic & security readiness via hybrid signer + Rekor submission and Vault PQ guidance
+ 2) Functional quantum compute readiness: adapters, job broker, MLflow logging and Argo workflow skeletons
+
+Quickstart
+----------
+1. Install dependencies:
+   python3 -m venv venv && source venv/bin/activate
+   pip install -r quantum/requirements.txt
+
+2. Run the smoke tests:
+   ./quantum/tests/smoke_run.sh
+
+3. Start the job broker (optional):
+   python3 quantum/job_broker/app.py &
+   python3 quantum/job_broker/worker.py &
+
+4. Run demo pipeline:
+   python3 quantum/notebooks/demo_quantum_pipeline.py
+
+Key files
+---------
+- quantum/crypto/hybrid_signer.py : hybrid signing CLI (cosign + PQ)
+- quantum/crypto/rekor_client.py : minimal Rekor submission stub (replace with official client in prod)
+- quantum/qbackend/* : adapters for Qiskit, Braket, Azure (skeletons)
+- quantum/job_broker/* : FastAPI job broker + worker + k8s manifest
+- quantum/mlflow_logger/quantum_logger.py : MLflow integration and hybrid signing integration
+- quantum/opapolicies/quantum_admission.rego : sample OPA policy requiring hybrid signature metadata
+- quantum/argo/quantum_job_workflow.yaml : Argo Workflows skeleton to orchestrate submission -> fetch
+- quantum/vault_pq/* : runbook + TF placeholder for PQ public keys and HSM integration
+
+Operational notes & next steps
+------------------------------
+- Replace the rekor_client placeholders with official Rekor/cosign integrations for production provenance.
+- Integrate PQ signing into HSM or vendor PKCS#11 when available; update hybrid_signer to call HSM.
+- For cloud QPUs (Braket/IBM/Azure), fill in provider-specific submission & result fetch logic in the adapters.
+- Add robust storage for job queue & results (move from SQLite to durable DB/managed queue for production).
+- Integrate admission controller webhook to enforce OPA policies for quantum artifacts.
+- Extend CI to build & scan quantum Docker images and add operator monitoring dashboards (Grafana).
+
+Security & compliance
+---------------------
+- Use Vault to store PQ public keys for verification and to store HSM access credentials securely.
+- Maintain rotation metadata and store key versions in compliance/evidence when rotating.
+- Ensure export control & legal compliance for quantum cryptography with counsel.
+
*** End Patch
*** End Patch
