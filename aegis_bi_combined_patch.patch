*** Begin Patch
*** Add File:bi/README_BI_RUNBOOK.md
````markdown
# Aegis Business Intelligence & Analytics Runbook

This runbook and artifacts let you extract Aegis evidence and operational data into a data warehouse, transform it with dbt, and visualize it in Superset. It targets a self-hosted AWS-friendly setup but is provider-agnostic in the transformations.

What this patch provides (high-level)
- dbt project skeleton with staging and mart models for:
  - job_runs (per-workflow/job telemetry & carbon)
  - reconciliation reports (estimated vs measured)
  - daily team rollups (carbon & cost)
- ingestion script to copy JSON evidence from S3 into a Postgres warehouse (ingest_s3_evidence.py).
- Airflow DAG to run ingestion -> dbt -> daily report generation.
- Superset dashboard JSON (carbon & cost summary) to import into Superset.
- Operator runbook with step-by-step staging instructions.

Pre-requisites
- A Postgres-compatible data warehouse (host, port, db, user, password) accessible by Airflow / ingest job.
- AWS credentials with S3 read access to the evidence bucket.
- Airflow (v2.x) deployed with access to run Python/Bash operators.
- dbt-core (installed in the environment where dbt run will execute) and a dbt profile configured to connect to your warehouse.
- Superset installed (or hosted) with import access.

Environment variables (examples)
- EVIDENCE_BUCKET=aegis-evidence
- EVIDENCE_PREFIX=trials/
- WAREHOUSE_DB=postgres://user:password@host:5432/aegis_dw
- AWS_REGION=us-west-2

Quick operator flow (summary)
1. Deploy the ingestion job (Airflow DAG) and ensure it can read S3 and connect to the warehouse.
2. Run the ingestion task to populate raw tables: raw_job_runs, raw_reconciliations, raw_costs.
3. Run dbt (via Airflow or manually) to build staging and marts: `dbt run`.
4. Import the Superset dashboard JSON into Superset and point datasources at the warehouse tables/views produced by dbt.
5. Validate numbers against S3 evidence and reconciliation reports.
6. Schedule the Airflow DAG daily (or hourly) for continuous ETL.

Acceptance criteria (for BI correctness)
- dbt models run successfully and produce materialized tables/views in the warehouse.
- Sample manual reconciliation: pick 20 jobs -> the computed median relative error and daily aggregated metrics match those in signed reconciliation reports.
- Superset dashboard loads and shows per-team daily carbon & cost.

Operator notes and next steps
- You can replace Postgres with BigQuery / Snowflake by changing dbt profile and ingest script.
- Consider adding data catalog (DataHub) and data quality tests (Great Expectations) after the first week of data.
- For scale, consider using bulk copy (COPY in Postgres or native cloud ingestion) rather than per-row inserts.
