*** Begin Patch
*** Add File:monitoring/runid-writer/requirements.txt
kubernetes
prometheus-client
pyyaml
*** End Patch
*** Begin Patch
*** Add File:monitoring/runid-writer/run_id_writer.py
#!/usr/bin/env python3
"""
Per-node Run-ID writer for node_exporter textfile collector.

Purpose:
 - Each daemon runs on a node and writes a small prometheus textfile metric that maps node -> active run_id.
 - Prometheus (node_exporter) will pick up the file and expose metric aegis_node_run_info{node="<node>",run_id="<run>"} 1
 - The carbon estimator can then query Prometheus with run_id labels to compute per-run energy.

Behavior:
 - Looks for pods scheduled on the same node that have label "aegis.run_id" (or annotation "aegis/run_id")
 - Picks the latest pod in Running state and writes the mapping.
 - Writes to /var/lib/node_exporter/textfile_collector/aegis_run_id.prom (hostPath mounted by DaemonSet)
 - Interval: configurable via env RUN_ID_WRITE_INTERVAL (seconds)
"""
import os
import time
import socket
from kubernetes import client, config
from kubernetes.client.rest import ApiException

TEXTFILE_DIR = os.environ.get("TEXTFILE_DIR", "/var/lib/node_exporter/textfile_collector")
INTERVAL = int(os.environ.get("RUN_ID_WRITE_INTERVAL", "15"))
NODE_NAME = os.environ.get("K8S_NODE_NAME") or os.environ.get("NODE_NAME") or socket.gethostname()
LABEL_KEY = os.environ.get("RUN_ID_LABEL_KEY", "aegis.run_id")
ANNOTATION_KEY = os.environ.get("RUN_ID_ANNOTATION_KEY", "aegis/run_id")
OUTPUT_FILE = os.path.join(TEXTFILE_DIR, "aegis_run_id.prom")

def get_k8s_client():
    try:
        config.load_incluster_config()
    except Exception:
        config.load_kube_config()
    return client.CoreV1Api()

def find_run_id_on_node(v1: client.CoreV1Api, node_name: str):
    # List pods on this node
    try:
        pods = v1.list_pod_for_all_namespaces(field_selector=f"spec.nodeName={node_name}")
    except ApiException as e:
        print("K8s API error:", e)
        return None
    # Look for pods with label or annotation
    candidates = []
    for p in pods.items:
        # skip completed/failed pods
        phase = p.status.phase
        if phase != "Running":
            continue
        run_id = None
        if p.metadata.labels and LABEL_KEY in p.metadata.labels:
            run_id = p.metadata.labels[LABEL_KEY]
        elif p.metadata.annotations and ANNOTATION_KEY in p.metadata.annotations:
            run_id = p.metadata.annotations[ANNOTATION_KEY]
        if run_id:
            # choose last started time as priority
            ts = p.status.start_time.timestamp() if p.status.start_time else 0
            candidates.append((ts, run_id, p.metadata.name, p.metadata.namespace))
    if not candidates:
        return None
    # pick latest
    candidates.sort(reverse=True)
    ts, run_id, name, ns = candidates[0]
    return run_id

def write_textfile(node_name: str, run_id: str):
    os.makedirs(TEXTFILE_DIR, exist_ok=True)
    content = ''
    if run_id:
        # expose as gauge 1 with labels
        content = f'aegis_node_run_info{{node="{node_name}",run_id="{run_id}"}} 1\n'
    else:
        # write empty (or zero) - we'll remove file to indicate no run
        content = f'# no run on node {node_name}\n'
    with open(OUTPUT_FILE, "w") as f:
        f.write(content)

def main():
    v1 = get_k8s_client()
    print("RunID writer starting on node:", NODE_NAME, "writing to", OUTPUT_FILE)
    while True:
        try:
            run_id = find_run_id_on_node(v1, NODE_NAME)
            write_textfile(NODE_NAME, run_id)
        except Exception as e:
            print("Error in runid loop:", e)
        time.sleep(INTERVAL)

if __name__ == "__main__":
    main()
*** End Patch
*** Begin Patch
*** Add File:monitoring/runid-writer/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: aegis-runid-writer
  namespace: monitoring
  labels:
    app: aegis-runid-writer
spec:
  selector:
    matchLabels:
      app: aegis-runid-writer
  template:
    metadata:
      labels:
        app: aegis-runid-writer
    spec:
      serviceAccountName: aegis-runid-writer-sa
      containers:
        - name: runid-writer
          image: ghcr.io/yourorg/aegis-runid-writer:latest
          env:
            - name: TEXTFILE_DIR
              value: "/var/lib/node_exporter/textfile_collector"
            - name: RUN_ID_WRITE_INTERVAL
              value: "15"
            - name: RUN_ID_LABEL_KEY
              value: "aegis.run_id"
            - name: RUN_ID_ANNOTATION_KEY
              value: "aegis/run_id"
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - name: textfile
              mountPath: /var/lib/node_exporter/textfile_collector
      volumes:
        - name: textfile
          hostPath:
            path: /var/lib/node_exporter/textfile_collector
            type: DirectoryOrCreate
      tolerations:
        - operator: "Exists"
*** End Patch
*** Begin Patch
*** Add File:monitoring/runid-writer/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aegis-runid-writer-sa
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aegis-runid-writer-role
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aegis-runid-writer-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: aegis-runid-writer-role
subjects:
  - kind: ServiceAccount
    name: aegis-runid-writer-sa
    namespace: monitoring
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/node-exporter-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      k8s-app: node-exporter
  namespaceSelector:
    any: true
  endpoints:
    - port: metrics
      path: /metrics
      interval: 15s
      honorLabels: true
*** End Patch
*** Begin Patch
*** Add File:carbon/estimator/runner_requirements.txt
mlflow
boto3
prometheus-api-client
requests
python-dateutil
*** End Patch
*** Begin Patch
*** Add File:carbon/estimator/runner.py
#!/usr/bin/env python3
"""
MLflow-driven estimator runner.

This cron-runner:
 - Lists recent MLflow runs (finished) in last N minutes
 - Skips runs already tagged 'carbon_processed' == 'true'
 - Calls estimator.estimator.py internally (import) or calls estimator.main to compute CO2e for the run's start/end
 - Attaches estimator evidence key to MLflow run and sets carbon_processed tag

Requires:
 - MLFLOW_TRACKING_URI env
 - PROM_URL, EVIDENCE_BUCKET, AWS_REGION and CARBON_API_URL / CARBON_API_KEY for estimator
"""
import os
import time
from datetime import datetime, timedelta, timezone
import mlflow
from mlflow.tracking import MlflowClient
import subprocess

MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
LOOKBACK_MIN = int(os.environ.get("CARBON_LOOKBACK_MIN", "60"))  # look for runs that ended in the last N minutes
ESTIMATOR_SCRIPT = os.environ.get("CARBON_ESTIMATOR_SCRIPT", "/opt/estimator/estimator.py")

if not MLFLOW_TRACKING_URI:
    raise RuntimeError("MLFLOW_TRACKING_URI not set")

mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
client = MlflowClient()

def iso_or_epoch_to_epoch(val):
    try:
        return int(val)
    except:
        try:
            return int(datetime.fromisoformat(val).replace(tzinfo=timezone.utc).timestamp())
        except:
            return None

def run_estimator_for_run(run):
    # fetch start and end times; prefer attributes if present
    info = run.info
    start_time = run.info.start_time // 1000 if run.info.start_time else None
    end_time = run.info.end_time // 1000 if run.info.end_time else None
    if not end_time:
        print("Run not finished:", run.info.run_id)
        return
    # call estimator script for the run time window
    cmd = [
        "python", ESTIMATOR_SCRIPT,
        "--run-id", run.info.run_id,
        "--start", str(start_time),
        "--end", str(end_time)
    ]
    env = os.environ.copy()
    print("Running estimator for run", run.info.run_id)
    subprocess.run(cmd, env=env, check=True)
    # mark run processed
    client.set_tag(run.info.run_id, "carbon_processed", "true")
    print("Marked run", run.info.run_id, "as carbon_processed")

def main():
    window_end = int(time.time())
    window_start = window_end - (LOOKBACK_MIN * 60)
    # find runs that ended in the window
    runs = client.search_runs(
        experiment_ids=None,
        filter_string=f"attributes.status = 'FINISHED'",
        max_results=1000
    )
    for r in runs:
        if r.info.end_time is None:
            continue
        end_ts = r.info.end_time // 1000
        if end_ts < window_start or end_ts > window_end:
            continue
        # skip if already processed
        tags = {t.key: t.value for t in r.data.tags}
        if tags.get("carbon_processed", "false") == "true":
            continue
        try:
            run_estimator_for_run(r)
        except Exception as e:
            print("Estimator failed for run", r.info.run_id, e)

if __name__ == "__main__":
    main()
*** End Patch
*** Begin Patch
*** Add File:carbon/estimator/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: carbon-estimator-runner
  namespace: aegis
spec:
  schedule: "*/15 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: carbon-estimator-sa
          containers:
            - name: estimator-runner
              image: ghcr.io/yourorg/aegis-carbon-estimator:latest
              env:
                - name: MLFLOW_TRACKING_URI
                  value: "REPLACE_MLFLOW_TRACKING_URI"
                - name: PROM_URL
                  value: "REPLACE_PROM_URL"
                - name: EVIDENCE_BUCKET
                  value: "REPLACE_EVIDENCE_BUCKET"
                - name: AWS_REGION
                  value: "REPLACE_AWS_REGION"
                - name: CARBON_API_URL
                  value: "REPLACE_CARBON_API_URL"
                - name: CARBON_API_KEY
                  value: "REPLACE_CARBON_API_KEY"
                - name: CARBON_ESTIMATOR_SCRIPT
                  value: "/opt/estimator/estimator.py"
                - name: CARBON_LOOKBACK_MIN
                  value: "60"
              volumeMounts:
                - name: estimator
                  mountPath: /opt/estimator
          restartPolicy: OnFailure
          volumes:
            - name: estimator
              configMap:
                name: carbon-estimator-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: carbon-estimator-config
  namespace: aegis
data:
  estimator.py: |
    # estimator.py should be the content of carbon/estimator/estimator.py from repo
    # For operator convenience, we mount a prepared image that contains estimator already; this is placeholder.
*** End Patch
*** Begin Patch
*** Add File:carbon/guard/requirements.txt
fastapi
uvicorn[standard]
redis
pydantic
*** End Patch
*** Begin Patch
*** Add File:carbon/guard/app.py
#!/usr/bin/env python3
"""
Carbon Guard — a small HTTP service used by a validating webhook or admission flow to reserve/check carbon budgets.

API:
 - POST /reserve
    { "tenant": "team-a", "estimate_kg": 12.3, "run_id": "r1" }
   -> { "allowed": true/false, "reason": "..." }

This service uses Redis-based CarbonAccounting (see cost/carbon_accounting.py) to atomically check and reserve budget.
"""
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from cost.carbon_accounting import CarbonAccounting

app = FastAPI(title="Aegis Carbon Guard")
REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379/0")
accounting = CarbonAccounting()

class ReserveRequest(BaseModel):
    tenant: str
    estimate_kg: float
    run_id: str = None

@app.post("/reserve")
def reserve(req: ReserveRequest):
    ok = accounting.check_and_reserve(req.tenant, req.estimate_kg)
    if ok:
        return {"allowed": True}
    else:
        return {"allowed": False, "reason": "budget_exceeded"}

@app.post("/release")
def release(req: ReserveRequest):
    # For simplicity, release reduces used amount. Implement carefully in operator flows.
    if accounting.r:
        accounting.r.decrbyfloat(f"carbon:{req.tenant}:used", req.estimate_kg)
    return {"ok": True}
*** End Patch
*** Begin Patch
*** Add File:carbon/guard/k8s-manifest.yaml
apiVersion: v1
kind: Service
metadata:
  name: aegis-carbon-guard
  namespace: aegis
spec:
  ports:
    - port: 8080
      targetPort: 8080
  selector:
    app: aegis-carbon-guard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aegis-carbon-guard
  namespace: aegis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: aegis-carbon-guard
  template:
    metadata:
      labels:
        app: aegis-carbon-guard
    spec:
      containers:
        - name: guard
          image: ghcr.io/yourorg/aegis-carbon-guard:latest
          env:
            - name: REDIS_URL
              value: "REPLACE_REDIS_URL"
          ports:
            - containerPort: 8080
*** End Patch
*** Begin Patch
*** Add File:carbon/webhook/validating-webhook-example.yaml
# NOTE: operator must generate TLS certs (cert-manager recommended) and populate caBundle
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: aegis-carbon-validate
webhooks:
  - name: carbon-guard.aegis.local
    clientConfig:
      service:
        name: aegis-carbon-webhook
        namespace: aegis
        path: "/validate"
      caBundle: "REPLACE_WITH_CA_BUNDLE"
    rules:
      - apiGroups: [""]
        apiVersions: ["v1"]
        operations: ["CREATE"]
        resources: ["pods"]
    admissionReviewVersions: ["v1"]
    sideEffects: None
    timeoutSeconds: 10
*** End Patch
*** Begin Patch
*** Add File:carbon/webhook/sample-admission-invoker.py
#!/usr/bin/env python3
"""
Example client showing how an external controller (or scheduler) would call Carbon Guard before admitting a job.
This is a template illustrating integration: the real integration is via mutating/validating webhook pointing to your admission endpoint.
"""
import requests
import os
def check_reserve(tenant, estimate_kg, run_id):
    url = os.environ.get("CARBON_GUARD_URL", "http://aegis-carbon-guard.aegis.svc.cluster.local:8080/reserve")
    r = requests.post(url, json={"tenant": tenant, "estimate_kg": estimate_kg, "run_id": run_id}, timeout=5)
    r.raise_for_status()
    return r.json()
*** End Patch
*** Begin Patch
*** Add File:policies/gatekeeper-carbon-constraint-template.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8scarbonpolicy
spec:
  crd:
    spec:
      names:
        kind: K8sCarbonPolicy
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8scarbonpolicy
        violation[{"msg": msg}] {
          input.review.object.metadata.annotations["carbon.defer"] == "true"
          # For Gatekeeper-based policy you'd rely on precomputed data (data.carbon) or external checks.
          # This RegO returns a violation if carbon.defer==true and no external approval tag present.
          not input.review.object.metadata.annotations["carbon.approved"]
          msg := "Pod requests defer but is not approved for high-carbon window"
        }
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/alerts-carbon-budget.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: aegis-carbon-alerts
  namespace: monitoring
spec:
  groups:
    - name: carbon-budget
      rules:
        - alert: CarbonBudgetNearLimit
          expr: sum(aegis_run_co2e_kg) by (tenant) > (0.9 * 1000)
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Tenant carbon budget near limit"
*** End Patch
*** Begin Patch
*** Add File:docs/carbon_finalization_readme.md
# Carbon-awareness Finalization (operator guide)

This patch implements the wiring necessary to map node GPUs to run_id, schedule and run the estimator on real runs, and wire budget enforcement into an admission flow.

Key pieces added:
- run-id-writer DaemonSet (monitoring/runid-writer) — writes textfile metrics consumed by node_exporter.
- ServiceMonitor for node-exporter scraping (monitoring/prometheus/node-exporter-servicemonitor.yaml).
- estimator runner CronJob (carbon/estimator/cronjob.yaml) that looks up finished MLflow runs and computes CO2e.
- Carbon Guard service (carbon/guard) provides /reserve to atomically check and reserve carbon budgets (Redis-backed).
- ValidatingWebhook example and Gatekeeper template examples to integrate budget enforcement into admission flows.
- Prometheus alert rules and Grafana dashboards (previously added) should be used to visualize.

Operator checklist (high level)
1. Build and publish run-id-writer image (from monitoring/runid-writer). Deploy DaemonSet and RBAC.
2. Ensure node_exporter textfile collector dir is writable (/var/lib/node_exporter/textfile_collector).
3. Deploy ServiceMonitor for node exporter and confirm Prometheus sees aegis_node_run_info{run_id=...}.
4. Configure MLFLOW_TRACKING_URI and run the estimator CronJob in namespace aegis; review S3 evidence & MLflow tags.
5. Deploy carbon-guard service and point your ValidatingWebhook or scheduler integration at it. Ensure Redis URL configured for budgets.
6. Configure Gatekeeper/Gatekeeper constraints to align with your org policy; use the validating webhook for atomic checks.
7. Verify end-to-end:
   - Start a run with label/annotation 'aegis.run_id' on pods
   - Confirm run_id metrics appear on nodes in Prometheus
   - Run estimator and see co2 evidence in S3 and MLflow; check carbon budget consumption
   - Attempt to create a pod with carbon.defer=true when over budget -> webhook denies

Notes & security
 - Webhook manifests require TLS; use cert-manager and populate caBundle in the ValidatingWebhookConfiguration.
 - No secrets are committed; put secrets (CARBON_API_KEY, REDIS_URL, COSIGN_KMS_ARN, etc.) in SecretsManager/ExternalSecrets before deploying.
 - The carbon-guard service must run in a secure namespace and be protected by RBAC and network policies.
*** End Patch
*** End Patch
