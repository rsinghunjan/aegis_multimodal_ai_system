diff --git a/playground/requirements.txt b/playground/requirements.txt
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/playground/requirements.txt
@@ -0,0 +1,6 @@
+flask==2.3.2
+requests==2.31.0
+python-dotenv==1.0.0
+psycopg2-binary==2.9.7
+jinja2==3.1.2
+
diff --git a/playground/app.py b/playground/app.py
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/playground/app.py
@@ -0,0 +1,240 @@
+#!/usr/bin/env python3
+"""
+Aegis Playground — simple web UI + API that lets users author prompts from templates,
+call the inference gateway, preview responses, and log the interaction to decision_log.
+
+Features:
+ - Serves a small web UI (Jinja) that loads prompt templates
+ - POST /api/generate proxies to configured Aegis inference gateway and returns response
+ - Writes an audit entry into decision_log via tools.decisionlog_client.insert_decision
+ - Minimal security: intended for internal/staging use behind auth/proxy
+
+Environment:
+ - INFERENCE_GATEWAY_URL (default: http://aegis-inference-gateway.aegis-ml.svc.cluster.local/generate)
+ - POSTGRES_URL (optional, for decision_log writes)
+ - DEFAULT_MODEL, DEFAULT_BACKEND
+"""
+import os
+import json
+import time
+import logging
+from flask import Flask, render_template, request, jsonify
+import requests
+
+from tools.decisionlog_client import insert_decision
+
+logging.basicConfig(level=logging.INFO)
+LOG = logging.getLogger("aegis.playground")
+
+app = Flask(__name__, template_folder="templates", static_folder="static")
+
+INFERENCE_GATEWAY = os.environ.get("INFERENCE_GATEWAY_URL", "http://aegis-inference-gateway.aegis-ml.svc.cluster.local/generate")
+DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "aegis-llm-default")
+DEFAULT_BACKEND = os.environ.get("DEFAULT_BACKEND", "vllm")
+
+TEMPLATES_PATH = os.path.join(os.path.dirname(__file__), "prompt_templates.json")
+try:
+    with open(TEMPLATES_PATH, "r") as fh:
+        PROMPT_TEMPLATES = json.load(fh)
+except Exception:
+    PROMPT_TEMPLATES = []
+
+@app.route("/")
+def index():
+    return render_template("index.html", templates=PROMPT_TEMPLATES, default_model=DEFAULT_MODEL, default_backend=DEFAULT_BACKEND)
+
+@app.route("/api/generate", methods=["POST"])
+def generate():
+    payload = request.get_json() or {}
+    prompt = payload.get("prompt", "")
+    template_id = payload.get("template_id")
+    model = payload.get("model", DEFAULT_MODEL)
+    backend = payload.get("backend", DEFAULT_BACKEND)
+    max_tokens = int(payload.get("max_tokens", 256))
+    temperature = float(payload.get("temperature", 0.0))
+
+    # If template selected, build the prompt (template may include placeholders)
+    if template_id:
+        templ = next((t for t in PROMPT_TEMPLATES if t.get("id") == template_id), None)
+        if templ:
+            prompt = templ.get("template", "").replace("{{input}}", prompt)
+
+    # Build gateway payload
+    gw_payload = {
+        "prompt": prompt,
+        "model": model,
+        "backend": backend,
+        "max_tokens": max_tokens,
+        "temperature": temperature,
+    }
+
+    # Log the request attempt in decision_log (pre-call)
+    pre_evidence = {"prompt_preview": prompt[:500], "model": model, "backend": backend}
+    dec_id = insert_decision(agent="aegis-playground", action="generate_request", payload={"model": model}, evidence=pre_evidence)
+
+    # Call inference gateway
+    try:
+        r = requests.post(INFERENCE_GATEWAY, json=gw_payload, timeout=60)
+        r.raise_for_status()
+        resp = r.json()
+    except Exception as e:
+        LOG.exception("Gateway call failed")
+        return jsonify({"error": "gateway call failed", "detail": str(e)}), 502
+
+    # Log the full response (redact if required) in decision_log
+    evidence = {"prompt_hash": __hash_prompt(prompt), "gateway_response": summarize_response(resp)}
+    insert_decision(agent="aegis-playground", action="generate_result", payload={"model": model}, evidence=evidence)
+
+    return jsonify({"decision_log_request": dec_id, "result": resp})
+
+def __hash_prompt(p: str) -> str:
+    import hashlib
+    return hashlib.sha256(p.encode()).hexdigest()
+
+def summarize_response(r):
+    # Keep lightweight evidence in decision_log: model, length, tokens (if present)
+    s = {}
+    try:
+        if isinstance(r, dict):
+            s["keys"] = list(r.keys())
+            if "answer" in r:
+                s["answer_snippet"] = (r["answer"][:400] + ("..." if len(r["answer"])>400 else ""))
+            if "usage" in r:
+                s["usage"] = r["usage"]
+    except Exception:
+        pass
+    return s
+
+if __name__ == "__main__":
+    port = int(os.environ.get("PORT", "8080"))
+    app.run(host="0.0.0.0", port=port, debug=False)
+
diff --git a/playground/templates/index.html b/playground/templates/index.html
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/playground/templates/index.html
@@ -0,0 +1,220 @@
+<!doctype html>
+<html lang="en">
+  <head>
+    <meta charset="utf-8" />
+    <meta name="viewport" content="width=device-width, initial-scale=1" />
+    <title>Aegis Playground</title>
+    <style>
+      body { font-family: Arial, sans-serif; margin: 24px; }
+      textarea { width: 100%; height: 160px; }
+      .row { display:flex; gap:12px; margin-top:8px; }
+      .col { flex:1; }
+      .btn { padding:8px 12px; cursor:pointer; background:#0366d6; color:white; border:none; border-radius:4px; }
+      .result { white-space:pre-wrap; background:#f6f8fa; padding:12px; margin-top:12px; border-radius:6px; }
+    </style>
+  </head>
+  <body>
+    <h2>Aegis Playground</h2>
+    <p>Author prompts from templates, call inference gateway and log audit evidence.</p>
+
+    <label for="template">Prompt template:</label>
+    <select id="template">
+      <option value="">-- raw input --</option>
+      {% for t in templates %}
+      <option value="{{t.id}}">{{t.name}} — {{t.description}}</option>
+      {% endfor %}
+    </select>
+
+    <div style="margin-top:8px;">
+      <label for="input">Your input ({{templates|length}} templates loaded):</label>
+      <textarea id="input" placeholder="Type your question or paste text..."></textarea>
+    </div>
+
+    <div class="row">
+      <div class="col">
+        <label for="model">Model</label>
+        <input id="model" value="{{ default_model }}" />
+      </div>
+      <div class="col">
+        <label for="backend">Backend</label>
+        <select id="backend">
+          <option value="vllm">vLLM</option>
+          <option value="triton">Triton</option>
+        </select>
+      </div>
+      <div class="col">
+        <label for="max_tokens">Max tokens</label>
+        <input id="max_tokens" type="number" value="256" />
+      </div>
+    </div>
+
+    <div style="margin-top:12px;">
+      <button id="call" class="btn">Generate</button>
+      <span id="status" style="margin-left:12px;color:#666;"></span>
+    </div>
+
+    <div id="output" class="result" style="display:none;"></div>
+
+    <script>
+      const callBtn = document.getElementById("call");
+      const out = document.getElementById("output");
+      const status = document.getElementById("status");
+      callBtn.addEventListener("click", async () => {
+        const prompt = document.getElementById("input").value;
+        const template = document.getElementById("template").value;
+        const model = document.getElementById("model").value;
+        const backend = document.getElementById("backend").value;
+        const max_tokens = parseInt(document.getElementById("max_tokens").value || "256");
+        status.textContent = "Calling gateway...";
+        out.style.display = "none";
+        try {
+          const resp = await fetch("/api/generate", {
+            method: "POST",
+            headers: {"Content-Type":"application/json"},
+            body: JSON.stringify({prompt, template_id: template, model, backend, max_tokens})
+          });
+          const j = await resp.json();
+          status.textContent = resp.ok ? "OK" : "Error";
+          out.style.display = "block";
+          out.textContent = JSON.stringify(j, null, 2);
+        } catch (e) {
+          status.textContent = "Request failed";
+          out.style.display = "block";
+          out.textContent = String(e);
+        }
+      });
+    </script>
+  </body>
+</html>
+
diff --git a/playground/static/app.js b/playground/static/app.js
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/playground/static/app.js
@@ -0,0 +1,40 @@
+// (Optional) future client-side enhancements: streaming, sanitization, local prompt history
+console.log("Aegis Playground static ready");
+
diff --git a/playground/prompt_templates.json b/playground/prompt_templates.json
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/playground/prompt_templates.json
@@ -0,0 +1,120 @@
+[
+  {
+    "id": "summarize_policy",
+    "name": "Summarize policy",
+    "description": "Summarize a policy document in plain language.",
+    "template": "Summarize the following policy document for a non-technical audience:\n\n{{input}}\n\nSummary:"
+  },
+  {
+    "id": "qa_from_context",
+    "name": "Q&A (with context)",
+    "description": "Answer a question given a context blob.",
+    "template": "You are a helpful assistant. Use the context below to answer the question. Context:\n\n{{input}}\n\nQuestion:\n"
+  },
+  {
+    "id": "safety_review",
+    "name": "Safety review",
+    "description": "Scan for potential PII or unsafe content and suggest redactions.",
+    "template": "Review the following text for PII and unsafe content. Provide redactions and recommendations:\n\n{{input}}\n\nReview:"
+  }
+]
+
diff --git a/playground/sdk/aegis_sdk.py b/playground/sdk/aegis_sdk.py
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/playground/sdk/aegis_sdk.py
@@ -0,0 +1,220 @@
+#!/usr/bin/env python3
+"""
+Aegis SDK (minimal) for programmatic playground use.
+Provides:
+ - generate(prompt, template_id, model, backend, max_tokens)
+ - logs to decision_log using tools.decisionlog_client
+"""
+import os
+import json
+import hashlib
+import requests
+from typing import Optional, Dict, Any
+from tools.decisionlog_client import insert_decision
+
+GATEWAY = os.environ.get("INFERENCE_GATEWAY_URL", "http://aegis-inference-gateway.aegis-ml.svc.cluster.local/generate")
+AGENT_NAME = "aegis-sdk"
+
+def hash_prompt(p: str) -> str:
+    return hashlib.sha256(p.encode()).hexdigest()
+
+def generate(prompt: str, template_id: Optional[str] = None, model: str = "aegis-llm-default", backend: str = "vllm", max_tokens: int = 256, temperature: float = 0.0) -> Dict[str, Any]:
+    payload = {"prompt": prompt, "template_id": template_id, "model": model, "backend": backend, "max_tokens": max_tokens, "temperature": temperature}
+    # pre-log
+    dec_id = insert_decision(agent=AGENT_NAME, action="generate_request", payload={"model": model}, evidence={"prompt_preview": prompt[:500]})
+    resp = requests.post(GATEWAY, json=payload, timeout=60)
+    resp.raise_for_status()
+    data = resp.json()
+    # post-log
+    insert_decision(agent=AGENT_NAME, action="generate_result", payload={"model": model}, evidence={"prompt_hash": hash_prompt(prompt), "result_keys": list(data.keys())})
+    return data
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--prompt", required=True)
+    p.add_argument("--model", default=os.environ.get("DEFAULT_MODEL","aegis-llm-default"))
+    args = p.parse_args()
+    print(generate(args.prompt, model=args.model))
+
diff --git a/k8s/manifests/playground-deployment.yaml b/k8s/manifests/playground-deployment.yaml
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/k8s/manifests/playground-deployment.yaml
@@ -0,0 +1,140 @@
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-playground
+  namespace: aegis-ml
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-playground
+  template:
+    metadata:
+      labels:
+        app: aegis-playground
+    spec:
+      serviceAccountName: aegis-agent-sa
+      containers:
+        - name: playground
+          image: <REGISTRY>/aegis-playground:latest
+          env:
+            - name: INFERENCE_GATEWAY_URL
+              value: "http://aegis-inference-gateway.aegis-ml.svc.cluster.local/generate"
+          ports:
+            - containerPort: 8080
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-playground
+  namespace: aegis-ml
+spec:
+  selector:
+    app: aegis-playground
+  ports:
+    - port: 80
+      targetPort: 8080
+
diff --git a/docs/PLAYGROUND_README.md b/docs/PLAYGROUND_README.md
new file mode 100644
index 0000000..8888888
--- /dev/null
+++ b/docs/PLAYGROUND_README.md
@@ -0,0 +1,200 @@
+# Aegis Playground — UI, SDK & Audit
+
+This patch implements an end-to-end playground: web UI, a small Python SDK, prompt templates and audit wiring (writes to decision_log).
+
+Files added
+- playground/app.py — Flask app and API proxy to inference gateway
+- playground/templates/index.html — simple UI for composing prompts from templates
+- playground/prompt_templates.json — small starter set of templates
+- playground/sdk/aegis_sdk.py — minimal SDK to call the gateway and log decision_log entries
+- k8s/manifests/playground-deployment.yaml — deployment + service (image placeholder)
+- tools/decisionlog_client.py — (shared) helper used to write audit events to decision_log
+- README & requirements
+
+Quick start (local)
+1. Install requirements:
+   ```bash
+   pip install -r playground/requirements.txt
+   ```
+2. Ensure the inference gateway is reachable (INFERENCE_GATEWAY_URL env) and POSTGRES_URL is set (optional).
+3. Start the app:
+   ```bash
+   INFERENCE_GATEWAY_URL=http://localhost:8080/generate POSTGRES_URL=postgresql://user:pass@host:5432/aegis python3 playground/app.py
+   ```
+4. Open http://localhost:8080 in a browser, pick a template, enter input and call Generate.
+
+Security & production notes
+- This playground is intended for staging/internal use. Protect with auth, ingress rules and network policies.
+- For production, replace plaintext k8s secrets with Vault injector/CSI and ensure decision_log writes are permissioned and scrubbed (avoid storing raw prompts if they may contain secrets).
+- Consider rate-limiting, quotas and prompting sanitization before exposing to end users.
+
+Next steps I can implement
+- Add interactive streaming responses (websockets) and token-by-token UI.
+- Add per-user audit trails and redaction rules.
+- Integrate with prompt versioning in LakeFS and CI tests for prompt behavior.
+
+Which next step would you like me to do?
