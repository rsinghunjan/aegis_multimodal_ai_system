*** Begin Patch
*** Add File: k8s/serving/kserve_inferenceservice.yaml
+apiVersion: "serving.kserve.io/v1beta1"
+kind: "InferenceService"
+metadata:
+  name: "aegis-model-v1"
+  namespace: "aegis"
+spec:
+  predictor:
+    model:
+      modelFormat:
+        name: "tensorflow"
+      storageUri: "s3://MODEL_BUCKET/model-archives/aegis-model-v1/"
+    resources:
+      limits:
+        cpu: "4"
+        memory: "8Gi"
+        # For GPU, attach appropriate nvidia.com/gpu limit; adjust per cluster
+        nvidia.com/gpu: "1"
+  canary:
+    trafficPercent: 10
+    predictor:
+      model:
+        storageUri: "s3://MODEL_BUCKET/model-archives/aegis-model-v1-canary/"
+      resources:
+        limits:
+          cpu: "2"
+          memory: "4Gi"
+          nvidia.com/gpu: "1"
+
+# Notes:
+# - Replace MODEL_BUCKET with your object store bucket.
+# - KServe will manage autoscaling if ClusterAutoscaler + HPA are installed.
+# - For low-latency inference use a VPA/HPA combination and tune readiness/liveness probes.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/serving/hpa_gpu_autoscaler.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-model-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-model-predictor
+  minReplicas: 1
+  maxReplicas: 10
+  metrics:
+  - type: Resource
+    resource:
+      name: cpu
+      target:
+        type: Utilization
+        averageUtilization: 70
+  # Example GPU-based autoscaling hint (requires custom metrics adapter):
+  # - type: Pods
+  #   pods:
+  #     metric:
+  #       name: nvidia_gpu_utilization  # custom metric provided by node exporter / adapter
+  #     target:
+  #       type: AverageValue
+  #       averageValue: "75"
+
+# NOTE: To autoscale on GPU utilization you must provide a custom metrics adapter
+# and exporter that collects GPU metrics (NVIDIA DCGM exporter) and exposes them
+# through the Kubernetes custom metrics API.
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/model_metrics_exporter.py
+#!/usr/bin/env python3
+"""
+Simple Prometheus metrics exporter for model inference metrics.
+Expose:
+ - aegis_model_request_count_total{model,name}
+ - aegis_model_request_latency_seconds histogram
+ - aegis_model_prediction_distribution (example gauge representing positive-rate)
+
+Usage: run next to your model inference service to scrape logs or receive forwarded metrics.
+"""
+from prometheus_client import start_http_server, Counter, Histogram, Gauge
+from flask import Flask, request, jsonify
+import time
+import os
+
+app = Flask(__name__)
+
+REQUEST_COUNT = Counter("aegis_model_request_count_total", "Total model requests", ["model"])
+REQUEST_LATENCY = Histogram("aegis_model_request_latency_seconds", "Inference latency seconds", ["model"])
+PRED_POS_RATE = Gauge("aegis_model_prediction_positive_rate", "Fraction positive predictions", ["model"])
+
+@app.route("/metrics_push", methods=["POST"])
+def metrics_push():
+    """
+    Accept JSON payload:
+    { "model": "aegis-model-v1", "latency": 0.12, "predictions": [0,1,0,1], "timestamp": 12345 }
+    """
+    payload = request.get_json() or {}
+    model = payload.get("model", "unknown")
+    latency = float(payload.get("latency", 0.0))
+    preds = payload.get("predictions", [])
+    REQUEST_COUNT.labels(model=model).inc()
+    REQUEST_LATENCY.labels(model=model).observe(latency)
+    if preds:
+        pos = sum(1 for p in preds if float(p) > 0.5) / float(len(preds))
+        PRED_POS_RATE.labels(model=model).set(pos)
+    return jsonify({"ok": True})
+
+@app.route("/health")
+def health():
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    port = int(os.environ.get("METRICS_PORT", "8080"))
+    start_http_server(8000)  # Prom client exposition (scraped by Prom)
+    # Flask front-end to receive push metrics if needed
+    app.run(host="0.0.0.0", port=port)
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/collector/readme.md
+Model metrics collector
+-----------------------
+This directory contains a simple Prometheus-compatible exporter and a push endpoint for inference
+services to forward batch metrics. Deploy this near the inference service (sidecar or separate deployment).
+
+Prometheus should scrape the /metrics path on port 8000 and Alertmanager rules can detect
+anomalies (e.g. high latency, sudden changes in positive rate).
+
*** End Patch
*** Begin Patch
*** Add File: scripts/drift_check.py
+#!/usr/bin/env python3
+"""
+Lightweight drift detection using Evidently (if available) or a simple statistical check.
+Inputs:
+ - reference.csv (historical features)
+ - production.csv (recent features/predictions)
+
+Output:
+ - JSON report with drift status for each feature and prediction distribution.
+ - exit code 2 if significant drift found (for CI/cron).
+"""
+import sys, os, json
+import pandas as pd
+
+THRESHOLD=0.2  # simple threshold for distribution shift (JS divergence placeholder)
+
+def simple_shift_score(ref_series, prod_series):
+    # quick heuristic: difference in means normalized by std
+    import math
+    if prod_series.std()==0:
+        return 0.0
+    return abs(prod_series.mean() - ref_series.mean()) / (ref_series.std()+1e-9)
+
+def main(ref_csv, prod_csv):
+    ref = pd.read_csv(ref_csv)
+    prod = pd.read_csv(prod_csv)
+    report = {"features":{}}
+    drift_found = False
+    for col in ref.columns:
+        if col not in prod.columns:
+            report["features"][col] = {"status":"missing_in_prod"}
+            continue
+        score = simple_shift_score(ref[col], prod[col])
+        status = "ok" if score < THRESHOLD else "drift"
+        report["features"][col] = {"score": float(score), "status": status}
+        if status == "drift":
+            drift_found = True
+    # prediction distribution check if present
+    if "prediction" in ref.columns and "prediction" in prod.columns:
+        ref_pos = (ref["prediction"]>0.5).mean()
+        prod_pos = (prod["prediction"]>0.5).mean()
+        report["prediction"] = {"ref_pos_rate": float(ref_pos), "prod_pos_rate": float(prod_pos)}
+        if abs(ref_pos - prod_pos) > 0.1:
+            report["prediction"]["status"] = "drift"
+            drift_found = True
+        else:
+            report["prediction"]["status"] = "ok"
+    out = "drift_report.json"
+    with open(out,"w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote", out)
+    return 2 if drift_found else 0
+
+if __name__ == "__main__":
+    if len(sys.argv) < 3:
+        print("Usage: drift_check.py reference.csv production.csv")
+        sys.exit(1)
+    sys.exit(main(sys.argv[1], sys.argv[2]))
+
*** End Patch
*** Begin Patch
*** Add File: argo/workflows/auto_retrain_on_drift.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-auto-retrain-
+  namespace: aegis
+spec:
+  entrypoint: retrain-pipeline
+  templates:
+  - name: retrain-pipeline
+    steps:
+    - - name: check-drift
+        template: drift-check
+    - - name: retrain
+        template: retrain
+        when: "{{steps.check-drift.outputs.parameters.drift}} == true"
+
+  - name: drift-check
+    container:
+      image: python:3.10-slim
+      command: [sh, -c]
+      args:
+        - |
+          set -e
+          # Expect ref and prod CSVs in a configured S3 location; adapt as needed.
+          aws s3 cp s3://$METRICS_BUCKET/reference.csv /tmp/reference.csv
+          aws s3 cp s3://$METRICS_BUCKET/production_recent.csv /tmp/production.csv
+          python3 scripts/drift_check.py /tmp/reference.csv /tmp/production.csv
+          if [ $? -eq 2 ]; then echo "drift=true" > /tmp/out; else echo "drift=false" > /tmp/out; fi
+          cat /tmp/out
+      outputs:
+        parameters:
+        - name: drift
+          valueFrom:
+            path: /tmp/out
+
+  - name: retrain
+    container:
+      image: ghcr.io/${{ github.repository }}/aegis-train:latest
+      command: [sh, -c]
+      args:
+        - |
+          set -e
+          # run training script that produces artifact and calls package_and_attest_rekor_canonical.sh
+          ./train_and_package.sh
+          echo "retrained"
+
+# Notes:
+# - Configure METRICS_BUCKET in Workflow's env or pass as parameter.
+# - This workflow runs retrain only when drift upload indicates drift.
+
*** End Patch
*** Begin Patch
*** Add File: experiments/compare_ui.py
+#!/usr/bin/env python3
+"""
+Lightweight Flask app to compare MLflow experiments / runs.
+Requires MLflow tracking server URL set in MLFLOW_TRACKING_URI env var.
+"""
+from flask import Flask, jsonify, request
+import os
+import mlflow
+
+app = Flask(__name__)
+mlflow.set_tracking_uri(os.environ.get("MLFLOW_TRACKING_URI","http://mlflow:5000"))
+
+@app.route("/experiments")
+def list_exps():
+    exps = mlflow.search_experiments()
+    out = [{"id":e.experiment_id,"name":e.name} for e in exps]
+    return jsonify(out)
+
+@app.route("/compare")
+def compare():
+    # Query runs by experiment id and return metrics summary
+    exp = request.args.get("exp_id")
+    if not exp:
+        return jsonify({"error":"exp_id required"}), 400
+    runs = mlflow.search_runs([exp], max_results=50)
+    results = []
+    for _, r in runs.iterrows():
+        results.append({"run_id": r.run_id, "metrics": r.to_dict().get("metrics", {})})
+    return jsonify(results)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=8081)
+
*** End Patch
*** Begin Patch
*** Add File: hpo/katib/job_template.yaml
+apiVersion: "kubeflow.org/v1beta1"
+kind: "Experiment"
+metadata:
+  name: aegis-hpo-example
+  namespace: aegis
+spec:
+  objective:
+    type: maximize
+    goal: 0.99
+    objectiveMetricName: validation_accuracy
+  algorithm:
+    algorithmName: random
+  parameters:
+    - name: --learning-rate
+      parameterType: double
+      feasibleSpace:
+        min: "0.0001"
+        max: "0.1"
+  trialTemplate:
+    primaryContainerName: training
+    trialParameters:
+      - name: learning-rate
+        description: "Learning rate"
+        reference: --learning-rate
+    trialSpec:
+      apiVersion: batch/v1
+      kind: Job
+      spec:
+        template:
+          spec:
+            containers:
+              - name: training
+                image: ghcr.io/yourorg/aegis-train:latest
+                command:
+                  - "python"
+                  - "train.py"
+                  - "--learning-rate"
+                  - "{{trialParameters.learning-rate}}"
+            restartPolicy: Never
+
+# Notes:
+# - Requires Katib deployed in the cluster.
+# - This template demonstrates an HPO experiment that runs training jobs with trial parameters.
+
*** End Patch
*** Begin Patch
*** Add File: explainability/shap_explain.py
+#!/usr/bin/env python3
+"""
+Run SHAP explainability for a saved model and sample dataset.
+Produces JSON with per-feature SHAP values (useful for dashboarding).
+"""
+import argparse, joblib, json
+import numpy as np
+import pandas as pd
+
+def main(model_path, data_path, out_path):
+    try:
+        import shap
+    except Exception:
+        print("shap library missing; pip install shap")
+        raise
+    model = joblib.load(model_path)
+    df = pd.read_csv(data_path)
+    X = df.drop(columns=["label"], errors="ignore")
+    explainer = shap.Explainer(model.predict, X)
+    shap_values = explainer(X)
+    # Convert to simple JSON (list of dicts per sample)
+    out = []
+    for i in range(X.shape[0]):
+        row = {col: float(shap_values.values[i, j]) for j, col in enumerate(X.columns)}
+        out.append(row)
+    with open(out_path, "w") as fh:
+        json.dump(out, fh)
+    print("Wrote SHAP explanations to", out_path)
+
+if __name__ == "__main__":
+    ap = argparse.ArgumentParser()
+    ap.add_argument("model_path")
+    ap.add_argument("data_path")
+    ap.add_argument("out_path")
+    args = ap.parse_args()
+    main(args.model_path, args.data_path, args.out_path)
+
*** End Patch
*** Begin Patch
*** Add File: labeling/streamlit_labeler.py
+"""
+Simple Streamlit labeling app to collect human labels for active learning.
+It pulls unlabeled examples from S3 (or local CSV) and allows a user to annotate.
+"""
+import streamlit as st
+import pandas as pd
+import boto3
+import os
+
+BUCKET = os.environ.get("LABEL_BUCKET","")
+S3_KEY = os.environ.get("LABEL_KEY","unlabeled.csv")
+
+def load_unlabeled():
+    if BUCKET:
+        s3 = boto3.client("s3")
+        obj = s3.get_object(Bucket=BUCKET, Key=S3_KEY)
+        return pd.read_csv(obj["Body"])
+    else:
+        return pd.read_csv("data/unlabeled.csv")
+
+def main():
+    st.title("Aegis Labeling UI (Simple)")
+    df = load_unlabeled()
+    idx = st.number_input("Row index", min_value=0, max_value=len(df)-1, value=0)
+    row = df.iloc[idx]
+    st.write(row.to_dict())
+    label = st.selectbox("Label", [0,1])
+    if st.button("Save label"):
+        # Append to labeled.csv; in production push to labeling DB or queue
+        outfn = "labeled.csv"
+        if not os.path.exists(outfn):
+            df_l = pd.DataFrame(columns=list(df.columns)+["label"])
+            df_l.to_csv(outfn, index=False)
+        df_l = pd.read_csv(outfn)
+        new = row.to_dict()
+        new["label"] = label
+        df_l = df_l.append(new, ignore_index=True)
+        df_l.to_csv(outfn, index=False)
+        st.success("Saved label locally to labeled.csv (replace with DB push in prod).")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/feedback_ingest.py
+#!/usr/bin/env python3
+"""
+Ingest online feedback (predictions + user corrections) and append to feedback store (S3 / DB).
+This is a small HTTP receiver to collect feedback from inference services or frontends.
+"""
+from flask import Flask, request, jsonify
+import os, json, boto3, datetime
+
+app = Flask(__name__)
+BUCKET = os.environ.get("FEEDBACK_BUCKET","")
+PREFIX = os.environ.get("FEEDBACK_PREFIX","feedback")
+
+@app.route("/feedback", methods=["POST"])
+def feedback():
+    payload = request.get_json()
+    ts = datetime.datetime.utcnow().isoformat()
+    key = f"{PREFIX}/{ts}.json"
+    if BUCKET:
+        s3 = boto3.client("s3")
+        s3.put_object(Bucket=BUCKET, Key=key, Body=json.dumps(payload))
+        return jsonify({"ok": True, "s3_key": key})
+    else:
+        with open(key.replace("/","_") + ".json","w") as fh:
+            json.dump(payload, fh)
+        return jsonify({"ok": True, "file": key})
+
+@app.route("/health")
+def health():
+    return jsonify({"ok": True})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8090")))
+
*** End Patch
*** Begin Patch
*** Add File: feature_store/feast_integration.py
+"""
+Lightweight Feast integration scaffold:
+- feature_repo that materializes online features to Redis (or alternative)
+- example functions to fetch online features for inference
+
+Requires Feast installed (pip install feast) and a configured Feast repo.
+"""
+from feast import FeatureStore
+import os
+
+FEAST_REPO = os.environ.get("FEAST_REPO", "./feature_repo")
+
+def get_store():
+    fs = FeatureStore(repo_path=FEAST_REPO)
+    return fs
+
+def fetch_online_features(entity_df):
+    fs = get_store()
+    # entity_df is a dataframe with entity keys (e.g., user_id)
+    # This returns a dataframe enriched with feature columns
+    features = ["my_feature_view:feature1","my_feature_view:feature2"]
+    return fs.get_online_features(features, entity_df).to_df()
+
+if __name__ == "__main__":
+    print("Feast integration scaffold. Set FEAST_REPO to your repo path.")
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/auto_retrain_on_drift.yml
+name: Auto Retrain on Drift (Argo trigger)
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 */6 * * *'  # check every 6 hours
+
+jobs:
+  check-and-trigger:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Check drift artifacts in S3 and submit Argo workflow
+        env:
+          OBJECT_STORE_BUCKET: ${{ secrets.OBJECT_STORE_BUCKET }}
+          METRICS_BUCKET: ${{ secrets.METRICS_BUCKET }}
+          KUBECONFIG: ${{ secrets.KUBECONFIG }}
+        run: |
+          # This is a simple controller: it relies on the Argo workflow in repo to perform drift-check and retrain.
+          kubectl apply -f argo/workflows/auto_retrain_on_drift.yaml -n aegis
+          echo "Submitted auto-retrain workflow"
+
*** End Patch
*** Begin Patch
*** Add File: docs/ML_FEATURES_README.md
+ML Platform Feature Additions (summary)
+-------------------------------------
+This patch introduces scaffolding to provide:
+- Model serving via KServe (InferenceService) with canary support and GPU hints.
+- HPA YAML with notes for GPU-based autoscaling (requires custom metrics adapter).
+- A Prometheus-friendly model metrics exporter and push endpoint for inference services.
+- A lightweight drift detection script (drift_check.py) and an Argo retrain workflow that retrains on detected drift.
+- A simple experiments comparison Flask app that queries MLflow.
+- Katib HPO experiment template (for Kubernetes Katib integration).
+- SHAP explainability script to produce per-sample explanations for model outputs.
+- Streamlit labeling UI scaffold to collect human labels.
+- A feedback ingestion HTTP receiver to collect online corrections and store them in S3.
+- Feast feature-store integration scaffold.
+
+How to use (high level)
+1. Deploy the KServe InferenceService in k8s/serving/kserve_inferenceservice.yaml (adjust storageUri).
+2. Deploy the model metrics exporter near your inference service and configure Prometheus to scrape it.
+3. Upload reference and production sample CSVs to the METRICS_BUCKET and run the Argo auto_retrain workflow to validate retrain triggers.
+4. Use the streamlit labeler to collect human labels and feed them into the feedback ingestion endpoint for retraining datasets.
+5. Use the Katib template to run HPO experiments when required.
+
+Notes & next steps
+- These are scaffolds intended to integrate into your existing Aegis pipelines. They require credential setup (S3/METRICS_BUCKET, MLFLOW_TRACKING_URI, Feast repo, Katib installed) and tuning per model.
+- Production-grade deployment will require packaging exporter scripts into container images, creating K8s Deployments and Services, and wiring Prometheus/Alertmanager integrations (alert rules in monitoring).
+
*** End Patch
*** End Patch
   
