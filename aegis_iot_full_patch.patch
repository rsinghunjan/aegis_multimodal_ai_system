*** Begin Patch
*** Add File:iot/crd/device_registry_crd.yaml
+apiVersion: apiextensions.k8s.io/v1
+kind: CustomResourceDefinition
+metadata:
+  name: deviceregistries.aegis.io
+spec:
+  group: aegis.io
+  names:
+    kind: DeviceRegistry
+    listKind: DeviceRegistryList
+    plural: deviceregistries
+    singular: deviceregistry
+  scope: Namespaced
+  versions:
+    - name: v1
+      served: true
+      storage: true
+      schema:
+        openAPIV3Schema:
+          type: object
+          properties:
+            spec:
+              type: object
+              properties:
+                deviceId:
+                  type: string
+                deviceType:
+                  type: string
+                firmwareVersion:
+                  type: string
+                provisioned:
+                  type: boolean
+                certRef:
+                  type: string
+                metadata:
+                  type: object
+            status:
+              type: object
+              properties:
+                lastSeen:
+                  type: string
+                state:
+                  type: string
+
*** End Patch
*** Begin Patch
*** Add File:argo/iot/onboarding_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: iot-onboard-
+  namespace: aegis
+spec:
+  entrypoint: onboard
+  templates:
+    - name: onboard
+      inputs:
+        parameters:
+          - name: device_id
+          - name: device_type
+      steps:
+        - - name: generate-csr
+            template: gen-csr
+            arguments:
+              parameters:
+                - name: device_id
+                  value: "{{inputs.parameters.device_id}}"
+        - - name: sign-cert
+            template: sign-cert
+            arguments:
+              parameters:
+                - name: csr_path
+                  value: "{{steps.generate-csr.outputs.parameters.csr}}"
+        - - name: register-device
+            template: register-device
+            arguments:
+              parameters:
+                - name: device_id
+                  value: "{{inputs.parameters.device_id}}"
+                - name: cert
+                  value: "{{steps.sign-cert.outputs.parameters.cert}}"
+
+    - name: gen-csr
+      inputs:
+        parameters:
+          - name: device_id
+      outputs:
+        parameters:
+          - name: csr
+            valueFrom:
+              path: /tmp/device.csr
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            pip install cryptography || true
+            python3 - <<PY
+from cryptography import x509
+from cryptography.x509.oid import NameOID
+from cryptography.hazmat.primitives import serialization, hashes
+from cryptography.hazmat.primitives.asymmetric import rsa
+key = rsa.generate_private_key(public_exponent=65537, key_size=2048)
+csr = x509.CertificateSigningRequestBuilder().subject_name(x509.Name([
+    x509.NameAttribute(NameOID.COMMON_NAME, "{{inputs.parameters.device_id}}"),
+])).sign(key, hashes.SHA256())
+open('/tmp/device.key','wb').write(key.private_bytes(serialization.Encoding.PEM, serialization.PrivateFormat.TraditionalOpenSSL, serialization.NoEncryption()))
+open('/tmp/device.csr','wb').write(csr.public_bytes(serialization.Encoding.PEM))
+print('wrote csr')
+PY
+
+    - name: sign-cert
+      inputs:
+        parameters:
+          - name: csr_path
+      outputs:
+        parameters:
+          - name: cert
+            valueFrom:
+              path: /tmp/device.crt
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - |
+            # Operator: replace with real CA or EST/ACME flow
+            cp "{{inputs.parameters.csr_path}}" /tmp/device.csr || true
+            echo "Signed-by-demo-CA" > /tmp/device.crt
+
+    - name: register-device
+      inputs:
+        parameters:
+          - name: device_id
+          - name: cert
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - |
+            DEV="{{inputs.parameters.device_id}}"
+            CRT="{{inputs.parameters.cert}}"
+            cat > /tmp/dr.yaml <<EOF
+apiVersion: aegis.io/v1
+kind: DeviceRegistry
+metadata:
+  name: ${DEV}
+spec:
+  deviceId: ${DEV}
+  deviceType: demo
+  firmwareVersion: "0.0.0"
+  provisioned: true
+  certRef: demo-cert
+EOF
+            kubectl apply -f /tmp/dr.yaml -n aegis || true
+            echo "registered ${DEV}"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/iot/onboard_tool.py
+#!/usr/bin/env python3
+"""
+Simple onboarding helper to simulate a device CSR submission and trigger Argo onboarding workflow.
+Usage: python3 scripts/iot/onboard_tool.py --device-id dev-001
+"""
+import argparse, subprocess, json
+
+def submit(device_id):
+    cmd = ["argo", "submit", "argo/iot/onboarding_workflow.yaml", "-n", "aegis", "-p", f"device_id={device_id}", "-p", "device_type=demo", "--watch"]
+    print("Running:", " ".join(cmd))
+    subprocess.run(cmd)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--device-id", required=True)
+    args = p.parse_args()
+    submit(args.device_id)
+
*** End Patch
*** Begin Patch
*** Add File:spire/README_SPIRE.md
+# SPIRE Integration (example)
+
+- This directory contains a small README describing how SPIRE (SPIFFE Runtime Environment) could be integrated:
+  - Run SPIRE server and agent on the cluster
+  - Use SPIRE to issue workload SVIDs to edge components (gateways) and services
+  - Use device onboarding to mint initial device certs and map device identity to SPIFFE ID for services
+
+Notes:
+- This is a guidance artifact; actual SPIRE manifests and secrets need operator credentials and environment-specific values.
+
*** End Patch
*** Begin Patch
*** Add File:emqx/values-emqx.yaml
+replicas: 3
+auth:
+  adminPassword: "change-me"
+mqtt_listeners:
+  - name: tls
+    port: 8883
+    ssl: true
+persist:
+  enabled: true
+  storageClass: "standard"
+resources:
+  limits:
+    cpu: 2000m
+    memory: 4Gi
+  requests:
+    cpu: 500m
+    memory: 1Gi
+
*** End Patch
*** Begin Patch
*** Add File:emqx/deployment_stub.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: emqx-broker
+  namespace: aegis
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: emqx
+  template:
+    metadata:
+      labels:
+        app: emqx
+    spec:
+      containers:
+        - name: emqx
+          image: emqx/emqx:latest
+          env:
+            - name: EMQX_NAME
+              value: "aegis-emqx"
+          ports:
+            - containerPort: 1883
+            - containerPort: 8883
+          resources:
+            limits:
+              cpu: "2000m"
+              memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:gateway/Dockerfile
+FROM golang:1.20-alpine AS build
+WORKDIR /src
+COPY gateway/main.go .
+RUN go build -o /bin/gateway main.go
+
+FROM alpine:3.18
+COPY --from=build /bin/gateway /usr/local/bin/gateway
+EXPOSE 1883 8883 8080
+ENTRYPOINT ["/usr/local/bin/gateway"]
+
*** End Patch
*** Begin Patch
*** Add File:gateway/main.go
+package main
+
+import (
+    "encoding/json"
+    "flag"
+    "fmt"
+    "log"
+    "net/http"
+    "os"
+    "time"
+    "github.com/syndtr/goleveldb/leveldb"
+)
+
+// This is a minimal gateway scaffold:
+// - exposes HTTP health endpoint
+// - stores inbound messages to local LevelDB (LMDB would be alternative)
+// - background forwarder attempts to push to Kafka (bridge stub)
+
+func main() {
+    dbPath := flag.String("db", "/data/gateway.db", "path to local DB")
+    forwardInterval := flag.Int("forward-interval", 5, "forward interval seconds")
+    flag.Parse()
+
+    db, err := leveldb.OpenFile(*dbPath, nil)
+    if err != nil {
+        log.Fatal("open db:", err)
+    }
+    defer db.Close()
+
+    http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
+        w.Write([]byte("ok"))
+    })
+
+    http.HandleFunc("/ingest", func(w http.ResponseWriter, r *http.Request) {
+        var m map[string]interface{}
+        decoder := json.NewDecoder(r.Body)
+        if err := decoder.Decode(&m); err != nil {
+            http.Error(w, "bad payload", 400)
+            return
+        }
+        key := fmt.Sprintf("%d", time.Now().UnixNano())
+        data, _ := json.Marshal(m)
+        db.Put([]byte(key), data, nil)
+        w.Write([]byte("stored"))
+    })
+
+    go func() {
+        for {
+            // stub: forward entries to Kafka or S3; here we just print count
+            iter := db.NewIterator(nil, nil)
+            count := 0
+            for iter.Next() {
+                count++
+            }
+            iter.Release()
+            log.Printf("local buffered messages: %d\n", count)
+            // in production: attempt to send a batch, remove on success
+            time.Sleep(time.Duration(*forwardInterval) * time.Second)
+        }
+    }()
+
+    log.Println("gateway starting on :8080")
+    log.Fatal(http.ListenAndServe(":8080", nil))
+}
+
*** End Patch
*** Begin Patch
*** Add File:argo/iot/gateway_deploy_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: gateway-deploy-
+  namespace: aegis
+spec:
+  entrypoint: deploy
+  templates:
+    - name: deploy
+      steps:
+        - - name: deploy-emqx
+            template: deploy-emqx
+        - - name: deploy-gateway
+            template: deploy-gateway
+
+    - name: deploy-emqx
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - kubectl apply -f emqx/deployment_stub.yaml -n aegis || true
+
+    - name: deploy-gateway
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - kubectl apply -f gateway/k8s_deployment.yaml -n aegis || true
+
*** End Patch
*** Begin Patch
*** Add File:gateway/k8s_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-gateway
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-gateway
+  template:
+    metadata:
+      labels:
+        app: aegis-gateway
+    spec:
+      containers:
+        - name: gateway
+          image: registry.example.com/aegis/gateway:latest
+          args: ["-db","/data/gateway.db"]
+          ports:
+            - containerPort: 8080
+          volumeMounts:
+            - name: gateway-data
+              mountPath: /data
+      volumes:
+        - name: gateway-data
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:gateway/bridge/kafka_bridge.py
+#!/usr/bin/env python3
+"""
+Simple bridge that polls the gateway DB or HTTP ingest and forwards to Kafka.
+This is a scaffold: replace with a robust bridge (Kafka Connect / Debezium / custom).
+"""
+import requests, time, json, os
+from kafka import KafkaProducer
+
+GATEWAY_URL = os.environ.get("GATEWAY_URL","http://aegis-gateway:8080")
+KAFKA_BOOTSTRAP = os.environ.get("KAFKA_BOOTSTRAP","kafka:9092")
+TOPIC = os.environ.get("KAFKA_TOPIC","iot-raw")
+
+producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP)
+
+def poll_and_forward():
+    # In this scaffold, the gateway exposes no read API; in production implement batch pull from DB
+    # Here we simulate by sending a heartbeat
+    msg = {"ts": int(time.time()*1000), "sample": "heartbeat"}
+    producer.send(TOPIC, json.dumps(msg).encode())
+    producer.flush()
+    print("sent heartbeat to kafka")
+
+if __name__ == "__main__":
+    while True:
+        try:
+            poll_and_forward()
+        except Exception as e:
+            print("bridge error", e)
+        time.sleep(5)
+
*** End Patch
*** Begin Patch
*** Add File:iot/agent/Dockerfile
+FROM golang:1.20-alpine AS build
+WORKDIR /src
+COPY iot/agent/main.go .
+RUN go build -o /bin/agent main.go
+FROM alpine:3.18
+COPY --from=build /bin/agent /usr/local/bin/agent
+ENTRYPOINT ["/usr/local/bin/agent"]
+
*** End Patch
*** Begin Patch
*** Add File:iot/agent/main.go
+package main
+
+import (
+    "flag"
+    "fmt"
+    "log"
+    "time"
+)
+
+// Minimal device agent scaffold:
+// - mTLS connection (not implemented in scaffold)
+// - local store and OTA verify hook
+// - plugin points for model runtime (TFLite Micro)
+
+func main() {
+    deviceId := flag.String("device-id","dev-000","device id")
+    server := flag.String("server","http://aegis-gateway:8080","gateway")
+    flag.Parse()
+    log.Printf("Device agent starting for %s, gateway=%s", *deviceId, *server)
+    for {
+        // stub: report heartbeat
+        fmt.Printf("heartbeat %s %s\n", *deviceId, time.Now().Format(time.RFC3339))
+        time.Sleep(10 * time.Second)
+    }
+}
+
*** End Patch
*** Begin Patch
*** Add File:pil/iot_pil_emulator.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: iot-pil-emulator-
+  namespace: aegis
+spec:
+  entrypoint: pil-emulate
+  templates:
+    - name: pil-emulate
+      steps:
+        - - name: run-agent-emulator
+            template: run-emulator
+        - - name: run-agent-tests
+            template: run-tests
+
+    - name: run-emulator
+      container:
+        image: registry.example.com/aegis/iot-agent:latest
+        command: ["/usr/local/bin/agent"]
+        args: ["-device-id","pil-dev-001","-server","http://aegis-gateway:8080"]
+
+    - name: run-tests
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests || true
+            python3 - <<PY
+import requests, time
+print("Testing gateway health")
+print(requests.get("http://aegis-gateway:8080/health").text)
+PY
+
*** End Patch
*** Begin Patch
*** Add File:ci/microtv_build.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "MicroTVM/TFLite cross-compile scaffold (operator must replace with real toolchain)"
+echo "Assumes model.onnx in /work/model/"
+MODEL=/work/model/model.onnx
+OUT=/work/out/micro
+mkdir -p "$OUT"
+if [ -f "$MODEL" ]; then
+  echo "Converting model (stub)"
+  cp "$MODEL" "$OUT/model.onnx" || true
+else
+  echo "No model found"
+fi
+echo "Micro build written to $OUT (stub)"
+
*** End Patch
*** Begin Patch
*** Add File:argo/iot/ota_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: iot-ota-pipeline-
+  namespace: aegis
+spec:
+  entrypoint: ota
+  templates:
+    - name: ota
+      steps:
+        - - name: build-package
+            template: build-package
+        - - name: sign-package
+            template: sign-package
+        - - name: publish
+            template: publish
+        - - name: staged-rollout
+            template: staged-rollout
+
+    - name: build-package
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh, -c]
+        args:
+          - echo "Build step (placeholder). Operator should produce OTA package at /tmp/ota_package.tar.gz"; touch /tmp/ota_package.tar.gz
+      outputs:
+        artifacts:
+          - name: package
+            path: /tmp/ota_package.tar.gz
+
+    - name: sign-package
+      inputs:
+        artifacts:
+          - name: package
+            path: /tmp/ota_package.tar.gz
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 || true
+            python3 scripts/iot/ota_sign_publish.py --package /tmp/ota_package.tar.gz --bucket ${MODEL_ARTIFACT_BUCKET:-""} --out /tmp/ota_signed.json || true
+
+    - name: publish
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - cat /tmp/ota_signed.json || true
+
+    - name: staged-rollout
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - echo "Staged rollout (placeholder). Implement device group selection and canary promotion scripts"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/iot/ota_sign_publish.py
+#!/usr/bin/env python3
+import argparse, json, os, time
+try:
+    import boto3
+except:
+    boto3 = None
+
+def sign_and_publish(pkg, bucket, out):
+    manifest = {"package": os.path.basename(pkg), "sha256": "dummychecksum", "timestamp": time.time()}
+    # Try HSM sign (placeholder)
+    try:
+        from production.policy.signing.sign_with_retry import sign_payload
+        sig, meta = sign_payload(open(pkg,"rb").read(), None)
+        manifest["signature"] = sig
+        manifest["signed_by"] = meta
+    except Exception as e:
+        manifest["signature_error"] = str(e)
+    if bucket and boto3:
+        s3 = boto3.client("s3")
+        key = f"ota/{manifest['package']}"
+        try:
+            s3.upload_file(pkg, bucket, key)
+            manifest["s3_key"] = key
+        except Exception as e:
+            manifest["upload_error"] = str(e)
+    open(out,"w").write(json.dumps(manifest))
+    print("Wrote", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--package", required=True)
+    p.add_argument("--bucket", default="")
+    p.add_argument("--out", default="/tmp/ota_signed.json")
+    args = p.parse_args()
+    sign_and_publish(args.package, args.bucket, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:device/verifier/tpm_verifier.sh
+#!/usr/bin/env bash
+# TPM attestation stub for device side
+set -euo pipefail
+if command -v tpm2_getrandom >/dev/null 2>&1; then
+  echo "TPM present — performing simple PCR read"
+  tpm2_pcrread sha256
+  echo "TPM PCRs read; in production verify PCRs against expected manifest"
+else
+  echo "No TPM utilities installed; skipping TPM attestation"
+fi
+echo "Verify signature (placeholder) — operator must implement signature verification using device root cert"
+
*** End Patch
*** Begin Patch
*** Add File:federated/aggregator/aggregator.py
+#!/usr/bin/env python3
+"""
+Simple secure aggregator scaffold.
+- Accept uploads from gateways (HTTP POST JSON meta) and produce aggregated model (demo).
+In production replace with CrypTen or secure MPC aggregator.
+"""
+import os, json, time
+from http.server import BaseHTTPRequestHandler, HTTPServer
+STORAGE="/tmp/fed_models"
+os.makedirs(STORAGE, exist_ok=True)
+
+class Handler(BaseHTTPRequestHandler):
+    def do_POST(self):
+        length = int(self.headers.get('Content-Length',0))
+        body = self.rfile.read(length)
+        meta = json.loads(body.decode())
+        fname = f"{STORAGE}/{int(time.time()*1000)}.json"
+        with open(fname,"w") as f:
+            json.dump(meta, f)
+        self.send_response(200)
+        self.end_headers()
+        self.wfile.write(b'{"status":"stored"}')
+
+def run(port=9200):
+    srv = HTTPServer(("0.0.0.0", port), Handler)
+    print("Federated aggregator running on", port)
+    srv.serve_forever()
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:federated/argo/federated_gateway_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: federated-gateway-
+  namespace: aegis
+spec:
+  entrypoint: federated-gateway
+  templates:
+    - name: federated-gateway
+      steps:
+        - - name: run-local-train
+            template: run-local
+        - - name: push-to-aggregator
+            template: push-agg
+
+    - name: run-local
+      container:
+        image: registry.example.com/aegis/fed-site-trainer:latest
+        command: [sh, -c]
+        args:
+          - python3 federated/local_train.py --out /tmp/local_model.json || true
+
+    - name: push-agg
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests || true
+            python3 - <<PY
+import requests, json, os
+with open('/tmp/local_model.json') as f:
+    meta = json.load(f)
+try:
+    requests.post(os.environ.get('FED_AGG_URL','http://fed-aggregator:9100'), json=meta, timeout=10)
+    print('pushed model')
+except Exception as e:
+    print('push failed', e)
+PY
+
*** End Patch
*** Begin Patch
*** Add File:privacy/opacus_accountant_stub.py
+#!/usr/bin/env python3
+"""
+DP accountant stub (Opacus demo). Replace with real Opacus integration.
+"""
+import json, time
+print("Running DP accountant demo (stub)")
+out = {"epsilon": 1.23, "delta": 1e-5, "timestamp": time.time()}
+open("/tmp/dp_report.json","w").write(json.dumps(out))
+print("Wrote /tmp/dp_report.json")
+
*** End Patch
*** Begin Patch
*** Add File:anomaly/detector.py
+#!/usr/bin/env python3
+"""
+Tiny anomaly detector using sklearn IsolationForest (not for constrained devices).
+Used on gateways for local anomaly detection; in production, replace with optimized C/Go runtime.
+"""
+import json, sys
+try:
+    from sklearn.ensemble import IsolationForest
+    import numpy as np
+except Exception:
+    print("sklearn not installed; this is a scaffold")
+    sys.exit(0)
+
+def detect(samples):
+    X = np.array(samples)
+    clf = IsolationForest(n_estimators=50).fit(X)
+    preds = clf.predict(X)
+    return preds.tolist()
+
+if __name__ == "__main__":
+    data = json.load(open(sys.argv[1]))
+    samples = [d["features"] for d in data]
+    print(detect(samples))
+
*** End Patch
*** Begin Patch
*** Add File:safety/rules_engine.py
+#!/usr/bin/env python3
+"""
+Simple rule engine for safety checks.
+Rules are defined as simple Python expressions evaluated against message dict.
+In production use Lua sandbox or WASM for safety.
+"""
+import json, sys
+
+def evaluate_rule(rule, msg):
+    try:
+        return eval(rule, {}, {"msg": msg})
+    except Exception as e:
+        return False
+
+if __name__ == "__main__":
+    rules = json.load(open(sys.argv[1]))
+    msg = json.load(open(sys.argv[2]))
+    results = {r["name"]: evaluate_rule(r["expr"], msg) for r in rules}
+    print(json.dumps(results))
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/exporter_gateway.py
+#!/usr/bin/env python3
+from http.server import BaseHTTPRequestHandler, HTTPServer
+import json, time
+
+class Handler(BaseHTTPRequestHandler):
+    def do_GET(self):
+        if self.path == "/metrics":
+            # simple plain text prometheus format
+            lines = []
+            lines.append('device_heartbeat_total 123')
+            lines.append('gateway_buffered_messages 42')
+            self.send_response(200)
+            self.send_header('Content-Type','text/plain; version=0.0.4')
+            self.end_headers()
+            self.wfile.write(("\n".join(lines)).encode())
+        else:
+            self.send_response(404)
+            self.end_headers()
+
+if __name__ == "__main__":
+    HTTPServer(('0.0.0.0', 9101), Handler).serve_forever()
+
*** End Patch
*** Begin Patch
*** Add File:grafana/dashboards/iot_fleet_dashboard.json
+{
+  "title": "Aegis IoT Fleet Dashboard",
+  "panels": [
+    {"type":"stat","title":"Devices Online %","gridPos":{"x":0,"y":0,"w":6,"h":3}},
+    {"type":"graph","title":"OTA Success Rate","gridPos":{"x":6,"y":0,"w":12,"h":4}},
+    {"type":"graph","title":"Gateway Buffer Size","gridPos":{"x":0,"y":3,"w":12,"h":4}},
+    {"type":"table","title":"Anomaly Events","gridPos":{"x":12,"y":3,"w":12,"h":4}}
+  ],
+  "schemaVersion": 16
+}
+
*** End Patch
*** Begin Patch
*** Add File:docs/IOT_RUNBOOK.md
+# Aegis IoT Runbook (scaffold)
+
+This runbook describes the artifacts added to support IoT: onboarding, gateway, agent, OTA, federated, anomaly, safety, and monitoring.
+
+Quickstart (dry-run)
+1. Deploy CRD:
+   kubectl apply -f iot/crd/device_registry_crd.yaml
+2. Deploy EMQX (demo stub) and Gateway:
+   argo submit argo/iot/gateway_deploy_workflow.yaml -n aegis --watch
+3. Run PIL emulator:
+   argo submit pil/iot_pil_emulator.yaml -n aegis --watch
+4. Run OTA pipeline (stub):
+   argo submit argo/iot/ota_pipeline.yaml -n aegis --watch
+5. Deploy federated aggregator:
+   kubectl apply -f federated/aggregator/aggregator_deployment.yaml -n aegis (not included; run python aggregator manually for demo)
+
+Testing & safety
+- All hardware-interactive steps are dry-run by default.
+- Before hardware rollout ensure independent kill switch and safety procedures.
+
+Next steps to production
+- Replace stubs with your CA / EST flow for cert signing, configure SPIRE if desired.
+- Build and push gateway and agent images to your registry and update image refs.
+- Harden bridge to Kafka (use Kafka Connect) and enable TLS/mTLS end-to-end.
+- Integrate HSM signing in CI and ensure devices perform signature verification (TPM attestation optional).
+- Run scale tests: simulate large number of devices and validate ingestion, gateway memory, and Kafka throughput.
+
*** End Patch
*** End Patch
