*** Begin Patch
*** Add File:terraform/rekor_rds/main.tf
+terraform {
+  required_providers {
+    aws = { source = "hashicorp/aws" version = "~> 5.0" }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_db_subnet_group" "rekor" {
+  name       = "${var.name}-subnet-group"
+  subnet_ids = var.subnet_ids
+}
+
+resource "aws_db_instance" "rekor" {
+  identifier              = "${var.name}-rekor-db"
+  engine                  = "postgres"
+  engine_version          = "15"
+  instance_class          = var.instance_class
+  allocated_storage       = 20
+  name                    = var.db_name
+  username                = var.db_user
+  password                = var.db_password
+  db_subnet_group_name    = aws_db_subnet_group.rekor.name
+  skip_final_snapshot     = true
+  publicly_accessible     = false
+  vpc_security_group_ids  = var.security_group_ids
+  backup_retention_period = 7
+  storage_encrypted       = true
+  multi_az                = true
+}
+
+output "rekor_rds_endpoint" {
+  value = aws_db_instance.rekor.address
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/rekor_rds/variables.tf
+variable "aws_region" { type = string, default = "us-west-2" }
+variable "name" { type = string, default = "aegis" }
+variable "subnet_ids" { type = list(string) }
+variable "security_group_ids" { type = list(string) }
+variable "instance_class" { type = string, default = "db.t3.medium" }
+variable "db_name" { type = string, default = "rekordb" }
+variable "db_user" { type = string, default = "rekor" }
+variable "db_password" { type = string, sensitive = true }
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/rekor-deployment-rds.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          env:
+            - name: REKOR_STORAGE_TYPE
+              value: "postgres"
+            - name: REKOR_DATABASE_URL
+              valueFrom:
+                secretKeyRef:
+                  name: rekor-db-secret
+                  key: REKOR_DATABASE_URL
+            - name: REKOR_API_PORT
+              value: "3000"
+          ports:
+            - containerPort: 3000
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 3000
+            initialDelaySeconds: 10
+            periodSeconds: 10
+---
+apiVersion: v1
+kind: Secret
+metadata:
+  name: rekor-db-secret
+  namespace: aegis
+type: Opaque
+stringData:
+  # Create this secret after RDS created: REKOR_DATABASE_URL=postgres://user:pass@host:5432/rekordb
+  REKOR_DATABASE_URL: "postgres://REPLACE_USER:REPLACE_PASS@REPLACE_HOST:5432/rekordb"
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/vault_init_secure.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Usage: VAULT_ADDR=https://vault.aegis.svc:8200 ./vault_init_secure.sh <kms-key-arn> <evidence-bucket>
+KMS_ARN=${1:-}
+EVIDENCE_BUCKET=${2:-}
+if [ -z "$KMS_ARN" ] || [ -z "$EVIDENCE_BUCKET" ]; then
+  echo "Usage: $0 <kms-key-arn> <evidence-bucket>"
+  exit 2
+fi
+
+echo "Waiting for Vault to be reachable at $VAULT_ADDR..."
+until curl -sSf --insecure ${VAULT_ADDR}/v1/sys/health >/dev/null 2>&1; do sleep 3; done
+
+if vault status -format=json | jq -e '.initialized' >/dev/null 2>&1; then
+  echo "Vault already initialized"
+  exit 0
+fi
+
+echo "Initializing Vault..."
+vault operator init -key-shares=1 -key-threshold=1 -format=json > /tmp/vault-init.json
+ROOT_TOKEN=$(jq -r '.root_token' /tmp/vault-init.json)
+UNSEAL_KEY=$(jq -r '.unseal_keys_b64[0]' /tmp/vault-init.json)
+
+echo "Storing initialization material to S3 with SSE-KMS..."
+aws s3 cp /tmp/vault-init.json s3://${EVIDENCE_BUCKET}/vault-init/vault-init-$(date +%s).json --sse aws:kms --sse-kms-key-id "${KMS_ARN}"
+
+echo "Enabling KMS auto-unseal (ensure helm values used during install)."
+echo "Storing root token in AWS Secrets Manager (encrypted)"
+aws secretsmanager create-secret --name aegis/vault/root-token --secret-string "${ROOT_TOKEN}" >/dev/null || aws secretsmanager put-secret-value --secret-id aegis/vault/root-token --secret-string "${ROOT_TOKEN}" >/dev/null
+
+echo "Enabling audit device to file and uploading via cron or direct push (operator should set audit device to file or socket)."
+echo "Creating basic policy and enabling Kubernetes auth..."
+export VAULT_TOKEN="${ROOT_TOKEN}"
+vault auth enable kubernetes || true
+vault write auth/kubernetes/config \
+  token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
+  kubernetes_host="https://kubernetes.default.svc" \
+  kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt || true
+
+echo "Creating 'signing-proxy' policy stub"
+cat > /tmp/signing-proxy.hcl <<'HCL'
+path "transit/*" {
+  capabilities = ["update","read"]
+}
+path "pki/issue/ci-client" {
+  capabilities = ["create","read"]
+}
+HCL
+vault policy write signing-proxy /tmp/signing-proxy.hcl || true
+
+echo "Vault initialization complete. Root token stored in Secrets Manager and init bundle archived in S3."
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/pki_role_setup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Usage: VAULT_ADDR=https://vault.aegis.svc:8200 VAULT_TOKEN=<token> ./pki_role_setup.sh
+echo "Enabling PKI at pki/"
+vault secrets enable -path=pki pki || true
+vault secrets tune -max-lease-ttl=87600h pki || true
+echo "Generating intermediate/root certificate (self-signed for example)"
+vault write -field=certificate pki/root/generate/internal common_name="aegis.example.com" ttl=87600h > /tmp/ca.crt || true
+vault write pki/config/urls issuing_certificates="$VAULT_ADDR/v1/pki/ca" crl_distribution_points="$VAULT_ADDR/v1/pki/crl" || true
+
+echo "Creating role ci-client with short TTLs"
+vault write pki/roles/ci-client allowed_domains="aegis.svc" allow_subdomains=true max_ttl="24h" allow_any_name=false enforce_hostnames=false || true
+
+echo "PKI role ready. CI can request certificates from path pki/issue/ci-client"
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/vault-integration-notes.md
+Signing-proxy Vault integration (ephemeral signing tokens)
+
+Flow:
+- CI runner authenticates to Vault (via OIDC/GitHub or via vault-agent with Kubernetes auth) and requests a short-lived client certificate from Vault PKI (pki/issue/ci-client).
+- CI uses that client cert to perform mTLS to signing-proxy or to authenticate to signing-proxy directly.
+- Signing-proxy maps client cert subject to a Vault policy and then requests ephemeral signing key/material from Vault transit or HSM (transit/encrypt or transit/sign).
+- Signing-proxy uses ephemeral secret to sign artifact and logs the event to Rekor (via Rekor API) and S3 evidence bucket.
+
+Implementation notes:
+- Ensure Vault PKI role 'ci-client' exists (deploy/vault/pki_role_setup.sh).
+- Use GitHub OIDC to login to Vault and create short-lived tokens (see .github/workflows/request_ephemeral_cert.yml).
+- Configure signing-proxy to call Vault transit or HSM directly. Provide VAULT_ADDR and role/policy via k8s secret.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/request_ephemeral_cert.yml
+name: Request ephemeral cert and sign via signing-proxy
+on:
+  workflow_dispatch:
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  request_and_sign:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Login to Vault via OIDC
+        uses: hashicorp/vault-action@v2
+        with:
+          url: ${{ secrets.VAULT_ADDR }}
+          method: jwt
+          role: ${{ secrets.VAULT_OIDC_ROLE }}
+          jwt: ${{ steps.token.outputs.token }}
+        id: vault_login
+
+      - name: Generate CSR and request client cert from Vault PKI
+        env:
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+        run: |
+          # create a private key & CSR
+          openssl genrsa -out /tmp/client.key 2048
+          openssl req -new -key /tmp/client.key -subj "/CN=ci-runner-${{ github.run_id }}" -out /tmp/client.csr
+          # request cert from Vault PKI role ci-client
+          curl -s --header "X-Vault-Token: $VAULT_TOKEN" --request POST --data "{\"common_name\":\"ci-runner-${{ github.run_id }}\"}" $VAULT_ADDR/v1/pki/issue/ci-client | jq -r '.data.certificate' > /tmp/client.crt
+          echo "Wrote /tmp/client.crt"
+
+      - name: Request signature from signing-proxy over mTLS
+        env:
+          SIGNING_PROXY_URL: ${{ secrets.SIGNING_PROXY_URL }}
+          IMAGE_NAME: ghcr.io/${{ github.repository_owner }}/aegis-sample:${{ github.sha }}
+        run: |
+          curl --cert /tmp/client.crt --key /tmp/client.key --cacert <(echo "${{ secrets.SIGNING_CLIENT_CA }}") \
+            -H "Content-Type: application/json" \
+            -d "{\"artifact\":\"${IMAGE_NAME}\"}" \
+            "${SIGNING_PROXY_URL}/sign" -o /tmp/sign_res.json -sS || (cat /tmp/sign_res.json && exit 1)
+          cat /tmp/sign_res.json
+
+      - name: Upload signature to evidence
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+        run: |
+          if [ -f /tmp/sign_res.json ]; then
+            aws s3 cp /tmp/sign_res.json s3://${EVIDENCE_BUCKET}/signatures/${{ github.sha }}-sign.json || true
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekor/rekor_backup_restore.sh
+#!/usr/bin/env bash
+set -euo pipefail
+OUT=/tmp/rekor_backup_$(date +%Y%m%d%H%M%S).sqlc
+PG_USER=${PG_USER:-rekor}
+PG_PASS=${PG_PASS:-REPLACE_WITH_SECRET}
+PG_HOST=${PG_HOST:-rekor-rds.aegis.internal}
+PG_DB=${PG_DB:-rekordb}
+export PGPASSWORD=$PG_PASS
+echo "Dumping Rekor DB to ${OUT}"
+pg_dump -h "${PG_HOST}" -U "${PG_USER}" -d "${PG_DB}" -F c -f "${OUT}"
+if [ -n "${EVIDENCE_BUCKET:-}" ]; then
+  aws s3 cp "${OUT}" "s3://${EVIDENCE_BUCKET}/rekor-backups/${OUT##*/}"
+  echo "Uploaded to s3://${EVIDENCE_BUCKET}/rekor-backups/"
+else
+  echo "EVIDENCE_BUCKET not set; local backup at ${OUT}"
+fi
+
+echo "To restore: pg_restore -h <host> -U <user> -d <db> -c <backupfile>"
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/tuning/auto_tuner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Run Gatekeeper dry-run tests and produce a report. Usage:
+# ./auto_tuner.sh --namespace aegis-staging --constraint cosign-required-staging --report s3://<bucket>/gk-reports/report.json
+NS="aegis-staging"
+CONSTRAINT="cosign-required-staging"
+REPORT_S3="${1:-}"
+TMP="/tmp/gk_report_$(date +%s).json"
+
+echo "Collect constraints and audit logs"
+kubectl get constraint -A -o json > /tmp/gk_constraints.json || true
+kubectl get gatekeeper.auditreports -A -o json > /tmp/gk_audit_raw.json || true
+
+# Run synthetic workload tests
+echo "Running synthetic workload tests..."
+kubectl -n ${NS} run gk-test-pod --image=nginx:alpine --restart=Never -- sleep 30 || true
+sleep 3
+kubectl -n ${NS} delete pod gk-test-pod || true
+
+cat > ${TMP} <<JSON
+{
+ "namespace": "${NS}",
+ "constraint": "${CONSTRAINT}",
+ "constraints_raw": $(jq . /tmp/gk_constraints.json),
+ "audit_raw": $(jq . /tmp/gk_audit_raw.json),
+ "note": "Use this report to tune enforcementAction from dryrun -> deny after review"
+}
+JSON
+
+echo "Report written to ${TMP}"
+if [ -n "${REPORT_S3}" ]; then
+  aws s3 cp ${TMP} ${REPORT_S3} || true
+  echo "Uploaded report to ${REPORT_S3}"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:falco/tuning/suppressions.yaml
+# Example suppressions to reduce noise; expand based on observed alerts
+suppressions:
+  - rule: Container added capabilities
+    imageRepository: "registry.gitlab.com/ci-runner"
+  - rule: Terminal shell in container
+    namespace: "aegis-staging"
+  - rule: Writing to sensitive mount
+    source: "kubelet"
+
*** End Patch
*** Begin Patch
*** Add File:falco/sidekick/configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: falco-sidekick-config
+  namespace: kube-system
+data:
+  sidekick.yml: |
+    driver: elasticsearch
+    elasticsearch:
+      url: https://elasticsearch.logging.svc:9200
+      user: ${ELASTIC_USER}
+      password: ${ELASTIC_PASS}
+      tls_verify: true
+
*** End Patch
*** Begin Patch
*** Add File:logging/fluentbit/elastic-output.conf
+[OUTPUT]
+    Name  es
+    Match *
+    Host  elasticsearch.logging.svc
+    Port  9200
+    HTTP_User  ${ELASTIC_USER}
+    HTTP_Passwd  ${ELASTIC_PASS}
+    tls   On
+    tls.verify   On
+    tls.ca_file  /fluent-bit/tls/ca.crt
+
*** End Patch
*** Begin Patch
*** Add File:velero/terraform_s3_with_object_lock.tf
+resource "aws_s3_bucket" "velero" {
+  bucket = var.velero_bucket
+  acl    = "private"
+  force_destroy = false
+  versioning {
+    enabled = true
+  }
+  object_lock_configuration {
+    object_lock_enabled = "Enabled"
+    rule {
+      default_retention {
+        mode = "GOVERNANCE"
+        days = var.object_lock_days
+      }
+    }
+  }
+  server_side_encryption_configuration {
+    rule {
+      apply_server_side_encryption_by_default {
+        sse_algorithm     = "aws:kms"
+        kms_master_key_id = var.kms_key_arn
+      }
+    }
+  }
+}
+
+output "velero_bucket" { value = aws_s3_bucket.velero.bucket }
+
*** End Patch
*** Begin Patch
*** Add File:velero/install_and_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+VELERO_BUCKET=${1:-aegis-velero-backups}
+EVIDENCE_BUCKET=${2:-aegis-evidence-bucket}
+AWS_REGION=${3:-us-west-2}
+
+echo "Ensure velero CLI installed and aws credentials exported"
+velero install --provider aws --bucket "${VELERO_BUCKET}" --secret-file ./credentials-velero --backup-location-config region=${AWS_REGION} || true
+
+BACKUP_NAME="drill-$(date +%s)"
+echo "Creating backup ${BACKUP_NAME} for namespace aegis"
+velero backup create ${BACKUP_NAME} --include-namespaces aegis
+velero backup wait ${BACKUP_NAME} --for=complete --timeout 10m
+
+RESTORE_NS="aegis-restore-$(date +%s)"
+kubectl create ns ${RESTORE_NS} || true
+velero restore create --from-backup ${BACKUP_NAME} --namespace-mappings "aegis:${RESTORE_NS}" "${BACKUP_NAME}-restore"
+velero restore wait "${BACKUP_NAME}-restore" --for=complete --timeout 15m
+
+echo "Signing proof-of-restore and uploading to evidence bucket"
+PROOF="/tmp/restore-proof-${BACKUP_NAME}.txt"
+echo "restore:${BACKUP_NAME} restored_to:${RESTORE_NS} time:$(date -u --iso-8601=seconds)" > ${PROOF}
+if [ -n "${COSIGN_KMS_KEY_ARN:-}" ]; then
+  cosign sign --key "awskms://${COSIGN_KMS_KEY_ARN}" ${PROOF} || true
+fi
+aws s3 cp ${PROOF} s3://${EVIDENCE_BUCKET}/restore-proofs/ || true
+
+echo "Restore drill completed. Please run smoke tests against ${RESTORE_NS} and then cleanup."
+
*** End Patch
*** Begin Patch
*** Add File:ops/ops_simplify_managed_services.md
+Recommendation: prefer managed services to reduce ops burden
+
+What to prefer:
+- MongoDB: Atlas (already scaffolded) — enable private endpoints & backups.
+- Couchbase: Capella (if you need sync or managed cluster) — otherwise run Autonomous Operator with careful ops team.
+- Kafka: prefer MSK / Confluent Cloud rather than Strimzi for production unless you have SRE for Kafka.
+
+Steps:
+1. Choose managed services in procurement & set up VPC peering to your platform VPC.
+2. Replace self-managed connection strings in k8s secrets with managed endpoints.
+3. Decommission self-managed clusters only after successful data migration and tests.
+
*** End Patch
*** Begin Patch
*** Add File:docs/PROD_HARDENING_CHECKLIST.md
+# Aegis Production Hardening Checklist — Fixes applied by patch
+
+Secrets & TLS (Vault)
+- Run terraform to provision KMS & S3 (or use existing). Populate KMS_ARN and EVIDENCE_BUCKET.
+- Deploy cert-manager and apply deploy/vault/cert-manager ClusterIssuer/Certificates (if not using external CA).
+- Install Vault via Helm using ha-values-raft.yaml with unsealer.kmsKeyId set to KMS_ARN.
+- Run deploy/vault/vault_init_secure.sh <kms-arn> <evidence-bucket> to initialize Vault, store root token in AWS Secrets Manager and archive init bundle to S3.
+- Run deploy/vault/pki_role_setup.sh with VAULT_TOKEN to enable PKI and create ci-client role.
+- Configure Vault audit device to write logs to a path and ensure CronJob uploader or native S3 audit device is used (audit uploader previously included).
+
+Signing trust
+- Use GitHub OIDC to authenticate to Vault; configure Vault OIDC/GitHub auth method and role for CI.
+- Use .github/workflows/request_ephemeral_cert.yml to request short-lived client certs from Vault PKI and call signing-proxy over mTLS.
+- Configure signing-proxy to call Vault transit or HSM to obtain ephemeral signing keys and log events to Rekor and S3.
+
+Rekor durability
+- Run terraform/rekor_rds to provision an RDS Postgres for Rekor; set rekor-db-secret in k8s with REKOR_DATABASE_URL.
+- Deploy deploy/rekor/rekor-deployment-rds.yaml and verify connectivity.
+- Schedule rekor backup using deploy/rekor/rekor_backup_restore.sh to EVIDENCE_BUCKET.
+
+Gatekeeper tuning
+- Keep constraints in dry-run for staging: gatekeeper/tuning/auto_tuner.sh helps collect audits and synthetic test results and uploads a report to S3.
+- Iterate on constraints and only flip to enforce when false-positive rate is acceptable.
+
+Falco & SIEM
+- Deploy Falco + falco-sidekick and configure falco/sidekick/configmap.yaml.
+- Use falco/tuning/suppressions.yaml to suppress noisy rules and tune per workload.
+- Configure Fluent Bit to forward to SIEM with TLS verify enabled (logging/fluentbit/elastic-output.conf).
+
+Backup & Restore
+- Use Terraform velero/terraform_s3_with_object_lock.tf to create Velero backup bucket with object-lock.
+- Run velero/install_and_restore_drill.sh to install velero (if not present) and run automated restore drills. Proofs are cosign-signed and uploaded to EVIDENCE_BUCKET.
+
+Ops simplification
+- Prefer managed services (MongoDB Atlas, Couchbase Capella, MSK) where possible — see ops/ops_simplify_managed_services.md
+
+Verification & acceptance
+- Verify Vault initialized and audit logs present in S3.
+- Run a full CI pipeline that requests ephemeral cert, signs via signing-proxy, and check Rekor entry exists.
+- Run Rekor backup & restore drill.
+- Run Gatekeeper auto-tuner script and validate reports before enforcing constraints.
+- Validate Falco alert volume is acceptable and SIEM receives events.
+
*** End Patch
*** End Patch
