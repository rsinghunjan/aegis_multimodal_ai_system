*** Begin Patch
*** Add File:ml/nlp/rag/train_finetune_accelerate.py
+#!/usr/bin/env python3
+"""
+HF fine-tune using ðŸ¤— Accelerate + Trainer pattern, checkpoint to S3 and export to ONNX for Triton.
+This is a productionizing example: use proper GPU node selectors in Argo.
+Env:
+  OUT_DIR, S3_BUCKET (optional), HF_TOKEN in Vault or env
+"""
+import os
+from datasets import load_dataset
+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
+import torch
+
+MODEL_NAME = os.environ.get("BASE_MODEL", "t5-small")
+OUT_DIR = os.environ.get("OUT_DIR", "out/model")
+S3_BUCKET = os.environ.get("MODEL_S3_BUCKET", "")
+
+def main():
+    ds = load_dataset("squad", split="train[:1%]")  # replace with production dataset
+    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
+
+    def preprocess(ex):
+        inputs = ["question: " + q + " context: " + c for q, c in zip(ex["question"], ex["context"])]
+        model_in = tokenizer(inputs, truncation=True, padding="max_length", max_length=256)
+        labels = tokenizer(ex["answers"]["text"], truncation=True, padding="max_length", max_length=64)
+        model_in["labels"] = labels["input_ids"]
+        return model_in
+
+    ds = ds.map(preprocess, batched=True, remove_columns=ds.column_names)
+    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
+    args = Seq2SeqTrainingArguments(output_dir=OUT_DIR, per_device_train_batch_size=4, num_train_epochs=1, logging_steps=10)
+    trainer = Seq2SeqTrainer(model=model, args=args, train_dataset=ds)
+    trainer.train()
+    trainer.save_model(OUT_DIR)
+
+    # Export to ONNX (simple path using torch.onnx for the encoder-decoder -- production use hf.onnx-exporter)
+    dummy_input = tokenizer("question: example context: example", return_tensors="pt", truncation=True, padding=True)
+    model.to("cpu")
+    onnx_path = os.path.join(OUT_DIR, "model.onnx")
+    torch.onnx.export(model, (dummy_input["input_ids"],), onnx_path, opset_version=13, input_names=["input_ids"], output_names=["output"])
+    print("Exported ONNX to", onnx_path)
+
+    if S3_BUCKET:
+        os.system(f"aws s3 cp {OUT_DIR} s3://{S3_BUCKET}/models/ --recursive || true")
+        print("Uploaded model artifacts to S3")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/nlp/rag/argo_train_rag_gpu.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: rag-train-gpu-
+  namespace: aegis
+spec:
+  entrypoint: train-rag-gpu
+  templates:
+    - name: train-rag-gpu
+      container:
+        image: ghcr.io/huggingface/transformers-training:latest
+        command: [sh, -c]
+        args:
+          - pip install accelerate datasets transformers[sentencepiece] && python3 ml/nlp/rag/train_finetune_accelerate.py
+      nodeSelector:
+        kubernetes.io/arch: "amd64"
+        cloud.google.com/gke-accelerator: "nvidia-tesla-t4" # example; adapt to your cluster
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      resources:
+        limits:
+          nvidia.com/gpu: 1
+
*** End Patch
*** Begin Patch
*** Add File:ml/nlp/rag/onnx_export.py
+#!/usr/bin/env python3
+"""
+Utility to prepare a Triton model repository layout for a HF-exported ONNX model.
+Generates a simple config.pbtxt for Triton.
+"""
+import os, json
+
+MODEL_DIR = os.environ.get("MODEL_DIR","out/model")
+MODEL_NAME = os.environ.get("MODEL_NAME","rag_reader")
+TRITON_MODEL_DIR = os.environ.get("TRITON_MODEL_DIR","triton_model_repo")
+
+os.makedirs(os.path.join(TRITON_MODEL_DIR, MODEL_NAME, "1"), exist_ok=True)
+# copy onnx
+os.system(f"cp {MODEL_DIR}/model.onnx {TRITON_MODEL_DIR}/{MODEL_NAME}/1/model.onnx || true")
+
+config = f"""
+name: "{MODEL_NAME}"
+platform: "onnxruntime_onnx"
+max_batch_size: 8
+input [
+  {{
+    name: "input_ids"
+    data_type: TYPE_INT32
+    dims: [-1]
+  }}
+]
+output [
+  {{
+    name: "output"
+    data_type: TYPE_FP32
+    dims: [-1]
+  }}
+]
+"""
+with open(os.path.join(TRITON_MODEL_DIR, MODEL_NAME, "config.pbtxt"), "w") as f:
+    f.write(config)
+print("Wrote Triton model repo at", TRITON_MODEL_DIR)
+
*** End Patch
*** Begin Patch
*** Add File:infra/triton/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: triton
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: triton
+  template:
+    metadata:
+      labels:
+        app: triton
+    spec:
+      containers:
+        - name: triton
+          image: nvcr.io/nvidia/tritonserver:23.03-py3
+          args: ["tritonserver", "--model-repository=/models"]
+          volumeMounts:
+            - name: models
+              mountPath: /models
+      volumes:
+        - name: models
+          persistentVolumeClaim:
+            claimName: triton-models-pvc
+
+---
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: triton-models-pvc
+  namespace: aegis
+spec:
+  accessModes:
+    - ReadWriteMany
+  resources:
+    requests:
+      storage: 20Gi
+
*** End Patch
*** Begin Patch
*** Add File:infra/triton/nlp_config.pbtxt
+name: "rag_reader"
+platform: "onnxruntime_onnx"
+max_batch_size: 8
+input [
+  {
+    name: "input_ids"
+    data_type: TYPE_INT32
+    dims: [-1]
+  }
+]
+output [
+  {
+    name: "output"
+    data_type: TYPE_FP32
+    dims: [-1]
+  }
+]
+
*** End Patch
*** Begin Patch
*** Add File:infra/milvus/ingest.py
+#!/usr/bin/env python3
+"""
+Create embeddings (sentence-transformers) and insert into Milvus collection.
+Production: run as Argo job with batching and checkpointing.
+"""
+import os
+from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection
+from sentence_transformers import SentenceTransformer
+from pymongo import MongoClient
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST","127.0.0.1")
+MILVUS_PORT = os.environ.get("MILVUS_PORT","19530")
+MONGO = os.environ.get("MONGO_URI","mongodb://localhost:27017")
+
+def main():
+    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
+    client = MongoClient(MONGO)
+    db = client['aegis']
+    docs = list(db.documents.find({}))
+    texts = [d['text'] for d in docs]
+    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
+    embs = model.encode(texts).tolist()
+
+    # create collection if not exists
+    fields = [
+        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
+        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=len(embs[0]))
+    ]
+    schema = CollectionSchema(fields, "Aegis documents")
+    coll_name = "aegis_documents"
+    if not Collection.exists(coll_name):
+        collection = Collection(coll_name, schema)
+        collection.create_index("embedding", {"index_type":"IVF_FLAT","metric_type":"L2","params":{"nlist":128}})
+        collection.load()
+    else:
+        collection = Collection(coll_name)
+    # insert
+    collection.insert([list(range(len(embs))), embs])
+    print("Inserted embeddings into Milvus collection", coll_name)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/cv/detectron2/train_full.py
+#!/usr/bin/env python3
+"""
+Detectron2 training pipeline (example). Requires GPU node and dataset in COCO format.
+This script uses detectron2's DefaultTrainer and saves model to OUT_DIR.
+"""
+import os
+from detectron2.engine import DefaultTrainer
+from detectron2.config import get_cfg
+
+OUT_DIR = os.environ.get("OUT_DIR","out/detectron2")
+DATASET_PATH = os.environ.get("DATASET_PATH","/data/coco")
+
+def main():
+    os.makedirs(OUT_DIR, exist_ok=True)
+    cfg = get_cfg()
+    cfg.merge_from_file("configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
+    cfg.DATASETS.TRAIN = ("coco_train",)
+    cfg.DATASETS.TEST = ("coco_val",)
+    cfg.DATALOADER.NUM_WORKERS = 4
+    cfg.SOLVER.IMS_PER_BATCH = 2
+    cfg.SOLVER.BASE_LR = 0.00025
+    cfg.SOLVER.MAX_ITER = 1000
+    cfg.MODEL.ROI_HEADS.NUM_CLASSES = int(os.environ.get("NUM_CLASSES","1"))
+    cfg.OUTPUT_DIR = OUT_DIR
+    trainer = DefaultTrainer(cfg)
+    trainer.resume_or_load(resume=False)
+    trainer.train()
+    print("Detectron2 training complete; model saved to", OUT_DIR)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/cv/onnx_export.py
+#!/usr/bin/env python3
+"""
+Export Detectron2/PyTorch model to ONNX (placeholder). For real pipelines, use torch.onnx.export after model preparation.
+"""
+import os
+OUT_DIR = os.environ.get("OUT_DIR","out/detectron2")
+print("Ensure model artifacts exist in", OUT_DIR)
+# Real export steps depend on model and pre/post processing; keep as a placeholder
+print("Export to ONNX is deployment-specific. Use torch.onnx.export with correct example inputs.")
+
*** End Patch
*** Begin Patch
*** Add File:ml/patterns/stream/kafka_ingest.py
+#!/usr/bin/env python3
+"""
+Kafka producer stub to simulate telemetry ingestion.
+Production: use a robust producer with batching and schema registry.
+"""
+from kafka import KafkaProducer
+import json, time, os, random
+
+BROKERS = os.environ.get("KAFKA_BOOTSTRAP","localhost:9092")
+TOPIC = os.environ.get("KAFKA_TOPIC","telemetry")
+
+def main():
+    p = KafkaProducer(bootstrap_servers=BROKERS, value_serializer=lambda v: json.dumps(v).encode('utf-8'))
+    for i in range(100):
+        msg = {"device_id": f"dev-{i%10}", "ts": time.time(), "value": random.random()}
+        p.send(TOPIC, msg)
+    p.flush()
+    print("Produced telemetry messages to", TOPIC)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ml/patterns/stream/consumer_and_anomaly.py
+#!/usr/bin/env python3
+"""
+Kafka consumer that writes telemetry into MongoDB and runs a simple threshold anomaly detector.
+If anomaly rate exceeds threshold, it triggers an Argo retrain workflow via Argo Server.
+"""
+from kafka import KafkaConsumer
+from pymongo import MongoClient
+import json, os, time, requests
+
+BROKERS = os.environ.get("KAFKA_BOOTSTRAP","localhost:9092")
+TOPIC = os.environ.get("KAFKA_TOPIC","telemetry")
+MONGO = os.environ.get("MONGO_URI","mongodb://localhost:27017")
+ARGO_SERVER = os.environ.get("ARGO_SERVER","http://argo-server.argo.svc:2746")
+RETRAIN_WORKFLOW_ENTRY = os.environ.get("RETRAIN_WF","retrain-autoencoder")
+
+def trigger_retrain():
+    wf = {
+      "apiVersion":"argoproj.io/v1alpha1",
+      "kind":"Workflow",
+      "metadata":{"generateName":"retrain-trigger-"},
+      "spec":{"entrypoint": RETRAIN_WORKFLOW_ENTRY}
+    }
+    try:
+        r = requests.post(f"{ARGO_SERVER}/api/v1/workflows/aegis", json=wf, timeout=10)
+        print("Triggered retrain workflow, status", r.status_code)
+    except Exception as e:
+        print("Failed to trigger retrain:", e)
+
+def main():
+    consumer = KafkaConsumer(TOPIC, bootstrap_servers=BROKERS, value_deserializer=lambda m: json.loads(m.decode('utf-8')))
+    client = MongoClient(MONGO)
+    db = client['aegis']
+    anomalies = 0
+    total = 0
+    for msg in consumer:
+        total += 1
+        doc = msg.value
+        db.telemetry.insert_one(doc)
+        if doc.get("value",0) > 0.95:
+            db.anomalies.insert_one({"doc":doc,"ts":time.time()})
+            anomalies += 1
+        if total % 100 == 0:
+            rate = anomalies / total
+            print("Anomaly rate", rate)
+            if rate > 0.05:
+                trigger_retrain()
+                anomalies = 0
+                total = 0
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/ae_train_gpu.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: ae-train-
+  namespace: aegis
+spec:
+  entrypoint: train-ae
+  templates:
+    - name: train-ae
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install numpy && python3 ml/patterns/autoencoder/train_ae.py
+      nodeSelector:
+        cloud.google.com/gke-accelerator: "nvidia-tesla-t4"
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      resources:
+        limits:
+          nvidia.com/gpu: 1
+
*** End Patch
*** Begin Patch
*** Add File:argo/retrain_trigger_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: retrain-evaluator-
+  namespace: aegis
+spec:
+  entrypoint: evaluate-and-trigger
+  templates:
+    - name: evaluate-and-trigger
+      steps:
+        - - name: run-drift-check
+            template: drift
+        - - name: decide
+            template: decide
+
+    - name: drift
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pymongo numpy || true; python3 monitoring/drift/drift_detector.py
+      outputs:
+        parameters:
+          - name: drift-flag
+            valueFrom:
+              path: /tmp/drift-flag.txt
+
+    - name: decide
+      container:
+        image: curlimages/curl:7.85.0
+        command: [sh, -c]
+        args:
+          - |
+            # read drift flag and trigger retrain if present (simplified)
+            if [ -f /tmp/drift-flag.txt ] && grep -q "DRIFT" /tmp/drift-flag.txt; then
+              echo "Drift detected â€” triggering retrain workflow"
+              curl -s -X POST "http://argo-server.argo.svc:2746/api/v1/workflows/aegis" -H "Content-Type: application/json" -d '{"apiVersion":"argoproj.io/v1alpha1","kind":"Workflow","metadata":{"generateName":"retrain-auto-"},"spec":{"entrypoint":"train-ae"}}' || true
+            else
+              echo "No drift"
+            fi
+
*** End Patch
*** Begin Patch
*** Add File:docs/PRODUCTIONIZE_RUNBOOK.md
+## Productionize NLP, CV, and Pattern Recognition â€” Runbook & Prioritized Tasks
+
+Goal: move the domain scaffolds to production-grade pipelines with SRE-approved runbooks, evidence capture, monitoring, and safe deployment patterns.
+
+High-level phases & estimated effort (team-weeks)
+- Phase 0 â€” Prep & infra (1â€“2w)
+  - Provision GPU-enabled Argo runners / node pools (1w)
+  - Securely store HF keys in Vault and provide CSI/Vault Agent injection (0.5w)
+  - Deploy production Milvus or managed vector DB (0.5w)
+
+- Phase 1 â€” NLP RAG productionize (2â€“4w)
+  - Replace demo dataset with production dataset & register in MongoDB (0.5w)
+  - Run argo_train_rag_gpu.yaml on GPU workers; validate outputs (0.5w)
+  - Export model to ONNX and prepare Triton model repo (1w)
+  - Deploy Triton & create k8s service + Flagger canary + safety-monitor webhook (1w)
+
+- Phase 2 â€” CV pipeline productionize (3â€“6w)
+  - Prepare COCO-like dataset and Label Studio annotation workflows (1w)
+  - Run Detectron2 training on GPU workers and validate (1â€“2w)
+  - Convert to ONNX/TorchScript and configure Triton (1w)
+  - Deploy Triton CV model with canaries and monitoring (1w)
+
+- Phase 3 â€” Pattern recognition & streaming (2â€“3w)
+  - Deploy Kafka (managed or Strimzi) and telemetry producers (0.5w)
+  - Deploy consumer/anomaly detector + thresholds + alerting (1w)
+  - Integrate retrain trigger workflow (0.5â€“1w)
+
+- Phase 4 â€” Governance & monitoring (2â€“3w)
+  - Integrate evidence attach_and_sign for all training & promotion workflows (1w)
+  - Add model metrics & drift detectors to Prometheus + Grafana dashboards (1w)
+  - Add cost/token budgets, quotas and SLO alerts (0.5â€“1w)
+
+Checklist: must-have before production rollout
+- All training & serving images SBOMed and scanned; artifacts signed & Rekor logged.
+- HF keys & other secrets in Vault; no plain secrets in k8s manifests.
+- Triton (or Seldon) autoscaler & GPU scheduling validated under load.
+- Safety-monitor and Gatekeeper checks in place for canary pre-rollout.
+- Velero backups + restore drills passed for model-storage and triton-models PVC.
+- KPI dashboards with lead time, signed artifact coverage, model performance, drift & cost.
+
+Recommended owners
+- Platform Infra: GPU infra, Triton, Milvus, Kafka
+- ML Engineering: training scripts, dataset registration, validators
+- SRE: monitoring, backups, runbooks, capacity
+- Security/Governance: Vault, cosign/Rekor, evidence retention & audits
+
+Runbook snippets
+- Training failure: check Argo logs, check GPU availability, inspect training container logs; reproduce locally if needed; attach_and_sign training evidence (even failed runs) for audit.
+- Canary rollback: Flagger will rollback automatically; investigate root cause and attach evidence; run retrain if model performance regresses due to data drift.
+
*** End Patch
*** End Patch
