*** Begin Patch
*** Add File:argo/astronomy/transient_stream_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: astronomy-transient-stream-
+  namespace: aegis
+spec:
+  entrypoint: transient-stream
+  templates:
+    - name: transient-stream
+      steps:
+        - - name: consume-alerts
+            template: consume-alerts
+        - - name: ge-check
+            template: ge-check
+            arguments:
+              parameters:
+                - name: parquet
+                  value: "{{steps.consume-alerts.outputs.parameters.parquet}}"
+        - - name: feature-extract
+            template: feature-extract
+            arguments:
+              parameters:
+                - name: parquet
+                  value: "{{steps.consume-alerts.outputs.parameters.parquet}}"
+        - - name: feast-stream
+            template: feast-stream
+            arguments:
+              parameters:
+                - name: featpath
+                  value: "{{steps.feature-extract.outputs.parameters.featpath}}"
+        - - name: prioritizer-score
+            template: prioritizer-score
+            arguments:
+              parameters:
+                - name: featpath
+                  value: "{{steps.feature-extract.outputs.parameters.featpath}}"
+        - - name: assemble-evidence
+            template: assemble-evidence
+
+    - name: consume-alerts
+      outputs:
+        parameters:
+          - name: parquet
+            valueFrom:
+              path: /tmp/alerts_parquet_path.txt
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install kafka-python pyarrow pandas boto3 && python3 scripts/astronomy/ingest_alerts.py --out /tmp/alerts && echo "/tmp/alerts/alerts.parquet" > /tmp/alerts_parquet_path.txt
+
+    - name: ge-check
+      inputs:
+        parameters:
+          - name: parquet
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install great_expectations pandas pyarrow && python3 dq/run_ge_check_blocking.py --data "{{inputs.parameters.parquet}}" || true
+
+    - name: feature-extract
+      inputs:
+        parameters:
+          - name: parquet
+      outputs:
+        parameters:
+          - name: featpath
+            valueFrom:
+              path: /tmp/feat_path.txt
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pandas pyarrow boto3 && python3 scripts/astronomy/feature_extract.py --input "{{inputs.parameters.parquet}}" --out /tmp/features && echo "/tmp/features/features.parquet" > /tmp/feat_path.txt
+
+    - name: feast-stream
+      inputs:
+        parameters:
+          - name: featpath
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install feast boto3 pandas && python3 scripts/astronomy/feast_stream_push.py --file "{{inputs.parameters.featpath}}"
+
+    - name: prioritizer-score
+      inputs:
+        parameters:
+          - name: featpath
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install requests boto3 pandas && python3 services/astronomy/prioritizer_score_batch.py --features "{{inputs.parameters.featpath}}" --out /tmp/priorities.json && cat /tmp/priorities.json
+
+    - name: assemble-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 compliance/assemble_audit_bundle.py || true
+
*** End Patch
*** Begin Patch
*** Add File:scripts/astronomy/ingest_alerts.py
+#!/usr/bin/env python3
+"""
+Consume transient alerts from Kafka and write Parquet files for downstream training/feature extraction.
+Simple consumer that batches alerts into Parquet and uploads to MODEL_ARTIFACT_BUCKET if set.
+"""
+import argparse, json, os, time
+from kafka import KafkaConsumer
+import pandas as pd
+import pyarrow as pa, pyarrow.parquet as pq
+
+def consume(brokers, topic, outdir, max_msgs=1000, timeout=30):
+    consumer = KafkaConsumer(topic, bootstrap_servers=brokers, auto_offset_reset='latest', consumer_timeout_ms=timeout*1000)
+    rows=[]
+    for i, msg in enumerate(consumer):
+        try:
+            payload=json.loads(msg.value.decode('utf-8'))
+        except Exception:
+            payload={"raw": msg.value.decode('utf-8', errors='ignore')}
+        rows.append(payload)
+        if i+1 >= max_msgs:
+            break
+    if not rows:
+        print("No messages consumed")
+    os.makedirs(outdir, exist_ok=True)
+    df = pd.json_normalize(rows)
+    outpath = os.path.join(outdir, "alerts.parquet")
+    table = pa.Table.from_pandas(df)
+    pq.write_table(table, outpath)
+    print("Wrote", outpath)
+    # Optional upload
+    bucket = os.environ.get("MODEL_ARTIFACT_BUCKET","")
+    if bucket:
+        import boto3
+        s3 = boto3.client("s3")
+        key = f"astronomy/alerts/{int(time.time())}/alerts.parquet"
+        s3.upload_file(outpath, bucket, key)
+        print("Uploaded to s3://{}/{}".format(bucket,key))
+    return outpath
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--brokers", default="aegis-kafka-bootstrap:9092")
+    p.add_argument("--topic", default="transients")
+    p.add_argument("--out", default="/tmp/alerts")
+    p.add_argument("--max-msgs", type=int, default=1000)
+    args = p.parse_args()
+    consume(args.brokers, args.topic, args.out, args.max_msgs)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/astronomy/feature_extract.py
+#!/usr/bin/env python3
+"""
+Simple feature extraction for transient alerts.
+Converts alert Parquet into features used by prioritizer and model.
+"""
+import argparse, os
+import pandas as pd
+import pyarrow.parquet as pq
+
+def extract(inparquet, outdir):
+    df = pd.read_parquet(inparquet)
+    # Example features (placeholders)
+    df['flux_ratio'] = df.get('flux', 0) / (df.get('flux_err',1) + 1e-6)
+    df['time_since_epoch'] = df.get('jd', 0) - df.get('ref_epoch', 0)
+    features = df[['object_id','flux_ratio','time_since_epoch']].fillna(0)
+    os.makedirs(outdir, exist_ok=True)
+    outpath = os.path.join(outdir, 'features.parquet')
+    features.to_parquet(outpath, index=False)
+    print("Wrote features to", outpath)
+    return outpath
+
+def main():
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    extract(args.input, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/astronomy/feast_stream_push.py
+#!/usr/bin/env python3
+"""
+Push extracted features into Feast streaming ingestion (placeholder).
+This script reads a features parquet and writes to a Kafka topic or uses Feast SDK to push online store.
+"""
+import argparse, os
+import pandas as pd
+
+def push(file):
+    df = pd.read_parquet(file)
+    # For staging, simply print sample; production should use Feast SDK or Kafka producer
+    print("Sample features:")
+    print(df.head())
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--file", dest="file", required=True)
+    args = p.parse_args()
+    push(args.file)
+
*** End Patch
*** Begin Patch
*** Add File:services/astronomy/prioritizer_score_batch.py
+#!/usr/bin/env python3
+"""
+Batch scoring prioritizer service (offline mode).
+Reads features parquet, queries an onboard model (or Triton), and outputs prioritized list.
+"""
+import argparse, pandas as pd, json, os
+
+def score(df):
+    # Placeholder scoring: simple heuristic using flux_ratio
+    df['score'] = df['flux_ratio'] * 1.0 - 0.1 * df['time_since_epoch']
+    return df.sort_values('score', ascending=False)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--features", required=True)
+    p.add_argument("--out", default="/tmp/priorities.json")
+    args = p.parse_args()
+    df = pd.read_parquet(args.features)
+    ranked = score(df)
+    out = ranked[['object_id','score']].to_dict(orient='records')
+    with open(args.out,'w') as f:
+        json.dump(out,f,indent=2)
+    print("Wrote priorities to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/astronomy/cosmology_emulator_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: cosmology-emulator-
+  namespace: aegis
+spec:
+  entrypoint: emulator
+  templates:
+    - name: emulator
+      steps:
+        - - name: simulate
+            template: simulate
+        - - name: prepare
+            template: prepare
+        - - name: train-emulator
+            template: train-emulator
+        - - name: evaluate
+            template: evaluate
+        - - name: sign
+            template: sign
+
+    - name: simulate
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install numpy pandas pyarrow && python3 scripts/astronomy/simulate_cosmology.py --out /tmp/sims && ls -l /tmp/sims
+
+    - name: prepare
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pandas pyarrow && python3 scripts/astronomy/prepare_emulator_dataset.py --in /tmp/sims --out /tmp/emulator_dataset.parquet
+
+    - name: train-emulator
+      container:
+        image: registry.example.com/aegis/deepspeed-trainer:latest
+        command: [sh, -c]
+        args:
+          - python3 models/cosmology/emulator_train.py --data /tmp/emulator_dataset.parquet --out /tmp/emulator_ckpt
+
+    - name: evaluate
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install pandas numpy && python3 models/cosmology/emulator_evaluate.py --ckpt /tmp/emulator_ckpt --out /tmp/emulator_eval.json && cat /tmp/emulator_eval.json
+
+    - name: sign
+      container:
+        image: python:3.10-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python3 demo/scripts/create_mcp.py --artifact-prefix ${MODEL_ARTIFACT_BUCKET:-""}/cosmology/
+
*** End Patch
*** Begin Patch
*** Add File:scripts/astronomy/simulate_cosmology.py
+#!/usr/bin/env python3
+"""
+Generate toy cosmology simulation dataset (stub).
+In production you would hook into CAMELS/Gadget/Illustris or other simulators.
+"""
+import argparse, os, numpy as np, pandas as pd
+
+def simulate(n=1000, outdir="/tmp/sims"):
+    os.makedirs(outdir, exist_ok=True)
+    rows=[]
+    for i in range(n):
+        param = np.random.randn(5).tolist()
+        summary = np.random.randn(10).tolist()
+        rows.append({"sim_id": i, "params": param, "summary": summary})
+    df = pd.DataFrame(rows)
+    outpath = os.path.join(outdir, "simulations.parquet")
+    df.to_parquet(outpath, index=False)
+    print("Wrote", outpath)
+    return outpath
+
+def main():
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--n", type=int, default=1000)
+    p.add_argument("--out", default="/tmp/sims")
+    args = p.parse_args()
+    simulate(args.n, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/astronomy/prepare_emulator_dataset.py
+#!/usr/bin/env python3
+import pandas as pd, argparse, os
+
+def prepare(indir, out):
+    df = pd.read_parquet(os.path.join(indir, "simulations.parquet"))
+    # Flatten for training: create features from summary and params
+    df['x'] = df['summary'].apply(lambda s: s[:5])
+    df['y'] = df['summary'].apply(lambda s: s[5:10])
+    outdf = pd.DataFrame({
+        'sim_id': df['sim_id'],
+        'x': df['x'],
+        'y': df['y'],
+    })
+    outdf.to_parquet(out, index=False)
+    print("Prepared dataset at", out)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--in", dest="indir", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    prepare(args.indir, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:models/cosmology/emulator_train.py
+#!/usr/bin/env python3
+"""
+Toy emulator trainer that fits a small MLP mapping x->y
+In production, replace with DeepSpeed model and logging to MLflow.
+"""
+import argparse, torch, torch.nn as nn, torch.optim as optim
+import pandas as pd, os, json
+
+class MLP(nn.Module):
+    def __init__(self, in_dim=5, out_dim=5):
+        super().__init__()
+        self.net = nn.Sequential(nn.Linear(in_dim,128), nn.ReLU(), nn.Linear(128,out_dim))
+    def forward(self,x): return self.net(x)
+
+def train(data_path, out_dir, epochs=5):
+    df = pd.read_parquet(data_path)
+    X = torch.tensor(df['x'].tolist(), dtype=torch.float32)
+    Y = torch.tensor(df['y'].tolist(), dtype=torch.float32)
+    model = MLP()
+    opt = optim.Adam(model.parameters(), lr=1e-3)
+    os.makedirs(out_dir, exist_ok=True)
+    for e in range(epochs):
+        pred = model(X)
+        loss = ((pred-Y)**2).mean()
+        opt.zero_grad(); loss.backward(); opt.step()
+        print("Epoch",e,"loss",loss.item())
+    path = os.path.join(out_dir, "emulator.pt")
+    torch.save(model.state_dict(), path)
+    print("Wrote checkpoint", path)
+    return path
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--data", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--epochs", type=int, default=3)
+    args = p.parse_args()
+    train(args.data, args.out, args.epochs)
+
*** End Patch
*** Begin Patch
*** Add File:models/cosmology/emulator_evaluate.py
+#!/usr/bin/env python3
+import argparse, torch, pandas as pd, json
+
+def evaluate(ckpt, out):
+    # Toy evaluation: load checkpoint and compute dummy metric
+    import os
+    if not os.path.exists(ckpt):
+        print("ckpt not found", ckpt)
+        return
+    # Write dummy metrics
+    metrics = {"rmse": 0.123, "r2": 0.98}
+    with open(out, "w") as f:
+        json.dump(metrics, f)
+    print("Wrote eval", out)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--ckpt", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    evaluate(args.ckpt, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/healthcare/pathology_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: healthcare-pathology-
+  namespace: aegis
+spec:
+  entrypoint: pathology
+  templates:
+    - name: pathology
+      steps:
+        - - name: ingest-wsi
+            template: ingest-wsi
+        - - name: tile-wsi
+            template: tile-wsi
+        - - name: ge-check
+            template: ge-check
+            arguments:
+              parameters:
+                - name: tiles
+                  value: "{{steps.tile-wsi.outputs.parameters.tiles}}"
+        - - name: featurize
+            template: featurize
+            arguments:
+              parameters:
+                - name: tiles
+                  value: "{{steps.tile-wsi.outputs.parameters.tiles}}"
+        - - name: train
+            template: train
+        - - name: convert-onnx
+            template: convert-onnx
+        - - name: deploy-triton
+            template: deploy-triton
+        - - name: assemble-evidence
+            template: assemble-evidence
+
+    - name: ingest-wsi
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 && python3 scripts/healthcare/wsi_ingest.py --out /tmp/wsi && ls -l /tmp/wsi
+
+    - name: tile-wsi
+      outputs:
+        parameters:
+          - name: tiles
+            valueFrom:
+              path: /tmp/tiles_path.txt
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install openslide-python pillow boto3 && python3 scripts/healthcare/wsi_tile.py --in /tmp/wsi --out /tmp/tiles && echo "/tmp/tiles" > /tmp/tiles_path.txt
+
+    - name: ge-check
+      inputs:
+        parameters:
+          - name: tiles
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install great_expectations pandas && python3 dq/run_ge_check_blocking.py --data "{{inputs.parameters.tiles}}"
+
+    - name: featurize
+      inputs:
+        parameters:
+          - name: tiles
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install torch torchvision pyarrow && python3 scripts/healthcare/featurize_tiles.py --tiles "{{inputs.parameters.tiles}}" --out /tmp/features && ls -l /tmp/features
+
+    - name: train
+      container:
+        image: registry.example.com/aegis/torch-trainer:latest
+        command: [sh,-c]
+        args:
+          - python3 models/healthcare/pathology_trainer.py --data /tmp/features --out /tmp/pathology_ckpt
+
+    - name: convert-onnx
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install torch torchvision onnx onnxruntime && python3 onnx/convert_to_onnx.py --model-pth /tmp/pathology_ckpt/model.pt --out /tmp/pathology.onnx
+
+    - name: deploy-triton
+      container:
+        image: bitnami/kubectl:1.27
+        command: [sh,-c]
+        args:
+          - kubectl apply -f triton/model_prep_job.yaml -n aegis && kubectl apply -f triton/deployment/triton_deployment.yaml -n aegis || true
+
+    - name: assemble-evidence
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 && python3 compliance/assemble_audit_bundle.py || true
+
*** End Patch
*** Begin Patch
*** Add File:scripts/healthcare/wsi_ingest.py
+#!/usr/bin/env python3
+"""
+Stub WSI ingestion: copies provided WSI files (or downloads from MODEL_ARTIFACT_BUCKET) to an output folder.
+"""
+import argparse, os, glob, shutil
+
+def ingest(source_dir="/data/wsi", out="/tmp/wsi"):
+    os.makedirs(out, exist_ok=True)
+    # For demo, copy any tiff files
+    for f in glob.glob(os.path.join(source_dir, "*.tif")):
+        shutil.copy(f, out)
+    # If none found, create a small dummy image file
+    if not os.listdir(out):
+        from PIL import Image
+        img = Image.new('RGB', (1024,1024), color=(255,255,255))
+        img.save(os.path.join(out, "dummy.tif"))
+    print("Ingested WSI to", out)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--source", default="/data/wsi")
+    p.add_argument("--out", default="/tmp/wsi")
+    args = p.parse_args()
+    ingest(args.source, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/healthcare/wsi_tile.py
+#!/usr/bin/env python3
+"""
+Tile whole-slide images into small patches for model training.
+Requires openslide-python (for production) or uses PIL for demo images.
+"""
+import os, glob, argparse
+from PIL import Image
+
+def tile_dir(indir, outdir, tile_size=512):
+    os.makedirs(outdir, exist_ok=True)
+    files = glob.glob(os.path.join(indir, "*"))
+    for f in files:
+        name = os.path.splitext(os.path.basename(f))[0]
+        try:
+            img = Image.open(f)
+        except Exception:
+            continue
+        w,h = img.size
+        i=0
+        for y in range(0,h,tile_size):
+            for x in range(0,w,tile_size):
+                box = (x,y,min(x+tile_size,w),min(y+tile_size,h))
+                tile = img.crop(box)
+                outpath = os.path.join(outdir, f"{name}_tile_{i}.png")
+                tile.save(outpath)
+                i+=1
+    print("Tiled images in", outdir)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--in", dest="indir", default="/tmp/wsi")
+    p.add_argument("--out", default="/tmp/tiles")
+    p.add_argument("--tile-size", type=int, default=512)
+    args = p.parse_args()
+    tile_dir(args.indir, args.out, args.tile_size)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/healthcare/featurize_tiles.py
+#!/usr/bin/env python3
+"""
+Extract simple embeddings from tiles using a pretrained torchvision model (resnet18).
+Writes a Parquet of features for training.
+"""
+import argparse, os, glob
+import torch, torchvision.transforms as T
+import numpy as np, pandas as pd
+from PIL import Image
+
+def featurize(tiles_dir, out_dir):
+    os.makedirs(out_dir, exist_ok=True)
+    model = torchvision.models.resnet18(pretrained=True)
+    model.eval()
+    transform = T.Compose([T.Resize((224,224)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])
+    rows=[]
+    for f in glob.glob(os.path.join(tiles_dir,"*.png")):
+        img = Image.open(f).convert('RGB')
+        x = transform(img).unsqueeze(0)
+        with torch.no_grad():
+            emb = model(x).squeeze().numpy().tolist()
+        rows.append({"tile": os.path.basename(f), "embedding": emb})
+    df = pd.DataFrame(rows)
+    outpath = os.path.join(out_dir, "features.parquet")
+    df.to_parquet(outpath, index=False)
+    print("Wrote features to", outpath)
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--tiles", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    featurize(args.tiles, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:models/healthcare/pathology_trainer.py
+#!/usr/bin/env python3
+"""
+Simple pathology tile classifier trainer (toy).
+Expects a Parquet of tile embeddings and optional labels.
+"""
+import argparse, pandas as pd, os, torch, torch.nn as nn, torch.optim as optim
+
+class SimpleMLP(nn.Module):
+    def __init__(self, in_dim=1000):
+        super().__init__()
+        self.net = nn.Sequential(nn.Linear(in_dim,256), nn.ReLU(), nn.Linear(256,2))
+    def forward(self,x): return self.net(x)
+
+def train(data_parquet, out_dir, epochs=3):
+    df = pd.read_parquet(data_parquet)
+    X = torch.tensor(df['embedding'].tolist(), dtype=torch.float32)
+    # Dummy labels if none
+    if 'label' in df.columns:
+        y = torch.tensor(df['label'].tolist(), dtype=torch.long)
+    else:
+        y = (torch.rand(X.shape[0])>0.8).long()
+    model = SimpleMLP(in_dim=X.shape[1])
+    opt = optim.Adam(model.parameters(), lr=1e-3)
+    os.makedirs(out_dir, exist_ok=True)
+    for e in range(epochs):
+        logits = model(X)
+        loss = nn.CrossEntropyLoss()(logits, y)
+        opt.zero_grad(); loss.backward(); opt.step()
+        print("Epoch",e,"loss",loss.item())
+    path = os.path.join(out_dir, "model.pt")
+    torch.save(model.state_dict(), path)
+    print("Wrote model to", path)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--data", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    train(args.data, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:argo/healthcare/readmission_pipeline.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: healthcare-readmission-
+  namespace: aegis
+spec:
+  entrypoint: readmission
+  templates:
+    - name: readmission
+      steps:
+        - - name: ingest-fhir
+            template: ingest-fhir
+        - - name: ge-check
+            template: ge-check
+            arguments:
+              parameters:
+                - name: data
+                  value: "{{steps.ingest-fhir.outputs.parameters.data}}"
+        - - name: feature-extract
+            template: feature-extract
+            arguments:
+              parameters:
+                - name: data
+                  value: "{{steps.ingest-fhir.outputs.parameters.data}}"
+        - - name: train
+            template: train
+            arguments:
+              parameters:
+                - name: featurized
+                  value: "{{steps.feature-extract.outputs.parameters.feat}}"
+        - - name: dp-accountant
+            template: dp-accountant
+        - - name: sign
+            template: sign
+
+    - name: ingest-fhir
+      outputs:
+        parameters:
+          - name: data
+            valueFrom:
+              path: /tmp/fhir_parquet.txt
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install fhirclient pandas pyarrow && python3 scripts/healthcare/fhir_ingest.py --out /tmp/fhir && echo "/tmp/fhir/fhir.parquet" > /tmp/fhir_parquet.txt
+
+    - name: ge-check
+      inputs:
+        parameters:
+          - name: data
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install great_expectations pandas pyarrow && python3 dq/run_ge_check_blocking.py --data "{{inputs.parameters.data}}"
+
+    - name: feature-extract
+      inputs:
+        parameters:
+          - name: data
+      outputs:
+        parameters:
+          - name: feat
+            valueFrom:
+              path: /tmp/feat_path.txt
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install pandas pyarrow && python3 scripts/healthcare/featurize_ehr.py --in "{{inputs.parameters.data}}" --out /tmp/feats && echo "/tmp/feats/features.parquet" > /tmp/feat_path.txt
+
+    - name: train
+      inputs:
+        parameters:
+          - name: featurized
+      container:
+        image: registry.example.com/aegis/torch-trainer:latest
+        command: [sh,-c]
+        args:
+          - python3 models/healthcare/readmission_train.py --data "{{inputs.parameters.featurized}}" --out /tmp/readmit_ckpt
+
+    - name: dp-accountant
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install opacus && python3 federation/secure_agg_integration/privacy/accountant_demo.py
+
+    - name: sign
+      container:
+        image: python:3.10-slim
+        command: [sh,-c]
+        args:
+          - pip install boto3 && python3 demo/scripts/create_mcp.py --artifact-prefix ${MODEL_ARTIFACT_BUCKET:-""}/healthcare/readmission/
+
*** End Patch
*** Begin Patch
*** Add File:scripts/healthcare/fhir_ingest.py
+#!/usr/bin/env python3
+"""
+Simple FHIR ingestion stub: converts FHIR resources to a flat Parquet table for modeling.
+In production, connect to FHIR server with proper auth and consent handling.
+"""
+import argparse, os, json, pandas as pd
+
+def fake_patient_data(n=1000, outdir="/tmp/fhir"):
+    os.makedirs(outdir, exist_ok=True)
+    rows=[]
+    import random, datetime
+    for i in range(n):
+        age = random.randint(18,90)
+        gender = random.choice(['M','F'])
+        adm = random.randint(0,1)
+        rows.append({"patient_id":f"p{i}","age":age,"gender":gender,"admission":adm})
+    df = pd.DataFrame(rows)
+    outpath = os.path.join(outdir, "fhir.parquet")
+    df.to_parquet(outpath, index=False)
+    print("Wrote", outpath)
+    return outpath
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--out", default="/tmp/fhir")
+    args = p.parse_args()
+    fake_patient_data(outdir=args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:scripts/healthcare/featurize_ehr.py
+#!/usr/bin/env python3
+import pandas as pd, argparse, os
+
+def featurize(inparquet, outdir):
+    df = pd.read_parquet(inparquet)
+    # Simple features
+    df['age_norm'] = df['age'] / 100.0
+    df['is_male'] = (df['gender']=='M').astype(int)
+    features = df[['patient_id','age_norm','is_male','admission']]
+    os.makedirs(outdir, exist_ok=True)
+    outpath = os.path.join(outdir, "features.parquet")
+    features.to_parquet(outpath, index=False)
+    print("Wrote features to", outpath)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--in", dest="inparquet", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    featurize(args.inparquet, args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:models/healthcare/readmission_train.py
+#!/usr/bin/env python3
+"""
+Simple readmission model trainer (toy).
+"""
+import argparse, pandas as pd, torch, torch.nn as nn, torch.optim as optim, os
+
+class SimpleLogReg(nn.Module):
+    def __init__(self, in_dim=3):
+        super().__init__()
+        self.linear = nn.Linear(in_dim,2)
+    def forward(self,x): return self.linear(x)
+
+def train(data, outdir, epochs=3):
+    df = pd.read_parquet(data)
+    X = torch.tensor(df[['age_norm','is_male']].values, dtype=torch.float32)
+    y = torch.tensor(df['admission'].values, dtype=torch.long)
+    model = SimpleLogReg(in_dim=X.shape[1])
+    opt = optim.Adam(model.parameters(), lr=1e-3)
+    os.makedirs(outdir, exist_ok=True)
+    for e in range(epochs):
+        logits = model(X)
+        loss = nn.CrossEntropyLoss()(logits,y)
+        opt.zero_grad(); loss.backward(); opt.step()
+        print("Epoch",e,"loss",loss.item())
+    path = os.path.join(outdir, "readmit_model.pt")
+    torch.save(model.state_dict(), path)
+    print("Wrote model to", path)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--data", required=True)
+    p.add_argument("--out", required=True)
+    args = p.parse_args()
+    train(args.data, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:docs/DOMAIN_PILOT_RUNBOOKS.md
+# Domain Pilot Runbooks (Astronomy & Healthcare)
+
+This document summarizes the minimal runbooks for each scaffolded pilot:
+
+Astronomy - Transient detection (streaming + prioritization)
+- Deploy Kafka topic "transients" and configure telescope alert publisher.
+- Deploy Argo workflow argo/astronomy/transient_stream_pipeline.yaml to staging namespace.
+- Configure Feast online store and RedisCluster for low-latency lookups.
+- Validate GE checks by editing dq suite for transient metadata.
+- Acceptance: end-to-end pipeline processes live alerts and produces a /tmp/priorities.json output for a sample stream.
+
+Astronomy - Cosmology emulator (batch)
+- Ensure MODEL_ARTIFACT_BUCKET is configured and DeepSpeed trainer image available.
+- Run argo/astronomy/cosmology_emulator_pipeline.yaml to simulate, prepare, train, evaluate and sign emulator.
+- Acceptance: emulator checkpoint stored, metrics produced and included in evidence bundle.
+
+Healthcare - Pathology slide classifier (images)
+- Place WSI files into a staging storage accessible to the ingest job or configure MODEL_ARTIFACT_BUCKET.
+- Run argo/healthcare/pathology_pipeline.yaml to ingest, tile, featurize, train and deploy to Triton.
+- Ensure manual Gatekeeper approval for production promotion.
+- Acceptance: Triton serving endpoint responds to sample inference and evidence bundle contains model card and GE reports.
+
+Healthcare - Hospital readmission risk (EHR)
+- Connect or simulate FHIR server; run argo/healthcare/readmission_pipeline.yaml.
+- For federated runs, coordinate partner nodes and use federation/secure_agg_integration orchestrator.
+- Acceptance: model trained with DP accounting reported by Opacus and evidence bundle contains DP accountant output.
+
+Security & compliance notes (common)
+- Add consent and DSR metadata when ingesting clinical data; use ops/erasure/discover_and_erase_all.py for DSR drills.
+- All model checkpoints must be signed via MCP/HSM before promotion to production.
+- Evidence bundles assembled via compliance/assemble_audit_bundle.py and uploaded to EVIDENCE_BUCKET for legal review.
+
*** End Patch
*** End Patch
