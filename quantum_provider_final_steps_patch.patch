*** Begin Patch
*** Add File:terraform/irsa/variables.tf
+variable "cluster_name" {
+  description = "EKS cluster name"
+  type        = string
+}
+
+variable "account_id" {
+  description = "AWS Account ID"
+  type        = string
+}
+
+variable "oidc_provider_arn" {
+  description = "EKS OIDC provider ARN"
+  type        = string
+}
+
+variable "policy_arn_braket" { type = string }
+variable "policy_arn_ibm"    { type = string }
+variable "policy_arn_azure"  { type = string }
+variable "policy_arn_rigetti" { type = string }
+
*** End Patch
*** Begin Patch
*** Add File:terraform/irsa/example.tfvars
+cluster_name = "my-aegis-cluster"
+account_id   = "123456789012"
+oidc_provider_arn = "arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE"
+policy_arn_braket = "arn:aws:iam::123456789012:policy/aegis-braket"
+policy_arn_ibm    = "arn:aws:iam::123456789012:policy/aegis-ibm"
+policy_arn_azure  = "arn:aws:iam::123456789012:policy/aegis-azure"
+policy_arn_rigetti = "arn:aws:iam::123456789012:policy/aegis-rigetti"
+
*** End Patch
*** Begin Patch
*** Add File:ops/eksctl_commands.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Template script to create IRSA iamserviceaccounts using eksctl.
+# Fill the POLICY ARNs and run the script.
+#
+CLUSTER=${1:-my-aegis-cluster}
+REGION=${2:-us-east-1}
+NS=${3:-aegis}
+
+BRAKET_POLICY=${BRAKET_POLICY:-arn:aws:iam::123456789012:policy/aegis-braket}
+IBM_POLICY=${IBM_POLICY:-arn:aws:iam::123456789012:policy/aegis-ibm}
+AZURE_POLICY=${AZURE_POLICY:-arn:aws:iam::123456789012:policy/aegis-azure}
+RIGETTI_POLICY=${RIGETTI_POLICY:-arn:aws:iam::123456789012:policy/aegis-rigetti}
+
+echo "Creating namespace $NS if missing"
+kubectl create ns "$NS" || true
+
+echo "Creating BRaket IRSA SA"
+eksctl create iamserviceaccount --name braket-sa --namespace "$NS" --cluster "$CLUSTER" --region "$REGION" --attach-policy-arn "$BRAKET_POLICY" --approve || true
+
+echo "Creating IBM IRSA SA"
+eksctl create iamserviceaccount --name ibm-quantum-sa --namespace "$NS" --cluster "$CLUSTER" --region "$REGION" --attach-policy-arn "$IBM_POLICY" --approve || true
+
+echo "Creating Azure IRSA SA"
+eksctl create iamserviceaccount --name azure-quantum-sa --namespace "$NS" --cluster "$CLUSTER" --region "$REGION" --attach-policy-arn "$AZURE_POLICY" --approve || true
+
+echo "Creating Rigetti IRSA SA"
+eksctl create iamserviceaccount --name rigetti-quantum-sa --namespace "$NS" --cluster "$CLUSTER" --region "$REGION" --attach-policy-arn "$RIGETTI_POLICY" --approve || true
+
+echo "IRSA creation done. Verify with: kubectl -n $NS get sa -o yaml"
+
*** End Patch
*** Begin Patch
*** Add File:ops/provider_connectors/aws_braket_pricing_connector.py
+#!/usr/bin/env python3
+"""
+AWS Braket pricing connector.
+Attempts to use AWS Pricing API to derive a per-shot cost for device types and falls back to a heuristic.
+Requires AWS credentials with Pricing API access (regional limitations apply).
+"""
+import boto3, json, argparse, math
+
+def query_pricing(service_code='AmazonBraket'):
+    pricing = boto3.client('pricing', region_name='us-east-1')
+    paginator = pricing.get_paginator('get_products')
+    results = []
+    try:
+        for page in paginator.paginate(ServiceCode=service_code, MaxResults=100):
+            results.extend(page.get('PriceList', []))
+    except Exception as e:
+        print("Pricing API unavailable or restricted:", e)
+        return []
+    return results
+
+def heuristic(shots, device, complexity=1.0):
+    base_sim = 0.0005
+    base_qpu = 0.005
+    per_shot = base_sim if 'simulator' in device else base_qpu
+    est = shots * per_shot * (1.0 + complexity*0.1)
+    return {"provider":"braket","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def estimate(shots, device, complexity):
+    skus = query_pricing()
+    if not skus:
+        return heuristic(shots, device, complexity)
+    # Best-effort: inspect skus and look for OnDemand price per unit (implementation may vary)
+    # This is a conservative fallback; parsing will need tailoring per account.
+    try:
+        # parse first SKU as fallback
+        sample = json.loads(skus[0])
+        return heuristic(shots, device, complexity)
+    except Exception:
+        return heuristic(shots, device, complexity)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="simulator")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(estimate(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/provider_connectors/ibm_billing_connector.py
+#!/usr/bin/env python3
+"""
+Stub for IBM Quantum billing connector.
+IBM billing is account-specific; implement the SDK call to your enterprise billing endpoint.
+This script returns a heuristic unless you provide IBMCLOUD_API_KEY and a billing endpoint.
+"""
+import os, json, argparse
+
+def heuristic(shots, device, complexity=1.0):
+    base = 0.001
+    est = shots * base * (1.0 + complexity*0.1)
+    return {"provider":"ibm","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def query_ibm_billing(shots, device, complexity):
+    api_key = os.environ.get("IBM_CLOUD_API_KEY")
+    endpoint = os.environ.get("IBM_BILLING_API_ENDPOINT")
+    if api_key and endpoint:
+        # Implement provider billing API call here
+        # Example: requests.get(endpoint, headers={'Authorization': 'Bearer ' + api_key})
+        return heuristic(shots, device, complexity)
+    return heuristic(shots, device, complexity)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="ibm")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(query_ibm_billing(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/provider_connectors/azure_billing_connector.py
+#!/usr/bin/env python3
+"""
+Azure Cost Management connector stub.
+Use Azure REST APIs (Cost Management Query) to retrieve actual cost/pricing information.
+Requires AZURE_CLIENT_ID/CLIENT_SECRET/TENANT_ID and subscription id.
+"""
+import os, json, argparse, requests
+
+def heuristic(shots, device, complexity=1.0):
+    base = 0.0009
+    est = shots * base * (1.0 + complexity*0.1)
+    return {"provider":"azure","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def query_azure(shots, device, complexity):
+    # Implement OAuth2 client credentials flow and call Cost Management API if you need accurate pricing.
+    if os.environ.get("AZURE_CLIENT_ID"):
+        # Placeholder for real implementation
+        return heuristic(shots, device, complexity)
+    return heuristic(shots, device, complexity)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="ionq")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(query_azure(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/argo_submitter.py
+#!/usr/bin/env python3
+"""
+Simple Argo submitter service to run inside a secure operator pod.
+It reads a submission JSON at /work/submit_request.json (mounted secret or created by operator),
+verifies cost via ops/qpu_cost_estimator.py, checks GitHub PR label if pr_number provided,
+and runs 'argo submit' if checks pass.
+
+This is intentionally simple; run this pod in a highly trusted operator namespace.
+"""
+import os, json, subprocess, time, argparse
+from pathlib import Path
+import requests
+
+GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
+
+def check_github_approval(repo, pr_number):
+    if not GITHUB_TOKEN:
+        print("No GITHUB_TOKEN; cannot check PR approval — failing-safe")
+        return False
+    headers = {"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}
+    url = f"https://api.github.com/repos/{repo}/pulls/{pr_number}/labels"
+    r = requests.get(url, headers=headers, timeout=10)
+    if r.status_code != 200:
+        print("Failed to query PR labels:", r.status_code, r.text)
+        return False
+    labels = [l['name'] for l in r.json()]
+    return "qpu-approved" in labels
+
+def estimate_cost(provider, shots, device, complexity):
+    out = subprocess.check_output(["python3","ops/qpu_cost_estimator.py","--provider",provider,"--shots",str(shots),"--device",device,"--circuit-complexity",str(complexity)])
+    return json.loads(out)
+
+def submit_argo(workflow_path, params):
+    cmd = ["argo","submit","-n","aegis", workflow_path]
+    for k,v in params.items():
+        cmd += ["-p", f"{k}={v}"]
+    print("Running:", " ".join(cmd))
+    subprocess.check_call(cmd)
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--request-file", default="/work/submit_request.json")
+    args = parser.parse_args()
+    req_path = Path(args.request_file)
+    if not req_path.exists():
+        print("No request file found at", req_path); exit(2)
+    req = json.loads(req_path.read_text())
+    provider = req.get("provider")
+    shots = req.get("shots",1000)
+    device = req.get("device","simulator")
+    complexity = req.get("circuit_complexity",1.0)
+    budget = float(req.get("budget_usd", 100.0))
+    pr = req.get("pr")  # optional: {"repo":"owner/repo","number":123}
+    workflow = req.get("workflow")  # path to argo workflow YAML in repo or full path
+    params = req.get("params", {})
+
+    est = estimate_cost(provider, shots, device, complexity)
+    est_usd = float(est.get("estimated_usd", 0.0))
+    print("Estimated USD:", est_usd)
+    if est_usd > budget:
+        print(f"Estimated cost {est_usd} exceeds budget {budget}; aborting")
+        exit(3)
+    if pr:
+        ok = check_github_approval(pr["repo"], pr["number"])
+        if not ok:
+            print("PR not approved with qpu-approved label; aborting")
+            exit(4)
+    submit_argo(workflow, params)
+    print("Submitted argo workflow")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/argo/submitter_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: qpu-argo-submitter
+  namespace: aegis-operator
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: qpu-argo-submitter
+  template:
+    metadata:
+      labels:
+        app: qpu-argo-submitter
+    spec:
+      serviceAccountName: aegis-operator-sa
+      containers:
+        - name: submitter
+          image: ghcr.io/yourorg/aegis-quantum:latest
+          command: ["/bin/bash","-lc"]
+          args:
+            - |
+              mkdir -p /work
+              # the operator writes submit request to /work/submit_request.json (e.g., via kubectl cp or mounted secret)
+              while true; do
+                if [ -f /work/submit_request.json ]; then
+                  python3 /opt/quantum/ops/argo_submitter.py --request-file /work/submit_request.json || true
+                  mv /work/submit_request.json /work/last_submit_$(date +%s).json
+                fi
+                sleep 10
+              done
+          env:
+            - name: GITHUB_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: qpu-github-token
+                  key: token
+          volumeMounts:
+            - name: workdir
+              mountPath: /work
+      volumes:
+        - name: workdir
+          emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/mlflow_qa_on_pr.yml
+name: MLflow Quantum QA on PR
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  mlflow-qa:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: 3.10
+      - name: Install deps
+        run: pip install mlflow jsonschema boto3
+      - name: Run MLflow Quantum QA scan
+        env:
+          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
+        run: |
+          python3 tests/mlflow_quantum_qa.py --tracking-uri "${MLFLOW_TRACKING_URI}" --lookback-seconds 86400 || exit 1
+
*** End Patch
*** Begin Patch
*** Add File:ops/provider_connectors/rigetti_billing_connector.py
+#!/usr/bin/env python3
+"""
+Rigetti billing connector stub (replace with Rigetti enterprise API integration).
+"""
+import json, argparse
+
+def heuristic(shots, device, complexity=1.0):
+    base = 0.0012
+    est = base * shots * (1.0 + complexity*0.1)
+    return {"provider":"rigetti","estimated_usd":round(est,4),"shots":shots,"device":device}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--shots", type=int, default=1000)
+    p.add_argument("--device", default="rigetti_qpu")
+    p.add_argument("--complexity", type=float, default=1.0)
+    args = p.parse_args()
+    print(json.dumps(heuristic(args.shots, args.device, args.complexity)))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/operator_runbook_filled_template.md
+# Operator Runbook — Quantum Provider Finalization (Template)
+
+This is a template. Replace ALL_CAPS placeholders with your values before running.
+
+Environment variables (set before running steps)
+export CLUSTER_NAME="ALL_APPLY_CLUSTER"
+export REGION="us-east-1"
+export AWS_ACCOUNT_ID="123456789012"
+export OIDC_PROVIDER_ARN="arn:aws:iam::123456789012:oidc-provider/oidc.eks.REGION.amazonaws.com/id/EXAMPLE"
+export EVIDENCE_BUCKET="aegis-evidence-12345"
+export GHCR_ORG="ghcr.io/yourorg"
+export IMAGE_TAG="latest"
+export MLFLOW_TRACKING_URI="https://mlflow.example.com"
+export COSIGN_KMS_ARN="kms://arn:aws:kms:REGION:ACCOUNT:key/EXAMPLE"
+export REKOR_URL="https://rekor.example.com"
+
+1) Create IAM policies (review files in iam/ and replace REPLACE_* placeholders)
+   aws iam create-policy --policy-name aegis-braket --policy-document file://iam/braket_policy.json
+   aws iam create-policy --policy-name aegis-ibm   --policy-document file://iam/ibm_policy.json
+   aws iam create-policy --policy-name aegis-azure --policy-document file://iam/azure_quantum_policy.json
+   aws iam create-policy --policy-name aegis-rigetti --policy-document file://iam/rigetti_policy.json
+
+2) Create IRSA roles via Terraform (recommended)
+   cd terraform/irsa
+   cp example.tfvars my.tfvars
+   # edit my.tfvars to replace placeholders
+   terraform init
+   terraform apply -var-file=my.tfvars
+
+3) Alternatively create IRSA via eksctl (quick)
+   export BRAKET_POLICY="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/aegis-braket"
+   ./ops/eksctl_commands.sh ${CLUSTER_NAME} ${REGION} aegis
+
+4) Build & push quantum image (CI)
+   ./ops/run_build_and_verify.sh ${IMAGE_TAG}
+
+5) Patch Argo workflows with real image & S3 prefixes
+   # e.g. replace ghcr.io/yourorg/aegis-quantum:latest and s3://REPLACE_BUCKET/... in argo/*.yaml
+
+6) Test simulate flows in staging
+   argo submit -n aegis argo/quantum_braket_workflow.yaml -p run-type=simulate -p image=${GHCR_ORG}/aegis-quantum:${IMAGE_TAG} -p circuit-file-s3=s3://${EVIDENCE_BUCKET}/quantum/circuits/example.py
+
+7) Use argo submitter
+   # write request JSON to a pod-mounted file or use kubectl cp to upload:
+   cat > submit_request.json <<EOF
+   {
+     "provider":"braket",
+     "shots":1000,
+     "device":"arn:aws:braket:::device/quantum-simulator/amazon/sv1",
+     "circuit_complexity":1.0,
+     "budget_usd":10,
+     "pr": {"repo":"OWNER/REPO","number":123},
+     "workflow":"argo/quantum_braket_workflow.yaml",
+     "params": {"circuit-file-s3":"s3://${EVIDENCE_BUCKET}/quantum/circuits/example.py"}
+   }
+   EOF
+   kubectl -n aegis-operator cp submit_request.json deploy/qpu-argo-submitter:/work/submit_request.json
+   # submitter pod will pick up the request and enforce budget & approval before running argo submit
+
+8) Run sanitizer for any inputs
+   python3 ops/legal_safe_sanitizer.py --input ./inputs/params.json --mode redact --out ./inputs/params.sanitized.json
+
+9) Validate MLflow run schema for a run (replace RUN_ID)
+   python3 mlflow/validate_quantum_mlflow.py --run-id RUN_ID --tracking-uri ${MLFLOW_TRACKING_URI}
+
+10) Nightly sandbox & reporting
+   - Configure repository secrets SANDBOX_QPU_S3_PREFIX, SANDBOX_QPU_BUDGET_USD and SANDBOX_MLFLOW_URI
+   - Enable .github/workflows/nightly_qpu_sandbox.yml
+
+Notes
+- Always obtain legal signoff and keep evidence of approval in ${EVIDENCE_BUCKET}/legal_signoff...
+- Do not run production QPU jobs until budgets, monitoring, and audit chain are validated in staging.
+
*** End Patch
*** Begin Patch
*** Add File:ops/provider_connectors/README.md
+Provider connectors and pricing hooks
+
+Files:
+- aws_braket_pricing_connector.py — calls AWS Pricing API or returns heuristic
+- ibm_billing_connector.py — stub for IBM billing API integration
+- azure_billing_connector.py — stub for Azure Cost API integration
+- rigetti_billing_connector.py — stub
+
+How to use:
+- For AWS: provide AWS credentials with pricing access, then run:
+  python3 ops/provider_connectors/aws_braket_pricing_connector.py --shots 1000 --device arn:aws:braket:::device/quantum-simulator/amazon/sv1
+
+Production note:
+- Provider price APIs can be complex or restricted; these scripts are a starting point and should be extended to parse official price SKUs or your internal price catalog.
+
*** End Patch
*** End Patch
