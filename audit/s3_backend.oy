 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
"""
        extra_args = {
            "ACL": "private",
            "Metadata": {"sha256": sha256, "request_id": str(rid)},
        }
        if self.kms_key:
            extra_args["ServerSideEncryption"] = "aws:kms"
            extra_args["SSEKMSKeyId"] = self.kms_key

        # idempotency: if object exists with same size & metadata, skip upload
        try:
            head = self.s3.head_object(Bucket=self.bucket, Key=key)
            existing_md5 = head.get("ContentLength", 0)
            if existing_md5 == len(body):
                meta = head.get("Metadata", {}) or {}
                if meta.get("sha256") == sha256:
                    logger.debug("S3Backend: object already exists and matches sha: %s", key)
                    return
        except ClientError as e:
            err_code = e.response.get("Error", {}).get("Code", "")
            if err_code not in ("404", "NotFound", "NoSuchKey"):
                logger.debug("S3Backend: head_object error (continuing to put): %s", e)

        # Simple exponential backoff retry loop for put_object
        attempt = 0
        while True:
            try:
                self.s3.put_object(Bucket=self.bucket, Key=key, Body=body, **extra_args)
                logger.debug("S3Backend: uploaded %s (sha256=%s)", key, sha256[:8])
                return
            except ClientError as e:
                attempt += 1
                logger.warning("S3Backend put_object attempt %d failed: %s", attempt, e)
                if attempt >= DEFAULT_RETRIES:
                    logger.exception("S3Backend: exhausted retries uploading audit event")
                    raise
                sleep = BACKOFF_BASE * (2 ** (attempt - 1))
                time.sleep(sleep)
            except Exception:
                attempt += 1
                logger.exception("S3Backend put_object unexpected error on attempt %d", attempt)
                if attempt >= DEFAULT_RETRIES:
                    raise
                time.sleep(BACKOFF_BASE * (2 ** (attempt - 1)))

    def prune_older_than(self, seconds: int) -> None:
        """
        Prune objects older than cutoff by listing prefix and deleting in batches.
        Note: For large buckets prefer S3 Lifecycle rules instead of running this.
        """
        cutoff_ts = int(time.time() - seconds)
        paginator = self.s3.get_paginator("list_objects_v2")
        prefix = self.prefix.rstrip("/") + "/"
        to_delete = []
        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                key = obj["Key"]
                # try to parse ts from filename: {ts}-{id}.json
                try:
                    basename = key.rsplit("/", 1)[-1]
                    ts_part = basename.split("-", 1)[0]
                    obj_ts = int(ts_part)
                except Exception:
                    obj_ts = int(obj["LastModified"].timestamp())
                if obj_ts < cutoff_ts:
                    to_delete.append({"Key": key})
                if len(to_delete) >= 1000:
                    self.s3.delete_objects(Bucket=self.bucket, Delete={"Objects": to_delete})
                    to_delete = []
        if to_delete:
            self.s3.delete_objects(Bucket=self.bucket, Delete={"Objects": to_delete})
aegis_multimodal_ai_system/audit/s3_backend.py

