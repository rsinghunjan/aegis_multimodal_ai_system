*** Begin Patch
*** Add File:ops/autonomy/grafana_import_instructions.md
+Grafana Autonomy Dashboard - Import Instructions
+===============================================
+
+This document explains two ways to add the "Aegis Autonomy Overview" dashboard to your Grafana instance:
+1) Provisioning (recommended for long-term, automated installs)
+2) Manual import via the Grafana UI or API
+
+Files in this repo
+- ops/autonomy/grafana_autonomy_dashboard_full.json  -> dashboard JSON
+- k8s/grafana/autonomy_dashboard_provision.yaml      -> Grafana provisioning provider config (for file-based provisioning)
+
+1) Provisioning (file-based) - recommended for infra-as-code
+------------------------------------------------------------
+Overview:
+- Grafana supports provisioning dashboards from files on disk. The repo includes a provisioning config that points to /var/lib/grafana/dashboards/aegis.
+Steps:
+1. Copy the dashboard JSON into your Grafana provisioning folder (example layout for Grafana container):
+   - Place JSON at: /var/lib/grafana/dashboards/aegis/aegis-autonomy-overview.json
+2. Ensure provisioning provider is present (use k8s/grafana/autonomy_dashboard_provision.yaml or add provider to your Grafana deployment):
+   - The provider file included (k8s/grafana/autonomy_dashboard_provision.yaml) instructs Grafana to read dashboards from /var/lib/grafana/dashboards/aegis.
+3. Restart Grafana (or ensure Grafana picks up new provisioning):
+   - If running as a container: restart the Grafana pod/deployment.
+   - Grafana will load the dashboard on startup automatically.
+
+Notes:
+- When provisioning from files, updates to the JSON will be picked up on Grafana restart or by reloading provisioning.
+- You can mount the repo path into the Grafana container as a volume:
+  volumeMounts:
+    - name: grafana-dashboards
+      mountPath: /var/lib/grafana/dashboards/aegis
+  volume (k8s hostPath or ConfigMap) may be used depending on your environment.
+
+2) Manual import (UI) or API
+----------------------------
+Manual UI:
+1. Open Grafana in your browser.
+2. Go to Dashboards -> Manage -> Import.
+3. Upload ops/autonomy/grafana_autonomy_dashboard_full.json or paste the JSON.
+4. Choose your Prometheus data source and complete import.
+
+API:
+1. Use Grafana's HTTP API to create a dashboard (example):
+   curl -s -H "Content-Type: application/json" \
+     -H "Authorization: Bearer <GRAFANA_API_KEY>" \
+     -X POST http://<grafana-host>/api/dashboards/db \
+     -d @ops/autonomy/grafana_autonomy_dashboard_full.json
+
+2. If the JSON contains a top-level object with "dashboard" field, wrap per the API schema:
+   curl -H "Authorization: Bearer <KEY>" -X POST http://<grafana>/api/dashboards/db \
+     -d '{"dashboard": <CONTENT_OF_JSON.dashboard>, "overwrite": true}'
+
+Troubleshooting
+- If panels show "no data", confirm Prometheus has the exported metrics from the autonomy exporter:
+  - aegis_autonomy_percent, aegis_ns_high_autonomy, aegis_recent_attestations, aegis_pending_promotions, aegis_circuit_open
+- Check ServiceMonitor (k8s/autonomy/autonomy_exporter_service_monitor.yaml) and Prometheus scrape config.
+
+That's it — once imported you should see Autonomy % and related KPIs.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hil_attestation_auto_pr.yml
+name: HIL Attestation Auto-PR
+
+on:
+  pull_request:
+    types: [labeled, opened, synchronize, reopened]
+
+jobs:
+  hil-matrix-and-attest:
+    name: Run HIL Matrix & Create Attestation
+    runs-on: ubuntu-latest
+    # only run when the PR is labeled with "run-hil" to avoid excessive CI
+    if: contains(github.event.pull_request.labels.*.name, 'run-hil')
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install boto3 kubernetes
+
+      - name: Determine artifact types (changed files heuristic)
+        id: artifact
+        run: |
+          FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }} || true)
+          echo "files=${FILES}"
+          # Produce a comma separated list of artifact types to run; heuristics can be customized
+          TYPES=""
+          if echo "$FILES" | grep -q "ops/generative/rlhf"; then TYPES="${TYPES},rlhf"; fi
+          if echo "$FILES" | grep -q "ops/milvus"; then TYPES="${TYPES},milvus"; fi
+          if echo "$FILES" | grep -q "ops/agentic"; then TYPES="${TYPES},infra"; fi
+          if [ -z "$TYPES" ]; then TYPES="model"; fi
+          # trim leading comma
+          TYPES=$(echo $TYPES | sed 's/^,//')
+          echo "types=$TYPES" >> $GITHUB_OUTPUT
+
+      - name: Run HIL matrix runner for each artifact type
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY }}
+          COSIGN_PUB: ${{ secrets.COSIGN_PUB }}
+        run: |
+          IFS=',' read -ra ARR <<< "${{ steps.artifact.outputs.types }}"
+          for t in "${ARR[@]}"; do
+            echo "Running HIL for type: $t"
+            python ops/autonomy/hil_matrix_runner.py --artifact "$t" --out "artifacts/$t" || echo "HIL runner failed for $t"
+          done
+
+      - name: Comment PR with attestation info
+        uses: actions/github-script@v7
+        with:
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          script: |
+            const pr = context.payload.pull_request.number;
+            await github.rest.issues.createComment({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              issue_number: pr,
+              body: "HIL matrix run complete. Attestations (if produced) were uploaded to the evidence bucket and stored as Kubernetes Secrets (prefix: aegis-hil-attest-)."
+            })
+
*** End Patch
*** Begin Patch
*** Add File:ops/autonomy/argo_hil_attest_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-argo-hil-attest-
+spec:
+  entrypoint: hil-matrix-argo
+  templates:
+    - name: hil-matrix-argo
+      steps:
+        - - name: run-hil-matrix
+            template: hil-matrix
+        - - name: create-attest
+            template: create-attest
+
+    - name: hil-matrix
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 && python /opt/aegis/hil_matrix_runner.py --artifact "{{workflow.parameters.artifact_type}}" --out /artifacts || true
+        volumeMounts:
+          - name: artifacts
+            mountPath: /artifacts
+          - name: code
+            mountPath: /opt/aegis
+      outputs:
+        artifacts:
+          - name: hil-artifacts
+            path: /artifacts
+
+    - name: create-attest
+      inputs:
+        artifacts:
+          - name: hil-artifacts
+            path: /artifacts
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install kubernetes boto3 && python /opt/aegis/hil_attestor.py --artifacts /artifacts --name "argo-{{workflow.name}}-{{pod.name}}" --namespace "aegis-system" --upload-s3
+        volumeMounts:
+          - name: artifacts
+            mountPath: /artifacts
+          - name: code
+            mountPath: /opt/aegis
+
+  arguments:
+    parameters:
+      - name: artifact_type
+        value: "model"
+
+  volumes:
+    - name: code
+      hostPath:
+        path: ./ops/autonomy
+        type: Directory
+    - name: artifacts
+      emptyDir: {}
+
*** End Patch
*** Begin Patch
*** Add File:docs/checklists/canary_to_full_promotion_checklist.md
+# Canary → Full Automation Promotion Checklist
+
+Purpose
+-------
+A concise, actionable one‑page checklist for safely promoting a namespace from canary automation scope to full automation (wider autonomy levels).
+
+Before you begin
+- Ensure you have operator-level access and access to Grafana/Prometheus, Argo, and Kubernetes cluster with Gatekeeper.
+- Review legal/compliance requirements; obtain any required signoffs (use ops/compliance/signoff_manager.py or the signoff webhook).
+
+Checklist
+---------
+1) Confirm Canary Stability (duration: recommended 72 hours)
+   - Grafana metrics: Autonomy % steady, no increase in failure/error rates.
+   - Prometheus checks: key metrics within thresholds for canary namespaces for at least 3 consecutive windows.
+   - Command examples:
+     - kubectl -n kube-system get configmap aegis-cost-canary -o yaml
+     - kubectl -n argo get workflows -l aegis/canary-namespace=<ns>
+
+2) Verify HIL Attestations & Audit Evidence
+   - All auto-promote candidates must have a signed HIL attestation.
+   - Confirm attestations present and verified via Attestation Service:
+     - curl -sS http://aegis-attest.aegis-system.svc.cluster.local:8088/verify/<attest_id>
+   - Check audit directory for records:
+     - ls /var/lib/aegis/auto_promote_audit | tail
+
+3) Historical-Success & RLHF Benchmarks
+   - Confirm historical-success model score for candidate plan >= configured threshold (default 0.75).
+     - POST plan to historical success service: curl -X POST http://aegis-hist.aegis-system.svc.cluster.local:8090/score -d '{"plan":[...]}'
+   - Ensure RLHF benchmark suite passed and suspend window is acceptable:
+     - argo logs <rlhf-workflow> or check /tmp/rlhf_report.json from workflow run.
+
+4) Circuit Breaker & Invariants
+   - Ensure circuit is closed:
+     - kubectl -n kube-system get configmap aegis-autonomy-circuit -o yaml
+   - Run invariants tests:
+     - python ops/agentic/invariants_test_runner.py
+
+5) Cost Calibration & Canary Constraint
+   - Ensure kWh calibration applied to canary namespaces and observed costs look reasonable.
+     - kubectl -n kube-system get configmap aegis-carbon-calibration -o yaml
+   - Confirm Gatekeeper canary constraint applied and limited to canary namespaces:
+     - kubectl get constraint aegis-carbon-canary -o yaml
+
+6) Legal / Finance Signoff (if required)
+   - Obtain legal signoff for production promotions affecting billing/actuators.
+   - Confirm signoff recorded:
+     - python ops/compliance/signoff_manager.py check legal:policy-<id>:<artifact-id>
+
+7) Final Dry‑Run Promotion (Optional)
+   - Execute a dry-run promotion to validate no unintended side-effects:
+     - kubectl apply --dry-run=client -f <promotion-manifest>
+   - Validate logs and metrics for a short window (5-15 minutes).
+
+8) Promote Scope Expansion (automated step)
+   - Add namespace(s) to canary scope pending list:
+     - kubectl -n kube-system create configmap aegis-canary-pending --from-literal=namespaces="ns1,ns2" --dry-run=client -o yaml | kubectl apply -f -
+   - The canary scope manager will expand automatically after stable windows (ops/carbon/canary_scope_manager.py).
+
+9) Monitor & Rollback Plan (first 24-72 hours)
+   - Watch autonomy KPI dashboard and monitoring alerts.
+   - If circuit trips or errors spike, revert namespace autonomy level:
+     - python ops/autonomy/autonomy_manager.py set <namespace> none
+   - Run rollback playbook if promotion caused service issues (ops/agentic/rollback_playbook.py).
+
+Acceptance Criteria
+-------------------
+- No significant increase in error rates, latency, or cost anomalies for the extended canary window.
+- Attestations present and verified for promoted artifacts.
+- Historical-success score and RLHF benchmarks meet thresholds.
+- Legal/finance signoff recorded where required.
+
+Notes
+-----
+- Start with a small set of namespaces and expand gradually.
+- Keep detailed audit logs for compliance; all promotions should have an audit record.
+- Automate monitoring and alerts so the team can act quickly on anomalies.
+
*** End Patch
*** End Patch
