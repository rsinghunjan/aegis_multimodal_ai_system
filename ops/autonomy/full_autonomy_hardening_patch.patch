*** Begin Patch
*** Add File:ops/autonomy/hil_matrix_expanded.json
+{
+  "model": {
+    "container": "ghcr.io/yourorg/hil-model-harness:latest",
+    "cmd": "/opt/hil/run_model_tests.sh --out /artifacts"
+  },
+  "rlhf": {
+    "container": "ghcr.io/yourorg/hil-rlhf-harness:latest",
+    "cmd": "/opt/hil/run_rlhf_tests.sh --out /artifacts"
+  },
+  "infra": {
+    "container": "ghcr.io/yourorg/hil-infra-harness:latest",
+    "cmd": "/opt/hil/run_infra_tests.sh --out /artifacts"
+  },
+  "milvus": {
+    "container": "ghcr.io/yourorg/hil-milvus-harness:latest",
+    "cmd": "/opt/hil/run_milvus_tests.sh --out /artifacts"
+  },
+  "gpu_mig": {
+    "container": "ghcr.io/yourorg/hil-gpu-mig-harness:latest",
+    "cmd": "/opt/hil/run_mig_tests.sh --out /artifacts"
+  },
+  "network": {
+    "container": "ghcr.io/yourorg/hil-network-harness:latest",
+    "cmd": "/opt/hil/run_network_tests.sh --out /artifacts"
+  },
+  "storage": {
+    "container": "ghcr.io/yourorg/hil-storage-harness:latest",
+    "cmd": "/opt/hil/run_storage_tests.sh --out /artifacts"
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/autonomy/hil_test_definitions.py
+#!/usr/bin/env python3
+"""
+Helpers to generate standardized HIL test manifests for different artifact classes.
+Used by hil_matrix_runner or CI to ensure consistent test coverage for models, RLHF, infra, Milvus, MIG, network, storage.
+"""
+import json
+import os
+from datetime import datetime
+
+ROOT = os.path.dirname(os.path.dirname(__file__))
+
+def sample_test_summary(passed=True, tests_executed=10, tests_failed=0, coverage=0.75, extra=None):
+    return {
+        "status": "ok" if passed and tests_failed == 0 else "fail",
+        "tests_executed": tests_executed,
+        "tests_failed": tests_failed,
+        "coverage": coverage,
+        "ts": datetime.utcnow().isoformat() + "Z",
+        "extra": extra or {}
+    }
+
+def write_summary(outdir, name="test_summary.json", **kwargs):
+    os.makedirs(outdir, exist_ok=True)
+    path = os.path.join(outdir, name)
+    with open(path, "w") as fh:
+        json.dump(sample_test_summary(**kwargs), fh, indent=2)
+    return path
+
+if __name__ == "__main__":
+    import sys
+    out = sys.argv[1] if len(sys.argv) > 1 else "/tmp/artifacts"
+    print(write_summary(out))
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing/cosign_kms_manager.py
+#!/usr/bin/env python3
+"""
+Cosign + KMS manager (AWS KMS example)
+
+Provides helper operations:
+ - create_kms_key(alias) -> returns key_arn
+ - generate_cosign_kms_keypair(alias) -> prints cosign KMS URI (operator runs cosign with the KMS URI)
+ - store_public_key_in_k8s(secret_name) -> stores cosign public key (for verification) in Kubernetes secret
+
+Notes:
+ - This script intentionally does not export private keys. It creates or returns a KMS key ARN which cosign can use via KMS integration.
+ - Operators must grant the executing principal (CI/cluster) kms:Sign kms:GetPublicKey on the KMS key.
+"""
+import os
+import argparse
+import boto3
+import time
+from kubernetes import client, config
+
+KMS_ALIAS_PREFIX = "alias/aegis-cosign-"
+
+def create_kms_key(alias, desc="Aegis cosign key"):
+    kms = boto3.client("kms")
+    resp = kms.create_key(Description=desc, KeyUsage="SIGN_VERIFY", CustomerMasterKeySpec="RSA_2048")
+    key_id = resp["KeyMetadata"]["KeyId"]
+    alias_name = KMS_ALIAS_PREFIX + alias
+    try:
+        kms.create_alias(AliasName=alias_name, TargetKeyId=key_id)
+    except Exception:
+        pass
+    key_arn = resp["KeyMetadata"]["Arn"]
+    return key_arn
+
+def cosign_kms_uri_for_keyalias(alias):
+    # cosign KMS URI format for AWS: "awskms://<key-id-or-alias>"
+    return f"awskms:///{KMS_ALIAS_PREFIX}{alias}"
+
+def store_public_key_in_k8s(secret_name, public_key_pem, ns="aegis-system"):
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    body = client.V1Secret(metadata=client.V1ObjectMeta(name=secret_name, namespace=ns), string_data={"cosign.pub": public_key_pem})
+    try:
+        core.create_namespaced_secret(ns, body)
+    except client.exceptions.ApiException:
+        core.replace_namespaced_secret(secret_name, ns, body)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--alias", required=True)
+    p.add_argument("--create", action="store_true")
+    p.add_argument("--store-secret", help="secret name to store public key in k8s")
+    args = p.parse_args()
+    if args.create:
+        key_arn = create_kms_key(args.alias)
+        print("created KMS key:", key_arn)
+    uri = cosign_kms_uri_for_keyalias(args.alias)
+    print("cosign kms uri:", uri)
+    if args.store_secret:
+        # Operator must have run: cosign public-key --kms <URI> public.pem
+        print("fetch public key using cosign public-key --kms <URI> public.pem and then run store_public_key_in_k8s")
+    print("done")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing/attestation_encrypt_upload.py
+#!/usr/bin/env python3
+"""
+Encrypt attestation with KMS and upload to S3.
+
+ - Reads attestation JSON file
+ - Uses AWS KMS to encrypt the blob (Encrypt API) with a provided KMS key id
+ - Uploads encrypted blob to S3 under secure prefix
+ - Optionally stores S3 URI and metadata in Kubernetes Secret for controllers to fetch
+"""
+import os
+import json
+import argparse
+import boto3
+from kubernetes import client, config
+
+def encrypt_blob(kms_key_id, data_bytes):
+    kms = boto3.client("kms")
+    resp = kms.encrypt(KeyId=kms_key_id, Plaintext=data_bytes)
+    return resp["CiphertextBlob"]
+
+def upload_s3(bucket, key, data_bytes):
+    s3 = boto3.client("s3")
+    s3.put_object(Bucket=bucket, Key=key, Body=data_bytes, ServerSideEncryption="aws:kms")
+    return f"s3://{bucket}/{key}"
+
+def store_metadata_k8s(secret_name, data, ns="aegis-system"):
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    body = client.V1Secret(metadata=client.V1ObjectMeta(name=secret_name, namespace=ns), string_data=data)
+    try:
+        core.create_namespaced_secret(ns, body)
+    except client.exceptions.ApiException:
+        core.replace_namespaced_secret(secret_name, ns, body)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--file", required=True)
+    p.add_argument("--kms-key-id", required=True)
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--s3-prefix", default="hil-attestations")
+    p.add_argument("--secret-name", help="optional k8s secret to store metadata")
+    args = p.parse_args()
+    with open(args.file, "rb") as fh:
+        data = fh.read()
+    enc = encrypt_blob(args.kms_key_id, data)
+    # upload encrypted blob
+    key = f"{args.s3_prefix}/{os.path.basename(args.file)}.enc"
+    uri = upload_s3(args.s3_bucket, key, enc)
+    print("uploaded:", uri)
+    if args.secret_name:
+        store_metadata_k8s(args.secret_name, {"attestation_s3": uri})
+        print("stored metadata secret:", args.secret_name)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/compliance/policy_registry.py
+#!/usr/bin/env python3
+"""
+Policy registry & machine-verifiability mapping.
+
+ - Register policies and indicate whether they can be validated automatically (machine_verifiable)
+ - Provide an HTTP endpoint to check if a policy + attestation satisfy machine rules
+ - Used by auto-promote controller to decide whether legal/manual signoff is required
+"""
+from flask import Flask, request, jsonify
+import os, json
+from kubernetes import client, config
+
+REGISTRY_CM = ("aegis-policy-registry", "kube-system")
+app = Flask(__name__)
+
+def load_registry():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    try:
+        cm = core.read_namespaced_config_map(REGISTRY_CM[0], REGISTRY_CM[1])
+        return json.loads(cm.data.get("registry","{}"))
+    except Exception:
+        return {}
+
+def save_registry(reg):
+    core = client.CoreV1Api()
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=REGISTRY_CM[0]), data={"registry": json.dumps(reg)})
+    try:
+        core.replace_namespaced_config_map(REGISTRY_CM[0], REGISTRY_CM[1], body)
+    except Exception:
+        core.create_namespaced_config_map(REGISTRY_CM[1], body)
+
+@app.route("/v1/policy/register", methods=["POST"])
+def register():
+    body = request.json or {}
+    pid = body.get("policy_id")
+    machine = body.get("machine_verifiable", False)
+    if not pid:
+        return jsonify({"ok": False, "error": "policy_id required"}), 400
+    reg = load_registry()
+    reg[pid] = {"machine_verifiable": bool(machine), "doc_ref": body.get("doc_ref")}
+    save_registry(reg)
+    return jsonify({"ok": True})
+
+@app.route("/v1/policy/check/<policy_id>", methods=["POST"])
+def check(policy_id):
+    body = request.json or {}
+    attestation = body.get("attestation")
+    reg = load_registry()
+    pol = reg.get(policy_id)
+    if not pol:
+        return jsonify({"ok": False, "error": "unknown policy"}), 404
+    if pol.get("machine_verifiable"):
+        # Basic checks: attestation must reference policy doc_ref and be signed
+        if not attestation:
+            return jsonify({"ok": False, "error": "attestation required"}), 400
+        att_doc = attestation.get("doc_ref")
+        if att_doc != pol.get("doc_ref"):
+            return jsonify({"ok": False, "error": "attestation does not reference policy doc"}), 403
+        # signature presence is required; actual signature verification done elsewhere (attestation service)
+        if not attestation.get("signature"):
+            return jsonify({"ok": False, "error": "attestation unsigned"}), 403
+        return jsonify({"ok": True, "machine_verifiable": True})
+    else:
+        return jsonify({"ok": False, "machine_verifiable": False})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8097")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/chaos/extended_chaos_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-chaos-extended-
+spec:
+  entrypoint: chaos-extended
+  templates:
+    - name: chaos-extended
+      steps:
+        - - name: partition-network
+            template: net-partition
+        - - name: spike-cpu
+            template: cpu-spike
+        - - name: fill-disk
+            template: disk-fill
+        - - name: kill-gpu-pod
+            template: kill-gpu-pod
+        - - name: run-dcgm-attrib
+            template: dcgm-attrib
+
+    - name: net-partition
+      container:
+        image: bitnami/kubectl:latest
+        command: [sh, -c]
+        args:
+          - |
+            # This uses kubectl to cordon nodes or apply network disruptions via kubectl annotate for CNI that supports it.
+            echo "Simulating network partition by cordoning one node"
+            NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
+            kubectl cordon $NODE || true
+            sleep 30
+            kubectl uncordon $NODE || true
+
+    - name: cpu-spike
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - |
+            # run a short CPU hog in a job
+            cat <<'EOF' > /tmp/cpu_hog.sh
+            #!/bin/sh
+            end=$((SECONDS+30))
+            while [ $SECONDS -lt $end ]; do :; done
+            EOF
+            chmod +x /tmp/cpu_hog.sh
+            /tmp/cpu_hog.sh || true
+
+    - name: disk-fill
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - |
+            # create a temp file to simulate disk pressure, clean up after a timeout
+            dd if=/dev/zero of=/tmp/fill.tmp bs=1M count=50 || true
+            sleep 10
+            rm -f /tmp/fill.tmp || true
+
+    - name: kill-gpu-pod
+      container:
+        image: bitnami/kubectl:latest
+        command: [sh, -c]
+        args:
+          - |
+            POD=$(kubectl -n default get pods -l gpu=true -o jsonpath='{.items[0].metadata.name}' || true)
+            if [ -n "$POD" ]; then kubectl -n default delete pod $POD --grace-period=0 --force || true; fi
+
+    - name: dcgm-attrib
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install requests && python /opt/aegis/carbon/run_attribution_experiment.sh || true
+      volumeMounts:
+        - name: code
+          mountPath: /opt/aegis
+
+  volumes:
+    - name: code
+      hostPath:
+        path: ./ops
+        type: Directory
+
*** End Patch
*** Begin Patch
*** Add File:ops/generative/drift_detector_improved.py
+#!/usr/bin/env python3
+"""
+Improved drift detector with adaptive baseline and Alertmanager integration.
+
+ - Maintains rolling baseline and moving-average drift score
+ - Emits Prometheus metrics via pushgateway or alerts via Alertmanager webhook
+ - Triggers RLHF retrain workflow when sustained drift observed
+"""
+import os
+import time
+import json
+import numpy as np
+import requests
+from datetime import datetime
+from kubernetes import client, config
+
+BASELINE_FILE = os.environ.get("DRIFT_BASELINE_FILE", "/var/lib/aegis/drift_baseline.npy")
+SCORES_FILE = os.environ.get("DRIFT_SCORES_FILE", "/var/lib/aegis/drift_scores.json")
+WINDOW = int(os.environ.get("DRIFT_WINDOW", "10"))
+THRESH = float(os.environ.get("DRIFT_THRESHOLD", "0.18"))
+ALERTMGR_URL = os.environ.get("ALERTMGR_URL", "")
+ARGO_NS = os.environ.get("ARGO_NS", "argo")
+RLHF_WORKFLOW = os.environ.get("RLHF_WORKFLOW", "ops/generative/rlhf/rlhf_distributed_workflow.yaml")
+
+def push_alert(summary, details):
+    if not ALERTMGR_URL:
+        print("alert:", summary, details)
+        return
+    payload = [{"labels": {"alertname": "AegisDrift"}, "annotations": {"summary": summary, "details": json.dumps(details)}}]
+    try:
+        requests.post(ALERTMGR_URL + "/api/v1/alerts", json=payload, timeout=5)
+    except Exception as e:
+        print("alert failed:", e)
+
+def sample_embeddings():
+    # reuse earlier sample_embeddings logic if available; fallback to reading file
+    path = "/var/lib/aegis/featurestore/latest_embs.npy"
+    if os.path.exists(path):
+        return np.load(path)
+    return np.array([])
+
+def cosine(a,b):
+    if a is None or b is None: return 1.0
+    na = np.linalg.norm(a); nb = np.linalg.norm(b)
+    if na==0 or nb==0: return 1.0
+    return 1.0 - float(np.dot(a,b)/(na*nb))
+
+def main():
+    scores = []
+    baseline = None
+    if os.path.exists(BASELINE_FILE):
+        try:
+            baseline = np.load(BASELINE_FILE)
+        except Exception:
+            baseline = None
+    while True:
+        embs = sample_embeddings()
+        if embs.size == 0:
+            time.sleep(60); continue
+        cur_mean = np.mean(embs, axis=0)
+        if baseline is None:
+            baseline = cur_mean
+            np.save(BASELINE_FILE, baseline)
+            scores.append(0.0)
+        else:
+            score = cosine(baseline, cur_mean)
+            scores.append(score)
+            if len(scores) > WINDOW: scores.pop(0)
+            avg = sum(scores)/len(scores)
+            with open(SCORES_FILE,"w") as fh:
+                json.dump({"scores": scores, "avg": avg, "ts": datetime.utcnow().isoformat()+"Z"}, fh)
+            print("drift avg", avg)
+            if avg > THRESH:
+                push_alert("Embedding drift detected", {"avg": avg})
+                # trigger RLHF retrain (best-effort)
+                try:
+                    os.system(f"argo submit -n {ARGO_NS} {RLHF_WORKFLOW} || true")
+                except Exception:
+                    pass
+        time.sleep(int(os.environ.get("DRIFT_POLL_S", "300")))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/generative/rlhf/rlhf_regression_detector.py
+#!/usr/bin/env python3
+"""
+RLHF regression detector
+
+ - Compares latest benchmark result JSON to historical baseline stored in S3 or local PVC
+ - Produces a delta and if regression beyond tolerance is detected, increases RLHF suspend window and records audit
+ - Optionally triggers rollback actions or notifies operators via Alertmanager
+"""
+import os
+import json
+import boto3
+import subprocess
+from datetime import datetime
+
+BASELINE_S3 = os.environ.get("RLHF_BASELINE_S3", "")
+BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+THRESHOLD_DELTA = float(os.environ.get("RLHF_REGRESSION_DELTA", "0.05"))
+SUSPEND_CONFIG_CM = ("rlhf-suspend-config", "kube-system")
+ALERTMGR = os.environ.get("ALERTMGR_URL","")
+
+def load_local(path):
+    if os.path.exists(path):
+        with open(path) as fh:
+            return json.load(fh)
+    return None
+
+def fetch_baseline_from_s3(key):
+    if not BUCKET:
+        return None
+    s3 = boto3.client("s3")
+    try:
+        tmp = "/tmp/rlhf_baseline.json"
+        s3.download_file(BUCKET, key, tmp)
+        with open(tmp) as fh:
+            return json.load(fh)
+    except Exception:
+        return None
+
+def alert(msg, data):
+    if ALERTMGR:
+        import requests
+        payload = [{"labels":{"alertname":"RLHFRegression"}, "annotations": {"summary": msg, "details": json.dumps(data)}}]
+        try:
+            requests.post(ALERTMGR + "/api/v1/alerts", json=payload, timeout=5)
+        except Exception as e:
+            print("alert failed", e)
+    else:
+        print("ALERT:", msg, data)
+
+def increase_suspend_window():
+    # patch ConfigMap to increase suspend window (best-effort)
+    try:
+        from kubernetes import client, config
+        config.load_incluster_config()
+        core = client.CoreV1Api()
+        cm = core.read_namespaced_config_map(SUSPEND_CONFIG_CM[0], SUSPEND_CONFIG_CM[1])
+        cur = int(cm.data.get("suspend_window_s","3600"))
+        new = min(cur * 2, 86400)
+        cm.data["suspend_window_s"] = str(new)
+        core.replace_namespaced_config_map(SUSPEND_CONFIG_CM[0], SUSPEND_CONFIG_CM[1], cm)
+        return new
+    except Exception as e:
+        print("increase_suspend_window failed", e)
+        return None
+
+def main():
+    latest = load_local("/tmp/rlhf_report.json")
+    if not latest:
+        print("no latest report")
+        return
+    baseline = None
+    if BASELINE_S3 and BUCKET:
+        baseline = fetch_baseline_from_s3(BASELINE_S3)
+    if not baseline:
+        baseline = load_local("/var/lib/aegis/rlhf_baseline.json")
+    if not baseline:
+        print("no baseline; storing current as baseline")
+        # persist locally and optionally to S3
+        with open("/var/lib/aegis/rlhf_baseline.json","w") as fh:
+            json.dump(latest, fh)
+        if BUCKET and BASELINE_S3:
+            s3 = boto3.client("s3")
+            s3.upload_file("/var/lib/aegis/rlhf_baseline.json", BUCKET, BASELINE_S3)
+        return
+    # compute regression metric (example: hallucination rate)
+    b_rate = baseline.get("hallucination",{}).get("rate", 1.0)
+    l_rate = latest.get("hallucination",{}).get("rate", 1.0)
+    delta = l_rate - b_rate
+    if delta > THRESHOLD_DELTA:
+        alert("RLHF regression detected", {"baseline": b_rate, "latest": l_rate, "delta": delta})
+        new_window = increase_suspend_window()
+        print("increased suspend window to", new_window)
+    else:
+        print("no regression", delta)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/audit/audit_retention_manager.py
+#!/usr/bin/env python3
+"""
+Audit retention & encryption manager
+
+ - Ensures S3 lifecycle rules exist (retention, transition)
+ - Ensures S3 bucket encryption with KMS
+ - Verifies signed attestation secrets presence and RBAC on secrets namespace
+ - Provides a scan/report to surface issues
+"""
+import os
+import json
+import boto3
+from kubernetes import client, config
+
+BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+KMS_KEY = os.environ.get("AUDIT_KMS_KEY", "")
+RETENTION_DAYS = int(os.environ.get("AUDIT_RETENTION_DAYS","365"))
+NS = "aegis-system"
+ATT_PREFIX = "aegis-hil-attest"
+
+def ensure_bucket_encryption(bucket, kms_key):
+    s3 = boto3.client("s3")
+    try:
+        s3.put_bucket_encryption(
+            Bucket=bucket,
+            ServerSideEncryptionConfiguration={
+                "Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "aws:kms", "KMSMasterKeyID": kms_key}}]
+            },
+        )
+        return True
+    except Exception as e:
+        print("encryption fail", e)
+        return False
+
+def ensure_lifecycle(bucket, days):
+    s3 = boto3.client("s3")
+    rule = {
+        'Rules': [{
+            'ID': 'aegis-attest-retention',
+            'Status': 'Enabled',
+            'Filter': {'Prefix': 'hil-attestations/'},
+            'Transitions': [],
+            'Expiration': {'Days': days},
+            'NoncurrentVersionExpiration': {'NoncurrentDays': days}
+        }]
+    }
+    try:
+        s3.put_bucket_lifecycle_configuration(Bucket=bucket, LifecycleConfiguration=rule)
+        return True
+    except Exception as e:
+        print("lifecycle fail", e)
+        return False
+
+def check_k8s_secrets_rbac():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    secrets = core.list_namespaced_secret(NS)
+    att_secrets = [s.metadata.name for s in secrets.items if s.metadata.name.startswith(ATT_PREFIX)]
+    # RBAC scanning requires a separate API (clusterrolebindings); here we just list secrets
+    return att_secrets
+
+def main():
+    report = {}
+    if BUCKET and KMS_KEY:
+        report["encryption_ok"] = ensure_bucket_encryption(BUCKET, KMS_KEY)
+    if BUCKET:
+        report["lifecycle_ok"] = ensure_lifecycle(BUCKET, RETENTION_DAYS)
+    report["attestation_secrets"] = check_k8s_secrets_rbac()
+    print(json.dumps(report, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/rbac/attestor-serviceaccount.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-attestor
+  namespace: aegis-system
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-attestor-role
+rules:
+  - apiGroups: [""]
+    resources: ["secrets", "configmaps"]
+    verbs: ["get", "list", "create", "update", "replace"]
+  - apiGroups: ["argoproj.io"]
+    resources: ["workflows"]
+    verbs: ["get", "list"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: aegis-attestor-binding
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: aegis-attestor-role
+subjects:
+  - kind: ServiceAccount
+    name: aegis-attestor
+    namespace: aegis-system
+
*** End Patch
*** End Patch
