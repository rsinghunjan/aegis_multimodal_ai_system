*** Begin Patch
*** Add File:ops/telemetry/collector.py
+#!/usr/bin/env python3
+"""
+Lightweight telemetry collector for Aegis.
+
+- Records per-request telemetry: model, model_version, prompt hash, token counts (if provided),
+  latency, memory_estimate, cost_estimate, accuracy signals and optional user feedback.
+- Exposes Prometheus metrics and writes JSONL events to local disk or S3 (if configured).
+
+Usage:
+  from ops.telemetry.collector import TelemetryCollector
+  tc = TelemetryCollector()
+  tc.record_request(model="opt-1.3b", model_version="v1", request=req, response=resp, metrics=...)
+  # Start metrics server: python -m ops.telemetry.collector (runs Prometheus on port)
+"""
+import os
+import json
+import time
+import hashlib
+from typing import Optional, Dict, Any
+from prometheus_client import start_http_server, Histogram, Counter, Gauge
+
+try:
+    import boto3
+except Exception:
+    boto3 = None
+
+TELEMETRY_DIR = os.environ.get("TELEMETRY_DIR", "/var/lib/aegis/telemetry")
+TELEMETRY_S3_BUCKET = os.environ.get("TELEMETRY_S3_BUCKET", "")
+METRICS_PORT = int(os.environ.get("TELEMETRICS_PORT", "9102"))
+
+LATENCY = Histogram("aegis_inference_latency_seconds", "Inference latency", ["model","version"])
+REQS = Counter("aegis_inference_requests_total", "Total inference requests", ["model","version","status"])
+TOKENS = Gauge("aegis_inference_tokens", "Tokens processed per request", ["model","version"])
+COST = Gauge("aegis_inference_cost_usd", "Estimated cost per request (USD)", ["model","version"])
+ACCURACY = Gauge("aegis_inference_accuracy", "Optional accuracy signal (0..1)", ["model","version"])
+
+class TelemetryCollector:
+    def __init__(self, flush_to_s3: bool = False):
+        os.makedirs(TELEMETRY_DIR, exist_ok=True)
+        self.flush_to_s3 = flush_to_s3 and bool(TELEMETRY_S3_BUCKET and boto3)
+        if self.flush_to_s3:
+            self.s3 = boto3.client("s3")
+
+    def _write_event(self, event: Dict[str, Any]):
+        fname = f"{int(time.time())}-{hashlib.sha1(json.dumps(event).encode()).hexdigest()[:8]}.jsonl"
+        path = os.path.join(TELEMETRY_DIR, fname)
+        with open(path, "a") as fh:
+            fh.write(json.dumps(event) + "\n")
+        if self.flush_to_s3:
+            key = f"telemetry/{fname}"
+            try:
+                self.s3.upload_file(path, TELEMETRY_S3_BUCKET, key)
+            except Exception:
+                # best-effort
+                pass
+
+    def record_request(self,
+                       model: str,
+                       model_version: str,
+                       request: Dict[str, Any],
+                       response: Dict[str, Any],
+                       latency_s: float,
+                       tokens_in: Optional[int] = None,
+                       tokens_out: Optional[int] = None,
+                       memory_estimate_mb: Optional[float] = None,
+                       cost_estimate_usd: Optional[float] = None,
+                       accuracy_signal: Optional[float] = None,
+                       user_feedback: Optional[Dict[str, Any]] = None):
+        status = "ok" if response.get("error") is None else "error"
+        LATENCY.labels(model=model, version=model_version).observe(latency_s)
+        REQS.labels(model=model, version=model_version, status=status).inc()
+        if tokens_in is not None:
+            TOKENS.labels(model=model, version=model_version).set(tokens_in + (tokens_out or 0))
+        if cost_estimate_usd is not None:
+            COST.labels(model=model, version=model_version).set(cost_estimate_usd)
+        if accuracy_signal is not None:
+            ACCURACY.labels(model=model, version=model_version).set(accuracy_signal)
+
+        event = {
+            "ts": int(time.time()),
+            "model": model,
+            "model_version": model_version,
+            "request": {"prompt_hash": hashlib.sha256(json.dumps(request).encode()).hexdigest()},
+            "response": {"status": status, "summary": (response.get("choices") or response.get("text", "") )[:200]},
+            "latency_s": latency_s,
+            "tokens_in": tokens_in,
+            "tokens_out": tokens_out,
+            "memory_mb": memory_estimate_mb,
+            "cost_usd": cost_estimate_usd,
+            "accuracy": accuracy_signal,
+            "feedback": user_feedback
+        }
+        self._write_event(event)
+
+if __name__ == "__main__":
+    start_http_server(METRICS_PORT)
+    print(f"Telemetry metrics available on :{METRICS_PORT}")
+    # keep running for metrics scrapers
+    import time
+    while True:
+        time.sleep(60)
+
*** End Patch
*** Begin Patch
*** Add File:ops/featurestore/store.py
+#!/usr/bin/env python3
+"""
+Simple feature/label store backed by JSONL files (local or S3).
+
+This is intentionally lightweight: each model has a store directory with timestamped JSONL files.
+Operators can mount a shared filesystem or use S3 for long-term retention.
+"""
+import os
+import json
+import time
+from typing import Dict, Any, List
+
+try:
+    import boto3
+except Exception:
+    boto3 = None
+
+STORE_ROOT = os.environ.get("FEATURESTORE_ROOT", "/var/lib/aegis/featurestore")
+S3_BUCKET = os.environ.get("FEATURESTORE_S3_BUCKET", "")
+
+def ensure_dir(path: str):
+    os.makedirs(path, exist_ok=True)
+
+def append_sample(model: str, sample: Dict[str, Any]):
+    ts = int(time.time())
+    model_dir = os.path.join(STORE_ROOT, model)
+    ensure_dir(model_dir)
+    fname = os.path.join(model_dir, f"{ts}.jsonl")
+    with open(fname, "a") as fh:
+        fh.write(json.dumps(sample) + "\n")
+    if S3_BUCKET and boto3:
+        s3 = boto3.client("s3")
+        s3.upload_file(fname, S3_BUCKET, f"featurestore/{model}/{os.path.basename(fname)}")
+
+def list_samples(model: str, limit: int = 100) -> List[Dict[str, Any]]:
+    model_dir = os.path.join(STORE_ROOT, model)
+    if not os.path.exists(model_dir):
+        return []
+    files = sorted(os.listdir(model_dir), reverse=True)[:limit]
+    samples = []
+    for f in files:
+        path = os.path.join(model_dir, f)
+        try:
+            with open(path) as fh:
+                for line in fh:
+                    samples.append(json.loads(line))
+        except Exception:
+            continue
+    return samples
+
+def sample_count(model: str) -> int:
+    model_dir = os.path.join(STORE_ROOT, model)
+    if not os.path.exists(model_dir):
+        return 0
+    return sum(1 for _ in open(os.path.join(model_dir, os.listdir(model_dir)[0])) ) if os.listdir(model_dir) else 0
+
*** End Patch
*** Begin Patch
*** Add File:ops/aiops/controller.py
+#!/usr/bin/env python3
+"""
+AIOps controller: polls Prometheus for alerts or metrics and invokes the planner to produce remediation plans.
+It then validates plans and either executes them (safe mode) or creates an approval request.
+
+This is a lightweight orchestrator that ties monitoring -> planner -> executor.
+"""
+import os
+import requests
+import time
+import json
+from typing import Dict, Any
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus:9090")
+POLL_S = int(os.environ.get("AIOPS_POLL_S", "30"))
+AUTOMATION_CONFIDENCE_THRESHOLD = float(os.environ.get("AIOPS_AUTO_THRESHOLD", "0.8"))
+
+# These imports touch local scaffolds (planner & executor)
+try:
+    from ops.agents.planner_validator import validate_plan, sanitize_plan
+    from ops.agents.executor_with_limits import execute_plan
+except Exception:
+    # fallbacks if modules missing in dev env
+    validate_plan = lambda p: True
+    sanitize_plan = lambda p: p
+    def execute_plan(p, prefix="agents/"): return {"steps": []}
+
+def query_prometheus(expr: str) -> Dict[str, Any]:
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def triage_and_remediate():
+    print("[aiops] polling prometheus for anomalies")
+    # example: high inference latency per model
+    resp = query_prometheus('histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le, model))')
+    # simplistic: if any model has quantile > 2s create a plan
+    results = resp.get("data", {}).get("result", [])
+    for r in results:
+        model = r.get("metric", {}).get("model","unknown")
+        val = float(r.get("value",[0,0])[1])
+        if val > 2.0:
+            print(f"[aiops] detected high tail latency for {model}: {val}s")
+            # simple planner: try scaling or swapping to quantized fallback
+            plan = [
+                {"id":"scale-1","tool":"echo","params":{"message":f"scale-up {model} replicas"}},
+                {"id":"swap-1","tool":"echo","params":{"message":f"swap-to-quantized {model}"}}
+            ]
+            try:
+                validate_plan(plan)
+                plan = sanitize_plan(plan)
+                # naive confidence heuristic
+                confidence = 0.9
+                if confidence >= AUTOMATION_CONFIDENCE_THRESHOLD:
+                    print("[aiops] executing plan automatically")
+                    trace = execute_plan(plan, provenance_prefix=f"aiops/{model}/")
+                    print("execution trace:", trace)
+                else:
+                    print("[aiops] creating approval request for plan")
+                    # integrate with approval_service (ops/automation)
+                    os.system(f"python3 ops/automation/approval_service.py create --id aiops-{model}-{int(time.time())} --who aiops-bot --note 'Remediation plan requires approval'")
+            except Exception as e:
+                print("plan validation failed:", e)
+
+if __name__ == "__main__":
+    while True:
+        try:
+            triage_and_remediate()
+        except Exception as e:
+            print("aiops controller error:", e)
+        time.sleep(POLL_S)
+
*** End Patch
*** Begin Patch
*** Add File:ops/cl/collect_samples.py
+#!/usr/bin/env python3
+"""
+Collect candidate samples from telemetry/featurestore for retraining.
+Writes a sampling file used by training jobs.
+"""
+import argparse
+import json
+from ops.featurestore.store import list_samples, append_sample
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--model", required=True)
+    p.add_argument("--out", default="/tmp/retrain_samples.jsonl")
+    p.add_argument("--limit", type=int, default=1000)
+    args = p.parse_args()
+    samples = list_samples(args.model, limit=args.limit)
+    with open(args.out, "w") as fh:
+        for s in samples[:args.limit]:
+            fh.write(json.dumps(s) + "\n")
+    print(f"Wrote {len(samples[:args.limit])} samples to {args.out}")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/cl/train_candidate.py
+#!/usr/bin/env python3
+"""
+Placeholder training script for candidate model.
+In production this will kick off an Argo/TF job and save artifacts to model registry.
+"""
+import argparse
+import time
+import json
+import os
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--samples", required=True)
+    p.add_argument("--out-model-dir", required=True)
+    args = p.parse_args()
+    print("Starting training on samples:", args.samples)
+    time.sleep(2)
+    os.makedirs(args.out_model_dir, exist_ok=True)
+    # write a placeholder metadata
+    meta = {"name":"candidate","version":"auto-"+str(int(time.time())),"precision":"fp16"}
+    with open(os.path.join(args.out_model_dir,"metadata.json"), "w") as fh:
+        json.dump(meta, fh)
+    print("Training completed, model written to", args.out_model_dir)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/cl/evaluate_candidate.py
+#!/usr/bin/env python3
+"""
+Evaluate candidate model against baseline using the A/B harness (lightweight).
+Outputs a JSON report and an integer success code (0 pass, 3 fail).
+"""
+import argparse
+import json
+import statistics
+import math
+
+def ttest_independent(a, b):
+    # Welch's t-test (approximate)
+    ma, mb = statistics.mean(a), statistics.mean(b)
+    sa, sb = statistics.pvariance(a), statistics.pvariance(b)
+    na, nb = len(a), len(b)
+    denom = math.sqrt(sa/na + sb/nb)
+    if denom == 0:
+        return 0.0
+    t = (ma - mb) / denom
+    # approximate degrees of freedom:
+    df = (sa/na + sb/nb)**2 / ((sa*sa)/(na*na*(na-1) if na>1 else 1) + (sb*sb)/(nb*nb*(nb-1) if nb>1 else 1))
+    return t
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--baseline-metrics", required=True)
+    p.add_argument("--candidate-metrics", required=True)
+    p.add_argument("--report", default="/tmp/eval_report.json")
+    p.add_argument("--threshold", type=float, default=0.05)  # simplistic
+    args = p.parse_args()
+    base = json.load(open(args.baseline_metrics))
+    cand = json.load(open(args.candidate_metrics))
+    # expect lists of per-sample scores
+    t = ttest_independent(base["scores"], cand["scores"])
+    report = {"t_stat": t, "baseline_mean": sum(base["scores"])/len(base["scores"]), "candidate_mean": sum(cand["scores"])/len(cand["scores"])}
+    with open(args.report,"w") as fh:
+        json.dump(report, fh)
+    print("Wrote report:", args.report)
+    # crude acceptance: candidate mean >= baseline_mean
+    if report["candidate_mean"] >= report["baseline_mean"]:
+        exit(0)
+    else:
+        exit(3)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/cl/retrain_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-retrain-
+spec:
+  entrypoint: retrain
+  templates:
+    - name: retrain
+      steps:
+        - - name: collect-samples
+            template: collect-samples
+        - - name: train-candidate
+            template: train-candidate
+        - - name: evaluate-candidate
+            template: evaluate-candidate
+        - - name: promote
+            template: promote
+
+    - name: collect-samples
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args: ["python ops/cl/collect_samples.py --model {{workflow.parameters.model}} --out /tmp/samples.jsonl"]
+      inputs:
+        parameters:
+          - name: model
+
+    - name: train-candidate
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args: ["python ops/cl/train_candidate.py --samples /tmp/samples.jsonl --out-model-dir /tmp/candidate"]
+
+    - name: evaluate-candidate
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args: ["python ops/cl/evaluate_candidate.py --baseline-metrics /tmp/baseline.json --candidate-metrics /tmp/cand.json --report /tmp/report.json"]
+
+    - name: promote
+      container:
+        image: alpine:3.17
+        command: [sh, -c]
+        args: ["echo promote step (implement promotion logic)"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/drift/trigger_service.py
+#!/usr/bin/env python3
+"""
+Polls drift metrics and triggers retrain Argo workflows when the drift score crosses threshold.
+"""
+import os
+import time
+import requests
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus:9090")
+DRIFT_THRESHOLD = float(os.environ.get("DRIFT_THRESHOLD", "0.8"))  # lower => more drift
+ARGO_WF_NS = os.environ.get("ARGO_WF_NS", "argo")
+
+def query_prometheus(expr: str):
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": expr}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def trigger_retrain(model: str):
+    print(f"[drift] Triggering retrain for {model} (placeholder)")
+    # In real: post Argo Workflow manifest to Argo server or apply kubectl workflow YAML with parameters
+    # Here we just log and optionally call `kubectl apply -f ops/cl/retrain_workflow.yaml -n {ARGO_WF_NS}`
+    os.system(f"kubectl -n {ARGO_WF_NS} apply -f ops/cl/retrain_workflow.yaml")
+
+if __name__ == "__main__":
+    while True:
+        try:
+            resp = query_prometheus('aegis_drift_score')
+            for r in resp.get("data",{}).get("result",[]):
+                model = r.get("metric",{}).get("model","unknown")
+                val = float(r.get("value",[0,0])[1])
+                print(f"[drift] model={model} score={val}")
+                if val < DRIFT_THRESHOLD:
+                    print(f"[drift] model {model} below threshold {DRIFT_THRESHOLD}; triggering retrain")
+                    trigger_retrain(model)
+        except Exception as e:
+            print("drift trigger error:", e)
+        time.sleep(int(os.environ.get("DRIFT_POLL_S","300")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/autopilot/meta_controller.py
+#!/usr/bin/env python3
+"""
+Meta-controller that selects interventions using a simple bandit (epsilon-greedy).
+Actions: scale, swap_model, retrain, rollback
+It records outcomes and updates the bandit.
+"""
+import time
+import json
+from ops.rl.bandit import EpsilonGreedyBandit
+
+MODEL_ACTIONS = ["scale","swap_model","retrain","rollback"]
+
+class MetaController:
+    def __init__(self, model_name: str, state_dir: str = "/var/lib/aegis/meta"):
+        self.model = model_name
+        self.bandit = EpsilonGreedyBandit(actions=MODEL_ACTIONS)
+        self.state_dir = state_dir
+
+    def observe_incident(self, signal: dict):
+        # choose action
+        action = self.bandit.select_action()
+        print(f"[meta] chosen action {action} for model {self.model}")
+        success = self.execute(action, signal)
+        reward = 1.0 if success else 0.0
+        self.bandit.update(action, reward)
+        # persist bandit state
+        with open(f"{self.state_dir}/{self.model}_bandit.json","w") as fh:
+            json.dump({"counts": self.bandit.counts, "values": self.bandit.values}, fh)
+
+    def execute(self, action: str, signal: dict) -> bool:
+        # placeholder: map action to concrete automation (scale via kubectl, retrain via Argo, etc.)
+        if action == "scale":
+            print("[meta] scaling deployment (placeholder)")
+            return True
+        if action == "swap_model":
+            print("[meta] swapping to fallback (placeholder)")
+            return True
+        if action == "retrain":
+            print("[meta] triggering retrain (placeholder)")
+            return True
+        if action == "rollback":
+            print("[meta] rolling back (placeholder)")
+            return True
+        return False
+
+if __name__ == "__main__":
+    mc = MetaController("opt-1.3b")
+    while True:
+        # in real loop, subscribe to incidents; here simulate
+        mc.observe_incident({"latency":2.3})
+        time.sleep(30)
+
*** End Patch
*** Begin Patch
*** Add File:ops/experiments/manager.py
+#!/usr/bin/env python3
+"""
+Experiment manager for A/B/canary tests.
+- Create experiments, collect per-variant metrics (as lists), compute simple Welch t-test,
+  and decide promotion based on thresholds.
+"""
+import json
+import math
+import time
+from typing import List
+
+def welch_ttest(a: List[float], b: List[float]):
+    ma, mb = sum(a)/len(a), sum(b)/len(b)
+    sa = sum((x-ma)**2 for x in a)/(len(a)-1) if len(a)>1 else 0.0
+    sb = sum((x-mb)**2 for x in b)/(len(b)-1) if len(b)>1 else 0.0
+    denom = math.sqrt(sa/len(a) + sb/len(b))
+    if denom == 0:
+        return 0.0
+    return (ma-mb)/denom
+
+class Experiment:
+    def __init__(self, name: str, variants: List[str], threshold: float=0.05):
+        self.name = name
+        self.variants = {v: [] for v in variants}
+        self.threshold = threshold
+        self.start = time.time()
+
+    def add_metric(self, variant: str, value: float):
+        if variant in self.variants:
+            self.variants[variant].append(value)
+
+    def analyze_and_promote(self):
+        # simplistic: compare first variant (baseline) to others
+        baseline = self.variants[self.variants.keys().__iter__().__next__()]
+        results = {}
+        for v, vals in self.variants.items():
+            if v == list(self.variants.keys())[0]:
+                continue
+            t = welch_ttest(baseline, vals) if baseline and vals else 0.0
+            results[v] = {"t_stat": t, "baseline_mean": sum(baseline)/len(baseline) if baseline else 0.0, "mean": sum(vals)/len(vals) if vals else 0.0}
+        # promotion decision: any variant with mean >= baseline_mean promotes
+        promotions = [v for v,r in results.items() if r["mean"] >= r["baseline_mean"]]
+        return {"results": results, "promote": promotions}
+
+if __name__ == "__main__":
+    e = Experiment("exp1", ["baseline","candA","candB"])
+    # simulate
+    for i in range(50):
+        e.add_metric("baseline", 0.8 + 0.05*math.sin(i))
+        e.add_metric("candA", 0.82 + 0.03*math.cos(i))
+        e.add_metric("candB", 0.7 + 0.1*math.sin(i/2))
+    print(e.analyze_and_promote())
+
*** End Patch
*** Begin Patch
*** Add File:ops/scheduler/sla_scheduler.py
+#!/usr/bin/env python3
+"""
+SLA / cost / carbon aware scheduler heuristic.
+Given a model metadata dict, choose a node pool with labels for placement.
+"""
+import os
+from typing import Dict
+
+NODE_POOLS = [
+    {"name":"gpu-premium","labels":{"node-role.kubernetes.io/gpu":"premium"}, "cost_usd_per_hour": 3.0, "carbon_score": 0.8},
+    {"name":"gpu-cheap","labels":{"node-role.kubernetes.io/gpu":"cheap"}, "cost_usd_per_hour": 1.2, "carbon_score": 0.5},
+    {"name":"cpu-edge","labels":{"node-role.kubernetes.io/edge":"true"}, "cost_usd_per_hour": 0.1, "carbon_score": 0.2},
+]
+
+def choose_pool(model_meta: Dict, sla_latency_ms: float = 200.0, prefer_low_cost: bool = False):
+    # simple heuristic: if latency requirement tight or model large -> premium
+    size = model_meta.get("params", {}).get("size_gb", 8.0)
+    if sla_latency_ms < 200 or size > 16.0:
+        return NODE_POOLS[0]
+    if prefer_low_cost:
+        return NODE_POOLS[1]
+    return NODE_POOLS[2]
+
+if __name__ == "__main__":
+    print(choose_pool({"params":{"size_gb": 30}}, sla_latency_ms=100))
+
*** End Patch
*** Begin Patch
*** Add File:ops/gov/policy_engine.py
+#!/usr/bin/env python3
+"""
+Policy engine wrapper:
+- Query Gatekeeper constraints (crudely via kubectl) and DLP service before allowing actions.
+"""
+import os
+import subprocess
+import json
+import requests
+
+DLP_URL = os.environ.get("DLP_URL", "http://localhost:8085/scan")
+
+def gatekeeper_allows(pod_manifest_yaml: str) -> bool:
+    # crude: use `opa`/gatekeeper not directly available; run `kubectl` dry-run to see admission response (requires k8s 1.18+)
+    try:
+        out = subprocess.check_output(["kubectl","apply","-f","-","--dry-run=server"], input=pod_manifest_yaml.encode(), stderr=subprocess.STDOUT)
+        return True
+    except subprocess.CalledProcessError as e:
+        print("gatekeeper deny/dry-run fail:", e.output.decode())
+        return False
+
+def dlp_scan_text(text: str):
+    try:
+        r = requests.post(DLP_URL, json={"text": text}, timeout=5)
+        r.raise_for_status()
+        return r.json().get("findings", [])
+    except Exception as e:
+        print("dlp scan failed:", e)
+        return [{"error":"dlp-unavailable"}]
+
+def approve_action(action_id: str, who: str, note: str):
+    # integrate with approval_service
+    os.system(f"python3 ops/automation/approval_service.py create --id {action_id} --who {who} --note {note}")
+
*** End Patch
*** Begin Patch
*** Add File:ops/marketplace/api.py
+#!/usr/bin/env python3
+"""
+Simple self-service marketplace API for model onboarding, retrain requests and catalog.
+Runs a small Flask app and stores registry entries under MODEL_REGISTRY_DIR.
+"""
+from flask import Flask, request, jsonify
+import os
+import json
+
+MODEL_REGISTRY_DIR = os.environ.get("MODEL_REGISTRY_DIR", "/models/registry")
+os.makedirs(MODEL_REGISTRY_DIR, exist_ok=True)
+
+app = Flask(__name__)
+
+@app.route("/catalog", methods=["GET"])
+def catalog():
+    models = []
+    for name in os.listdir(MODEL_REGISTRY_DIR):
+        meta = {}
+        try:
+            with open(os.path.join(MODEL_REGISTRY_DIR, name, "metadata.json")) as fh:
+                meta = json.load(fh)
+        except Exception:
+            meta = {"name": name}
+        models.append(meta)
+    return jsonify(models)
+
+@app.route("/onboard", methods=["POST"])
+def onboard():
+    body = request.json or {}
+    name = body.get("name")
+    if not name:
+        return jsonify({"error":"name required"}), 400
+    model_dir = os.path.join(MODEL_REGISTRY_DIR, name)
+    os.makedirs(model_dir, exist_ok=True)
+    meta = body.get("metadata", {})
+    with open(os.path.join(model_dir,"metadata.json"), "w") as fh:
+        json.dump(meta, fh)
+    return jsonify({"status":"onboarded","name":name})
+
+@app.route("/request-retrain", methods=["POST"])
+def request_retrain():
+    body = request.json or {}
+    model = body.get("model")
+    reason = body.get("reason","user_request")
+    if not model:
+        return jsonify({"error":"model required"}), 400
+    # Queue a retrain via Argo/CL loop - placeholder: write request file
+    req_dir = os.path.join(MODEL_REGISTRY_DIR, model, "retrain_requests")
+    os.makedirs(req_dir, exist_ok=True)
+    ts = int(time.time())
+    with open(os.path.join(req_dir, f"{ts}.json"), "w") as fh:
+        json.dump({"ts":ts,"reason":reason}, fh)
+    return jsonify({"status":"queued","model":model,"ts":ts})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("MARKETPLACE_PORT", "8082")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/rl/bandit.py
+#!/usr/bin/env python3
+"""
+Simple epsilon-greedy bandit implementation to learn from operations.
+"""
+import random
+from typing import List, Dict
+
+class EpsilonGreedyBandit:
+    def __init__(self, actions: List[str], epsilon: float = 0.1):
+        self.actions = actions
+        self.epsilon = epsilon
+        self.counts = {a:0 for a in actions}
+        self.values = {a:0.0 for a in actions}
+
+    def select_action(self) -> str:
+        # explore
+        if random.random() < self.epsilon:
+            return random.choice(self.actions)
+        # exploit
+        best = max(self.actions, key=lambda a: self.values.get(a,0.0))
+        return best
+
+    def update(self, action: str, reward: float):
+        self.counts[action] += 1
+        n = self.counts[action]
+        old = self.values[action]
+        # incremental mean
+        self.values[action] = old + (reward - old) / n
+
*** End Patch
*** End Patch
