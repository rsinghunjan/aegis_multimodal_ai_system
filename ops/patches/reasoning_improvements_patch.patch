*** Begin Patch
*** Add File:ops/reasoning/plan_decomposer.py
+#!/usr/bin/env python3
+"""
+Plan decomposer: split long-horizon tasks into verifiable micro-tasks.
+ - Basic heuristic decomposition based on sentence boundaries and conjunctions.
+ - Returns ordered steps with minimal premises so each step can be verified by CAS/NLI.
+ - Intended to be used by chain_runner.py to run, verify and compose results.
+"""
+import re
+from typing import List, Dict
+
+SPLIT_PATTERN = re.compile(r"[;\.\n]\s+| and then | then | and |, then ")
+
+def decompose_text_to_steps(text: str, max_steps: int = 12) -> List[Dict]:
+    """
+    Decompose text into an ordered list of micro-tasks.
+    Each micro-task is a dict: {"id": i, "text": step_text, "prereqs": []}
+    """
+    text = text.strip()
+    # naive split on sentences and sequencing cues
+    parts = [p.strip() for p in SPLIT_PATTERN.split(text) if p.strip()]
+    # cap and combine if too many short fragments
+    if len(parts) > max_steps:
+        # group into roughly equal buckets
+        n = max_steps
+        chunk_size = max(1, len(parts) // n)
+        chunks = []
+        for i in range(0, len(parts), chunk_size):
+            chunks.append(" ".join(parts[i:i+chunk_size]))
+        parts = chunks[:max_steps]
+    steps = []
+    for i, p in enumerate(parts):
+        steps.append({"id": i+1, "text": p, "prereqs": [j+1 for j in range(i)]})
+    return steps
+
+def simple_decompose_example():
+    t = "Compute the stopping distance for a car at 60 km/h, then compare with safe limit 40m and report whether to brake. Also recommend a control action."
+    return decompose_text_to_steps(t)
+
+if __name__ == "__main__":
+    import json
+    print(json.dumps(simple_decompose_example(), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/reasoning/premise_normalizer.py
+#!/usr/bin/env python3
+"""
+Premise extractor & normalizer.
+ - Extracts candidate premises from an input text (sentences, numeric facts, named entities).
+ - Normalizes units, numeric formats and canonicalizes entity names (best-effort).
+ - Uses spaCy if available for improved entity extraction; falls back to regex heuristics.
+"""
+import re
+import json
+from typing import List, Dict
+
+try:
+    import spacy
+    NLP = spacy.load("en_core_web_sm")
+except Exception:
+    NLP = None
+
+NUM_RE = re.compile(r"([-+]?\d+(?:\.\d+)?)\s*(km/h|kph|m/s|m|km|s|ms|g|kg)?", re.I)
+
+UNIT_MAP = {
+    "kph": "km/h",
+    "km/h": "km/h",
+    "m/s": "m/s",
+    "m": "m",
+    "km": "km",
+}
+
+def normalize_number(match):
+    num = float(match.group(1))
+    unit = match.group(2) or ""
+    unit = UNIT_MAP.get(unit.lower(), unit) if unit else ""
+    return {"value": num, "unit": unit}
+
+def extract_sentences(text: str) -> List[str]:
+    # simple sentence split
+    s = re.split(r'(?<=[\.\?\!])\s+', text.strip())
+    return [x.strip() for x in s if x.strip()]
+
+def extract_entities(text: str) -> List[Dict]:
+    ents = []
+    if NLP:
+        doc = NLP(text)
+        for e in doc.ents:
+            ents.append({"text": e.text, "label": e.label_})
+    else:
+        # heuristic: capitalized words groups
+        for m in re.finditer(r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b", text):
+            ents.append({"text": m.group(1), "label": "PROPER"})
+    return ents
+
+def extract_numeric_facts(text: str) -> List[Dict]:
+    facts = []
+    for m in NUM_RE.finditer(text):
+        facts.append(normalize_number(m))
+    return facts
+
+def normalize_premises(text: str) -> Dict:
+    """
+    Returns a structure:
+      {"sentences": [...], "entities": [...], "numbers": [...], "normalized_text": "..."}
+    """
+    sents = extract_sentences(text)
+    ents = extract_entities(text)
+    nums = extract_numeric_facts(text)
+    norm_text = text
+    # replace numbers with canonical form
+    for n in nums:
+        if n["unit"]:
+            norm_text = re.sub(rf"{n['value']}\s*{re.escape(n['unit'])}", f"{n['value']} {n['unit']}", norm_text, flags=re.I)
+    return {"sentences": sents, "entities": ents, "numbers": nums, "normalized_text": norm_text}
+
+if __name__ == "__main__":
+    import sys
+    txt = " ".join(sys.argv[1:]) or "If speed is 60 km/h the stopping distance is approx 50 m."
+    print(json.dumps(normalize_premises(txt), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/reasoning/commonsense_checker.py
+#!/usr/bin/env python3
+"""
+Commonsense / plausibility checker.
+ - Best-effort heuristics plus optional Wikipedia lookup to validate common-sense facts.
+ - Returns a score and reasons; intended as a pre-check before trusting model inferences.
+"""
+import requests
+import re
+import json
+from typing import Tuple, Dict
+
+WIKIPEDIA_SUMMARY = "https://en.wikipedia.org/api/rest_v1/page/summary/{}"
+
+def quick_heuristic_check(premise: str) -> Tuple[float, Dict]:
+    """
+    Fast local heuristics: flag extreme numeric claims or simple contradictions.
+    Returns (score 0..1 confidence, details)
+    """
+    details = {}
+    # extreme numbers
+    nums = re.findall(r"[-+]?\d+(?:\.\d+)?", premise)
+    for n in nums:
+        val = float(n)
+        if abs(val) > 1e6:
+            return 0.1, {"reason": "extreme_number", "value": val}
+    # trivial contradictions (example)
+    if "sun rises in the west" in premise.lower():
+        return 0.0, {"reason": "obvious_contradiction"}
+    return 0.7, {"reason": "heuristic_ok"}
+
+def wikipedia_check(concept: str) -> Tuple[float, Dict]:
+    """
+    Query Wikipedia summary for the concept; if page exists return higher plausibility.
+    Note: network call; caller should handle timeouts.
+    """
+    try:
+        title = concept.strip().replace(" ", "_")
+        r = requests.get(WIKIPEDIA_SUMMARY.format(title), timeout=5)
+        if r.status_code == 200:
+            data = r.json()
+            summary = data.get("extract", "")
+            return 0.9, {"source": "wikipedia", "summary": summary[:500]}
+        else:
+            return 0.4, {"source": "wikipedia", "status": r.status_code}
+    except Exception as e:
+        return 0.5, {"error": str(e)}
+
+def check_premise_with_kb(premise: str, concept_hint: str = None) -> Dict:
+    """
+    Composite check: run quick heuristics and optionally a KB/Wikipedia lookup.
+    Returns a dict with score and details.
+    """
+    score, det = quick_heuristic_check(premise)
+    out = {"score": score, "heuristic": det}
+    if concept_hint:
+        kb_score, kb_det = wikipedia_check(concept_hint)
+        out["kb_score"] = kb_score
+        out["kb_detail"] = kb_det
+        # combine naively
+        out["combined_score"] = min(1.0, 0.6 * score + 0.4 * kb_score)
+    return out
+
+if __name__ == "__main__":
+    import sys
+    p = " ".join(sys.argv[1:]) or "A car going 60 km/h stops in 3 meters"
+    print(json.dumps(check_premise_with_kb(p, concept_hint="Car"), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/invariant_registry_enhancer.py
+#!/usr/bin/env python3
+"""
+Invariant registry enhancer and coverage reporter.
+ - Scans invariants registry (JSON format) and verification logs to report prover coverage, runtime monitor coverage, and gaps.
+ - Produces a coverage report for operators to prioritize invariant engineering.
+"""
+import os
+import json
+from datetime import datetime
+
+INVAR_PATH = os.environ.get("INVARIANT_REGISTRY", "ops/formal/invariant_registry.json")
+VERIF_LOG_DIR = os.environ.get("VERIFIER_LOG_DIR", "/tmp/verifier_logs")
+OUT = os.environ.get("INVARIANT_COVERAGE_OUT", "/tmp/invariant_coverage_report.json")
+
+def load_invariants():
+    if not os.path.exists(INVAR_PATH):
+        return {}
+    return json.load(open(INVAR_PATH))
+
+def scan_verifier_logs():
+    res = {}
+    if not os.path.exists(VERIF_LOG_DIR):
+        return res
+    for fn in os.listdir(VERIF_LOG_DIR):
+        if not fn.endswith(".json"):
+            continue
+        try:
+            j = json.load(open(os.path.join(VERIF_LOG_DIR, fn)))
+            for inv in j.get("invariants", []):
+                key = inv.get("id") or inv.get("name")
+                rec = res.setdefault(key, {"attempts":0, "passed":0})
+                rec["attempts"] += 1
+                if inv.get("status")=="proved":
+                    rec["passed"] += 1
+        except Exception:
+            continue
+    return res
+
+def produce_report():
+    invs = load_invariants()
+    logs = scan_verifier_logs()
+    report = {"ts": datetime.utcnow().isoformat()+"Z", "invariants_total": len(invs), "entries": []}
+    for k,v in invs.items():
+        attempts = logs.get(k, {}).get("attempts", 0)
+        passed = logs.get(k, {}).get("passed", 0)
+        report["entries"].append({"id": k, "spec": v, "prover_attempts": attempts, "prover_passed": passed})
+    with open(OUT, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote invariant coverage report to", OUT)
+    return OUT
+
+if __name__ == "__main__":
+    produce_report()
+
*** End Patch
*** Begin Patch
*** Add File:ops/retrieval/provenance_validator.py
+#!/usr/bin/env python3
+"""
+Provenance validator for retrieved documents.
+ - Validates retrieved items have required provenance (url, domain, published date if available)
+ - Computes a provenance score and flags weak items to be deprioritized or re-queried.
+"""
+import os
+import re
+import json
+from urllib.parse import urlparse
+from datetime import datetime
+
+ALLOWLIST = set(os.environ.get("PROV_ALLOWLIST", "wikipedia.org,arxiv.org,github.com").split(","))
+OUT = os.environ.get("PROV_VALIDATION_OUT", "/tmp/provenance_validation.json")
+
+def domain_score(url: str) -> float:
+    try:
+        dom = urlparse(url).netloc.lower()
+        for a in ALLOWLIST:
+            if a and a in dom:
+                return 1.0
+        # penalize unknown domains
+        return 0.3
+    except Exception:
+        return 0.0
+
+def recency_score(meta: dict) -> float:
+    # meta may include "published" timestamp
+    pub = meta.get("published")
+    if not pub:
+        return 0.5
+    try:
+        dt = datetime.fromisoformat(pub.replace("Z",""))
+        age_days = (datetime.utcnow() - dt).days
+        if age_days < 30:
+            return 1.0
+        if age_days < 365:
+            return 0.8
+        return 0.5
+    except Exception:
+        return 0.5
+
+def validate_retrieved_items(items):
+    """
+    items: list of {"url":..., "title":..., "meta": {...}, "text": "..."}
+    returns list of validation entries with score
+    """
+    out = []
+    for it in items:
+        url = it.get("url","")
+        if not url:
+            score = 0.0
+            reason = "missing_url"
+        else:
+            ds = domain_score(url)
+            rs = recency_score(it.get("meta", {}))
+            # basic text length check
+            length = len(it.get("text","") or "")
+            length_score = min(1.0, length / 500.0)
+            score = 0.5*ds + 0.3*rs + 0.2*length_score
+            reason = None
+        out.append({"url": url, "score": score, "reason": reason, "title": it.get("title")})
+    with open(OUT, "w") as fh:
+        json.dump({"ts": datetime.utcnow().isoformat()+"Z", "results": out}, fh, indent=2)
+    return out
+
+if __name__ == "__main__":
+    import sys
+    sample = [{"url": "https://en.wikipedia.org/wiki/Car", "title":"Car", "text":"Cars are vehicles...", "meta":{"published":"2020-01-01T00:00:00Z"}}]
+    print(json.dumps(validate_retrieved_items(sample), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/calibration.py
+#!/usr/bin/env python3
+"""
+NLI calibration helpers.
+ - Temperature scaling via simple grid search to minimize negative log-likelihood on a validation set.
+ - Consistency tester: runs multiple seeds/replicas and reports disagreement/confidence calibration.
+ - Expects validation logits/probs and labels stored in JSON lines or numpy arrays (simple JSON interface provided).
+"""
+import json
+import math
+import os
+from typing import List
+
+def nll_with_temp(probs, labels, temp):
+    # probs: list of [p_true, p_false] or scalar prob_true; labels: 0/1
+    nll = 0.0
+    eps = 1e-12
+    for p, y in zip(probs, labels):
+        p_t = p if isinstance(p, float) else p[0]
+        # scale via temp on logits: logit = log(p/(1-p)) / temp
+        if p_t <= 0 or p_t >= 1:
+            p_s = max(eps, min(1-eps, p_t))
+        else:
+            p_s = p_t
+        logit = math.log(p_s/(1.0-p_s))
+        scaled = 1.0/(1.0 + math.exp(-logit / temp))
+        p_y = scaled if y==1 else (1.0 - scaled)
+        nll -= math.log(max(eps, p_y))
+    return nll / len(probs)
+
+def find_temperature_grid(probs: List[float], labels: List[int], grid=None):
+    if grid is None:
+        grid = [0.5, 0.7, 0.9, 1.0, 1.2, 1.5, 2.0, 3.0]
+    best_t = 1.0
+    best_nll = float("inf")
+    for t in grid:
+        nll = nll_with_temp(probs, labels, t)
+        if nll < best_nll:
+            best_nll = nll
+            best_t = t
+    return best_t, best_nll
+
+def load_validation_json(path):
+    # expects JSON with {"probs":[...], "labels":[...]}
+    j = json.load(open(path))
+    return j.get("probs", []), j.get("labels", [])
+
+def apply_temperature(probs, temp):
+    out = []
+    for p in probs:
+        p_t = p if isinstance(p, float) else p[0]
+        logit = math.log(max(1e-12, p_t)/(1.0-max(1e-12,p_t)))
+        scaled = 1.0/(1.0 + math.exp(-logit / temp))
+        out.append(scaled)
+    return out
+
+if __name__ == "__main__":
+    import sys
+    if len(sys.argv) < 2:
+        print("usage: calibration.py validation.json")
+    else:
+        probs, labels = load_validation_json(sys.argv[1])
+        t, nll = find_temperature_grid(probs, labels)
+        print("best_temp", t, "nll", nll)
+
*** End Patch
*** Begin Patch
*** Add File:ops/reasoning/chain_runner.py
+#!/usr/bin/env python3
+"""
+Chain runner with decomposition, premise normalization, commonsense checking and per-step verification hooks.
+ - Exposes run_task(task_text, llm_fn, verifier_fn, options)
+ - llm_fn(signature: prompt) -> { "answer": str, "confidence": float, "logits": optional }
+ - verifier_fn(signature: step_text, context) -> { "verdict": "pass|fail", "evidence": path_or_obj }
+ - The runner will decompose, normalize premises, run llm_fn for each micro-task, call verifier_fn,
+   and aggregate results. If a step fails verification, runner will stop and return partial evidence.
+"""
+import json
+from typing import Callable, Dict, Any
+from .plan_decomposer import decompose_text_to_steps
+from .premise_normalizer import normalize_premises
+from .commonsense_checker import check_premise_with_kb
+
+def run_task(task_text: str,
+             llm_fn: Callable[[str], Dict[str, Any]],
+             verifier_fn: Callable[[str, Dict], Dict[str, Any]],
+             max_steps: int = 12,
+             commonsense_hint: bool = True) -> Dict:
+    steps = decompose_text_to_steps(task_text, max_steps=max_steps)
+    results = {"task": task_text, "steps": [], "status": "ok"}
+    for s in steps:
+        sid = s["id"]
+        step_txt = s["text"]
+        # normalize premises
+        prem = normalize_premises(step_txt)
+        # commonsense check
+        cs = None
+        if commonsense_hint:
+            # hint with first proper noun if available
+            hint = prem["entities"][0]["text"] if prem["entities"] else None
+            cs = check_premise_with_kb(step_txt, concept_hint=hint) if hint else check_premise_with_kb(step_txt, None)
+            if cs.get("combined_score", cs.get("score", 0)) < 0.2:
+                results["steps"].append({"id": sid, "text": step_txt, "status": "rejected_by_commonsense", "commonsense": cs})
+                results["status"] = "blocked"
+                return results
+        # call LLM to solve microtask
+        prompt = f"Step {sid}: {step_txt}\nProvide concise result and justification."
+        llm_res = llm_fn(prompt)
+        # verify step
+        ver = verifier_fn(llm_res.get("answer", ""), {"premises": prem, "llm": llm_res})
+        entry = {"id": sid, "text": step_txt, "premises": prem, "llm": llm_res, "verify": ver}
+        results["steps"].append(entry)
+        if ver.get("verdict") != "pass":
+            results["status"] = "failed_verification"
+            return results
+    return results
+
+if __name__ == "__main__":
+    # example demo llm and verifier stubs
+    def demo_llm(prompt):
+        return {"answer": "42", "confidence": 0.7}
+    def demo_verifier(ans, ctx):
+        # trivial accept if numeric
+        import re
+        if re.match(r"^\d+(\.\d+)?$", str(ans).strip()):
+            return {"verdict":"pass","evidence":None}
+        return {"verdict":"fail","reason":"non_numeric"}
+    task = "Compute 6 * 7 then add 0 and report the result."
+    print(json.dumps(run_task(task, demo_llm, demo_verifier), indent=2))
+
*** End Patch
*** End Patch
