*** Begin Patch
*** Add File:ops/hil/adapters/acme_hil_adapter.py
+#!/usr/bin/env python3
+"""
+Acme HIL vendor adapter (REST + verification helpers)
+ - Implements prepare_replay, start_run, poll_status, fetch_artifacts against Acme's REST API.
+ - Expects env:
+     ACME_HIL_URL - base URL for vendor API
+     ACME_HIL_TOKEN - bearer token
+ - Fallbacks to local simulator when ACME_HIL_URL is not set (useful for CI).
+ - Contains a small CLI for quick smoke checks.
+"""
+import os
+import time
+import json
+from typing import Dict, Any, List
+
+ACME_URL = os.environ.get("ACME_HIL_URL", "")
+ACME_TOKEN = os.environ.get("ACME_HIL_TOKEN", "")
+
+try:
+    import requests
+except Exception:
+    requests = None
+
+class AcmeHILAdapter:
+    def __init__(self, url: str = None, token: str = None):
+        self.url = url or ACME_URL
+        self.token = token or ACME_TOKEN
+
+    def _headers(self):
+        h = {"Content-Type":"application/json"}
+        if self.token:
+            h["Authorization"] = f"Bearer {self.token}"
+        return h
+
+    def list_backends(self) -> List[Dict[str,Any]]:
+        if not self.url or not requests:
+            return [{"rack_id":"acme-sim","slot":1,"status":"idle","simulated":True}]
+        r = requests.get(f"{self.url}/racks", headers=self._headers(), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def prepare_replay(self, payload: Dict[str,Any]) -> Dict[str,Any]:
+        if not self.url or not requests:
+            return {"mounted": payload.get("manifest",{}).get("bag","local"), "ok": True, "simulated": True}
+        r = requests.post(f"{self.url}/racks/prepare", headers=self._headers(), json=payload, timeout=30)
+        r.raise_for_status()
+        return r.json()
+
+    def start_run(self, payload: Dict[str,Any]) -> Dict[str,Any]:
+        if not self.url or not requests:
+            jid = f"acme-sim-{int(time.time()*1000)}"
+            return {"job_id": jid, "status": "submitted", "simulated": True}
+        r = requests.post(f"{self.url}/runs", headers=self._headers(), json=payload, timeout=30)
+        r.raise_for_status()
+        return r.json()
+
+    def poll_status(self, job_id: str) -> Dict[str,Any]:
+        if not self.url or not requests:
+            return {"state":"succeeded"}
+        r = requests.get(f"{self.url}/runs/{job_id}/status", headers=self._headers(), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def fetch_artifacts(self, job_id: str, out_dir: str) -> List[str]:
+        os.makedirs(out_dir, exist_ok=True)
+        if not self.url or not requests:
+            p = os.path.join(out_dir, f"{job_id}_hil_log.json")
+            json.dump({"job_id": job_id, "status":"sim_complete"}, open(p,"w"))
+            return [p]
+        r = requests.get(f"{self.url}/runs/{job_id}/artifacts", headers=self._headers(), timeout=30)
+        r.raise_for_status()
+        arts = r.json().get("artifacts", [])
+        local = []
+        for a in arts:
+            try:
+                rr = requests.get(a, headers=self._headers(), timeout=60)
+                rr.raise_for_status()
+                fname = os.path.join(out_dir, a.split("/")[-1])
+                open(fname,"wb").write(rr.content)
+                local.append(fname)
+            except Exception:
+                continue
+        return local
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--list", action="store_true")
+    args = p.parse_args()
+    a = AcmeHILAdapter()
+    if args.list:
+        print(a.list_backends())
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hil_onboard_vendor.yml
+name: HIL Vendor Onboard & Certify
+on:
+  workflow_dispatch:
+
+jobs:
+  certify:
+    runs-on: ubuntu-latest
+    environment: production
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: python -m pip install --upgrade pip requests boto3 || true
+      - name: Run certify_rack
+        env:
+          ACME_HIL_URL: ${{ secrets.ACME_HIL_URL }}
+          ACME_HIL_TOKEN: ${{ secrets.ACME_HIL_TOKEN }}
+          EVIDENCE_SIGN_BACKEND: ${{ secrets.EVIDENCE_SIGN_BACKEND }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+        run: |
+          python ops/hil/certify_rack.py --adapter ops.hil.adapters.acme_hil_adapter.AcmeHILAdapter --manifest ops/hil/playbook/example_replay_manifest.json --name acme-rack-1 --repeats 10 --s3-bucket "${{ secrets.EVIDENCE_BUCKET }}"
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/rt_rollout_playbook.md
+RT Fleet Rollout Playbook
+=========================
+
+Purpose
+-------
+Automate building RT node images, labeling nodes, deploying RT agents and executing WCET validation across fleet nodes.
+
+Steps (high-level)
+1. Build & push RT image:
+   ./ops/rt/build_rt_image.sh <reg>/aegis/rt-node:stable
+2. Label candidate nodes:
+   ./ops/rt/label_nodes.sh "gpu=true" "aegis/rt=true"
+3. Deploy RT agent via Helm:
+   helm upgrade --install aegis-rt-node charts/rt-node -f charts/rt-node/values.prod.yaml
+4. Run WCET K8s job on nodes:
+   kubectl apply -f ops/rt/wcet_k8s_job.yaml
+5. For remote vehicles, provision SSH entries in ops/rt/ssh_wcet_config_example.json and run remote_wcet_runner.py.
+6. Aggregate results and fail promotion if p99 exceeds budgets.
+
+Acceptance criteria
+- p99 per control binary within annotated budget across sampled nodes.
+- Prometheus alerts configured to trigger on regression.
+- Signed WCET aggregate uploaded to evidence store with index entry.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/rt_rollout.yml
+name: RT Rollout & WCET Validation
+on:
+  workflow_dispatch:
+
+jobs:
+  build-and-rollout:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build RT image
+        run: |
+          ./ops/rt/build_rt_image.sh ${{ secrets.REGISTRY }}/aegis/rt-node:${{ github.sha }}
+      - name: Push image
+        run: |
+          echo "Push handled by CI/publish step (assume image available)"
+      - name: Label nodes (operator needed)
+        run: |
+          echo "Labeling nodes requires kubectl access; ensure runner has kubeconfig"
+          ./ops/rt/label_nodes.sh "vehicle=true" "aegis/rt=true" || true
+      - name: Trigger WCET job on k8s
+        run: |
+          kubectl apply -f ops/rt/wcet_k8s_job.yaml || true
+      - name: Run remote WCET (SSH-configured)
+        env:
+          SSH_CONFIG: ${{ secrets.WCET_SSH_CONFIG }}
+        run: |
+          python ops/rt/remote_wcet_runner.py --config ops/rt/ssh_wcet_config_example.json
+      - name: Validate aggregate p99
+        run: |
+          python - <<PY
+import json,sys
+try:
+    data=json.load(open('/tmp/wcet_aggregate_remote.json'))
+except Exception:
+    print('No aggregate'); sys.exit(2)
+for r in data.get('reports',[]):
+    p99=r.get('p99') or r.get('p99_ms') or 0
+    if p99 and float(p99) > 0.2:
+        print('p99 too high', p99); sys.exit(2)
+print('RT p99 validated')
+PY
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/annotation_vendor_connector.py
+#!/usr/bin/env python3
+"""
+Annotation vendor connector (generic)
+ - Configuration: ops/perception/annotation_vendor_config.example.json
+ - Supports multipart upload and polling for completion
+ - Fallback to S3 + human operator when vendor URL is not configured
+"""
+import os
+import json
+import time
+
+CONFIG_PATH = os.environ.get("ANNOTATION_VENDOR_CONFIG", "ops/perception/annotation_vendor_config.example.json")
+
+try:
+    import requests
+except Exception:
+    requests = None
+
+def load_config():
+    if os.path.exists(CONFIG_PATH):
+        return json.load(open(CONFIG_PATH))
+    return {}
+
+def submit(bundle_path):
+    cfg = load_config()
+    url = cfg.get("vendor_url")
+    token = cfg.get("api_token")
+    if not url or not requests:
+        # upload to S3 or place in /tmp/annotation_tasks for manual handling
+        dst = "/tmp/annotation_tasks/" + os.path.basename(bundle_path)
+        os.makedirs("/tmp/annotation_tasks", exist_ok=True)
+        import shutil
+        shutil.copy(bundle_path, dst)
+        return {"task_id": os.path.basename(dst), "mode": "manual", "path": dst}
+    with open(bundle_path,"rb") as fh:
+        r = requests.post(f"{url}/upload", headers={"Authorization": f"Bearer {token}"}, files={"file": fh}, timeout=120)
+        r.raise_for_status()
+        return r.json()
+
+def poll(task_id, timeout=86400, interval=30):
+    cfg = load_config()
+    url = cfg.get("vendor_url")
+    token = cfg.get("api_token")
+    if not url or not requests:
+        p = f"/tmp/annotation_tasks/{task_id}.json"
+        waited=0
+        while waited < timeout:
+            if os.path.exists(p):
+                return json.load(open(p))
+            time.sleep(interval); waited += interval
+        return {"status":"timeout"}
+    r = requests.get(f"{url}/tasks/{task_id}", headers={"Authorization": f"Bearer {token}"}, timeout=30)
+    r.raise_for_status()
+    return r.json()
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--submit", help="bundle path")
+    p.add_argument("--poll", help="task id")
+    args = p.parse_args()
+    if args.submit:
+        print(submit(args.submit))
+    if args.poll:
+        print(poll(args.poll))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/annotation_vendor_config.example.json
+{
+  "vendor_url": "",
+  "api_token": "",
+  "s3_bucket_fallback": "my-aegis-annotations"
+}
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/perception_production.yml
+name: Perception Production Pipeline (demo)
+on:
+  workflow_dispatch:
+
+jobs:
+  production-perception:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: python -m pip install --upgrade pip requests boto3 || true
+      - name: Ingest annotation results
+        run: |
+          python ops/perception/annotation_result_ingest.py || true
+      - name: Run training shards (launched via train_launcher)
+        run: |
+          # launching is idempotent if k8s available
+          for s in /tmp/dataset_shards/manifest_shard_*.json; do
+            python ops/perception/train_launcher.py --shard "$s" --image "${{ secrets.TRAINER_IMAGE }}" || true
+          done
+      - name: Run evaluation & promote (manual approval step recommended)
+        run: |
+          echo "Evaluation and promote should be run manually or via separate gated job"
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/sme_onboard_playbook.md
+SME Onboarding Playbook for Formal Invariants
+============================================
+
+Goal
+----
+Help domain SMEs submit authoritative SMT encodings and iterate until proofs pass CI.
+
+Workflow
+--------
+1. SME creates a PR under ops/formal/onboard_submissions with:
+   - <id>.smt2 (SMT-LIB v2 encoding)
+   - <id>.meta.json containing id, description, priority, responsible
+2. CI (sme_review_workflow.yml) runs prover_batch_runner and uploads proof artifacts.
+3. Reviewer verifies artifacts and either marks the registry entry as proved or requests changes.
+4. When accepted, maintainer runs ops/formal/onboard_invariant_api.py to add to invariant_registry_high_priority.json.
+
+Acceptance criteria
+- SMT file parses and runs (check-sat present).
+- Prover artifacts (*.proof.json) generated by prover_batch_runner exist.
+- SME signs off on composed proof artifact and documents rationale.
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/pr_template.md
+Formal Invariant PR Template
+---------------------------
+Please fill in the fields below when submitting a new invariant SMT PR.
+
+Invariant id:
+Description (plain English):
+Priority (high|medium|low):
+Responsible SME (name/team):
+SMT file path: ops/formal/onboard_submissions/<id>.smt2
+SMT rationale and assumptions:
+Tests / representative scenarios included (how to reproduce):
+
+CI will run the prover and produce artifacts; please attach any domain notes.
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/oidc_kms_iam_policies.md
+OIDC / KMS / IAM Policy Templates & Guidance
+===========================================
+
+This file contains example least-privilege snippets and guidance. Adapt to your org.
+
+AWS: minimal policy for CI signing (example)
+------------------------------------------
+1. Create an IAM role trust for GitHub OIDC (see ops/governance/create_oidc_role.sh).
+2. Attach a KMS usage policy allowing Sign with specified key:
+{
+  "Version":"2012-10-17",
+  "Statement":[
+    {
+      "Effect":"Allow",
+      "Action":["kms:Sign","kms:GetPublicKey"],
+      "Resource":["arn:aws:kms:REGION:ACCOUNT:key/YOUR_KEY_ID"]
+    },
+    {
+      "Effect":"Allow",
+      "Action":["s3:PutObject","s3:GetObject"],
+      "Resource":["arn:aws:s3:::evidence-bucket/*"]
+    }
+  ]
+}
+
+GCP/Azure: similar patterns using Workload Identity / Federated credentials.
+
+Guidance
+--------
+- Use short-lived credentials in CI via OIDC.
+- Limit KMS key usage to a dedicated signing key with key policy audit logs enabled.
+- Require human approval for changing role trust or adding new repositories to OIDC trust.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/oidc_kms_signing.yml
+name: OIDC KMS Signing Demo (CI)
+on:
+  workflow_dispatch:
+
+jobs:
+  assume-and-sign:
+    runs-on: ubuntu-latest
+    permissions:
+      id-token: write
+      contents: read
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Configure AWS credentials via OIDC
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          role-to-assume: ${{ secrets.AEGIS_OIDC_ROLE_ARN }}
+          aws-region: ${{ secrets.AWS_REGION }}
+      - name: Run KMS sign example
+        env:
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+        run: |
+          echo "hello" > /tmp/to_sign.txt
+          aws kms sign --key-id ${AWS_KMS_KEY_ID} --message fileb:///tmp/to_sign.txt --signing-algorithm RSASSA_PSS_SHA_256 --output text --query Signature | base64 --decode > /tmp/sig.bin || true
+          echo "Signing attempted (demo)."
+
*** End Patch
*** Begin Patch
*** Add File:ops/sre/grafana_dashboard_rt.json
+{
+  "title": "Aegis RT Dashboard",
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Control Loop P99",
+      "targets": [{"expr":"aegis_control_loop_latency_seconds_p99"}]
+    },
+    {
+      "type": "stat",
+      "title": "HIL Jobs Success Rate",
+      "targets": [{"expr":"increase(aegis_hil_jobs_succeeded_total[7d]) / increase(aegis_hil_jobs_total[7d])"}]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/ci/hardware_smoke_ci.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# CI helper to attempt a hardware-targeted smoke test if staging nodes are available.
+#
+if [ -f /tmp/staging_lab_registry.json ]; then
+  node=$(jq -r 'keys[0]' /tmp/staging_lab_registry.json)
+  host=$(jq -r ".[\"$node\"].host" /tmp/staging_lab_registry.json)
+  echo "Found staging node $node -> $host; attempting SSH smoke test..."
+  ssh -o BatchMode=yes -o ConnectTimeout=10 "$host" "echo smoke-ok" || { echo "SSH smoke failed"; exit 2; }
+  echo "Hardware smoke OK"
+else
+  echo "No staging registry; skipping hardware smoke (allowed in fast lanes)."
+fi
+
*** End Patch
*** End Patch
