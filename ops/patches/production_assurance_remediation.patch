*** Begin Patch
*** Add File:ops/hsm/auditor_evidence_recorder.py
+#!/usr/bin/env python3
+"""
+Record additional auditor-session artifacts and store signoff metadata in a ConfigMap.
+
+ - Wraps existing live auditor session runner and collects:
+   - evidence bundle path (local/S3)
+   - live signing transcript
+   - operator checklist answers
+   - optional video recording link (operator-provided)
+ - Writes a ConfigMap entry 'aegis-auditor-signoffs' with structured JSON per session
+"""
+import os
+import json
+from datetime import datetime
+from kubernetes import client, config
+from ops.hsm.auditor_session_runner import run_session
+
+CM_NAME = os.environ.get("AUDIT_SIGNOFF_CM", "aegis-auditor-signoffs")
+CM_NS = os.environ.get("AUDIT_SIGNOFF_NS", "kube-system")
+
+def k8s_client():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+def persist_signoff(session_report, operator_answers=None, video_link=None):
+    core = k8s_client()
+    try:
+        cm = core.read_namespaced_config_map(CM_NAME, CM_NS)
+        data = cm.data or {}
+    except Exception:
+        data = {}
+    sid = "aud-" + str(int(datetime.utcnow().timestamp()))
+    entry = {
+        "id": sid,
+        "report": session_report,
+        "operator_answers": operator_answers or {},
+        "video_link": video_link or "",
+        "state": "pending",
+        "created": datetime.utcnow().isoformat() + "Z"
+    }
+    data[sid] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=CM_NAME), data=data)
+    try:
+        core.replace_namespaced_config_map(CM_NAME, CM_NS, body)
+    except Exception:
+        core.create_namespaced_config_map(CM_NS, body)
+    return entry
+
+def run_and_record(sample, kms_key_id=None, cloudhsm_cluster_id=None, operator_answers=None, video_link=None):
+    print("Running auditor session...")
+    session = run_session(sample, kms_key_id, cloudhsm_cluster_id, operator_notes=operator_answers.get("notes") if operator_answers else "")
+    print("Persisting signoff entry...")
+    entry = persist_signoff(session, operator_answers, video_link)
+    print("Recorded signoff:", entry["id"])
+    return entry
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--sample", default="ops/hsm/audit_sample.bin")
+    p.add_argument("--kms")
+    p.add_argument("--cluster")
+    p.add_argument("--video", help="URL to auditor session recording")
+    p.add_argument("--notes", help="operator notes")
+    args = p.parse_args()
+    answers = {"notes": args.notes}
+    print(run_and_record(args.sample, args.kms, args.cluster, answers, args.video))
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariants_catalog_extended.json
+{
+  "planner_time_budget_ms": {"type":"planner_budget", "max_ms": 50, "description":"planner must respond within 50ms"},
+  "stopping_distance_margin_m": {"type":"stopping_distance", "params":{"min_margin_m": 0.5}},
+  "max_lateral_accel_m_s2": {"type":"numeric_limit", "variable":"lateral_accel", "op":"le", "value":4.0},
+  "no_entry_zones": {"type":"forbidden_area", "areas":[{"id":"zone_a", "polygon":[[0,0],[10,0],[10,10],[0,10]]}]},
+  "controller_gain_bounds": {"type":"controller_gain_check", "params":{"max_gain":1.5}},
+  "velocity_range": {"type":"numeric_range","variable":"speed","min":0.0,"max":5.0}
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/fuzz_falsifier.py
+#!/usr/bin/env python3
+"""
+Lightweight fuzz/falsification harness for planner/controller invariants.
+
+ - Mutates plan/state fixtures to search for invariant violations
+ - Uses ops/formal/formal_prover_integration.prove_invariants to check each mutated state
+ - Outputs counterexamples to /tmp/formal_fuzz_counterexamples.jsonl
+"""
+import os
+import json
+import random
+from datetime import datetime
+from ops.formal.formal_prover_integration import prove_invariants
+
+SEED_FIXTURE = os.environ.get("PROVER_FIXTURE","fixtures/sample_plan_state.json")
+INV_FILE = os.environ.get("INVARIANTS_FILE","ops/formal/invariants_catalog_extended.json")
+OUT = os.environ.get("FUZZ_OUT","/tmp/formal_fuzz_counterexamples.jsonl")
+
+def mutate(state):
+    s = dict(state)
+    # mutate a few numeric fields
+    for k in ["lateral_accel","long_decel","speed","planner_ms","obstacle_dist"]:
+        if k in s:
+            perturb = (random.random()*2-1) * (abs(s.get(k,1.0)) + 1.0)
+            s[k] = s.get(k,0) + perturb
+    # random position within 50x50 area
+    s["position"] = [random.uniform(-50,50), random.uniform(-50,50)]
+    return s
+
+def run(iterations=200):
+    base = json.load(open(SEED_FIXTURE))
+    inv = json.load(open(INV_FILE))
+    with open(OUT,"w") as fh:
+        for i in range(iterations):
+            m = mutate(base)
+            res = prove_invariants(m, inv)
+            if not res.get("ok"):
+                record = {"ts": datetime.utcnow().isoformat()+"Z", "mutant": m, "result": res}
+                fh.write(json.dumps(record) + "\n")
+    print("Fuzz results written to", OUT)
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--iters", type=int, default=200)
+    args = p.parse_args()
+    run(args.iters)
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/pr_hil_formal_extended.yml
+name: PR HIL Formal Extended
+on:
+  pull_request:
+    types: [opened, synchronize]
+
+jobs:
+  formal-extended:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install Z3
+        run: |
+          python -m pip install z3-solver
+      - name: Run formal prover against extended invariants
+        run: |
+          mkdir -p fixtures
+          if [ ! -f fixtures/sample_plan_state.json ]; then echo '{"planner_ms":10,"lateral_accel":1.0,"long_decel":-2.0,"speed":1.0,"position":[1,1],"obstacle_dist":100.0}' > fixtures/sample_plan_state.json; fi
+          python ops/formal/pr_hil_prover.py --plan-state fixtures/sample_plan_state.json --invariants ops/formal/invariants_catalog_extended.json --out /tmp/pr_formal_proof.json
+      - name: Run fuzz/falsifier (fast)
+        run: |
+          python ops/formal/fuzz_falsifier.py --iters 50 || true
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: pr-formal-artifacts
+          path: |
+            /tmp/pr_formal_proof.json
+            /tmp/formal_fuzz_counterexamples.jsonl
+
*** End Patch
*** Begin Patch
*** Add File:ops/validation/autoscaler_canary_runner.py
+#!/usr/bin/env python3
+"""
+Run autoscaler canary tests and rollback scenarios.
+
+ - Deploys a canary ASG/nodepool change in a controlled manner, observes metrics, and rolls back on failure
+ - Integrates with ops/infra/gpu_autoscaler_improved or ASG APIs
+ - Produces a canary_report.json
+"""
+import os
+import json
+import time
+from datetime import datetime
+import requests
+
+CANARY_CONFIGMAP = os.environ.get("CANARY_CM","aegis-canary-changes")
+NAMESPACE = os.environ.get("CANARY_NS","kube-system")
+PROM_URL = os.environ.get("PROM_URL","http://prometheus:9090")
+
+def apply_canary(desired_replicas):
+    # write desired to ConfigMap for operator or external controller to pick up
+    # (simple implementation: call gpu_autoscaler_improved's CM)
+    import subprocess
+    # write a small file marker - real infra would use k8s API
+    marker = {"canary_desired": desired_replicas, "ts": datetime.utcnow().isoformat()+"Z"}
+    path = "/tmp/canary_marker.json"
+    with open(path,"w") as fh:
+        json.dump(marker, fh)
+    return path
+
+def monitor_window(seconds=120, metric_query="aegis_quantum_sim_p95_latency"):
+    end = time.time() + seconds
+    samples = []
+    while time.time() < end:
+        try:
+            r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": metric_query}, timeout=5)
+            r.raise_for_status()
+            data = r.json().get("data",{}).get("result",[])
+            val = float(data[0]["value"][1]) if data else None
+        except Exception:
+            val = None
+        samples.append({"ts": datetime.utcnow().isoformat()+"Z", "value": val})
+        time.sleep(5)
+    return samples
+
+def run_canary(desired, monitor_s=120, threshold=5.0):
+    marker = apply_canary(desired)
+    samples = monitor_window(monitor_s)
+    failures = [s for s in samples if s["value"] is not None and s["value"] > threshold]
+    ok = len(failures) == 0
+    if not ok:
+        # rollback - operator action or simple marker
+        apply_canary(1)
+    report = {"marker": marker, "samples": samples, "ok": ok, "rolled_back": not ok, "ts": datetime.utcnow().isoformat()+"Z"}
+    out = "/tmp/canary_report.json"
+    with open(out,"w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote", out)
+    return out
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--desired", type=int, default=5)
+    args = p.parse_args()
+    run_canary(args.desired)
+
*** End Patch
*** Begin Patch
*** Add File:ops/providers/provider_sandbox_registry.py
+#!/usr/bin/env python3
+"""
+Maintain sandbox test registry and expected error-code classification per provider.
+
+ - Provides an executable that returns provider error taxonomy (used by CI and QE)
+ - Stores mapping in a simple JSON file that can be extended by infra/QE after sandbox runs
+"""
+REG = "/tmp/provider_sandbox_registry.json"
+
+DEFAULT = {
+  "braket": {"transient": ["ThrottlingException","TooManyRequestsException","ServiceUnavailable"], "fatal": ["AccessDeniedException","InvalidParameterException"]},
+  "ibmq": {"transient": ["CONNECTION_ERROR","TIMEOUT"], "fatal": ["AUTH_ERROR","INVALID_BACKEND"]}
+}
+
+def ensure():
+    if not os.path.exists(REG):
+        with open(REG,"w") as fh:
+            json.dump(DEFAULT, fh, indent=2)
+    return REG
+
+if __name__=="__main__":
+    print("registry:", ensure())
+
*** End Patch
*** Begin Patch
*** Add File:ops/finance/reconcile_signoff_workflow.py
+#!/usr/bin/env python3
+"""
+Run reconciliation, produce CSV, email (or CM) Finance and record signoff placeholder.
+
+ - Wraps ops/finance/multi_provider_reconciler.merge_and_write
+ - Posts entry to aegis-finance-signoffs ConfigMap and writes /tmp/finance_reconcile_result.json
+"""
+import json
+from datetime import datetime
+from ops.finance.multi_provider_reconciler import merge_and_write
+from kubernetes import client, config
+
+CM_NAME = "aegis-finance-signoffs"
+CM_NS = "kube-system"
+
+def k8s_client():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+def record_signoff(csv_path, owner="finance@example.com"):
+    core = k8s_client()
+    try:
+        cm = core.read_namespaced_config_map(CM_NAME, CM_NS)
+        data = cm.data or {}
+    except Exception:
+        data = {}
+    rid = "finance-" + str(int(datetime.utcnow().timestamp()))
+    entry = {"id": rid, "csv": csv_path, "owner": owner, "state":"pending", "ts": datetime.utcnow().isoformat()+"Z"}
+    data[rid] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=CM_NAME), data=data)
+    try:
+        core.replace_namespaced_config_map(CM_NAME, CM_NS, body)
+    except Exception:
+        core.create_namespaced_config_map(CM_NS, body)
+    return entry
+
+if __name__=="__main__":
+    csv = merge_and_write()
+    entry = record_signoff(csv)
+    out = {"csv": csv, "signoff_entry": entry}
+    with open("/tmp/finance_reconcile_result.json","w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote /tmp/finance_reconcile_result.json")
+
*** End Patch
*** Begin Patch
*** Add File:ops/ota/device_canary_injector.py
+#!/usr/bin/env python3
+"""
+Controlled canary injector to simulate failure scenarios in device OTA tests.
+
+ - Issues simulated failure flags to test fleet validation rollback logic
+ - Writes per-device test control records to /tmp/device_canary_controls.json
+"""
+import os
+import json
+from datetime import datetime
+
+CONTROLS = "/tmp/device_canary_controls.json"
+
+def set_device_failure(device_id, fail=True, reason="test_inject"):
+    if os.path.exists(CONTROLS):
+        controls = json.load(open(CONTROLS))
+    else:
+        controls = {}
+    controls[device_id] = {"fail": bool(fail), "reason": reason, "ts": datetime.utcnow().isoformat()+"Z"}
+    with open(CONTROLS,"w") as fh:
+        json.dump(controls, fh, indent=2)
+    return controls[device_id]
+
+def clear_controls():
+    if os.path.exists(CONTROLS):
+        os.remove(CONTROLS)
+    return {}
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--device")
+    p.add_argument("--fail", action="store_true")
+    p.add_argument("--clear", action="store_true")
+    args = p.parse_args()
+    if args.clear:
+        print(clear_controls())
+    else:
+        print(set_device_failure(args.device, args.fail))
+
*** End Patch
*** Begin Patch
*** Add File:ops/sandbox/seccomp_matrix_expand.json
+[
+  {"image":"python:3.11-slim","cmd":"python -c \"print('ok')\""},
+  {"image":"alpine:3.18","cmd":"sh -c \"echo hello\""},
+  {"image":"node:18","cmd":"node -e \"console.log('ok')\""},
+  {"image":"golang:1.20","cmd":"go version"},
+  {"image":"rust:1.72","cmd":"rustc --version"}
+]
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/device_agent.py
+#!/usr/bin/env python3
+"""
+A small agent to run on devices to benchmark quantized models and report results.
+
+ - Exposes simple HTTP endpoint for control (run benchmark, report)
+ - Reports to central collector via POST if QA_COLLECTOR_URL set
+"""
+from flask import Flask, request, jsonify
+import subprocess
+import json
+import os
+from datetime import datetime
+import requests
+
+app = Flask(__name__)
+COLLECTOR = os.environ.get("QA_COLLECTOR_URL","")
+
+def run_benchmark_cmd(model_path, payload_path, iterations=50):
+    # placeholder: operator should implement real runner
+    # simulate results
+    return {"p50":0.02,"p95":0.06,"p99":0.12,"memory_mb":200,"thermal_c":55}
+
+@app.route("/v1/benchmark", methods=["POST"])
+def bench():
+    body = request.json or {}
+    res = run_benchmark_cmd(body.get("model"), body.get("payload"), body.get("iters",50))
+    res["ts"] = datetime.utcnow().isoformat()+"Z"
+    if COLLECTOR:
+        try:
+            requests.post(COLLECTOR, json={"device": os.environ.get("DEVICE_ID","device-1"), "result": res}, timeout=5)
+        except Exception:
+            pass
+    return jsonify(res)
+
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("DEVICE_AGENT_PORT", "8087")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantum/manifest_enforcer_hook.py
+#!/usr/bin/env python3
+"""
+Pre-promotion check: ensure that experiment run has a signed manifest and provenance bundle.
+
+ - Intended as a gating script in release pipelines or runtime promotion hooks
+ - Checks manifest signature (cosign or HSM) and presence of provenance bundle
+"""
+import os
+import json
+import subprocess
+
+def has_signature(manifest_path):
+    sig = manifest_path + ".sig"
+    return os.path.exists(sig)
+
+def has_provenance(manifest_path):
+    # expect bundle with manifest hash in evidence bucket or local dir
+    manifest_hash = None
+    try:
+        manifest_hash = subprocess.check_output(["sha256sum", manifest_path]).decode().split()[0]
+    except Exception:
+        pass
+    # check /tmp/quantum_provenance for matching bundles (simple)
+    prov_dir = "/tmp"
+    for f in os.listdir(prov_dir):
+        if f.startswith("qprov_") or f.endswith(".zip"):
+            # naive check
+            if manifest_hash and manifest_hash in f:
+                return True
+    return False
+
+def main(manifest):
+    ok_sig = has_signature(manifest)
+    ok_prov = has_provenance(manifest)
+    if not ok_sig:
+        print("manifest signature missing")
+        raise SystemExit(2)
+    if not ok_prov:
+        print("provenance bundle missing")
+        raise SystemExit(2)
+    print("manifest & provenance present")
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    args = p.parse_args()
+    main(args.manifest)
+
*** End Patch
*** Begin Patch
*** Add File:ops/metrics/baseline_collector.py
+#!/usr/bin/env python3
+"""
+Collect baseline metrics for quantum KPIs over a sampling period and suggest alert thresholds.
+
+ - Samples aegis_quantum_job_fidelity and shots for configured window
+ - Writes suggested thresholds to /tmp/quantum_thresholds.json
+"""
+import os
+import time
+import requests
+import json
+from datetime import datetime
+
+PROM_URL = os.environ.get("PROM_URL","http://prometheus:9090")
+SAMPLE_S = int(os.environ.get("BASELINE_SAMPLING_S","300"))
+INTERVAL = int(os.environ.get("BASELINE_INTERVAL_S","10"))
+
+def sample(metric, duration=SAMPLE_S):
+    values = []
+    end = time.time() + duration
+    while time.time() < end:
+        try:
+            r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": metric}, timeout=5)
+            r.raise_for_status()
+            data = r.json().get("data",{}).get("result",[])
+            val = float(data[0]["value"][1]) if data else None
+        except Exception:
+            val = None
+        if val is not None:
+            values.append(val)
+        time.sleep(INTERVAL)
+    return values
+
+def suggest_thresholds():
+    fidelity = sample("aegis_quantum_job_fidelity")
+    shots = sample("aegis_quantum_job_shots")
+    import numpy as np
+    th = {}
+    if fidelity:
+        th["fidelity_warn"] = float(max(0.0, np.percentile(fidelity, 5)))
+    if shots:
+        th["shots_warn"] = int(np.percentile(shots, 95))
+    out = {"suggested": th, "collected_at": datetime.utcnow().isoformat()+"Z"}
+    with open("/tmp/quantum_thresholds.json","w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote /tmp/quantum_thresholds.json")
+    return out
+
+if __name__=="__main__":
+    suggest_thresholds()
+
*** End Patch
*** Begin Patch
*** Add File:ops/security/sbom_generator.py
+#!/usr/bin/env python3
+"""
+Generate a lightweight SBOM (packages list) for a given Docker image or workspace.
+
+ - For images: uses 'docker run --rm image pip freeze' heuristic for Python-based images
+ - For repo: walks requirements.txt and package manifests
+ - Writes /tmp/sbom.json
+"""
+import os
+import json
+import subprocess
+
+def sbom_from_requirements(req="requirements.txt"):
+    pkgs = []
+    if os.path.exists(req):
+        for l in open(req):
+            l = l.strip()
+            if l and not l.startswith("#"):
+                pkgs.append(l)
+    out = {"packages": pkgs}
+    with open("/tmp/sbom.json","w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote /tmp/sbom.json")
+    return "/tmp/sbom.json"
+
+def sbom_from_image(image):
+    # best-effort: extract pip freeze by running container
+    try:
+        out = subprocess.check_output(["docker","run","--rm",image,"python","-m","pip","freeze"], stderr=subprocess.STDOUT, timeout=60).decode().splitlines()
+        pkgs = [p for p in out if p]
+    except Exception:
+        pkgs = []
+    with open("/tmp/sbom.json","w") as fh:
+        json.dump({"image": image, "packages": pkgs}, fh, indent=2)
+    print("Wrote /tmp/sbom.json")
+    return "/tmp/sbom.json"
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--req", default="requirements.txt")
+    p.add_argument("--image")
+    args = p.parse_args()
+    if args.image:
+        sbom_from_image(args.image)
+    else:
+        sbom_from_requirements(args.req)
+
*** End Patch
*** Begin Patch
*** Add File:ops/security/secrets_audit.py
+#!/usr/bin/env python3
+"""
+Simple scan of repository for high-entropy strings and common secret patterns.
+Writes /tmp/secrets_audit.json with findings.
+"""
+import os
+import re
+import json
+
+PATTERNS = [
+    re.compile(r"AKIA[0-9A-Z]{16}"),  # AWS keys
+    re.compile(r"-----BEGIN PRIVATE KEY-----"),
+    re.compile(r"AIza[0-9A-Za-z-_]{35}"),  # Google API key
+]
+
+def scan(path="."):
+    findings = []
+    for root, dirs, files in os.walk(path):
+        for f in files:
+            if f.endswith((".pyc",".exe",".bin")):
+                continue
+            try:
+                p = os.path.join(root,f)
+                txt = open(p, errors="ignore").read()
+                for pat in PATTERNS:
+                    for m in pat.finditer(txt):
+                        findings.append({"file": p, "match": m.group(0)[:64]})
+            except Exception:
+                continue
+    with open("/tmp/secrets_audit.json","w") as fh:
+        json.dump(findings, fh, indent=2)
+    print("Wrote /tmp/secrets_audit.json")
+    return findings
+
+if __name__=="__main__":
+    scan()
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/definition_of_done_signoff.md
+Definition of Done â€” Quantum Release Signoff
+==========================================
+
+This runbook lists the minimum artifacts and approvals required to declare a quantum release "production ready".
+
+Mandatory artifacts:
+- HSM evidence bundle + auditor signoff (ConfigMap aegis-auditor-signoffs entry).
+- Signed manifest and manifest.sig for each canonical experiment.
+- Provenance bundle (qprov_*.zip) uploaded to EVIDENCE_BUCKET.
+- Full-scale validation summary (fullscale_summary.json) with 3 consecutive SLO pass runs.
+- Formal proof bundles (pr_formal_proof.json) for all critical invariants.
+- Calibration report (calibration_report.json) for representative hardware.
+- Finance reconciliation CSV and signoff entry in aegis-finance-signoffs.
+- Release bundle created by ops/release/release_packager.py with manifest signature and meta.json.
+
+Approvals required:
+- Security/Infra: written auditor acceptance recorded in aegis-auditor-signoffs.
+- Safety/Verification: formal proofs validated and FMEA mapping recorded.
+- SRE: full-scale validation passes and tuned autoscaler configurations committed.
+- Finance: chargeback reconciliation and signoff present in ConfigMap.
+- Product/Platform Owner: release bundle approved for rollout.
+
+How to mark release done:
+1. Run ops/hsm/live_auditor_session_orchestrator.py during the auditor meeting and persist signoff via ops/hsm/auditor_evidence_recorder.py.
+2. Execute ops/validation/fullscale_validation_runner.py and ensure 3/3 passes.
+3. Ensure ops/formal/pr_hil_prover outputs were uploaded and attach with ops/formal/proof_bundle_collector.py.
+4. Run ops/quantum/hardware_calibration_ci.py and attach calibration_report.json to release.
+5. Run ops/finance/reconcile_signoff_workflow.py and obtain finance signoff entry.
+6. Run ops/release/run_release_and_signoff.py to package artifacts and create a release signoff record.
+
+Keep this doc updated with owner names and contact emails for each role.
+
*** End Patch
*** End Patch
