*** Begin Patch
*** Add File:ops/verifier/premise_normalizer.py
+#!/usr/bin/env python3
+"""
+Premise normalizer & sanitizer for math/hypothesis text produced by LLMs.
+ - Attempts to detect math claims, extract canonical expressions, sanitize and canonicalize them.
+ - Uses SymPy to validate parseable mathematical expressions where appropriate.
+ - Exposes normalize_premise(text) -> {"kind":"math"|"text", "normalized":..., "error": None|str}
+"""
+import re
+from typing import Dict
+
+def looks_like_math(text: str) -> bool:
+    # heuristics: presence of digits and math operators or math verbs
+    if re.search(r"[0-9]", text) and re.search(r"[\+\-\*\/\=\^]", text):
+        return True
+    if any(w in text.lower() for w in ("integrate", "differentiate", "derivative", "solve", "limit", "sum", "product")):
+        return True
+    return False
+
+def extract_first_math_expr(text: str) -> str:
+    # crude extraction: find first substring containing digits/operators up to sentence end
+    m = re.search(r"([0-9A-Za-z\(\)\[\]\+\-\*\/\=\^\.,\s]+)", text)
+    if m:
+        return m.group(1).strip()
+    return text.strip()
+
+def sanitize_expr(expr: str) -> bool:
+    # basic reject of suspicious tokens
+    if "__" in expr or "import" in expr or "os." in expr or "sys." in expr:
+        return False
+    # reject unacceptable characters
+    if re.search(r"[;`<>\\\$]", expr):
+        return False
+    return True
+
+def try_sympy_parse(expr: str):
+    try:
+        import sympy as sp
+        sp.sympify(expr)
+        return True, None
+    except Exception as e:
+        return False, str(e)
+
+def normalize_premise(text: str) -> Dict:
+    """
+    Normalize LLM premise/hypothesis.
+    Returns dict:
+      { "kind": "math" | "text", "normalized": str, "error": None|str }
+    """
+    text = (text or "").strip()
+    if not text:
+        return {"kind":"text","normalized":"","error":"empty"}
+    if looks_like_math(text):
+        expr = extract_first_math_expr(text)
+        if not sanitize_expr(expr):
+            return {"kind":"math","normalized":expr,"error":"rejected_by_sanitizer"}
+        ok, err = try_sympy_parse(expr)
+        if ok:
+            # canonicalize using sympy.srepr or simplify
+            try:
+                import sympy as sp
+                s = sp.sympify(expr, evaluate=True)
+                norm = str(sp.simplify(s))
+                return {"kind":"math","normalized":norm,"error":None}
+            except Exception:
+                return {"kind":"math","normalized":expr,"error":None}
+        else:
+            return {"kind":"math","normalized":expr,"error":f"sympy_parse_error:{err}"}
+    # non-math: return trimmed text
+    return {"kind":"text","normalized":text,"error":None}
+
+if __name__ == "__main__":
+    tests = [
+        "I think 2+2=4.",
+        "The solution is x^2 - 2 = 0 has roots sqrt(2)",
+        "This is not math, just a statement."
+    ]
+    for t in tests:
+        print(t, "->", normalize_premise(t))
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/verifier_entrypoint.py
+#!/usr/bin/env python3
+"""
+Entrypoint wrapper for verification that:
+ - normalizes and sanitizes LLM-generated premises
+ - enforces rejection of malformed premises prior to calling verifier_with_cas
+ - emits a standard response used by verification_gate and agents
+"""
+from ops.verifier.premise_normalizer import normalize_premise
+from ops.verifier.verifier_with_cas import verify_claim
+from typing import List, Dict
+
+def verify_hypothesis(hypothesis: str, snippets: List[Dict], threshold: float = 0.7) -> Dict:
+    """
+    Normalizes hypothesis; returns:
+      {"ok": bool, "method": "cas|nli|mixed", "normalized": str, "error": None|str, "details": {...}}
+    """
+    norm = normalize_premise(hypothesis)
+    if norm.get("error"):
+        # reject malformed math premises outright
+        return {"ok": False, "method": "normalize", "normalized": norm.get("normalized"), "error": norm.get("error"), "details": {}}
+    # if normalized kind is math, pass canonicalized string to verifier_with_cas
+    if norm.get("kind") == "math":
+        # use normalized math string as claim
+        res = verify_claim(norm.get("normalized"), snippets, threshold=threshold)
+        return {"ok": res.get("ok"), "method": res.get("method"), "normalized": norm.get("normalized"), "error": None, "details": res}
+    else:
+        res = verify_claim(norm.get("normalized"), snippets, threshold=threshold)
+        return {"ok": res.get("ok"), "method": res.get("method"), "normalized": norm.get("normalized"), "error": None, "details": res}
+
+if __name__ == "__main__":
+    # quick self-test
+    snips = [{"chunk_text":"2+2 equals 4"}]
+    print(verify_hypothesis("I believe 2+2=4", snips))
+    print(verify_hypothesis("This statement is unsafe; import os; os.system('rm -rf /')", snips))
+
*** End Patch
*** Begin Patch
*** Add File:services/nli_service/app.py
+"""
+Local NLI (entailment) service to host a local model with concurrency control and health checks.
+ - Provides /v1/entail {premise, hypothesis} -> {label, score}
+ - Provides /health for readiness/liveness
+ - Loads transformers pipeline model at startup (configurable by env LOCAL_NLI_MODEL)
+"""
+import os
+import logging
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+import asyncio
+
+LOCAL_MODEL = os.environ.get("LOCAL_NLI_MODEL", "ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli")
+MAX_CONCURRENCY = int(os.environ.get("NLI_MAX_CONCURRENCY", "2"))
+
+logger = logging.getLogger("aegis.nli_service")
+logger.setLevel(logging.INFO)
+
+app = FastAPI(title="Aegis Local NLI Service")
+
+# guarded global
+PIPELINE = None
+SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENCY)
+
+class EntailRequest(BaseModel):
+    premise: str
+    hypothesis: str
+
+@app.on_event("startup")
+async def startup_event():
+    global PIPELINE
+    try:
+        from transformers import pipeline
+        PIPELINE = pipeline("text-classification", model=LOCAL_MODEL, return_all_scores=True)
+        # warmup
+        PIPELINE("Premise </s> Hypothesis")
+        logger.info("Loaded NLI model %s", LOCAL_MODEL)
+    except Exception as e:
+        logger.exception("Failed loading local NLI model: %s", e)
+        PIPELINE = None
+
+@app.get("/health")
+def health():
+    return {"ok": PIPELINE is not None}
+
+@app.post("/v1/entail")
+async def entail(req: EntailRequest):
+    if PIPELINE is None:
+        raise HTTPException(status_code=503, detail="nli_model_unavailable")
+    async with SEMAPHORE:
+        # run in threadpool to avoid blocking event loop
+        import asyncio
+        loop = asyncio.get_running_loop()
+        def run_pipe():
+            return PIPELINE(f"{req.premise} </s> {req.hypothesis}")
+        out = await loop.run_in_executor(None, run_pipe)
+        # find ENTAIL/ SUPPORT class
+        scores = out[0] if isinstance(out, list) else out
+        ent = next((s for s in scores if s["label"].lower().startswith("entail")), None)
+        if ent:
+            return {"label": "ENTAILMENT", "score": float(ent["score"])}
+        # fallback: return best label mapped
+        best = max(scores, key=lambda x: x["score"])
+        lab = best["label"].upper()
+        if lab.startswith("CONTR"):
+            lab = "CONTRADICTION"
+        elif lab.startswith("ENT"):
+            lab = "ENTAILMENT"
+        else:
+            lab = "NEUTRAL"
+        return {"label": lab, "score": float(best["score"])}
+
+if __name__ == "__main__":
+    import uvicorn
+    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("NLI_PORT", "8200")), log_level="info")
+
*** End Patch
*** Begin Patch
*** Add File:k8s/calc_service_hardened.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: calc-service-hardened
+  namespace: aegis-retriever
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: calc-service-hardened
+  template:
+    metadata:
+      labels:
+        app: calc-service-hardened
+      annotations:
+        seccomp.security.alpha.kubernetes.io/pod: "runtime/default"
+    spec:
+      securityContext:
+        runAsUser: 65534   # nobody
+        runAsGroup: 65534
+        fsGroup: 65534
+      containers:
+        - name: calc
+          image: aegis/calc-service:latest
+          imagePullPolicy: IfNotPresent
+          ports:
+            - containerPort: 8100
+          resources:
+            limits:
+              cpu: "1000m"
+              memory: "1Gi"
+            requests:
+              cpu: "250m"
+              memory: "256Mi"
+          securityContext:
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            capabilities:
+              drop: ["ALL"]
+            runAsNonRoot: true
+          volumeMounts:
+            - name: tmp
+              mountPath: /tmp
+              readOnly: false
+      volumes:
+        - name: tmp
+          emptyDir: {}
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: calc-service
+  namespace: aegis-retriever
+spec:
+  selector:
+    app: calc-service-hardened
+  ports:
+    - port: 8100
+      targetPort: 8100
+
*** End Patch
*** Begin Patch
*** Add File:ops/scaling/calc_pool.py
+#!/usr/bin/env python3
+"""
+Lightweight compute queue for CAS invocations to manage concurrency, queuing and simple metrics.
+ - Accepts POST /v1/compute (same payload as calc_service) and enqueues jobs.
+ - Runs a bounded number of worker subprocesses invoking calc_worker_sandbox.py to limit parallel resource use.
+ - Exposes /metrics for Prometheus (jobs_total, jobs_pending, jobs_success, jobs_failed)
+"""
+import os
+import asyncio
+import json
+import uuid
+import time
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+from prometheus_client import start_http_server, Counter, Gauge
+import subprocess
+
+MAX_WORKERS = int(os.environ.get("CALC_POOL_WORKERS", "2"))
+WORKER_SCRIPT = os.environ.get("CALC_WORKER_SCRIPT", os.path.join(os.path.dirname(__file__), "..", "math", "calc_worker_sandbox.py"))
+
+app = FastAPI(title="Aegis Calc Pool")
+
+# metrics
+JOBS_TOTAL = Counter("aegis_calc_jobs_total", "Total CAS jobs")
+JOBS_PENDING = Gauge("aegis_calc_jobs_pending", "Pending CAS jobs")
+JOBS_SUCCESS = Counter("aegis_calc_jobs_success", "Successful CAS jobs")
+JOBS_FAILED = Counter("aegis_calc_jobs_failed", "Failed CAS jobs")
+
+QUEUE = asyncio.Queue()
+
+class CalcRequest(BaseModel):
+    mode: str
+    expr: str
+    precision: int = 50
+
+@app.post("/v1/compute")
+async def compute(req: CalcRequest):
+    job_id = str(uuid.uuid4())
+    payload = {"mode": req.mode, "expr": req.expr, "precision": req.precision}
+    await QUEUE.put((job_id, payload))
+    JOBS_TOTAL.inc()
+    JOBS_PENDING.set(QUEUE.qsize())
+    # return job id immediately; client can poll or use /v1/compute_sync
+    return {"job_id": job_id, "status": "queued"}
+
+@app.post("/v1/compute_sync")
+async def compute_sync(req: CalcRequest):
+    # convenience: run synchronously via local worker
+    try:
+        proc = subprocess.Popen(["python", WORKER_SCRIPT], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
+        stdout, stderr = proc.communicate(input=json.dumps({"mode": req.mode, "expr": req.expr, "precision": req.precision}), timeout=10)
+        if stdout:
+            data = json.loads(stdout)
+            if data.get("ok"):
+                JOBS_SUCCESS.inc()
+            else:
+                JOBS_FAILED.inc()
+            return data
+        else:
+            JOBS_FAILED.inc()
+            raise HTTPException(status_code=500, detail=f"worker_error: {stderr[:200]}")
+    except Exception as e:
+        JOBS_FAILED.inc()
+        raise HTTPException(status_code=500, detail=str(e))
+
+async def worker_loop():
+    while True:
+        job_id, payload = await QUEUE.get()
+        JOBS_PENDING.set(QUEUE.qsize())
+        try:
+            proc = subprocess.Popen(["python", WORKER_SCRIPT], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
+            stdout, stderr = proc.communicate(input=json.dumps(payload), timeout=12)
+            if stdout:
+                data = json.loads(stdout)
+                if data.get("ok"):
+                    JOBS_SUCCESS.inc()
+                else:
+                    JOBS_FAILED.inc()
+            else:
+                JOBS_FAILED.inc()
+        except Exception:
+            JOBS_FAILED.inc()
+        finally:
+            QUEUE.task_done()
+
+@app.on_event("startup")
+async def startup():
+    # start prometheus client
+    start_http_server(int(os.environ.get("METRICS_PORT", "9101")))
+    # spawn worker coroutines
+    for _ in range(MAX_WORKERS):
+        asyncio.create_task(worker_loop())
+
+if __name__ == "__main__":
+    import uvicorn
+    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("CALC_POOL_PORT", "8110")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariant_registry.py
+#!/usr/bin/env python3
+"""
+Invariant registry and authoring helper for domain experts.
+ - Allows adding invariant JSON snippets to ops/formal/continuous_invariants_templates.json
+ - Provides a quick CLI to propose an invariant and run targeted prover to check immediately
+"""
+import os
+import json
+from datetime import datetime
+
+INV_FILE = os.environ.get("CONT_INVARIANTS", "ops/formal/continuous_invariants_templates.json")
+
+def load_invariants():
+    if os.path.exists(INV_FILE):
+        return json.load(open(INV_FILE))
+    return {}
+
+def save_invariants(d):
+    os.makedirs(os.path.dirname(INV_FILE), exist_ok=True)
+    with open(INV_FILE, "w") as fh:
+        json.dump(d, fh, indent=2)
+
+def add_invariant(key, spec):
+    inv = load_invariants()
+    inv[key] = spec
+    save_invariants(inv)
+    return key
+
+def cli_add():
+    key = input("Invariant key (id): ").strip()
+    print("Enter invariant JSON (single line):")
+    line = input().strip()
+    try:
+        spec = json.loads(line)
+    except Exception as e:
+        print("invalid json:", e)
+        return
+    add_invariant(key, spec)
+    print("Added invariant:", key)
+    # attempt targeted prove
+    try:
+        import ops.formal.targeted_prover as tp
+        print("Running targeted prover for", key)
+        tp.run_all(INV_FILE)
+    except Exception:
+        print("targeted prover unavailable")
+
+if __name__ == "__main__":
+    cli_add()
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/benchmark_orchestrator.py
+#!/usr/bin/env python3
+"""
+Orchestrate full math & device benchmarks across models and devices.
+ - Runs expanded math bench, device fleet tests, and aggregates artifacts into /tmp/benchmark_orchestration.json
+ - Optionally uploads to EVIDENCE_BUCKET when configured
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+OUT = "/tmp/benchmark_orchestration.json"
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def run_cmd(cmd, timeout=600):
+    try:
+        subprocess.check_call(cmd, shell=True, timeout=timeout)
+        return {"ok": True}
+    except subprocess.CalledProcessError as e:
+        return {"ok": False, "error": str(e)}
+
+def collect_artifacts(paths):
+    coll = {}
+    for p in paths:
+        if os.path.exists(p):
+            try:
+                coll[p] = open(p).read()
+            except Exception:
+                coll[p] = "<read_error>"
+        else:
+            coll[p] = None
+    return coll
+
+def run_all():
+    summary = {"ts": datetime.utcnow().isoformat()+"Z", "steps": []}
+    # expanded math bench
+    summary["steps"].append({"name":"math_bench_expanded", "result": run_cmd("python ops/ci/math_bench_expanded.py")})
+    # device fleet run
+    summary["steps"].append({"name":"device_fleet", "result": run_cmd("python ops/edge/device_fleet_runner.py")})
+    # continuous fuzz
+    summary["steps"].append({"name":"continuous_fuzz", "result": run_cmd("python ops/formal/continuous_fuzz.py")})
+    # collect artifacts
+    arts = collect_artifacts(["/tmp/math_bench_expanded_results.json", "/tmp/device_fleet_reports.json", "/tmp/continuous_fuzz_counterexamples.jsonl"])
+    summary["artifacts"] = {k:(v[:10000] if isinstance(v,str) else v) for k,v in arts.items()}
+    with open(OUT, "w") as fh:
+        json.dump(summary, fh, indent=2)
+    # optional upload
+    if EVIDENCE_BUCKET:
+        try:
+            import boto3
+            s3 = boto3.client("s3")
+            key = f"bench_orchestrator/{os.path.basename(OUT)}"
+            s3.upload_file(OUT, EVIDENCE_BUCKET, key)
+            summary["uploaded"] = f"s3://{EVIDENCE_BUCKET}/{key}"
+        except Exception as e:
+            summary["upload_error"] = str(e)
+    return summary
+
+if __name__ == "__main__":
+    print(run_all())
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/benchmark_orchestration.yml
+name: Benchmark Orchestration (full)
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 4 * * *' # nightly off-peak
+
+jobs:
+  orchestrate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install datasets transformers sympy mpmath requests prometheus_client
+      - name: Run orchestrator
+        env:
+          LLM_API: ${{ secrets.LLM_API }}
+          DEVICE_LIST: ${{ secrets.DEVICE_LIST }}
+        run: |
+          python ops/ci/benchmark_orchestrator.py || true
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: benchmark-orchestrator
+          path: /tmp/benchmark_orchestration.json
+
*** End Patch
*** End Patch
