*** Begin Patch
*** Add File:ops/hil/onboard_automation.py
+#!/usr/bin/env python3
+"""
+Automate vendor rack onboarding and validation.
+ - Register a vendor rack in the registry
+ - Run health checks and smoke run(s)
+ - Execute N determinism repeats and create a signed determinism summary
+ - Mark rack as "approved_for_cert" in registry if acceptance criteria met
+
+Usage:
+  python ops/hil/onboard_automation.py --adapter <adapter_path> --manifest <manifest.json> --repeats 10
+"""
+import os
+import json
+import argparse
+import subprocess
+from datetime import datetime
+
+from ops.hil.hil_provisioning import register_rack, health_check, create_replay_lock
+
+REG_PATH = os.environ.get("HIL_RACK_REG", "ops/hil/hil_racks.json")
+
+def run_repeats(adapter, manifest, firmware_cmd, repeats):
+    cmd = ["python", "ops/hil/replay_verification_loop.py", "--adapter", adapter, "--manifest", manifest, "--repeats", str(repeats)]
+    if firmware_cmd:
+        cmd += ["--firmware-cmd", firmware_cmd]
+    subprocess.check_call(cmd)
+    # move summary to standardized location if produced
+    s = "/tmp/hil_replay_summary.json"
+    if os.path.exists(s):
+        dst = os.path.join("/tmp", f"hil_replay_summary_{int(datetime.utcnow().timestamp())}.json")
+        os.rename(s, dst)
+        return dst
+    return None
+
+def sign_if_available(path):
+    if os.environ.get("EVIDENCE_SIGN_BACKEND"):
+        try:
+            subprocess.check_call(["python","ops/evidence/sign_with_kms_or_vault.py", path])
+            return path + ".sig"
+        except Exception:
+            return None
+    return None
+
+def approve_rack(name, adapter, manifest, firmware_cmd, repeats=10, score_threshold=80, max_fail_rate=0.05):
+    print("Registering rack", name)
+    reg = register_rack(name, adapter, os.environ.get("HIL_VENDOR_URL"), os.environ.get("HIL_VENDOR_TOKEN"))
+    ok, info = health_check(adapter)
+    print("Health check:", ok, info)
+    if not ok:
+        raise RuntimeError("Health check failed: " + str(info))
+    lock = create_replay_lock(manifest)
+    print("Created replay lock:", lock)
+    summary = run_repeats(adapter, manifest, firmware_cmd, repeats)
+    if not summary:
+        raise RuntimeError("No determinism summary produced")
+    print("Determinism summary:", summary)
+    # analyze summary heuristically
+    sdata = json.load(open(summary))
+    scores = sdata.get("scores", [])
+    failures = sdata.get("failures", [])
+    mean_score = sum(scores)/len(scores) if scores else 0.0
+    fail_rate = len(failures)/repeats if repeats else 1.0
+    print(f"mean_score={mean_score}, fail_rate={fail_rate}")
+    if mean_score >= score_threshold and fail_rate <= max_fail_rate:
+        sig = sign_if_available(summary)
+        # update registry to approved
+        racks = json.load(open(REG_PATH)) if os.path.exists(REG_PATH) else {}
+        racks[name]["approved_for_cert"] = True
+        racks[name]["approval_ts"] = datetime.utcnow().isoformat()+"Z"
+        racks[name]["determinism_summary"] = summary
+        if sig:
+            racks[name]["determinism_summary_sig"] = sig
+        json.dump(racks, open(REG_PATH,"w"), indent=2)
+        print("Rack approved for certification:", name)
+        return True
+    else:
+        print("Rack failed acceptance criteria")
+        return False
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--adapter", required=True)
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--repeats", type=int, default=10)
+    p.add_argument("--firmware-cmd", default=None)
+    p.add_argument("--name", default="unnamed-rack")
+    args = p.parse_args()
+    approve_rack(args.name, args.adapter, args.manifest, args.firmware_cmd, repeats=args.repeats)
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/wcet_k8s_job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-wcet-run
+  namespace: aegis-rt
+spec:
+  template:
+    spec:
+      nodeSelector:
+        aegis/rt: "true"
+      containers:
+        - name: wcet
+          image: ${WCET_IMAGE:-aegis/wcet-runner:latest}
+          command: ["python","/app/ops/rt/wcet_analyzer.py","--cmd","/workspace/bin/control_loop","--samples","1000","--rt"]
+          resources:
+            limits:
+              cpu: "2000m"
+              memory: "2Gi"
+      restartPolicy: Never
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/ssh_wcet_config_example.json
+{
+  "entries": [
+    {"name": "vehicle-node-1", "host": "ubuntu@10.0.0.10", "cmd": "/opt/aegis/bin/control_loop"},
+    {"name": "vehicle-node-2", "host": "ubuntu@10.0.0.11", "cmd": "/opt/aegis/bin/control_loop"}
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/dataset_sharder.py
+#!/usr/bin/env python3
+"""
+Shard a large curated dataset manifest into N shards for distributed training.
+Input: curated manifest JSON with "entries" list.
+Output: shard files manifest_shard_<i>.json
+"""
+import os
+import json
+from math import ceil
+
+def shard(manifest_path, shards=10, out_dir="/tmp/dataset_shards"):
+    os.makedirs(out_dir, exist_ok=True)
+    j = json.load(open(manifest_path))
+    entries = j.get("entries", [])
+    per = max(1, ceil(len(entries)/shards))
+    outs = []
+    for i in range(shards):
+        s = entries[i*per:(i+1)*per]
+        outp = os.path.join(out_dir, f"manifest_shard_{i}.json")
+        json.dump({"shard": i, "count": len(s), "entries": s}, open(outp,"w"), indent=2)
+        outs.append(outp)
+    return outs
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--shards", type=int, default=10)
+    p.add_argument("--out", default="/tmp/dataset_shards")
+    args = p.parse_args()
+    print(shard(args.manifest, args.shards, args.out))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/training_job_template.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-detector-train-{{ shard }}
+  namespace: aegis-ml
+spec:
+  template:
+    spec:
+      containers:
+        - name: trainer
+          image: ${TRAINER_IMAGE:-aegis/detector-trainer:latest}
+          command: ["python","/app/train.py","--manifest","/data/manifest_shard_{{ shard }}.json","--out","/data/model_{{ shard }}.bin"]
+          volumeMounts:
+            - name: data
+              mountPath: /data
+          resources:
+            limits:
+              cpu: "8000m"
+              memory: "32Gi"
+      restartPolicy: Never
+      nodeSelector:
+        aegis/gpu: "true"
+  backoffLimit: 2
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/annotation_pipeline_connector.py
+#!/usr/bin/env python3
+"""
+Robust annotation pipeline connector:
+ - Upload harvested bundle (zip) to annotation vendor or S3
+ - Poll for completion and fetch annotated outputs
+ - Integrates with local mock if ANNOTATION_URL not set
+"""
+import os
+import time
+import json
+import subprocess
+from ops.perception.annotation_api_client import upload_bundle, poll_task
+
+def submit_and_wait(bundle_path, timeout=86400):
+    tid = upload_bundle(bundle_path)
+    print("Submitted annotation bundle, task id:", tid)
+    res = poll_task(tid, timeout=timeout, poll_interval=30)
+    print("Annotation result:", res)
+    if res.get("status") != "done":
+        raise RuntimeError("Annotation timed out or failed")
+    return res.get("result")
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--bundle", required=True)
+    args = p.parse_args()
+    print(submit_and_wait(args.bundle))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/promote_and_deploy.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Promote a model after evaluation and deploy serving + HPA
+#
+MODEL_PATH=${1:-}
+EVAL_SUMMARY=${2:-}
+MODEL_NAME=${3:-detector-promoted}
+
+if [ -z "$MODEL_PATH" ] || [ -z "$EVAL_SUMMARY" ]; then
+  echo "Usage: promote_and_deploy.sh <model_path> <eval_summary.json> [model_name]"; exit 2
+fi
+
+python ops/perception/promote_model_ci.py --model "$MODEL_PATH" --eval "$EVAL_SUMMARY"
+
+# create deployment manifest using scaffold (simple kubectl apply)
+cat > /tmp/${MODEL_NAME}_deployment.yaml <<EOF
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: ${MODEL_NAME}
+  namespace: aegis-ml
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: ${MODEL_NAME}
+  template:
+    metadata:
+      labels:
+        app: ${MODEL_NAME}
+    spec:
+      containers:
+      - name: detector
+        image: ${MODEL_PATH}
+        resources:
+          limits:
+            cpu: "2000m"
+            memory: "4Gi"
+EOF
+
+kubectl apply -f /tmp/${MODEL_NAME}_deployment.yaml
+
+# apply HPA
+kubectl apply -f ops/perception/serving_hpa.yaml || true
+echo "Model promoted and deployed: $MODEL_NAME"
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/serving_hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: detector-serving-hpa
+  namespace: aegis-ml
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: detector-promoted
+  minReplicas: 2
+  maxReplicas: 20
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/sme_review_workflow.yml
+name: SME SMT Onboard & Prove
+on:
+  pull_request:
+    paths:
+      - 'ops/formal/onboard_submissions/**'
+
+jobs:
+  run-prover:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install Z3
+        run: |
+          sudo apt-get update && sudo apt-get install -y z3 || true
+      - name: Run prover batch on submission
+        run: |
+          python ops/formal/prover_batch_runner.py
+      - name: Upload proof artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: proof-artifacts
+          path: /tmp/verifier_logs/*.proof.json
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/create_kms_key.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create AWS KMS key and minimal policy for CI signing (example)
+#
+if [ -z "${AWS_REGION:-}" ] || [ -z "${AWS_PROFILE:-}" ]; then
+  echo "Set AWS_REGION and AWS_PROFILE"; exit 2
+fi
+
+KEY_DESC="Aegis evidence signing key"
+KEY_ID=$(aws kms create-key --description "$KEY_DESC" --query KeyMetadata.KeyId --output text --region "$AWS_REGION" --profile "$AWS_PROFILE")
+echo "Created KMS key: $KEY_ID"
+aws kms create-alias --alias-name alias/aegis-evidence --target-key-id "$KEY_ID" --region "$AWS_REGION" --profile "$AWS_PROFILE"
+echo "Alias created: alias/aegis-evidence"
+echo "Export AWS_KMS_KEY_ID=$KEY_ID"
+
*** End Patch
*** Begin Patch
*** Add File:ops/sre/grafana_dashboard_hil.json
+{
+  "annotations": {
+    "list": []
+  },
+  "panels": [
+    {
+      "type": "graph",
+      "title": "HIL Determinism Score",
+      "targets": [
+        {
+          "expr": "aegis_hil_determinism_score",
+          "legendFormat": "{{rack}}"
+        }
+      ]
+    },
+    {
+      "type": "table",
+      "title": "Recent HIL Jobs",
+      "targets": [
+        {
+          "expr": "aegis_hil_jobs_total",
+          "legendFormat": "jobs"
+        }
+      ]
+    }
+  ],
+  "title": "Aegis HIL Dashboard"
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/hardware_integration/check_hil_presence.py
+#!/usr/bin/env python3
+"""
+Ensure a vendor rack is reachable and has an idle slot.
+Used by CI gate to fail fast if hardware is required but not available.
+"""
+import os
+import sys
+import importlib
+
+ADAPTER = os.environ.get("HIL_VENDOR_ADAPTER", "ops.hil.vendor_adapter_rest.RESTVendorAdapter")
+
+def load_adapter(path):
+    mod, cls = path.rsplit(".",1)
+    m = importlib.import_module(mod)
+    C = getattr(m, cls)
+    return C()
+
+def main():
+    adapter = load_adapter(ADAPTER)
+    backends = adapter.list_backends()
+    if not backends:
+        print("No racks/backends reported by adapter; failing")
+        sys.exit(2)
+    idle = [b for b in backends if b.get("status") in ("idle","available")]
+    if not idle:
+        print("No idle slots found; failing")
+        sys.exit(2)
+    print("Found idle backend:", idle[0])
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hardware_integration_gate.yml
+name: Hardware Integration Gate
+on:
+  workflow_dispatch:
+
+jobs:
+  hw-check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Check HIL presence
+        env:
+          HIL_VENDOR_ADAPTER: ${{ secrets.HIL_VENDOR_ADAPTER }}
+          HIL_VENDOR_URL: ${{ secrets.HIL_VENDOR_URL }}
+          HIL_VENDOR_TOKEN: ${{ secrets.HIL_VENDOR_TOKEN }}
+        run: |
+          python ops/tests/hardware_integration/check_hil_presence.py
+
*** End Patch
*** Begin Patch
*** Add File:ops/sre/runbook_hil_onboarding.md
+HIL Onboarding Runbook (summary)
+================================
+
+1. Preconditions
+   - Vendor endpoint and credentials (HIL_VENDOR_URL, HIL_VENDOR_TOKEN)
+   - Manifest(s) for representative scenarios
+   - Evidence signing backend configured (optional for demo)
+
+2. Steps
+   - Run: python ops/hil/onboard_automation.py --adapter <adapter> --manifest <manifest> --repeats 10 --name <rack-name>
+   - Inspect /tmp/hil_replay_summary_*.json and determinism reports
+   - Verify firmware snapshots were saved and hashed
+   - Mark rack as approved in ops/hil/hil_racks.json
+
+3. Acceptance
+   - mean determinism score >= 80, failure rate < 5%
+   - Signed determinism summary present
+   - Health check stable for 24h
+
*** End Patch
*** End Patch
