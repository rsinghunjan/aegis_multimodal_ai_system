*** Begin Patch
*** Add File:ops/hil/certify_rack.py
+#!/usr/bin/env python3
+"""
+Certify a vendor HIL rack for use in certification flows.
+ - Runs onboarding health checks and determinism repeats (replay_verification_loop)
+ - Validates determinism summary against thresholds
+ - Uploads summary and artifacts to S3 evidence store and indexes them
+ - Marks rack approved in the registry (ops/hil/hil_racks.json)
+
+Usage:
+  python ops/hil/certify_rack.py --adapter <adapter> --manifest <manifest.json> --name <rack-name> --repeats 10 --s3-bucket my-evidence-bucket
+"""
+import os
+import argparse
+import json
+import subprocess
+from datetime import datetime
+
+from ops.hil.hil_provisioning import register_rack, health_check, create_replay_lock
+
+REG_PATH = os.environ.get("HIL_RACK_REG", "ops/hil/hil_racks.json")
+
+def sign_and_upload(path, s3_bucket=None):
+    sig = None
+    if os.environ.get("EVIDENCE_SIGN_BACKEND"):
+        try:
+            subprocess.check_call(["python", "ops/evidence/sign_with_kms_or_vault.py", path])
+            sig = path + ".sig"
+        except Exception:
+            sig = None
+    if s3_bucket:
+        try:
+            # uses s3_retention_uploader
+            subprocess.check_call(["python", "ops/evidence/s3_retention_uploader.py", "--pattern", path, "--bucket", s3_bucket])
+        except Exception as e:
+            print("S3 upload failed:", e)
+    return sig
+
+def certify(adapter, manifest, name, repeats, firmware_cmd, s3_bucket, score_threshold=80, max_fail_rate=0.05):
+    print("Registering rack:", name)
+    register_rack(name, adapter, os.environ.get("HIL_VENDOR_URL"), os.environ.get("HIL_VENDOR_TOKEN"))
+    ok, info = health_check(adapter)
+    if not ok:
+        raise RuntimeError("Health check failed: " + str(info))
+    lock = create_replay_lock(manifest)
+    print("Created lock:", lock)
+    # Run repeats
+    subprocess.check_call(["python", "ops/hil/replay_verification_loop.py", "--adapter", adapter, "--manifest", manifest, "--repeats", str(repeats)])
+    # Find latest summary
+    cand = sorted([p for p in os.listdir("/tmp") if p.startswith("hil_replay_summary") and p.endswith(".json")], reverse=True)
+    if not cand:
+        raise RuntimeError("No determinism summary produced")
+    summary = os.path.join("/tmp", cand[0])
+    print("Summary produced:", summary)
+    sdata = json.load(open(summary))
+    scores = sdata.get("scores", [])
+    failures = sdata.get("failures", [])
+    mean_score = sum(scores)/len(scores) if scores else 0.0
+    fail_rate = len(failures)/repeats if repeats else 1.0
+    print("Mean score:", mean_score, "Fail rate:", fail_rate)
+    if mean_score >= score_threshold and fail_rate <= max_fail_rate:
+        sig = sign_and_upload(summary, s3_bucket)
+        # record approval in registry
+        reg = {}
+        if os.path.exists(REG_PATH):
+            reg = json.load(open(REG_PATH))
+        reg[name] = {"approved_for_cert": True, "approved_at": datetime.utcnow().isoformat()+"Z", "summary": summary, "sig": sig}
+        json.dump(reg, open(REG_PATH,"w"), indent=2)
+        print("Rack certified and recorded in", REG_PATH)
+        return True
+    else:
+        print("Rack failed certification criteria")
+        return False
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--adapter", required=True)
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--name", required=True)
+    p.add_argument("--repeats", type=int, default=10)
+    p.add_argument("--firmware-cmd", default=None)
+    p.add_argument("--s3-bucket", default=None)
+    args = p.parse_args()
+    certify(args.adapter, args.manifest, args.name, args.repeats, args.firmware_cmd, args.s3_bucket)
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/wcet_stress_harness.py
+#!/usr/bin/env python3
+"""
+RT stress harness to run on-vehicle or on-node to exercise timing under load.
+ - Runs stress-ng and cyclictest to create realistic load and measure latencies.
+ - Produces JSON report with p50/p95/p99 and missed deadlines.
+"""
+import subprocess
+import json
+import time
+from datetime import datetime
+
+OUT="/tmp/rt_stress_report.json"
+
+def run_stress(duration_s=60):
+    # Run a background stress workload (cpu) during cyclictest measurement
+    try:
+        # Start stress-ng in background
+        sproc = subprocess.Popen(["stress-ng", "-c", "4", "--timeout", str(duration_s)+"s"])
+    except Exception:
+        sproc = None
+    # Run cyclictest to measure latencies
+    try:
+        proc = subprocess.run(["cyclictest", "-p", "80", "-n", "-t", "4", "-l", str(duration_s), "-i", "1000"], capture_output=True, text=True, timeout=duration_s+20)
+        out = proc.stdout + proc.stderr
+    except Exception as e:
+        out = str(e)
+    if sproc:
+        sproc.terminate()
+    # Heuristic parse
+    report = {"ts": datetime.utcnow().isoformat()+"Z", "raw": out}
+    # look for "max" or "p99" lines
+    for line in out.splitlines():
+        if "max" in line and "min" in line:
+            report["summary"] = line
+    with open(OUT,"w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote", OUT)
+    return OUT
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--duration", type=int, default=60)
+    args = p.parse_args()
+    run_stress(args.duration)
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/rt_wcet_validation.yml
+name: RT WCET Fleet Validation
+on:
+  workflow_dispatch:
+
+jobs:
+  run-wcet:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Run remote WCET runner
+        env:
+          SSH_CONFIG: ${{ secrets.WCET_SSH_CONFIG }}
+        run: |
+          python ops/rt/remote_wcet_runner.py --config ops/rt/ssh_wcet_config_example.json
+      - name: Validate p99 threshold
+        run: |
+          if [ -f /tmp/wcet_aggregate_remote.json ]; then
+            python - <<PY
+import json
+data=json.load(open('/tmp/wcet_aggregate_remote.json'))
+for r in data.get('reports',[]):
+    p99=r.get('p99') or r.get('p99_ms') or 0
+    if p99 and float(p99) > 0.2:
+        print('p99 too high', p99); raise SystemExit(2)
+print('p99 ok')
+PY
+          else
+            echo "No aggregate found" ; exit 2
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/annotation_result_ingest.py
+#!/usr/bin/env python3
+"""
+Poll annotation results (local or S3) and ingest annotated datasets into the curated dataset.
+ - For each completed annotation task, download/collect annotated JSONL and merge into curated manifest
+ - Trigger dataset sharding and submit training jobs via the training launcher
+"""
+import os
+import json
+import subprocess
+from glob import glob
+from datetime import datetime
+
+ANNOTATION_TASK_DIR = os.environ.get("ANNOTATION_TASK_DIR", "/tmp/annotation_tasks")
+CURATED_MANIFEST = os.environ.get("CURATED_MANIFEST", "/tmp/curated_dataset/manifest.json")
+
+def ingest_local_tasks():
+    os.makedirs(os.path.dirname(CURATED_MANIFEST), exist_ok=True)
+    tasks = glob(os.path.join(ANNOTATION_TASK_DIR, "*.json"))
+    merged = []
+    for t in tasks:
+        try:
+            j = json.load(open(t))
+        except Exception:
+            continue
+        if j.get("status") != "done":
+            continue
+        res = j.get("result")
+        # append result entries to curated manifest
+        if res and os.path.exists(res):
+            for l in open(res):
+                merged.append(json.loads(l))
+        # archive task
+        os.rename(t, t + ".ingested")
+    # merge into curated manifest
+    base = {"ts": datetime.utcnow().isoformat()+"Z", "entries": merged}
+    json.dump(base, open(CURATED_MANIFEST,"w"), indent=2)
+    print("Wrote curated manifest:", CURATED_MANIFEST)
+    # shard and submit training
+    subprocess.check_call(["python","ops/perception/dataset_sharder.py","--manifest",CURATED_MANIFEST,"--shards","4"])
+    # launch training jobs for shards
+    for i in range(4):
+        shard = f"/tmp/dataset_shards/manifest_shard_{i}.json"
+        subprocess.check_call(["python","ops/perception/train_launcher.py","--shard",shard,"--image","${TRAINER_IMAGE:-aegis/detector-trainer:latest}"])
+
+if __name__ == "__main__":
+    ingest_local_tasks()
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/train_launcher.py
+#!/usr/bin/env python3
+"""
+Launch training jobs in Kubernetes for dataset shards.
+ - Generates a Job manifest per shard using a simple Jinja-like template and applies via kubectl or the Kubernetes API (kubectl fallback).
+ - Requires kubectl configured for the target cluster/namespace.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+TEMPLATE = """apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-train-{{shard_id}}-{{ts}}
+  namespace: aegis-ml
+spec:
+  template:
+    spec:
+      containers:
+        - name: trainer
+          image: {{image}}
+          command: ["python","/app/train.py","--manifest","/data/{{shard_file}}","--out","/data/model_{{shard_id}}.bin"]
+          volumeMounts:
+            - name: data
+              mountPath: /data
+      restartPolicy: Never
+  backoffLimit: 2
+"""
+
+def render(shard_file, image):
+    shard_id = os.path.basename(shard_file).split("_")[-1].split(".")[0]
+    ts = datetime.utcnow().strftime("%Y%m%d%H%M%S")
+    return TEMPLATE.replace("{{shard_id}}", shard_id).replace("{{ts}}", ts).replace("{{image}}", image).replace("{{shard_file}}", os.path.basename(shard_file))
+
+def launch(shard_file, image):
+    manifest = render(shard_file, image)
+    path = f"/tmp/train_job_{os.path.basename(shard_file)}.yaml"
+    open(path,"w").write(manifest)
+    subprocess.check_call(["kubectl","apply","-f",path])
+    print("Launched training job for", shard_file)
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--shard", required=True)
+    p.add_argument("--image", required=True)
+    args = p.parse_args()
+    launch(args.shard, args.image)
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/check_invariants_registry.py
+#!/usr/bin/env python3
+"""
+CI helper that enforces that a minimum set of high-priority invariants are onboarded and have proof artifacts.
+ - Reads ops/formal/invariant_registry_high_priority.json (list of ids and smt paths)
+ - Verifies a proof artifact exists in VERIF_LOG_DIR for each high priority invariant
+ - Exits non-zero if an invariant lacks proof or a monitored fallback declaration
+"""
+import os
+import json
+from glob import glob
+
+REG = os.environ.get("INVARIANT_REG", "ops/formal/invariant_registry_high_priority.json")
+VERIF_LOG_DIR = os.environ.get("VERIF_LOG_DIR", "/tmp/verifier_logs")
+
+def main():
+    if not os.path.exists(REG):
+        print("Invariant registry missing:", REG); return 2
+    reg = json.load(open(REG))
+    failures = []
+    for iid, info in reg.items():
+        if info.get("priority","").lower() != "high":
+            continue
+        smt = info.get("file")
+        # check for proof artifact
+        proof_files = glob(os.path.join(VERIF_LOG_DIR, os.path.basename(smt) + ".proof.json"))
+        if proof_files:
+            continue
+        # allow monitored fallback
+        if info.get("monitored", False):
+            continue
+        failures.append(iid)
+    if failures:
+        print("Missing proofs or monitors for invariants:", failures)
+        return 2
+    print("All high-priority invariants covered")
+    return 0
+
+if __name__ == "__main__":
+    raise SystemExit(main())
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/create_oidc_role.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create an AWS IAM role for GitHub Actions OIDC trust (example).
+# Use aws cli configured with administrative credentials.
+#
+ROLE_NAME=${1:-aegis-github-oidc-role}
+POLICY_NAME=${2:-AegisOIDCPolicy}
+ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
+
+TRUST_POLICY=$(cat <<EOF
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Effect": "Allow",
+      "Principal": {
+        "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/token.actions.githubusercontent.com"
+      },
+      "Action": "sts:AssumeRoleWithWebIdentity",
+      "Condition": {
+        "StringEquals": {
+          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
+        }
+      }
+    }
+  ]
+}
+EOF
+)
+
+echo "$TRUST_POLICY" > /tmp/trust.json
+aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file:///tmp/trust.json
+echo "Created role $ROLE_NAME"
+echo "Attach least-privilege policies separately; this script only creates role trust."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/staging_integration.yml
+name: Staging Integration & Load Test
+on:
+  workflow_dispatch:
+
+jobs:
+  staging:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup-kind
+        uses: engineerd/setup-kind@v0.5.0
+      - name: Create kind cluster
+        run: |
+          kind create cluster --name aegis-staging
+      - name: Install Helm charts
+        run: |
+          helm install aegis-model-server charts/model-server || helm upgrade --install aegis-model-server charts/model-server
+          helm install aegis-rt-node charts/rt-node || helm upgrade --install aegis-rt-node charts/rt-node
+      - name: Wait for pods
+        run: |
+          kubectl wait --for=condition=ready pod -l app=aegis-model-server --timeout=120s || true
+      - name: Run integration tests
+        run: |
+          chmod +x tests/integration/run_integration_tests.sh
+          ./tests/integration/run_integration_tests.sh
+      - name: Run load test
+        run: |
+          chmod +x ops/sre/loadtest.sh
+          ./ops/sre/loadtest.sh
+      - name: Tear down
+        if: always()
+        run: kind delete cluster --name aegis-staging
+
*** End Patch
*** Begin Patch
*** Add File:ops/sre/loadtest.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Simple load test that calls the model-server health endpoint to generate load.
+#
+ENDPOINT=${1:-http://127.0.0.1:8080/health}
+DURATION=${2:-30}
+RATE=${3:-50}
+
+echo "Running simple load test against $ENDPOINT for $DURATION seconds at rate $RATE rps"
+end=$((SECONDS + DURATION))
+while [ $SECONDS -lt $end ]; do
+  for i in $(seq 1 $RATE); do
+    curl -sS -o /dev/null -w "%{http_code}" "$ENDPOINT" &
+  done
+  wait
+  sleep 1
+done
+echo "Load test complete"
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/staging/lab_manager.py
+#!/usr/bin/env python3
+"""
+Staging lab manager to register test hardware (nodes) into a simple registry and orchestrate smoke tests.
+ - Allows CI to know which test nodes are available and run hardware tests via SSH.
+"""
+import os
+import json
+from datetime import datetime
+
+REG="/tmp/staging_lab_registry.json"
+
+def register_node(name, host, tags=None):
+    reg = {}
+    if os.path.exists(REG):
+        reg = json.load(open(REG))
+    reg[name] = {"host": host, "tags": tags or [], "registered_at": datetime.utcnow().isoformat()+"Z"}
+    json.dump(reg, open(REG,"w"), indent=2)
+    print("Registered node", name)
+
+def list_nodes():
+    if not os.path.exists(REG):
+        print("No nodes registered")
+        return
+    print(open(REG).read())
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--register", nargs=2, metavar=("name","host"))
+    p.add_argument("--list", action="store_true")
+    args = p.parse_args()
+    if args.register:
+        register_node(args.register[0], args.register[1])
+    if args.list:
+        list_nodes()
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/serving_canary.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: detector-canary
+  namespace: aegis-ml
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: detector-canary
+  template:
+    metadata:
+      labels:
+        app: detector-canary
+    spec:
+      containers:
+        - name: detector
+          image: registry/detector:canary
+          ports:
+            - containerPort: 8080
+          resources:
+            limits:
+              cpu: "2000m"
+              memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/check_staging_hardware_and_run_gate.py
+#!/usr/bin/env python3
+"""
+CI gate that ensures staging hardware is registered and runs a quick hardware-targeted smoke test.
+ - Reads staging_lab_registry.json
+ - SSHs to a node and runs a smoke test script (if provided)
+ - Fails if no nodes registered or smoke test fails
+"""
+import os
+import json
+import subprocess
+from glob import glob
+
+REG="/tmp/staging_lab_registry.json"
+
+def main():
+    if not os.path.exists(REG):
+        print("No staging lab registry found; failing"); return 2
+    reg = json.load(open(REG))
+    if not reg:
+        print("No nodes registered; failing"); return 2
+    # pick first node
+    name, info = next(iter(reg.items()))
+    host = info.get("host")
+    print("Using node", name, host)
+    # run remote quick check (assumes passwordless SSH)
+    try:
+        subprocess.check_call(["ssh", host, "echo ok && uname -a"], timeout=30)
+    except Exception as e:
+        print("Remote smoke test failed:", e); return 2
+    print("Staging hardware smoke test passed")
+    return 0
+
+if __name__ == "__main__":
+    raise SystemExit(main())
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/staging_hardware_gate.yml
+name: Staging Hardware Gate
+on:
+  workflow_dispatch:
+
+jobs:
+  staging-hw:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Check staging hardware
+        run: |
+          python ops/ci/check_staging_hardware_and_run_gate.py
+
*** End Patch
*** End Patch
