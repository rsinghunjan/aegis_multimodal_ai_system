*** Begin Patch
*** Add File:ops/hil/wiring_checklist.md
+Acme HIL Onboarding & Wiring Checklist
+=====================================
+
+Goal
+----
+Walk an operator through the final wiring/verification steps to onboard a real Acme HIL rack and produce certifiable determinism evidence.
+
+Preconditions
+- ACME_HIL_URL and ACME_HIL_TOKEN (store in Vault and as GitHub Actions secrets)
+- EVIDENCE_BUCKET (S3) and evidence signing configured (AWS KMS or Vault) or ability to produce .sig files for demo
+- Representative replay manifest(s) in ops/hil/playbook/
+- CI runner or operator machine with network access to vendor and Kubernetes cluster (if needed)
+
+Step 0 — Quick validations
+- Ensure system time is synced (NTP) on operator machine and vendor rack.
+- Ensure firmware management policy: capture pre/post firmware snapshot commands from vendor.
+
+Step 1 — Smoke check against vendor
+1. Configure env (locally or in CI):
+   export ACME_HIL_URL="https://acme.example.com"
+   export ACME_HIL_TOKEN="...(stored in Vault/Secret manager)..."
+
+2. Run quick adapter smoke:
+   python -c "from ops.hil.adapters.acme_hil_adapter import AcmeHILAdapter; print(AcmeHILAdapter().list_backends())"
+
+Step 2 — Health & prepare
+1. Run health check via onboarding helper:
+   python ops/hil/onboard_automation.py --adapter ops.hil.adapters.acme_hil_adapter.AcmeHILAdapter --manifest ops/hil/playbook/example_replay_manifest.json --repeats 1 --name acme-rack-1
+2. Confirm vendor reported idle slot(s) and firmware snapshot step works.
+
+Step 3 — Determinism repeats & certification
+1. From operator machine (or via CI with secrets):
+   python ops/hil/certify_rack.py --adapter ops.hil.adapters.acme_hil_adapter.AcmeHILAdapter --manifest ops/hil/playbook/example_replay_manifest.json --name acme-rack-1 --repeats 12 --s3-bucket my-evidence-bucket
+2. Review produced summary in /tmp/hil_replay_summary_*.json — check mean score and failures.
+3. If EVIDENCE_SIGN_BACKEND configured, summary will be signed and uploaded. Otherwise, sign via ops/governance/kms_sign_helper.py or manually create .sig.
+
+Step 4 — Indexing & registry
+1. Confirm ops/hil/hil_racks.json lists the rack and approved_for_cert = true
+2. Confirm evidence index DB has an entry and S3 index exists (s3_retention_uploader created index).
+
+Step 5 — CI integration & gating
+1. Add secrets to GitHub repo: ACME_HIL_URL, ACME_HIL_TOKEN, AEGIS_OIDC_ROLE_ARN, AWS_KMS_KEY_ID, EVIDENCE_BUCKET.
+2. Trigger .github/workflows/hil_onboard_acme.yml to reproduce certification via CI.
+3. Ensure production gating workflows (production_gates.yml) accept evidence from this rack (registry matching).
+
+Notes / Troubleshooting
+- If vendor returns nonstandard artifact URLs, add normalization in ops/hil/adapters/acme_hil_adapter.py.
+- If deterministic_verifier fails due to environment variance, collect hw telemetry and rerun; consider reducing ambient variance (thermal, power).
+- Keep replay manifests immutable and use create_replay_lock() to prevent accidental re-runs with altered manifests.
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/onboard_runbook.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Operator helper to run HIL onboarding/certify steps locally (wraps certify_rack).
+#
+# Usage:
+#   ACME_HIL_URL=... ACME_HIL_TOKEN=... EVIDENCE_BUCKET=... ./ops/hil/onboard_runbook.sh acme-rack-1 ops/hil/playbook/example_replay_manifest.json
+
+RACK_NAME=${1:-acme-rack-1}
+MANIFEST=${2:-ops/hil/playbook/example_replay_manifest.json}
+REPEATS=${3:-12}
+S3_BUCKET=${EVIDENCE_BUCKET:-}
+
+echo "Starting onboarding for rack: $RACK_NAME with manifest: $MANIFEST"
+echo "Repeats: $REPEATS"
+
+if [ -z "${ACME_HIL_URL:-}" ] || [ -z "${ACME_HIL_TOKEN:-}" ]; then
+  echo "ACME_HIL_URL and ACME_HIL_TOKEN must be set (store in Vault or export locally)"; exit 2
+fi
+
+python ops/hil/certify_rack.py --adapter ops.hil.adapters.acme_hil_adapter.AcmeHILAdapter --manifest "$MANIFEST" --name "$RACK_NAME" --repeats "$REPEATS" --s3-bucket "$S3_BUCKET"
+
+echo "Onboarding run complete. Check /tmp for hil_replay_summary_*.json and cert record ops/hil/hil_racks.json"
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/rt_rollout_script.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# RT rollout helper: build RT image, optionally push, label nodes, deploy via helm, run WCET job and collect results.
+#
+# Usage:
+#   ./ops/rt/rt_rollout_script.sh <registry/repo:tag>
+
+IMAGE=${1:-aegis/rt-node:stable}
+
+echo "Building RT image: $IMAGE"
+./ops/rt/build_rt_image.sh "$IMAGE"
+
+echo "Labeling candidate nodes for RT workloads (requires kubectl configured)"
+./ops/rt/label_nodes.sh "vehicle=true" "aegis/rt=true" || true
+
+echo "Deploying RT node helm chart (ensure chart values correct)"
+helm upgrade --install aegis-rt-node charts/rt-node -f charts/rt-node/values.prod.yaml || true
+
+echo "Launching WCET job on k8s (targets labeled RT nodes)"
+kubectl apply -f ops/rt/wcet_k8s_job.yaml || true
+
+echo "If you have remote vehicle hosts configured in ops/rt/ssh_wcet_config_example.json, run remote_wcet_runner.py to collect reports"
+python ops/rt/remote_wcet_runner.py --config ops/rt/ssh_wcet_config_example.json || true
+
+echo "Aggregate expected at /tmp/wcet_aggregate_remote.json — sign/upload if EVIDENCE_SIGN_BACKEND set"
+if [ -f /tmp/wcet_aggregate_remote.json ] && [ -n "${EVIDENCE_SIGN_BACKEND:-}" ]; then
+  python ops/evidence/s3_retention_uploader.py --pattern /tmp/wcet_aggregate_remote.json --bucket "${EVIDENCE_BUCKET:-}" || true
+fi
+
+echo "RT rollout script complete. Inspect /tmp/wcet_aggregate_remote.json"
+
*** End Patch
*** Begin Patch
*** Add File:tests/artifacts/example_evaluation_summary.json
+{
+  "model": "registry/detector:example-v1",
+  "dataset": "curated/2026-01-09",
+  "metrics": {
+    "overall": {
+      "precision": 0.942,
+      "recall": 0.915,
+      "f1": 0.928
+    },
+    "scenarios": {
+      "urban_day": {"precision": 0.95, "recall": 0.93},
+      "highway_night": {"precision": 0.89, "recall": 0.86},
+      "occlusion_longtail": {"precision": 0.78, "recall": 0.74}
+    }
+  },
+  "evaluation_ts": "2026-01-09T12:00:00Z",
+  "signature": "",
+  "provenance": {
+    "trainer_job": "aegis-train-20260109-1234",
+    "commit": "deadbeef",
+    "evidence_index": "/tmp/evidence_index.json"
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:tests/artifacts/example_evaluation_summary.json.sig.b64
+VGhpcyBpcyBhIGRlbW8gYmFzZTY0IHNpZ25hdHVyZSBmb3IgZXhhbXBsZSBldmFsdWF0aW9uIHN1bW1hcnku
+
*** End Patch
*** Begin Patch
*** Add File:tests/artifacts/mock_proof/obstacle_stop.proof.json
+{
+  "invariant_id": "emergency_stop_invariant",
+  "smt_file": "ops/formal/smt_examples/emergency_stop_invariant.smt2",
+  "proved": true,
+  "solver": "z3",
+  "timestamp": "2026-01-09T12:15:00Z",
+  "notes": "Mock proof artifact for CI gating demo. In real runs produced by prover_batch_runner."
+}
+
*** End Patch
*** Begin Patch
*** Add File:tests/artifacts/mock_proof/obstacle_stop.proof.json.sig.b64
+TW9jayBzaWduYXR1cmUgYmFzZTY0IGZvciBtb2NrIHByb29mIGFydGlmYWN0Lg==
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/mock_proof_generator.py
+#!/usr/bin/env python3
+"""
+Create mock proof artifact and base64 signature for local CI testing.
+ - Writes tests/artifacts/mock_proof/<id>.proof.json and .sig.b64
+ - If EVIDENCE_SIGN_BACKEND and AWS creds are present, will attempt to sign using KMS (via kms_sign_helper.py)
+"""
+import os
+import json
+import base64
+from datetime import datetime
+import subprocess
+
+OUT_DIR = "tests/artifacts/mock_proof"
+os.makedirs(OUT_DIR, exist_ok=True)
+
+def create_mock(invariant_id):
+    p = {
+        "invariant_id": invariant_id,
+        "smt_file": f"ops/formal/smt_examples/{invariant_id}.smt2",
+        "proved": True,
+        "solver": "z3-mock",
+        "timestamp": datetime.utcnow().isoformat() + "Z",
+        "notes": "Auto-generated mock proof for CI gating"
+    }
+    out = os.path.join(OUT_DIR, f"{invariant_id}.proof.json")
+    with open(out, "w") as fh:
+        json.dump(p, fh, indent=2)
+    # attempt real signing if configured
+    sig_b64_path = out + ".sig.b64"
+    if os.environ.get("EVIDENCE_SIGN_BACKEND") and os.environ.get("AWS_KMS_KEY_ID"):
+        try:
+            subprocess.check_call(["python", "ops/governance/kms_sign_helper.py", "--file", out, "--key-id", os.environ.get("AWS_KMS_KEY_ID")])
+            # write base64 encoded copy for portability
+            with open(out + ".sig", "rb") as fh:
+                sig = fh.read()
+            with open(sig_b64_path, "wb") as fh:
+                fh.write(base64.b64encode(sig))
+            print("Wrote signed proof and base64 copy")
+            return out, out + ".sig"
+        except Exception as e:
+            print("KMS signing failed:", e)
+    # fallback: write simple base64 placeholder
+    with open(sig_b64_path, "wb") as fh:
+        fh.write(base64.b64encode(b"mock-proof-sig"))
+    print("Wrote mock proof and .sig.b64")
+    return out, sig_b64_path
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--id", default="obstacle_stop")
+    args = p.parse_args()
+    print(create_mock(args.id))
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/generate_example_signed_artifact.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Generate example evaluation summary and sign it (KMS preferred), else create a base64 .sig.b64 placeholder.
+#
+OUT_JSON="tests/artifacts/example_evaluation_summary.json"
+OUT_SIG="${OUT_JSON}.sig"
+
+if [ -n "${AWS_KMS_KEY_ID:-}" ]; then
+  echo "Attempting to sign with AWS KMS (requires AWS creds via OIDC or env)..."
+  python ops/governance/kms_sign_helper.py --file "$OUT_JSON" --key-id "$AWS_KMS_KEY_ID" || true
+  if [ -f "$OUT_SIG" ]; then
+    echo "Signature written to $OUT_SIG"
+    base64 "$OUT_SIG" > "$OUT_SIG.b64" || true
+  fi
+else
+  echo "No AWS_KMS_KEY_ID; writing demo base64 sig"
+  echo "VGhpcyBpcyBhIGRlbW8gc2lnbmF0dXJl" > "$OUT_SIG.b64"
+fi
+
+echo "Done. Evaluation summary available at $OUT_JSON and signature at ${OUT_SIG}.b64"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/test_gates_with_examples.yml
+name: Test Promotion Gates with Example Artifacts
+on:
+  workflow_dispatch:
+
+jobs:
+  test-gates:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: python -m pip install --upgrade pip || true
+      - name: Place example artifacts in /tmp
+        run: |
+          mkdir -p /tmp/perception_sweep /tmp/verifier_logs
+          cp tests/artifacts/example_evaluation_summary.json /tmp/perception_sweep/sweep_summary.json
+          cp tests/artifacts/example_evaluation_summary.json.sig.b64 /tmp/perception_sweep/sweep_summary.json.sig.b64
+          # convert base64 sig to raw if needed for local checks
+          base64 -d /tmp/perception_sweep/sweep_summary.json.sig.b64 > /tmp/perception_sweep/sweep_summary.json.sig || true
+          cp tests/artifacts/mock_proof/obstacle_stop.proof.json /tmp/verifier_logs/obstacle_stop.proof.json
+          cp tests/artifacts/mock_proof/obstacle_stop.proof.json.sig.b64 /tmp/verifier_logs/obstacle_stop.proof.json.sig.b64
+          base64 -d /tmp/verifier_logs/obstacle_stop.proof.json.sig.b64 > /tmp/verifier_logs/obstacle_stop.proof.json.sig || true
+      - name: Run CI gate check (should pass if signatures present)
+        run: |
+          python ops/tests/integration/ci_gate_check.py || (echo "Gate failed" && exit 2)
+      - name: Run promotion example (simulate)
+        run: |
+          python ops/perception/promote_model_ci.py --model "registry/detector:example-v1" --eval /tmp/perception_sweep/sweep_summary.json || true
+
*** End Patch
*** End Patch
