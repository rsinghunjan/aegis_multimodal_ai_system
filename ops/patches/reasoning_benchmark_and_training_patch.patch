*** Begin Patch
*** Add File:ops/ci/reasoning_benchmark_runner.py
+#!/usr/bin/env python3
+"""
+Reasoning benchmark runner
+ - Runs a dataset (GSM8K/MATH) through the chain_runner with a pluggable LLM call and verifier function.
+ - Produces a JSON report with: examples, per-step verification results, chain success, verification pass rate, accuracy (if gold provided), and calibration stats.
+
+Usage:
+  python ops/ci/reasoning_benchmark_runner.py --dataset data/gsm8k_sample.jsonl --out report.json --max-examples 200
+
+Environment:
+ - LLM_API_URL optional: if set, runner will POST {"prompt": "..."} and expect JSON {"answer": "...", "confidence": ...}
+ - VERIFIER_API_URL optional: if set, runner will POST {"step": "...", "context": {...}} and expect {"verdict":"pass|fail","confidence":float}
+"""
+import os
+import sys
+import json
+import time
+import argparse
+from typing import Dict, Any
+from ops.reasoning.chain_runner import run_task
+
+import requests
+
+def llm_fn_local(prompt: str) -> Dict[str, Any]:
+    # Best-effort: call remote LLM API if available, otherwise simple eval stub
+    url = os.environ.get("LLM_API_URL", "")
+    if url:
+        try:
+            r = requests.post(url, json={"prompt": prompt}, timeout=30)
+            r.raise_for_status()
+            return r.json()
+        except Exception:
+            pass
+    # stub: return the prompt as answer (for smoke)
+    return {"answer": "0", "confidence": 0.01}
+
+def verifier_fn_local(step_text: str, context: Dict) -> Dict[str, Any]:
+    url = os.environ.get("VERIFIER_API_URL", "")
+    if url:
+        try:
+            r = requests.post(url, json={"step": step_text, "context": context}, timeout=20)
+            r.raise_for_status()
+            return r.json()
+        except Exception:
+            pass
+    # fallback: simple numeric verifier (accept if the step contains a number)
+    import re
+    m = re.search(r"[-+]?\d+(\.\d+)?", str(step_text))
+    if m:
+        return {"verdict": "pass", "confidence": 0.9}
+    return {"verdict": "fail", "confidence": 0.1}
+
+def load_dataset(path, limit=None):
+    items = []
+    with open(path) as fh:
+        for i, line in enumerate(fh):
+            if limit and i >= limit:
+                break
+            try:
+                items.append(json.loads(line))
+            except Exception:
+                continue
+    return items
+
+def evaluate(dataset_path, out_path, max_examples=200, max_steps=12):
+    data = load_dataset(dataset_path, limit=max_examples)
+    stats = {"examples": len(data), "processed": 0, "chain_success": 0, "verified_steps_total":0, "verified_steps_passed":0, "accuracy_count":0, "examples_with_gold":0}
+    results = []
+    start = time.time()
+    for ex in data:
+        prompt = ex.get("question") or ex.get("input") or ex.get("task") or ex.get("text")
+        gold = ex.get("answer") or ex.get("gold")
+        res = run_task(prompt, llm_fn_local, verifier_fn_local, max_steps=max_steps)
+        # tally verification
+        ver_total = 0
+        ver_pass = 0
+        for s in res.get("steps", []):
+            v = s.get("verify", {})
+            if v:
+                ver_total += 1
+                if v.get("verdict") == "pass":
+                    ver_pass += 1
+        stats["verified_steps_total"] += ver_total
+        stats["verified_steps_passed"] += ver_pass
+        if res.get("status") == "ok":
+            stats["chain_success"] += 1
+        # crude answer extraction: last step llm answer
+        last = res.get("steps", [])[-1] if res.get("steps") else {}
+        llm_ans = last.get("llm", {}).get("answer")
+        correct = None
+        if gold is not None:
+            stats["examples_with_gold"] += 1
+            # simple string compare or numeric compare
+            try:
+                # numeric compare tolerance
+                if llm_ans is None:
+                    correct = False
+                else:
+                    if isinstance(gold,(int,float)) or str(gold).replace('.','',1).isdigit():
+                        correct = abs(float(str(llm_ans).strip()) - float(str(gold).strip())) < 1e-6
+                    else:
+                        correct = str(llm_ans).strip() == str(gold).strip()
+                if correct:
+                    stats["accuracy_count"] += 1
+            except Exception:
+                pass
+        results.append({"prompt": prompt, "result": res, "llm_answer": llm_ans, "gold": gold, "correct": correct})
+        stats["processed"] += 1
+    duration = time.time() - start
+    metrics = {
+        "examples": stats["examples"],
+        "processed": stats["processed"],
+        "chain_success_rate": stats["chain_success"] / max(1, stats["processed"]),
+        "verified_step_pass_rate": stats["verified_steps_passed"] / max(1, stats["verified_steps_total"]),
+        "accuracy": stats["accuracy_count"] / max(1, stats["examples_with_gold"]),
+        "duration_s": duration
+    }
+    out = {"metrics": metrics, "stats": stats, "results": results}
+    with open(out_path, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote benchmark report to", out_path)
+    return out
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--max-examples", type=int, default=200)
+    p.add_argument("--max-steps", type=int, default=12)
+    args = p.parse_args()
+    evaluate(args.dataset, args.out, max_examples=args.max_examples, max_steps=args.max_steps)
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/reasoning_benchmark.yml
+name: Reasoning Benchmark (GSM8K/MATH) with Chain Runner & Verifier
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 4 * * 1'  # weekly run
+
+jobs:
+  benchmark:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests
+      - name: Prepare data (provide small sample files)
+        run: |
+          mkdir -p data
+          # user should replace these with full datasets; we include a tiny example for CI
+          cat > data/gsm8k_sample.jsonl <<'EOF'
+{"question":"What is 2+3?","answer":"5"}
+{"question":"Compute 6 * 7 then add 0 and report the result.","answer":"42"}
+EOF
+      - name: Run benchmark
+        run: |
+          python ops/ci/reasoning_benchmark_runner.py --dataset data/gsm8k_sample.jsonl --out /tmp/reasoning_benchmark_report.json --max-examples 20
+      - name: Upload report
+        uses: actions/upload-artifact@v4
+        with:
+          name: reasoning-benchmark-report
+          path: /tmp/reasoning_benchmark_report.json
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/README_DATA_FORMAT.md
+NLI Fine-tuning Data Format
+---------------------------
+We expect a JSONL file where each line is a JSON object:
+
+{
+  "premise": "The car is moving at 60 km/h.",
+  "hypothesis": "The car is moving fast.",
+  "label": "entailment"   # or "contradiction" or "neutral"
+}
+
+Labels mapping for training: entailment->0, neutral->1, contradiction->2 (or adjust in training args).
+
+Place validation files in ops/nli/data/ and production pipelines read from NLI_VALIDATION_DIR for calibration.
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/fine_tune_nli.py
+#!/usr/bin/env python3
+"""
+Fine-tune an NLI model (classification) using Hugging Face Transformers Trainer.
+ - Expects JSONL train/validation files in the data format described in README_DATA_FORMAT.md
+ - Produces a saved model and emits probabilities for validation set to be used by calibration pipeline.
+
+Usage:
+  python ops/nli/fine_tune_nli.py --train data/train.jsonl --valid data/valid.jsonl --out_dir /tmp/nli_model
+"""
+import os
+import argparse
+import json
+from datasets import load_dataset, Dataset
+from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
+import numpy as np
+
+LABEL_MAP = {"entailment":0, "neutral":1, "contradiction":2}
+
+def load_jsonl(path):
+    data = []
+    with open(path) as fh:
+        for l in fh:
+            data.append(json.loads(l))
+    return data
+
+def prepare_dataset(records):
+    texts = []
+    labels = []
+    for r in records:
+        texts.append((r["premise"], r["hypothesis"]))
+        labels.append(LABEL_MAP.get(r["label"],1))
+    ds = Dataset.from_dict({"premise":[p[0] for p in texts], "hypothesis":[p[1] for p in texts], "label":labels})
+    return ds
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--train", required=True)
+    p.add_argument("--valid", required=True)
+    p.add_argument("--out_dir", required=True)
+    p.add_argument("--model", default="roberta-large-mnli")
+    args = p.parse_args()
+
+    train_records = load_jsonl(args.train)
+    valid_records = load_jsonl(args.valid)
+    train_ds = prepare_dataset(train_records)
+    valid_ds = prepare_dataset(valid_records)
+
+    tokenizer = AutoTokenizer.from_pretrained(args.model)
+
+    def tokenize_fn(ex):
+        return tokenizer(ex["premise"], ex["hypothesis"], truncation=True, padding="max_length", max_length=256)
+
+    train_ds = train_ds.map(tokenize_fn, batched=True)
+    valid_ds = valid_ds.map(tokenize_fn, batched=True)
+    train_ds.set_format(type="np", columns=["input_ids","attention_mask","label"])
+    valid_ds.set_format(type="np", columns=["input_ids","attention_mask","label"])
+
+    model = AutoModelForSequenceClassification.from_pretrained(args.model, num_labels=3)
+    training_args = TrainingArguments(
+        output_dir=args.out_dir,
+        per_device_train_batch_size=8,
+        per_device_eval_batch_size=8,
+        num_train_epochs=1,
+        evaluation_strategy="epoch",
+        save_strategy="epoch",
+        logging_steps=10,
+        fp16=False,
+    )
+    def compute_metrics(eval_pred):
+        logits, labels = eval_pred
+        preds = np.argmax(logits, axis=-1)
+        acc = (preds == labels).mean().item()
+        return {"accuracy": acc}
+
+    trainer = Trainer(
+        model=model,
+        args=training_args,
+        train_dataset=train_ds,
+        eval_dataset=valid_ds,
+        compute_metrics=compute_metrics,
+    )
+    trainer.train()
+    trainer.save_model(args.out_dir)
+    # emit validation probabilities for calibration
+    preds = trainer.predict(valid_ds)
+    logits = preds.predictions
+    probs = (np.exp(logits) / np.exp(logits).sum(-1, keepdims=True)).tolist()
+    out_prob_path = os.path.join(args.out_dir, "validation_probs.json")
+    json.dump({"probs": probs, "labels": valid_ds["label"]}, open(out_prob_path, "w"))
+    print("Saved model to", args.out_dir, "and validation probs to", out_prob_path)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/calibration_pipeline.py
+#!/usr/bin/env python3
+"""
+Run the calibration pipeline after NLI fine-tuning.
+ - Expects validation probs JSON produced by fine_tune_nli.py
+ - Finds best temperature and writes calibration artifact to NLI_CALIB_OUT (default /tmp/nli_temperature.json)
+"""
+import os
+import json
+from ops.nli.recalibrate_ci import find_and_write
+
+if __name__ == "__main__":
+    res = find_and_write()
+    print("Calibration pipeline result:", res)
+
*** End Patch
*** Begin Patch
*** Add File:ops/planning/llm_decomposer.py
+#!/usr/bin/env python3
+"""
+LLM-based decomposer with few-shot templates.
+ - Calls LLM_API_URL with a decomposition prompt (env var LLM_API_URL)
+ - Writes proposed decompositions to a review queue directory for human validation.
+ - Output format: JSON files in PLANNING_QUEUE_DIR with fields {task, proposed_steps, prompt, ts}
+"""
+import os
+import json
+import time
+import requests
+from typing import List
+
+PLANNING_QUEUE_DIR = os.environ.get("PLANNING_QUEUE_DIR", "/tmp/planning_queue")
+LLM_API_URL = os.environ.get("LLM_API_URL", "")
+DEFAULT_SHOTS = [
+    "Task: Bake a simple cake\nSteps:\n1. Preheat oven\n2. Mix ingredients\n3. Bake\n",
+    "Task: Plan a short road trip\nSteps:\n1. Pick destination\n2. Map route\n3. Check vehicle\n"
+]
+
+def call_llm(prompt: str) -> str:
+    if not LLM_API_URL:
+        # no LLM configured; return empty
+        return ""
+    r = requests.post(LLM_API_URL, json={"prompt": prompt}, timeout=30)
+    r.raise_for_status()
+    return r.json().get("answer","")
+
+def propose(task_text: str, shots: List[str]=None) -> dict:
+    shots = shots or DEFAULT_SHOTS
+    prompt = "Decompose the task below into short numbered steps. Provide only numbered steps.\n\n"
+    for s in shots:
+        prompt += s + "\n"
+    prompt += f"Task: {task_text}\n\nSteps:\n"
+    answer = call_llm(prompt)
+    # naive parse
+    lines = [l.strip() for l in (answer or "").splitlines() if l.strip()]
+    steps = []
+    for line in lines:
+        import re
+        m = re.match(r"^\s*(\d+)[\.\)]\s*(.*)$", line)
+        if m:
+            steps.append(m.group(2).strip())
+        else:
+            steps.append(line)
+    os.makedirs(PLANNING_QUEUE_DIR, exist_ok=True)
+    fname = os.path.join(PLANNING_QUEUE_DIR, f"proposal_{int(time.time()*1000)}.json")
+    obj = {"task": task_text, "proposed_steps": steps, "prompt": prompt, "answer": answer, "ts": time.time()}
+    with open(fname, "w") as fh:
+        json.dump(obj, fh, indent=2)
+    return obj
+
+if __name__ == "__main__":
+    import sys
+    t = " ".join(sys.argv[1:]) or "Compute safe speed profile for a 2 km downhill segment with curves"
+    print(json.dumps(propose(t), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/planning/human_validation_loop.py
+#!/usr/bin/env python3
+"""
+Human validation loop for decomposition proposals.
+ - Reads JSON proposals from PLANNING_QUEUE_DIR and writes validated proposals to PLANNING_VALIDATED_DIR.
+ - Provides a CLI to accept/reject and optionally edit proposed steps before accepting.
+"""
+import os
+import json
+from glob import glob
+
+PLANNING_QUEUE_DIR = os.environ.get("PLANNING_QUEUE_DIR", "/tmp/planning_queue")
+PLANNING_VALIDATED_DIR = os.environ.get("PLANNING_VALIDATED_DIR", "/tmp/planning_validated")
+
+def list_proposals():
+    return sorted(glob(os.path.join(PLANNING_QUEUE_DIR, "proposal_*.json")))
+
+def load_proposal(path):
+    return json.load(open(path))
+
+def accept_proposal(path, editor_func=None):
+    p = load_proposal(path)
+    if editor_func:
+        p = editor_func(p)
+    os.makedirs(PLANNING_VALIDATED_DIR, exist_ok=True)
+    out = os.path.join(PLANNING_VALIDATED_DIR, os.path.basename(path))
+    json.dump(p, open(out, "w"), indent=2)
+    os.remove(path)
+    print("Accepted proposal ->", out)
+
+def reject_proposal(path, reason="rejected"):
+    rej = {"rejected": True, "reason": reason}
+    obj = load_proposal(path)
+    obj["_rejection"] = rej
+    os.makedirs(PLANNING_VALIDATED_DIR, exist_ok=True)
+    out = os.path.join(PLANNING_VALIDATED_DIR, os.path.basename(path))
+    json.dump(obj, open(out, "w"), indent=2)
+    os.remove(path)
+    print("Rejected proposal ->", out)
+
+def cli():
+    proposals = list_proposals()
+    if not proposals:
+        print("No proposals in", PLANNING_QUEUE_DIR)
+        return
+    for p in proposals:
+        obj = load_proposal(p)
+        print("Task:", obj.get("task"))
+        print("Proposed steps:")
+        for i,s in enumerate(obj.get("proposed_steps",[]), start=1):
+            print(f"  {i}. {s}")
+        resp = input("Accept (y), Reject (n), Edit (e), Skip (s)? ")
+        if resp.lower() == "y":
+            accept_proposal(p)
+        elif resp.lower() == "n":
+            reason = input("Reason for rejection: ")
+            reject_proposal(p, reason=reason)
+        elif resp.lower() == "e":
+            print("Enter edited steps, one per line. Empty line to finish.")
+            edited = []
+            while True:
+                line = input()
+                if not line.strip():
+                    break
+                edited.append(line.strip())
+            def editor(o):
+                o["proposed_steps"] = edited
+                o["edited_by"] = os.environ.get("USER","human")
+                return o
+            accept_proposal(p, editor_func=editor)
+        else:
+            continue
+
+if __name__ == "__main__":
+    cli()
+
*** End Patch
*** End Patch
