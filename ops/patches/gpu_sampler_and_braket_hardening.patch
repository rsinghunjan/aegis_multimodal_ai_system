*** Begin Patch
*** Add File:ops/carbon/Dockerfile
+FROM python:3.11-slim
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y --no-install-recommends \\
+    gcc libffi-dev libssl-dev libxml2-dev libxslt1-dev curl && \\
+    rm -rf /var/lib/apt/lists/*
+
+# Install Python deps
+RUN pip install --no-cache-dir psutil pynvml requests
+
+# Copy sampler and calibration tools
+COPY ops/carbon/gpu_sampler.py /usr/local/bin/gpu_sampler.py
+COPY ops/carbon/aggregate_calibration.py /usr/local/bin/aggregate_calibration.py
+COPY ops/carbon/emission_calculator.py /usr/local/bin/emission_calculator.py
+RUN chmod +x /usr/local/bin/gpu_sampler.py /usr/local/bin/aggregate_calibration.py /usr/local/bin/emission_calculator.py
+
+WORKDIR /work
+CMD ["/usr/local/bin/gpu_sampler.py"]
+
*** End Patch
*** Begin Patch
*** Add File:k8s/daemonsets/gpu-sampler-daemonset.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-observability
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-gpu-sampler
+  namespace: aegis-observability
+---
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: aegis-gpu-sampler
+  namespace: aegis-observability
+spec:
+  selector:
+    matchLabels:
+      app: aegis-gpu-sampler
+  template:
+    metadata:
+      labels:
+        app: aegis-gpu-sampler
+    spec:
+      serviceAccountName: aegis-gpu-sampler
+      tolerations:
+        - operator: "Exists"
+      containers:
+        - name: gpu-sampler
+          image: ${GPU_SAMPLER_IMAGE:-aegis/gpu-sampler:latest}
+          imagePullPolicy: IfNotPresent
+          env:
+            - name: NODE_POWER_DIR
+              value: /var/log/node_power
+            - name: INSTANCE_TYPE
+              valueFrom:
+                fieldRef:
+                  fieldPath: spec.nodeName
+            - name: HOST_SAMPLE_INTERVAL
+              value: "5.0"
+          volumeMounts:
+            - name: node-log
+              mountPath: /var/log/node_power
+            - name: etc-aegis
+              mountPath: /etc/aegis
+      volumes:
+        - name: node-log
+          hostPath:
+            path: /var/log/node_power
+            type: DirectoryOrCreate
+        - name: etc-aegis
+          hostPath:
+            path: /etc/aegis
+            type: DirectoryOrCreate
+
*** End Patch
*** Begin Patch
*** Add File:k8s/cronjobs/aggregate-calibration-cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-aggregate-calibration
+  namespace: aegis-observability
+spec:
+  schedule: "0 * * * *"  # hourly
+  concurrencyPolicy: Forbid
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: aegis-gpu-sampler
+          restartPolicy: OnFailure
+          containers:
+            - name: aggregate-calibration
+              image: ${GPU_SAMPLER_IMAGE:-aegis/gpu-sampler:latest}
+              imagePullPolicy: IfNotPresent
+              command: ["/usr/local/bin/aggregate_calibration.py"]
+              env:
+                - name: NODE_POWER_DIR
+                  value: /var/log/node_power
+                - name: CARBON_CALIB_PATH
+                  value: /etc/aegis/carbon_calibration.json
+              volumeMounts:
+                - name: node-log
+                  mountPath: /var/log/node_power
+                - name: etc-aegis
+                  mountPath: /etc/aegis
+          volumes:
+            - name: node-log
+              hostPath:
+                path: /var/log/node_power
+                type: Directory
+            - name: etc-aegis
+              hostPath:
+                path: /etc/aegis
+                type: Directory
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/build_and_push_gpu_sampler.yml
+name: Build & Push GPU Sampler Image
+on:
+  workflow_dispatch:
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v2
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.DOCKER_REGISTRY }}
+          username: ${{ secrets.DOCKER_USERNAME }}
+          password: ${{ secrets.DOCKER_PASSWORD }}
+      - name: Build and push
+        uses: docker/build-push-action@v4
+        with:
+          context: .
+          file: ops/carbon/Dockerfile
+          push: true
+          tags: ${{ secrets.DOCKER_REGISTRY }}/aegis/gpu-sampler:${{ github.sha }}
+      - name: Output image tag
+        run: echo "GPU_SAMPLER_IMAGE=${{ secrets.DOCKER_REGISTRY }}/aegis/gpu-sampler:${{ github.sha }}" >> $GITHUB_ENV
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantum/providers/aws_braket_adapter_v2.py
+#!/usr/bin/env python3
+"""
+AWS Braket adapter v2 — hardened:
+ - Uses boto3, robust retries, S3 output fetching (when configured), cost estimation stub, and provenance enrichment.
+ - Expects environment:
+    AWS_REGION, BRAKET_OUTPUT_BUCKET (optional), BRAKET_DEVICE_ARN (optional)
+ - Provides methods: submit_circuit, poll_status, fetch_result, estimate_cost, get_provenance
+"""
+import os
+import json
+import time
+import botocore
+from datetime import datetime
+from typing import Dict, Any
+
+try:
+    import boto3
+    BOTO3_AVAILABLE = True
+except Exception:
+    BOTO3_AVAILABLE = False
+
+from ops.quantum.qpu_adapter import QPUAdapter
+
+RETRY_MAX = int(os.environ.get("BRAKET_RETRY_MAX", "5"))
+RETRY_BACKOFF = float(os.environ.get("BRAKET_RETRY_BACKOFF", "1.5"))
+
+class AwsBraketAdapterV2(QPUAdapter):
+    def __init__(self, region_name: str = None):
+        if not BOTO3_AVAILABLE:
+            raise RuntimeError("boto3 required for AwsBraketAdapterV2")
+        self.region = region_name or os.environ.get("AWS_REGION", "us-west-2")
+        self.client = boto3.client("braket", region_name=self.region)
+        self.s3 = boto3.client("s3", region_name=self.region)
+        self.output_bucket = os.environ.get("BRAKET_OUTPUT_BUCKET")
+        self.device_arn = os.environ.get("BRAKET_DEVICE_ARN")  # e.g., emulator/simulator or real device
+
+    def list_backends(self):
+        try:
+            resp = self.client.search_devices()
+            return [d.get("deviceArn") for d in resp.get("devices", [])]
+        except Exception:
+            return []
+
+    def estimate_cost(self, circuit_spec: Dict[str, Any], shots: int = 1024) -> float:
+        # Placeholder: allow mapping by backend or use env var BRAKET_PRICE_PER_SHOT_USD
+        price_per_shot = float(os.environ.get("BRAKET_PRICE_PER_SHOT_USD", "0.0005"))
+        return shots * price_per_shot
+
+    def _create_quantum_task(self, payload: Dict[str, Any]) -> str:
+        """
+        Create a quantum task. For production, create a proper task with source and action.
+        This function uses create_quantum_task with simplified parameters and relies on BRAKET_OUTPUT_BUCKET.
+        """
+        params = {
+            "deviceArn": self.device_arn or os.environ.get("BRAKET_DEVICE_ARN"),
+            "shots": int(payload.get("shots", 1024)),
+        }
+        # For managed simulator, minimal fields; for real usage, craft 'action' or 'source' per Braket SDK
+        if self.output_bucket:
+            params["outputS3Bucket"] = self.output_bucket
+        # Best effort: pass clientToken to ensure idempotency
+        params["clientToken"] = payload.get("clientToken") or f"ae-{int(time.time()*1000)}"
+        # Use retries around boto3
+        attempt = 0
+        while True:
+            try:
+                resp = self.client.create_quantum_task(**params)
+                return resp.get("quantumTaskArn")
+            except botocore.exceptions.ClientError as e:
+                attempt += 1
+                if attempt >= RETRY_MAX:
+                    raise
+                time.sleep((RETRY_BACKOFF ** attempt))
+
+    def submit_circuit(self, circuit_spec: Dict[str, Any], shots: int = 1024) -> str:
+        payload = {"circuit": circuit_spec, "shots": shots}
+        return self._create_quantum_task(payload)
+
+    def poll_status(self, job_id: str) -> Dict[str, Any]:
+        try:
+            resp = self.client.get_quantum_task(quantumTaskArn=job_id)
+            status = resp.get("status", {}).get("state") if isinstance(resp.get("status"), dict) else resp.get("status")
+            # For older Braket responses, status may be direct field
+            if not status:
+                status = resp.get("status", "unknown")
+            return {"state": status.lower() if isinstance(status, str) else status, "raw": resp}
+        except botocore.exceptions.ClientError as e:
+            return {"state": "error", "error": str(e)}
+
+    def fetch_result(self, job_id: str) -> Dict[str, Any]:
+        """
+        Attempt to fetch results via get_quantum_task and, if present, fetch output from S3.
+        Returns raw response plus optional fetched artifact path or S3 location.
+        """
+        resp = self.client.get_quantum_task(quantumTaskArn=job_id)
+        result = {"raw": resp}
+        # try to extract S3 output information from response
+        try:
+            output = resp.get("outputS3Location") or resp.get("deviceTaskMetadata", {}).get("outputS3Location")
+            if output:
+                # output looks like s3://bucket/key or dict with bucket/key
+                if isinstance(output, str) and output.startswith("s3://"):
+                    _,_,path = output.partition("s3://")
+                    bucket, _, key = path.partition("/")
+                elif isinstance(output, dict):
+                    bucket = output.get("bucket")
+                    key = output.get("key")
+                else:
+                    bucket = None
+                    key = None
+                if bucket and key:
+                    # Download object to /tmp
+                    local_path = f"/tmp/braket_output_{job_id.replace('/','_')}.tar.gz"
+                    try:
+                        self.s3.download_file(bucket, key, local_path)
+                        result["fetched"] = local_path
+                    except Exception:
+                        result["s3_error"] = "failed to download s3 output"
+        except Exception:
+            pass
+        return result
+
+    def get_provenance(self, job_id: str) -> Dict[str, Any]:
+        try:
+            resp = self.client.get_quantum_task(quantumTaskArn=job_id)
+            prov = {
+                "deviceArn": resp.get("deviceArn"),
+                "createdAt": resp.get("createdAt"),
+                "shots": resp.get("shots"),
+                "status": resp.get("status"),
+                "outputS3Location": resp.get("outputS3Location") or resp.get("deviceTaskMetadata", {}).get("outputS3Location")
+            }
+            return prov
+        except Exception as e:
+            return {"error": str(e)}
+
*** End Patch
*** Begin Patch
*** Add File:docs/ops/carbon_sampler_readme.md
+GPU Sampler & Calibration — README
+---------------------------------
+
+What this adds
+ - A container image that runs ops/carbon/gpu_sampler.py to collect host+GPU power samples on nodes and writes JSONL to /var/log/node_power.
+ - A DaemonSet manifest to deploy the sampler on all nodes (k8s/daemonsets/gpu-sampler-daemonset.yaml).
+ - An hourly CronJob that runs aggregate_calibration.py and writes /etc/aegis/carbon_calibration.json.
+ - A GitHub Actions workflow to build & push the sampler image.
+
+Quick start
+ 1. Build and push the sampler image via CI (set DOCKER_* secrets) or build locally:
+    docker build -t <registry>/aegis/gpu-sampler:latest -f ops/carbon/Dockerfile .
+    docker push <registry>/aegis/gpu-sampler:latest
+ 2. Edit k8s/daemonsets/gpu-sampler-daemonset.yaml to set GPU_SAMPLER_IMAGE env or replace placeholder.
+ 3. Apply manifests:
+    kubectl apply -f k8s/daemonsets/gpu-sampler-daemonset.yaml
+    kubectl apply -f k8s/cronjobs/aggregate-calibration-cronjob.yaml
+ 4. Check samples:
+    ls /var/log/node_power
+    tail -n 20 /var/log/node_power/node_power_<nodename>.jsonl
+ 5. After samples accumulate, verify calibration DB:
+    kubectl exec -n aegis-observability <aggregate-job-pod> -- cat /etc/aegis/carbon_calibration.json
+
+Notes
+ - Ensure nodes running the daemonset allow access to NVML (NVIDIA driver) if GPUs present.
+ - HostPath volumes are used for simplicity; consider using a centralized collector in production (Fluentd/Logstash).
+
*** End Patch
*** Begin Patch
*** Add File:docs/quantum/aws_braket_adapter_readme.md
+AWS Braket Adapter v2 — README
+------------------------------
+
+Capabilities
+ - Submits quantum tasks with retries and backoff
+ - Polls task status and fetches S3 output artifacts when present
+ - Provides a get_provenance method that returns deviceArn, createdAt, shots and output location
+ - estimate_cost stub (configurable by env)
+
+Configuration
+ - Set AWS_REGION, AWS credentials in environment (or via instance role)
+ - Optionally set BRAKET_OUTPUT_BUCKET to enable S3 output capture
+ - Optionally set BRAKET_DEVICE_ARN to choose device/simulator
+
+Usage example
+ - In ProviderManager use adapter name pointing to ops.quantum.providers.aws_braket_adapter_v2.AwsBraketAdapterV2
+ - Submit job and poll for status via ProviderManager or directly via the adapter
+
+Security & billing
+ - Use IAM roles for least privilege (braket:CreateQuantumTask, braket:GetQuantumTask, s3:GetObject for output bucket)
+ - Set BRAKET_PRICE_PER_SHOT_USD or rely on estimate_cost; integrate billing_guard for enforcement
+
+Notes
+ - This adapter is a pragmatic wrapper. For production, expand to produce valid Braket program sources (OpenQASM/IR) and robust S3 output parsing.
+
*** End Patch
*** End Patch
