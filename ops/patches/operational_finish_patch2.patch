*** Begin Patch
*** Add File:ops/deploy/provision_gpu_nodepool.sh
+#!/usr/bin/env bash
+set -euo pipefail
+
+echo "Provision GPU nodepool helper (cloud-specific snippets)."
+echo "This script provides commands you can run or copy into infra pipelines."
+echo
+echo "GKE (Google Kubernetes Engine) sample:"
+echo "  gcloud container node-pools create nli-gpu-pool \\"
+echo "    --cluster=<CLUSTER_NAME> --region=<REGION> \\"
+echo "    --machine-type=n1-standard-8 --accelerator type=nvidia-tesla-t4,count=1 \\"
+echo "    --num-nodes=1 --enable-autoscaling --min-nodes=1 --max-nodes=4 \\"
+echo "    --node-labels=nvidia.com/gpu.present=true"
+echo
+echo "EKS (AWS) sample via eksctl:"
+echo "  eksctl create nodegroup --cluster=<CLUSTER> --name nli-gpu-ng \\"
+echo "    --node-type p3.2xlarge --nodes 1 --nodes-min 1 --nodes-max 4 \\"
+echo "    --labels nvidia.com/gpu.present=true"
+echo
+echo "AKS (Azure) sample:"
+echo "  az aks nodepool add --resource-group <RG> --cluster-name <CLUSTER> --name nli-gpu \\"
+echo "    --node-vm-size Standard_NC6 --node-count 1 --enable-cluster-autoscaler --min-count 1 --max-count 4 \\"
+echo "    --labels nvidia.com/gpu.present=true"
+echo
+echo "After creating nodepool, ensure nodes are labelled (kubectl get nodes --show-labels) and update nli-service nodeSelector if necessary."
+echo "Note: This script intentionally does not run destructive commands. Copy the appropriate snippet into your infra pipeline."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/servicemonitor.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: aegis-services-monitor
+  namespace: aegis-retriever
+  labels:
+    release: prometheus
+spec:
+  selector:
+    matchLabels:
+      # select all services in our namespace - each service should have this label in manifests
+      app: realtime-retriever
+  namespaceSelector:
+    matchNames:
+      - aegis-retriever
+  endpoints:
+    - port: http-metrics
+      interval: 15s
+      path: /metrics
+      honorLabels: true
+  jobLabel: aegis-prometheus
+
+---
+# ServiceMonitor for calc pool (assumes manifest includes label app=calc-pool and metrics port)
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: calc-pool-monitor
+  namespace: aegis-retriever
+spec:
+  selector:
+    matchLabels:
+      app: calc-pool
+  namespaceSelector:
+    matchNames:
+      - aegis-retriever
+  endpoints:
+    - port: metrics
+      interval: 15s
+      path: /metrics
+
+---
+# ServiceMonitor for nli service (assumes app=nli-service)
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: nli-service-monitor
+  namespace: aegis-retriever
+spec:
+  selector:
+    matchLabels:
+      app: nli-service
+  namespaceSelector:
+    matchNames:
+      - aegis-retriever
+  endpoints:
+    - port: metrics
+      interval: 15s
+      path: /metrics
+
*** End Patch
*** Begin Patch
*** Add File:grafana/dashboards/aegis_math_dashboard.json
+{
+  "annotations": {
+    "list": []
+  },
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Calc Pool - Pending Jobs",
+      "targets": [
+        {
+          "expr": "aegis_calc_jobs_pending",
+          "legendFormat": "pending"
+        }
+      ],
+      "id": 1
+    },
+    {
+      "type": "graph",
+      "title": "Calc Service - p95 Latency",
+      "targets": [
+        {
+          "expr": "histogram_quantile(0.95, sum(rate(aegis_calc_job_duration_seconds_bucket[5m])) by (le))",
+          "legendFormat": "calc_p95"
+        }
+      ],
+      "id": 2
+    },
+    {
+      "type": "graph",
+      "title": "NLI - p95 Latency",
+      "targets": [
+        {
+          "expr": "histogram_quantile(0.95, sum(rate(aegis_nli_request_duration_seconds_bucket[5m])) by (le))",
+          "legendFormat": "nli_p95"
+        }
+      ],
+      "id": 3
+    },
+    {
+      "type": "graph",
+      "title": "Verification Gate - Blocks (5m)",
+      "targets": [
+        {
+          "expr": "increase(aegis_verification_blocked_total[5m])",
+          "legendFormat": "blocked_5m"
+        }
+      ],
+      "id": 4
+    },
+    {
+      "type": "graph",
+      "title": "Benchmarks - Recent Results (artifact indicator)",
+      "targets": [
+        {
+          "expr": "increase(aegis_benchmark_runs_total[24h])",
+          "legendFormat": "bench_runs_24h"
+        }
+      ],
+      "id": 5
+    }
+  ],
+  "schemaVersion": 16,
+  "title": "Aegis Math Overview",
+  "version": 1
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/scaling/auto_tuner.py
+#!/usr/bin/env python3
+"""
+Auto tuner: reads capacity plan generated by ops/scaling/capacity_planner.py and applies Kubernetes scaling recommendations.
+ - Requires KUBECONFIG or in-cluster access.
+ - Performs best-effort scaling via kubectl or k8s API.
+ - Outputs /tmp/auto_tuner_result.json summarizing actions.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+IN = "/tmp/capacity_plan.json"
+OUT = "/tmp/auto_tuner_result.json"
+
+def apply_scale(target, action):
+    if target == "calc_pool" and action == "increase_CALC_POOL_WORKERS":
+        # Increase replicas of calc-pool deployment
+        # This is conservative: scale to 3 if currently 1
+        try:
+            subprocess.check_call(["kubectl","scale","deployment","calc-pool","--replicas=3","-n","aegis-retriever"])
+            return {"ok": True, "action": "scale_calc_pool_to_3"}
+        except Exception as e:
+            return {"ok": False, "error": str(e)}
+    if target == "nli-service" and action == "increase_replicas_or_gpu":
+        try:
+            subprocess.check_call(["kubectl","scale","deployment","nli-service","--replicas=2","-n","aegis-retriever"])
+            return {"ok": True, "action": "scale_nli_to_2"}
+        except Exception as e:
+            return {"ok": False, "error": str(e)}
+    return {"ok": False, "error": "unknown_action"}
+
+def main():
+    if not os.path.exists(IN):
+        print("No capacity plan found at", IN)
+        return
+    plan = json.load(open(IN))
+    recs = plan.get("recommendations", [])
+    results = []
+    for r in recs:
+        res = apply_scale(r["target"], r["action"])
+        results.append({"recommendation": r, "result": res})
+    out = {"ts": datetime.utcnow().isoformat()+"Z", "results": results}
+    with open(OUT, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Auto-tuner results written to", OUT)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/runtime_hardening_checker.py
+#!/usr/bin/env python3
+"""
+Runtime hardening checker (CI): validates that running pods follow hardened expectations:
+ - images come from allowed registries (config)
+ - securityContext runAsNonRoot / readOnlyRootFilesystem are respected
+ - seccomp annotation present on pod templates
+ - cgroup/rlimit enforcement cannot be fully checked from outside but we assert container security contexts exist
+Exits non-zero when severe issues detected.
+"""
+import os
+import sys
+import json
+from kubernetes import client, config
+
+ALLOWED_REGISTRIES = os.environ.get("ALLOWED_IMAGE_REGISTRIES", "ghcr.io,aegis-registry").split(",")
+NS = "aegis-retriever"
+
+def load():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.AppsV1Api(), client.CoreV1Api()
+
+def check_images(deps):
+    bad = []
+    for d in deps:
+        for c in d.spec.template.spec.containers:
+            img = c.image or ""
+            if not any(img.startswith(r) for r in ALLOWED_REGISTRIES):
+                bad.append({"deployment": d.metadata.name, "container": c.name, "image": img})
+    return bad
+
+def check_security(deps):
+    issues = []
+    for d in deps:
+        spec = d.spec.template.spec
+        ann = d.spec.template.metadata.annotations or {}
+        if "seccomp.security.alpha.kubernetes.io/pod" not in ann:
+            issues.append({"deployment": d.metadata.name, "issue": "missing_seccomp_annotation"})
+        for c in spec.containers:
+            sc = c.security_context or {}
+            if not sc.run_as_non_root and not sc.get('runAsNonRoot', False):
+                issues.append({"deployment": d.metadata.name, "container": c.name, "issue": "runAsNonRoot_missing"})
+            if not sc.read_only_root_filesystem and not sc.get('readOnlyRootFilesystem', False):
+                issues.append({"deployment": d.metadata.name, "container": c.name, "issue": "readOnlyRootFilesystem_missing"})
+    return issues
+
+def main():
+    apps, core = load()
+    deps = apps.list_namespaced_deployment(NS).items
+    bad_images = check_images(deps)
+    sec_issues = check_security(deps)
+    report = {"bad_images": bad_images, "security_issues": sec_issues}
+    print(json.dumps(report, indent=2))
+    if bad_images or sec_issues:
+        sys.exit(2)
+    print("Runtime hardening checks passed.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariant_authoring.md
+# Invariant Authoring Guide
+
+This document provides a short workflow and templates for domain experts to author invariants for continuous/dynamical systems in Aegis.
+
+1. Locate triage tasks:
+   - Fuzz triage creates tasks in /tmp/invariant_tasks_to_review.json
+   - Use ops/formal/invariant_registry.py to add canonical invariants.
+
+2. Invariant template format:
+   {
+     "type": "numeric_limit" | "stopping_distance" | "lyapunov",
+     "variable": "<name>",
+     "op": "le|ge",
+     "value": 5.0,
+     "description": "human-readable rationale"
+   }
+
+3. Create and test:
+   - Add invariant via invariant_registry.add_invariant(key, spec)
+   - Run targeted prover: python ops/formal/targeted_prover.py
+   - If prover cannot discharge analytically, provide numeric witness thresholds to be checked by fuzzing and monitoring.
+
+4. Prover strategies:
+   - For hybrid continuous/discrete systems consider using Lyapunov-style proofs, interval arithmetic, or reachability tools (Flow*, CORA).
+   - If SMT alone cannot prove, add runtime monitors with alerts and capture counterexamples for offline theorem engineering.
+
+5. Review & commit:
+   - Invariants should be reviewed by Safety/Domain owner
+   - Add metadata: owner, approver, created_at to the invariant file
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/nightly_full_bench.yml
+name: Nightly Full Bench & Device Tests
+on:
+  schedule:
+    - cron: '0 2 * * *'
+  workflow_dispatch:
+
+jobs:
+  ensure-budget:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure budget permitting full run
+        run: |
+          python ops/ci/benchmark_cost_guard.py nightly
+
+  run-full-bench:
+    needs: ensure-budget
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install datasets transformers sympy mpmath requests
+      - name: Run full-suite orchestrator
+        env:
+          LLM_API: ${{ secrets.LLM_API }}
+          DEVICE_LIST: ${{ secrets.DEVICE_LIST }}
+        run: |
+          python ops/ci/benchmark_orchestrator.py
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: nightly-bench
+          path: /tmp/benchmark_orchestration.json
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/ambiguity_detector_improved.py
+#!/usr/bin/env python3
+"""
+Improved ambiguity detector:
+ - Uses spaCy if available to detect unresolved pronouns and coreferences (best-effort).
+ - Falls back to simple heuristics otherwise.
+ - Enqueues ambiguous items to human_review_queue.
+"""
+import re
+from ops.process.human_review_queue import enqueue_review
+
+try:
+    import spacy
+    NLP = spacy.load("en_core_web_sm")
+except Exception:
+    NLP = None
+
+PRONOUNS = {"it","they","them","this","that","he","she","we","you","their","its"}
+
+def detect_ambiguity(text: str):
+    issues = []
+    if NLP:
+        doc = NLP(text)
+        # simple coref-like heuristic: look for pronouns without antecedent in previous sentences
+        sentences = list(doc.sents)
+        for i, sent in enumerate(sentences):
+            for tok in sent:
+                if tok.text.lower() in PRONOUNS:
+                    # search previous sentences for candidate noun
+                    prev = " ".join([s.text for s in sentences[:i]])
+                    if not prev or not re.search(r"\b[A-Za-z]{2,}\b", prev):
+                        issues.append({"type":"pronoun_no_antecedent", "word": tok.text, "sentence": sent.text[:200]})
+    else:
+        toks = re.findall(r"[A-Za-z]+", text.lower())
+        pronouns = [t for t in toks if t in PRONOUNS]
+        if pronouns:
+            issues.append({"type":"pronoun", "examples": pronouns[:3]})
+    return {"ambiguous": bool(issues), "issues": issues}
+
+def check_and_enqueue(text: str, metadata: dict):
+    res = detect_ambiguity(text)
+    if res["ambiguous"]:
+        enqueue_review({"text": text[:2000], "metadata": metadata, "issues": res["issues"]})
+    return res
+
+if __name__ == "__main__":
+    print(check_and_enqueue("If it is 2 then x equals 3", {"src":"demo"}))
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kyverno/policies.yaml
+apiVersion: kyverno.io/v1
+kind: ClusterPolicy
+metadata:
+  name: aegis-security-policy
+spec:
+  validationFailureAction: enforce
+  rules:
+    - name: require-readonlyrootfs
+      match:
+        resources:
+          kinds:
+            - Pod
+      validate:
+        message: "Containers must set securityContext.readOnlyRootFilesystem=true"
+        pattern:
+          spec:
+            containers:
+              - securityContext:
+                  readOnlyRootFilesystem: true
+    - name: require-runasnonroot
+      match:
+        resources:
+          kinds:
+            - Pod
+      validate:
+        message: "Containers must run as non-root"
+        pattern:
+          spec:
+            securityContext:
+              runAsNonRoot: true
+    - name: disallow-privileged
+      match:
+        resources:
+          kinds:
+            - Pod
+      validate:
+        message: "Privileged containers are disallowed"
+        deny:
+          anyPattern:
+            spec:
+              containers:
+                - securityContext:
+                    privileged: true
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/incident_response_math_incorrect.md
+# Incident response: Incorrect math result or verifier regression
+
+Purpose
+-------
+Provide immediate steps when a math-related production incident occurs (incorrect numeric/symbolic output, verification gate bypass, or invariants failing).
+
+Steps
+1. Triage
+   - Identify the affected service (calc-service, calc-pool, nli-service, retriever).
+   - Check Prometheus dashboards: calc_p95, nli_p95, verification blocked spikes.
+   - Pull the verification audit log (/tmp/verification_audit.log or centralized location).
+
+2. Containment
+   - If a deployed model or service produced incorrect results and auto-actions are affected: set FAIL_CLOSED=true in environment/config to ensure manual gating.
+   - Scale down risky endpoints if necessary: kubectl scale deployment/<name> --replicas=0 -n aegis-retriever
+
+3. Root cause
+   - Reproduce the failing input via calc_service or verifier_entrypoint locally.
+   - Run worker fuzzer and check /tmp/worker_fuzzer_findings.jsonl for related crashes.
+   - If invariant-related, collect continuous_fuzz counterexamples and run targeted_prover.py locally.
+
+4. Remediation
+   - If CAS produced incorrect numeric result: pin a known-good calc_worker image, restart calc-pool.
+   - If NLI drift suspected: roll back NLI model to last known good version (use model_manager records).
+   - If invariant gap found: add temporary runtime monitor and create an invariant task via invariant_registry.
+
+5. Postmortem
+   - Document incident in incident tracker and include evidence bundle (snapshots, provenance, verification logs, model versions).
+   - Update invariants or add additional tests to PR shard to prevent recurrence.
+
*** End Patch
*** End Patch
