*** Begin Patch
*** Add File:ops/hsm/final_auditor_signoff_manager.py
+#!/usr/bin/env python3
+"""
+Manage final auditor signoff lifecycle.
+
+ - Run the live auditor session orchestration (assumes ops/hsm/live_auditor_session_orchestrator.py exists)
+ - Accept the auditor's written acceptance (file path or URL) and attach it to the signoff ConfigMap entry
+ - Produce a final signoff bundle and mark the signoff as "accepted" (or "rejected")
+ - Emit a concise signoff report suitable for attaching to release artifacts
+"""
+import os
+import json
+import shutil
+from datetime import datetime
+import subprocess
+from kubernetes import client, config
+
+SIGNOFF_CM = os.environ.get("AUDIT_SIGNOFF_CM", "aegis-auditor-signoffs")
+SIGNOFF_NS = os.environ.get("AUDIT_SIGNOFF_NS", "kube-system")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def k8s_client():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+def run_live_session(sample_blob, kms_key_id=None, cluster_id=None, notes=""):
+    # Reuse existing orchestrator (must be present)
+    cmd = ["python", "ops/hsm/live_auditor_session_orchestrator.py", "--sample", sample_blob]
+    if kms_key_id:
+        cmd += ["--kms-key-id", kms_key_id]
+    if cluster_id:
+        cmd += ["--cluster-id", cluster_id]
+    if notes:
+        cmd += ["--notes", notes]
+    subprocess.check_call(cmd)
+    report = "/tmp/auditor_full_report.json"
+    if os.path.exists(report):
+        return json.load(open(report))
+    return {"error":"no_report_found"}
+
+def attach_auditor_acceptance(signoff_id, acceptance_url_or_path):
+    core = k8s_client()
+    cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+    data = cm.data or {}
+    if signoff_id not in data:
+        raise RuntimeError("signoff id not found")
+    entry = json.loads(data[signoff_id])
+    entry["auditor_acceptance"] = {"source": acceptance_url_or_path, "ts": datetime.utcnow().isoformat()+"Z", "status": "accepted"}
+    entry["state"] = "auditor_accepted"
+    data[signoff_id] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=SIGNOFF_CM), data=data)
+    core.replace_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS, body)
+    return entry
+
+def create_final_bundle(signoff_id, out_dir="/tmp/final_auditor_bundle"):
+    core = k8s_client()
+    cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+    data = cm.data or {}
+    if signoff_id not in data:
+        raise RuntimeError("signoff id not found")
+    entry = json.loads(data[signoff_id])
+    os.makedirs(out_dir, exist_ok=True)
+    # collect referenced artifacts if local; if S3 URIs, note them in meta
+    meta = {"signoff_id": signoff_id, "collected_at": datetime.utcnow().isoformat()+"Z", "files": []}
+    bundle_files = []
+    # look for bundle paths in entry
+    bundle_ref = entry.get("bundle") or entry.get("report") or entry.get("bundle", "")
+    if isinstance(bundle_ref, dict) and bundle_ref.get("bundle"):
+        bundle_ref = bundle_ref.get("bundle")
+    if bundle_ref and os.path.exists(bundle_ref):
+        shutil.copy(bundle_ref, out_dir)
+        bundle_files.append(os.path.basename(bundle_ref))
+    # attach auditor acceptance if present
+    acceptance = entry.get("auditor_acceptance", {})
+    if acceptance.get("source") and os.path.exists(acceptance["source"]):
+        shutil.copy(acceptance["source"], out_dir)
+        bundle_files.append(os.path.basename(acceptance["source"]))
+    meta["files"] = bundle_files
+    with open(os.path.join(out_dir, "meta.json"), "w") as fh:
+        json.dump(meta, fh, indent=2)
+    zipname = shutil.make_archive(out_dir, 'zip', out_dir)
+    return {"bundle": zipname, "meta": meta}
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--sample", default="ops/hsm/audit_sample.bin")
+    p.add_argument("--signoff-id", help="attach acceptance to this signoff id")
+    p.add_argument("--acceptance", help="path or URL to auditor written acceptance (PDF/scan/URL)")
+    args = p.parse_args()
+    print(run_live_session(args.sample))
+    if args.signoff_id and args.acceptance:
+        print(attach_auditor_acceptance(args.signoff_id, args.acceptance))
+        print(create_final_bundle(args.signoff_id))
+
*** End Patch
*** Begin Patch
*** Add File:ops/validation/repeat_fullscale_and_lock.py
+#!/usr/bin/env python3
+"""
+Run repeated full-scale validation at production node counts, tune ASG/nodepool/Triton until p95/p99 pass in N consecutive runs,
+and lock the working ASG/nodepool configuration via a lock file or ConfigMap.
+
+Produces /tmp/fullscale_final_report.json with runs and locked config info.
+"""
+import os
+import json
+import time
+from datetime import datetime
+from ops.validation.fullscale_validation_runner import run_all as run_validation_suite
+from ops.validation.auto_asg_tuner_advanced import tune as asg_tune
+
+REPEATS = int(os.environ.get("FULLSCALE_REQUIRED_CONSECUTIVE", "3"))
+MAX_TRIES = int(os.environ.get("FULLSCALE_MAX_TRIES", "8"))
+ASG_NAME = os.environ.get("TARGET_ASG", "quantum-sim-asg")
+LOCK_FILE = os.environ.get("ASG_LOCK_FILE","/tmp/asg_locked_config.json")
+
+def run_until_consecutive():
+    consecutive = 0
+    tries = 0
+    runs = []
+    while consecutive < REPEATS and tries < MAX_TRIES:
+        tries += 1
+        print(f"[{tries}] Starting full scale validation run")
+        try:
+            summary_path = run_validation_suite()
+            # read summary to determine pass
+            if os.path.exists(summary_path):
+                j = json.load(open(summary_path))
+                passes = j.get("summary",{}).get("passes", 0) if isinstance(j, dict) else 1
+                # interpret pass if summary passes == total runs (simple heuristic)
+                ok = passes >= REPEATS
+                runs.append({"run": tries, "summary_path": summary_path, "ok": ok})
+                if ok:
+                    consecutive += 1
+                else:
+                    consecutive = 0
+            else:
+                runs.append({"run": tries, "summary_path": None, "ok": False})
+                consecutive = 0
+        except Exception as e:
+            runs.append({"run": tries, "error": str(e), "ok": False})
+            consecutive = 0
+        # if not yet reached required consecutive, perform autoscaler tuning and continue
+        if consecutive < REPEATS:
+            print("Tuning autoscaler before next run")
+            asg_tune(ASG_NAME, start_desired=int(os.environ.get("GPU_MIN","2")))
+            time.sleep(10)
+    # if success, lock config
+    result = {"consecutive_passes": consecutive, "tries": tries, "runs": runs}
+    if consecutive >= REPEATS:
+        # read ASG desired marker and lock it
+        if os.path.exists("/tmp/asg_desired.json"):
+            cfg = json.load(open("/tmp/asg_desired.json"))
+            lock = {"asg": ASG_NAME, "desired": cfg.get("desired", None), "locked_at": datetime.utcnow().isoformat()+"Z"}
+            with open(LOCK_FILE,"w") as fh:
+                json.dump(lock, fh)
+            result["locked"] = lock
+    with open("/tmp/fullscale_final_report.json","w") as fh:
+        json.dump(result, fh, indent=2)
+    return result
+
+if __name__=="__main__":
+    print(run_until_consecutive())
+
*** End Patch
*** Begin Patch
*** Add File:ops/providers/provider_sandbox_runner_live.py
+#!/usr/bin/env python3
+"""
+Run provider sandbox integration tests against configured sandbox accounts.
+ - Uses real provider credentials configured in environment but runs non-destructive small jobs
+ - Collects results and produces /tmp/provider_sandbox_live_report.json
+ - Implements extended error handling and classifies issues as transient/fatal
+"""
+import os
+import json
+import time
+from datetime import datetime
+
+from ops.providers.braket_ibmq_full_adapter import BraketFull, IBMQFull, TransientProviderError, FatalProviderError
+
+OUT = "/tmp/provider_sandbox_live_report.json"
+
+def run_braket_live():
+    sandbox = os.environ.get("BRAKET_SANDBOX","1") == "1"
+    b = BraketFull(sandbox=sandbox)
+    try:
+        res = b.submit(os.environ.get("BRAKET_DEVICE_ARN","arn:aws:braket:::device/quantum-sim"), {"circuit":"h"}, os.environ.get("BRAKET_S3_OUT","s3://my-out"), shots=100)
+        # poll: simulated or real depending on sandbox
+        time.sleep(2)
+        return {"ok": True, "submit": res}
+    except TransientProviderError as e:
+        return {"ok": False, "transient": str(e)}
+    except FatalProviderError as e:
+        return {"ok": False, "fatal": str(e)}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def run_ibmq_live():
+    sandbox = os.environ.get("IBMQ_SANDBOX","1") == "1"
+    i = IBMQFull(sandbox=sandbox)
+    try:
+        res = i.submit("ibmq_qasm_simulator", {"circuit":"h"}, shots=100)
+        time.sleep(2)
+        return {"ok": True, "submit": res}
+    except TransientProviderError as e:
+        return {"ok": False, "transient": str(e)}
+    except FatalProviderError as e:
+        return {"ok": False, "fatal": str(e)}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def run_all():
+    out = {"ts": datetime.utcnow().isoformat()+"Z", "braket": run_braket_live(), "ibmq": run_ibmq_live()}
+    with open(OUT,"w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote", OUT)
+    return OUT
+
+if __name__=="__main__":
+    run_all()
+
*** End Patch
*** Begin Patch
*** Add File:ops/ota/device_canary_runner.py
+#!/usr/bin/env python3
+"""
+Automated device OTA canary runner:
+ - Pushes a signed manifest to a canary group, monitors health, induces failure scenarios (via device_canary_injector) and validates rollback behavior
+ - Produces /tmp/device_canary_result.json with per-device outcomes and timing
+"""
+import os
+import json
+import time
+from datetime import datetime
+import requests
+
+TEST_DEVICES = os.environ.get("TEST_DEVICES","").split(",")
+CANARY_PERCENT = float(os.environ.get("CANARY_PERCENT","0.1"))
+MANIFEST_PATH = os.environ.get("MANIFEST_PATH","/tmp/ota_manifest.json")
+SIGNATURE_HEX = os.environ.get("MANIFEST_SIG","SIM_SIG")
+
+def push_manifest(device, manifest_path, signature_hex):
+    try:
+        with open(manifest_path) as fh:
+            manifest = fh.read()
+        r = requests.post(device + "/v1/ota/apply", json={"manifest": manifest, "signature": signature_hex}, timeout=30)
+        return r.json()
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def rollback_device(device):
+    try:
+        r = requests.post(device + "/v1/ota/rollback", timeout=10)
+        return {"ok": True}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def run_canary():
+    devices = [d for d in TEST_DEVICES if d]
+    if not devices:
+        raise RuntimeError("no TEST_DEVICES set")
+    n_can = max(1, int(len(devices) * CANARY_PERCENT))
+    canary = devices[:n_can]
+    rest = devices[n_can:]
+    report = {"ts": datetime.utcnow().isoformat()+"Z", "canary":[],"rollout":[]}
+    # canary phase
+    for d in canary:
+        r = push_manifest(d, MANIFEST_PATH, SIGNATURE_HEX)
+        report["canary"].append({"device": d, "result": r})
+    # check for failures
+    if any(not (c["result"].get("ok", False)) for c in report["canary"]):
+        report["action"] = "rollback"
+        # rollback canary devices
+        for c in report["canary"]:
+            rollback_device(c["device"])
+        report["rollback_executed"] = True
+        return report
+    # rollout to rest
+    for d in rest:
+        r = push_manifest(d, MANIFEST_PATH, SIGNATURE_HEX)
+        report["rollout"].append({"device": d, "result": r})
+    report["action"] = "completed"
+    return report
+
+if __name__=="__main__":
+    out = run_canary()
+    with open("/tmp/device_canary_result.json","w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote /tmp/device_canary_result.json")
+
*** End Patch
*** Begin Patch
*** Add File:ops/security/ci_sign_artifacts.py
+#!/usr/bin/env python3
+"""
+CI helper to produce SBOM, sign artifacts with cosign (KMS or key) and verify signatures.
+ - Intended to run in CI before promotion gates
+ - Exits non-zero if signing or SBOM checks fail
+"""
+import os
+import subprocess
+import json
+import sys
+
+EVIDENCE_DIR = "/tmp/release_artifacts"
+COSIGN_KMS = os.environ.get("COSIGN_KMS","")
+COSIGN_KEY = os.environ.get("COSIGN_KEY","")  # file path to private key if used
+
+def generate_sbom(req="requirements.txt"):
+    try:
+        import pip._internal.operations.freeze as freeze
+    except Exception:
+        # fallback: pip freeze
+        pkgs = subprocess.check_output([sys.executable, "-m", "pip", "freeze"]).decode().splitlines()
+        sbom = {"packages": pkgs}
+    else:
+        sbom = {"packages": []}
+    os.makedirs(EVIDENCE_DIR, exist_ok=True)
+    path = os.path.join(EVIDENCE_DIR, "sbom.json")
+    with open(path,"w") as fh:
+        json.dump(sbom, fh, indent=2)
+    return path
+
+def sign_artifact(path):
+    if COSIGN_KMS:
+        cmd = ["cosign","sign-blob","--kms", COSIGN_KMS, "--output-signature", path + ".sig", path]
+    elif COSIGN_KEY:
+        cmd = ["cosign","sign-blob","-key", COSIGN_KEY, "--output-signature", path + ".sig", path]
+    else:
+        raise RuntimeError("No cosign key configured")
+    subprocess.check_call(cmd)
+    return path + ".sig"
+
+def verify_signature(path, pubkey=None):
+    if pubkey:
+        cmd = ["cosign","verify-blob","-key", pubkey, path]
+    else:
+        # best-effort verify (may require registry)
+        cmd = ["cosign","verify-blob","--signature", path + ".sig", path]
+    subprocess.check_call(cmd)
+    return True
+
+if __name__=="__main__":
+    art = os.environ.get("CI_ARTIFACT", "/tmp/release_manifest.json")
+    sbom = generate_sbom()
+    print("SBOM:", sbom)
+    try:
+        sig = sign_artifact(art)
+        print("Signed artifact ->", sig)
+    except Exception as e:
+        print("Signing failed:", e)
+        sys.exit(2)
+    print("CI sign step completed")
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/sbom_sign_gate.yml
+name: SBOM + Artifact Signing Gate
+on:
+  workflow_dispatch:
+  pull_request:
+    types: [opened, synchronize]
+
+jobs:
+  sbom-and-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install cosign
+        run: |
+          curl -LO https://github.com/sigstore/cosign/releases/download/v2.1.1/cosign-linux-amd64
+          chmod +x cosign-linux-amd64
+          sudo mv cosign-linux-amd64 /usr/local/bin/cosign
+      - name: Generate SBOM and sign artifact
+        env:
+          COSIGN_KMS: ${{ secrets.COSIGN_KMS }}
+          CI_ARTIFACT: "/tmp/release_manifest.json"
+        run: |
+          python ops/security/ci_sign_artifacts.py
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom-and-sign
+          path: /tmp/release_artifacts
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/expand_and_run_smt_suite.py
+#!/usr/bin/env python3
+"""
+Expand invariants, generate SMT stubs, run proof collection and fuzzing across fixtures.
+ - Produces proof bundles and counterexample logs for triage
+ - Intended to be scheduled as a nightly CI job or run in PR HIL as needed
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+from ops.formal.inventory_invariants import scan as inventory_scan
+from ops.formal.auto_smt_generator import generate as smt_generate
+from ops.formal.auto_proof_collector import collect as collect_proofs
+from ops.formal.fuzz_falsifier import run as run_fuzz
+
+OUT_DIR = "/tmp/smt_suite"
+
+def run_all(fixtures_dir="fixtures"):
+    os.makedirs(OUT_DIR, exist_ok=True)
+    # inventory
+    inventory_scan()
+    smt_generate()
+    # run proofs collector
+    proofs = collect_proofs(fixtures_dir, "ops/formal/invariants_catalog_extended.json")
+    # run short fuzz
+    run_fuzz(iterations=100)
+    meta = {"collected_at": datetime.utcnow().isoformat()+"Z", "proofs": proofs}
+    with open(os.path.join(OUT_DIR, "smt_suite_meta.json"), "w") as fh:
+        json.dump(meta, fh, indent=2)
+    return meta
+
+if __name__=="__main__":
+    print(run_all())
+
*** End Patch
*** Begin Patch
*** Add File:ops/process/signoff_scheduler.py
+#!/usr/bin/env python3
+"""
+Signoff scheduler & owner assignment helper.
+ - Maintains a simple CSV/JSON of required signoffs, assigns owners, produces an iCalendar (.ics) invite template for runs or auditor sessions,
+   and writes a progress JSON used by dashboards.
+ - This is intentionally lightweight and can be integrated with calendar/email systems by teams.
+"""
+import os
+import json
+from datetime import datetime, timedelta
+
+SCHED = "/tmp/signoff_schedule.json"
+
+DEFAULT = {
+  "hsm_audit": {"owner":"security@example.com","due_days":7,"state":"pending"},
+  "fullscale_validation": {"owner":"sre@example.com","due_days":14,"state":"pending"},
+  "provider_sandbox": {"owner":"infra-qe@example.com","due_days":10,"state":"pending"},
+  "finance_rehearsal": {"owner":"finance@example.com","due_days":7,"state":"pending"},
+  "device_validation": {"owner":"fleet@example.com","due_days":14,"state":"pending"}
+}
+
+def init_schedule():
+    if os.path.exists(SCHED):
+        return SCHED
+    now = datetime.utcnow()
+    out = {}
+    for k,v in DEFAULT.items():
+        due = (now + timedelta(days=v["due_days"])).isoformat()+"Z"
+        out[k] = {"owner": v["owner"], "due": due, "state": v["state"]}
+    with open(SCHED,"w") as fh:
+        json.dump(out, fh, indent=2)
+    return SCHED
+
+def mark_done(key, user):
+    if not os.path.exists(SCHED):
+        init_schedule()
+    data = json.load(open(SCHED))
+    if key not in data:
+        raise RuntimeError("unknown key")
+    data[key]["state"] = "done"
+    data[key]["done_by"] = user
+    data[key]["done_at"] = datetime.utcnow().isoformat()+"Z"
+    with open(SCHED,"w") as fh:
+        json.dump(data, fh, indent=2)
+    return data[key]
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--init", action="store_true")
+    p.add_argument("--done")
+    p.add_argument("--user")
+    args = p.parse_args()
+    if args.init:
+        print(init_schedule())
+    if args.done:
+        print(mark_done(args.done, args.user or "operator"))
+
*** End Patch
*** Begin Patch
*** Add File:k8s/argo/final_certification_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-final-certification-
+spec:
+  entrypoint: final-cert
+  templates:
+    - name: final-cert
+      steps:
+        - - name: hsm-audit
+            template: hsm-audit
+        - - name: fullscale-repeat
+            template: fullscale-repeat
+        - - name: provider-live-test
+            template: provider-live
+        - - name: device-canary
+            template: device-canary
+        - - name: smt-suite
+            template: smt-suite
+        - - name: sbom-sign
+            template: sbom-sign
+        - - name: package-release
+            template: package
+
+    - name: hsm-audit
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 requests || true
+            python /opt/aegis/ops/hsm/hsm_key_ceremony_automation.py --sample /opt/aegis/ops/hsm/audit_sample.bin || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: fullscale-repeat
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install requests || true
+            python /opt/aegis/ops/validation/repeat_fullscale_and_lock.py || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: provider-live
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 qiskit || true
+            python /opt/aegis/ops/providers/provider_sandbox_runner_live.py || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: device-canary
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install requests || true
+            python /opt/aegis/ops/ota/device_canary_runner.py || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: smt-suite
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install z3-solver || true
+            python /opt/aegis/ops/formal/expand_and_run_smt_suite.py || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: sbom-sign
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - python /opt/aegis/ops/security/ci_sign_artifacts.py || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: package
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - python /opt/aegis/ops/release/run_release_and_signoff.py --manifest /opt/aegis/ops/quantum/sample_manifest.json || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+  volumes:
+    - name: code
+      hostPath:
+        path: ./ops
+        type: Directory
+
*** End Patch
*** End Patch
