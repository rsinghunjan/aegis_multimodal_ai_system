*** Begin Patch
*** Add File:ops/models/register_models.py
+#!/usr/bin/env python3
+"""
+Model registry for vision/AV and LLM endpoints.
+ - Register model metadata, SLA, provenance, endpoint URL and required artifacts.
+ - Registry stored at ops/models/model_registry.json
+"""
+import os
+import json
+from datetime import datetime
+
+REG_PATH = os.environ.get("MODEL_REGISTRY_PATH", "ops/models/model_registry.json")
+
+def load_registry():
+    if os.path.exists(REG_PATH):
+        try:
+            return json.load(open(REG_PATH))
+        except Exception:
+            return {}
+    return {}
+
+def save_registry(r):
+    os.makedirs(os.path.dirname(REG_PATH), exist_ok=True)
+    with open(REG_PATH, "w") as fh:
+        json.dump(r, fh, indent=2)
+
+def register_model(name, kind, endpoint, version, sla: dict = None, provenance: dict = None):
+    reg = load_registry()
+    reg[name] = {
+        "name": name,
+        "kind": kind,  # e.g., "vision-encoder","detector","vlm","llm"
+        "endpoint": endpoint,
+        "version": version,
+        "sla": sla or {"p99_latency_s": 1.0, "availability": 0.99},
+        "provenance": provenance or {},
+        "registered_at": datetime.utcnow().isoformat()+"Z"
+    }
+    save_registry(reg)
+    return reg[name]
+
+def get_model(name):
+    return load_registry().get(name)
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--name", required=True)
+    p.add_argument("--kind", required=True)
+    p.add_argument("--endpoint", required=True)
+    p.add_argument("--version", default="0.0.1")
+    p.add_argument("--p99", type=float, default=1.0)
+    p.add_argument("--avail", type=float, default=0.99)
+    args = p.parse_args()
+    print(register_model(args.name, args.kind, args.endpoint, args.version, sla={"p99_latency_s":args.p99,"availability":args.avail}))
+
*** End Patch
*** Begin Patch
*** Add File:ops/models/detector_adapter.py
+#!/usr/bin/env python3
+"""
+Simple detector adapter that exposes a predict(image_bytes) -> list(boxes,labels,score)
+ - Pluggable to real detectors (YOLO/Detectron) by implementing load_model() and run_model()
+ - Provides a small local HTTP wrapper if needed
+"""
+import os
+import base64
+try:
+    from PIL import Image
+except Exception:
+    Image = None
+
+MODEL = None
+
+def load_model(path=None):
+    # Placeholder: load a real detector in production
+    global MODEL
+    MODEL = {"name": "stub-detector", "version": "0.1"}
+    return MODEL
+
+def run_model(image_bytes):
+    # stub: return single central box with high confidence label "person"
+    if Image:
+        from io import BytesIO
+        im = Image.open(BytesIO(image_bytes))
+        w,h = im.size
+    else:
+        w,h = (640,480)
+    box = [w*0.4, h*0.4, w*0.6, h*0.8]  # x1,y1,x2,y2
+    return [{"box": box, "label": "person", "score": 0.95}]
+
+if __name__ == "__main__":
+    # quick smoke
+    import sys
+    p = sys.argv[1] if len(sys.argv)>1 else None
+    if not p:
+        print("usage: detector_adapter.py image.jpg")
+        raise SystemExit(2)
+    b = open(p,"rb").read()
+    load_model()
+    print(run_model(b))
+
*** End Patch
*** Begin Patch
*** Add File:ops/pipeline/multimodal_pipeline.py
+#!/usr/bin/env python3
+"""
+Multimodal embedding pipeline:
+ - Accepts image bytes + text query
+ - Produces image embedding (CLIP adapter), text embedding (via text->LLM API or simple TF-IDF stub),
+   concatenates/normalizes and calls reranker adapter to rank candidates.
+ - Integrates with ops/retrieval/vision_reranker_adapter.py
+"""
+import os
+import base64
+import json
+import requests
+from ops.models.vision.clip_adapter import embed_image_bytes
+from ops.retrieval.vision_reranker_adapter import score_candidates
+
+LLM_EMBED_URL = os.environ.get("TEXT_EMBED_URL", "")  # optional text embedding service
+
+def embed_text(text):
+    if LLM_EMBED_URL:
+        try:
+            r = requests.post(LLM_EMBED_URL, json={"text": text}, timeout=10)
+            r.raise_for_status()
+            return r.json().get("embedding")
+        except Exception:
+            pass
+    # fallback simple bag-of-words vector represented as sparse dict (not used by reranker directly)
+    return None
+
+def rank_with_image_text(image_bytes, query_text, candidates):
+    img_emb = embed_image_bytes(image_bytes)
+    txt_emb = embed_text(query_text)
+    # The reranker uses image_embedding + query_text
+    return score_candidates(query_text, img_emb, candidates)
+
+if __name__ == "__main__":
+    import sys
+    img = open(sys.argv[1],"rb").read()
+    cands = ["approach speed limit","person crossing","stop sign"]
+    print(rank_with_image_text(img, "is there a pedestrian?", cands))
+
*** End Patch
*** Begin Patch
*** Add File:ops/sim/carla/scenario_expander.py
+#!/usr/bin/env python3
+"""
+Expand scenario bank across weather, lighting, sensor noise and daylight cycles.
+Writes many scenario JSONs into ops/sim/carla/scenario_library/generated_expanded_*.json
+"""
+import os
+import json
+import itertools
+from datetime import datetime
+
+OUT_DIR = os.environ.get("CARLA_SCENARIO_OUT", "ops/sim/carla/scenario_library")
+os.makedirs(OUT_DIR, exist_ok=True)
+
+WEATHERS = ["clear","rain","fog","storm"]
+TIMES = ["dawn","day","dusk","night"]
+NOISE_LEVELS = [0.0, 0.05, 0.15]  # fraction of sensor noise
+PEDESTRIAN_BEHAVIORS = ["walk","run","jog","hesitate"]
+
+def expand(base_scenarios):
+    idx = 0
+    for base in base_scenarios:
+        base_obj = json.load(open(base))
+        for w,t,n,bh in itertools.product(WEATHERS, TIMES, NOISE_LEVELS, PEDESTRIAN_BEHAVIORS):
+            obj = dict(base_obj)
+            obj["id"] = f"exp_{os.path.splitext(os.path.basename(base))[0]}_{idx}"
+            obj["weather"] = w
+            obj["time_of_day"] = t
+            obj["sensor_noise"] = n
+            obj["pedestrian"]["behavior"] = bh
+            obj["seed"] = (base_obj.get("seed", 0) + idx) % 2**31
+            fn = os.path.join(OUT_DIR, f"generated_exp_{idx}.json")
+            with open(fn, "w") as fh:
+                json.dump(obj, fh, indent=2)
+            idx += 1
+    return idx
+
+if __name__ == "__main__":
+    import argparse, glob
+    p = argparse.ArgumentParser()
+    p.add_argument("--base-dir", default="ops/sim/carla/scenario_library")
+    args = p.parse_args()
+    bases = glob.glob(os.path.join(args.base_dir, "scenario_*.json"))
+    n = expand(bases)
+    print("Generated", n, "expanded scenarios")
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/eval_metrics.py
+#!/usr/bin/env python3
+"""
+Perception evaluation metrics instrumentation.
+ - Computes TP/FP/FN, detection distance (if ego/obj positions present), latency stats and writes a metrics JSON.
+ - Intended to be used by scenario runner after each run.
+"""
+import os
+import json
+import math
+from datetime import datetime
+
+def compute_iou(boxA, boxB):
+    xA = max(boxA[0], boxB[0])
+    yA = max(boxA[1], boxB[1])
+    xB = min(boxA[2], boxB[2])
+    yB = min(boxA[3], boxB[3])
+    inter = max(0, xB-xA) * max(0, yB-yA)
+    aA = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])
+    aB = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])
+    if aA + aB - inter == 0:
+        return 0.0
+    return inter / float(aA + aB - inter)
+
+def evaluate_detections(gt_boxes, pred_boxes, iou_thresh=0.5):
+    tp=0; fp=0; fn=0
+    used = [False]*len(gt_boxes)
+    for p in pred_boxes:
+        matched=False
+        for i,g in enumerate(gt_boxes):
+            if used[i]:
+                continue
+            if compute_iou(p["box"], g["box"]) >= iou_thresh:
+                matched=True; used[i]=True; break
+        if matched:
+            tp+=1
+        else:
+            fp+=1
+    fn = sum(1 for u in used if not u)
+    prec = tp / (tp+fp) if (tp+fp)>0 else 0.0
+    rec = tp / (tp+fn) if (tp+fn)>0 else 0.0
+    return {"tp":tp,"fp":fp,"fn":fn,"precision":prec,"recall":rec}
+
+def write_metrics(out_path, metrics):
+    metrics["ts"] = datetime.utcnow().isoformat()+"Z"
+    os.makedirs(os.path.dirname(out_path), exist_ok=True)
+    with open(out_path,"w") as fh:
+        json.dump(metrics, fh, indent=2)
+    return out_path
+
*** End Patch
*** Begin Patch
*** Add File:ops/llm/llm_server_provenance.py
+#!/usr/bin/env python3
+"""
+LLM endpoint wrapper with temperature control, provenance logging and verifier hook.
+ - POST /generate {"text": "...", "temp": 0.2, "verify": true}
+ - Logs generation provenance to PROV_DIR and optionally calls verifier (ops/nli/verify_nli.py)
+"""
+import os
+import time
+import json
+import uuid
+from datetime import datetime
+from flask import Flask, request, jsonify
+import subprocess
+
+LLM_BACKEND_URL = os.environ.get("LLM_API_URL", "")  # if empty, use stub
+PROV_DIR = os.environ.get("LLM_PROV_DIR", "/tmp/llm_prov")
+os.makedirs(PROV_DIR, exist_ok=True)
+
+app = Flask(__name__)
+
+def call_llm_backend(text, temp=0.0):
+    if LLM_BACKEND_URL:
+        import requests
+        r = requests.post(LLM_BACKEND_URL, json={"text":text,"temperature":temp}, timeout=30)
+        r.raise_for_status()
+        return r.json().get("output")
+    # stub: echo reversed text
+    time.sleep(0.02)
+    return text[::-1]
+
+def run_verifier(output, claim):
+    # call NLI verifier script; returns dict with verdict and score
+    try:
+        proc = subprocess.run(["python","ops/nli/verify_nli.py","--hyp",output,"--premise",claim,"--json"], capture_output=True, text=True, timeout=30)
+        if proc.returncode == 0:
+            return json.loads(proc.stdout)
+    except Exception:
+        pass
+    return {"verdict":"unknown","score":0.0}
+
+@app.route("/generate", methods=["POST"])
+def generate():
+    body = request.get_json() or {}
+    text = body.get("text","")
+    temp = float(body.get("temp", 0.0))
+    verify = body.get("verify", False)
+    req_id = str(uuid.uuid4())
+    start = time.time()
+    output = call_llm_backend(text, temp=temp)
+    latency = time.time() - start
+    prov = {
+        "id": req_id,
+        "input": text,
+        "temperature": temp,
+        "output": output,
+        "latency_s": latency,
+        "backend": LLM_BACKEND_URL or "stub",
+        "ts": datetime.utcnow().isoformat()+"Z"
+    }
+    prov_path = os.path.join(PROV_DIR, f"llm_{req_id}.json")
+    with open(prov_path, "w") as fh:
+        json.dump(prov, fh, indent=2)
+    verification = None
+    if verify and body.get("claim"):
+        verification = run_verifier(output, body.get("claim"))
+    return jsonify({"id": req_id, "output": output, "latency_s": latency, "provenance": prov_path, "verification": verification})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("LLM_PORT","8400")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/verify_nli.py
+#!/usr/bin/env python3
+"""
+Simple NLI verifier stub. For production, plug a real NLI model.
+ - Usage: python ops/nli/verify_nli.py --premise "..." --hyp "..." --json
+ - Returns JSON {"verdict":"entailment|contradiction|neutral","score":0.9}
+"""
+import argparse
+import json
+import sys
+def simple_nli(premise, hyp):
+    # naive: if premise words overlap with hyp -> entailment
+    ps = set(premise.lower().split())
+    hs = set(hyp.lower().split())
+    inter = ps & hs
+    score = len(inter)/max(1,len(hs))
+    if score > 0.6:
+        return {"verdict":"entailment","score":score}
+    if score < 0.1:
+        return {"verdict":"contradiction","score":score}
+    return {"verdict":"neutral","score":score}
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--premise", required=True)
+    p.add_argument("--hyp", required=True)
+    p.add_argument("--json", action="store_true")
+    args = p.parse_args()
+    out = simple_nli(args.premise, args.hyp)
+    if args.json:
+        print(json.dumps(out))
+    else:
+        print(out)
+    sys.exit(0)
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/generate_nli_evidence.yml
+name: Generate -> NLI -> Evidence (demo)
+on:
+  workflow_dispatch:
+
+jobs:
+  gen-verify:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests
+      - name: Start LLM provenance server (background)
+        run: |
+          (python ops/llm/llm_server_provenance.py &) ; sleep 2
+      - name: Run generation and verification
+        run: |
+          python - <<'PY'
+import requests, json, time
+payload = {"text":"The car must stop for pedestrians", "temp":0.1, "verify":True, "claim":"The car will stop for pedestrians"}
+res = requests.post("http://127.0.0.1:8400/generate", json=payload, timeout=30).json()
+print("Generation result:", res)
+# Collect provenance file and package minimal evidence
+prov = res.get("provenance")
+with open("/tmp/evidence_bundle.json","w") as fh:
+    json.dump({"generation":res, "prov":prov}, fh, indent=2)
+print("Wrote evidence bundle")
+PY
+      - name: Attempt sign (requires KMS/Vault secrets)
+        env:
+          EVIDENCE_SIGN_BACKEND: ${{ secrets.EVIDENCE_SIGN_BACKEND }}
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+          VAULT_TRANSIT_KEY: ${{ secrets.VAULT_TRANSIT_KEY }}
+        run: |
+          python ops/evidence/sign_with_kms_or_vault.py /tmp/evidence_bundle.json || true
+      - name: Upload evidence
+        uses: actions/upload-artifact@v4
+        with:
+          name: gen-nli-evidence
+          path: /tmp/evidence_bundle.json*
+
*** End Patch
*** Begin Patch
*** Add File:ops/agents/orchestrator_v2.py
+#!/usr/bin/env python3
+"""
+Hardened stateful orchestrator v2:
+ - SQLite-backed persistent plan store with step-level state and retry policies
+ - Preemption, step retries, step timeout, persistent memory (key-value), decision logging
+ - Exposes HTTP API: submit_plan, get_status, preempt, step_result, memory_get/set
+"""
+import os
+import sqlite3
+import threading
+import time
+import json
+from flask import Flask, request, jsonify
+from datetime import datetime, timedelta
+
+DB = os.environ.get("AGENT_DB_V2", "/tmp/aegis_agent_v2.db")
+DECISION_LOG = os.environ.get("DECISION_LOG", "/tmp/decision_log.jsonl")
+
+app = Flask(__name__)
+
+def init_db():
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    c.execute("CREATE TABLE IF NOT EXISTS plans (id TEXT PRIMARY KEY, owner TEXT, plan_json TEXT, state TEXT, created_at TEXT, updated_at TEXT)")
+    c.execute("CREATE TABLE IF NOT EXISTS steps (id TEXT PRIMARY KEY, plan_id TEXT, step_idx INTEGER, step_json TEXT, state TEXT, retries INTEGER, next_retry_at TEXT)")
+    c.execute("CREATE TABLE IF NOT EXISTS memory (k TEXT PRIMARY KEY, v TEXT)")
+    conn.commit(); conn.close()
+
+def log_decision(entry):
+    entry["ts"] = datetime.utcnow().isoformat()+"Z"
+    with open(DECISION_LOG, "a") as fh:
+        fh.write(json.dumps(entry) + "\n")
+
+def add_plan(plan_id, owner, plan):
+    conn = sqlite3.connect(DB)
+    c = conn.cursor()
+    ts = datetime.utcnow().isoformat()+"Z"
+    c.execute("INSERT OR REPLACE INTO plans (id,owner,plan_json,state,created_at,updated_at) VALUES (?,?,?,?,?,?)",
+              (plan_id, owner, json.dumps(plan), "pending", ts, ts))
+    # insert steps
+    for i,step in enumerate(plan.get("steps",[])):
+        sid = f"{plan_id}_s{i}"
+        c.execute("INSERT OR REPLACE INTO steps (id,plan_id,step_idx,step_json,state,retries,next_retry_at) VALUES (?,?,?,?,?,?,?)",
+                  (sid, plan_id, i, json.dumps(step), "pending", 0, None))
+    conn.commit(); conn.close()
+    log_decision({"action":"submit_plan","plan_id":plan_id,"owner":owner})
+
+def worker():
+    while True:
+        conn = sqlite3.connect(DB)
+        c = conn.cursor()
+        # pick next pending step whose next_retry_at is null or in past
+        c.execute("SELECT id,plan_id,step_idx,step_json,retries FROM steps WHERE state IN ('pending','retry') AND (next_retry_at IS NULL OR next_retry_at <= ?) ORDER BY rowid LIMIT 1", (datetime.utcnow().isoformat()+"Z",))
+        row = c.fetchone()
+        if row:
+            sid, pid, idx, step_json, retries = row
+            step = json.loads(step_json)
+            # mark running
+            c.execute("UPDATE steps SET state=?, next_retry_at=? WHERE id=?", ("running", None, sid))
+            conn.commit()
+            # execute simulated step with timeout
+            timeout = step.get("timeout_s", 5)
+            try:
+                # simulate call to action endpoint
+                time.sleep(step.get("duration", 0.1))
+                # success
+                c.execute("UPDATE steps SET state=?, retries=?, next_retry_at=? WHERE id=?", ("succeeded", retries, None, sid))
+                log_decision({"action":"step_succeeded","plan":pid,"step":idx,"step_id":sid})
+            except Exception as e:
+                retries += 1
+                if retries > step.get("max_retries", 3):
+                    c.execute("UPDATE steps SET state=? WHERE id=?", ("failed", sid))
+                    log_decision({"action":"step_failed","plan":pid,"step":idx,"error":str(e)})
+                else:
+                    next_at = (datetime.utcnow() + timedelta(seconds=(2 ** retries))).isoformat()+"Z"
+                    c.execute("UPDATE steps SET state=?, retries=?, next_retry_at=? WHERE id=?", ("retry", retries, next_at, sid))
+                    log_decision({"action":"step_retry","plan":pid,"step":idx,"retry":retries,"next_retry_at":next_at})
+            conn.commit()
+        conn.close()
+        time.sleep(0.5)
+
+@app.route("/submit_plan", methods=["POST"])
+def submit_plan_api():
+    body = request.get_json() or {}
+    pid = body.get("id")
+    owner = body.get("owner","unknown")
+    plan = body.get("plan")
+    if not pid or not plan:
+        return jsonify({"error":"id and plan required"}), 400
+    add_plan(pid, owner, plan)
+    return jsonify({"status":"submitted","id":pid})
+
+@app.route("/preempt/<plan_id>", methods=["POST"])
+def preempt(plan_id):
+    conn = sqlite3.connect(DB); c = conn.cursor()
+    c.execute("UPDATE plans SET state=?, updated_at=? WHERE id=?", ("preempted", datetime.utcnow().isoformat()+"Z", plan_id))
+    conn.commit(); conn.close()
+    log_decision({"action":"preempt","plan":plan_id})
+    return jsonify({"status":"preempted"})
+
+@app.route("/memory/set", methods=["POST"])
+def mem_set():
+    body = request.get_json() or {}
+    k = body.get("k"); v = body.get("v")
+    if not k:
+        return jsonify({"error":"k required"}), 400
+    conn = sqlite3.connect(DB); c=conn.cursor()
+    c.execute("INSERT OR REPLACE INTO memory (k,v) VALUES (?,?)", (k, json.dumps(v)))
+    conn.commit(); conn.close()
+    return jsonify({"status":"ok"})
+
+@app.route("/memory/get")
+def mem_get():
+    k = request.args.get("k")
+    if not k:
+        return jsonify({"error":"k required"}), 400
+    conn = sqlite3.connect(DB); c=conn.cursor()
+    c.execute("SELECT v FROM memory WHERE k=?", (k,))
+    row = c.fetchone(); conn.close()
+    if not row:
+        return jsonify({"found":False})
+    return jsonify({"found":True,"v":json.loads(row[0])})
+
+@app.route("/status/<plan_id>")
+def status(plan_id):
+    conn = sqlite3.connect(DB); c=conn.cursor()
+    c.execute("SELECT id,owner,plan_json,state,created_at,updated_at FROM plans WHERE id=?", (plan_id,))
+    row = c.fetchone()
+    if not row:
+        return jsonify({"error":"not found"}),404
+    c.execute("SELECT id,step_idx,state,retries,next_retry_at FROM steps WHERE plan_id=? ORDER BY step_idx", (plan_id,))
+    steps = [{"id":r[0],"idx":r[1],"state":r[2],"retries":r[3],"next_retry_at":r[4]} for r in c.fetchall()]
+    conn.close()
+    return jsonify({"plan":{"id":row[0],"owner":row[1],"plan":json.loads(row[2]),"state":row[3]},"steps":steps})
+
+if __name__ == "__main__":
+    init_db()
+    t = threading.Thread(target=worker, daemon=True)
+    t.start()
+    app.run(host="0.0.0.0", port=int(os.environ.get("ORCH_PORT","8600")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/action_contracts.json
+{
+  "StopVehicleAction": {
+    "inputs": ["command_id","urgency","timestamp"],
+    "outputs": ["ack"],
+    "assumptions": ["brake_actuator_functional"],
+    "guarantees": ["vehicle_will_brake_within_0.5s"],
+    "critical": true
+  },
+  "SetSpeedAction": {
+    "inputs": ["command_id","speed","timestamp"],
+    "outputs": ["ack"],
+    "assumptions": ["control_loop_running"],
+    "guarantees": ["command_applied_within_0.1s"],
+    "critical": true
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/enforce_action_contracts.py
+#!/usr/bin/env python3
+"""
+CI check to ensure deployments referencing action APIs include contract annotations and safe images.
+ - For k8s manifests deploying "safety-" namespaces, ensure annotations:
+     metadata.annotations["aegis/action-contract"] = "<ContractName>"
+ - Fail CI if contract is missing or contract is unknown in ops/formal/action_contracts.json
+"""
+import os
+import json
+import yaml
+from glob import glob
+
+CONTRACTS = json.load(open("ops/formal/action_contracts.json"))
+
+def check_manifest(path):
+    docs = list(yaml.safe_load_all(open(path)))
+    issues = []
+    for d in docs:
+        if not isinstance(d, dict):
+            continue
+        meta = d.get("metadata", {})
+        ns = meta.get("namespace","default")
+        if ns.startswith("safety-"):
+            ann = meta.get("annotations", {})
+            contract = ann.get("aegis/action-contract")
+            if not contract:
+                issues.append(f"{path}: missing aegis/action-contract annotation")
+            elif contract not in CONTRACTS:
+                issues.append(f"{path}: unknown contract {contract}")
+    return issues
+
+if __name__ == "__main__":
+    import sys
+    files = glob("k8s/**/*.yaml", recursive=True)
+    all_issues = []
+    for f in files:
+        all_issues += check_manifest(f)
+    if all_issues:
+        print("Action contract enforcement failed:")
+        for i in all_issues:
+            print(i)
+        raise SystemExit(2)
+    print("All manifests satisfied action contract checks.")
+
*** End Patch
*** Begin Patch
*** Add File:ops/rl/rl_sandbox.py
+#!/usr/bin/env python3
+"""
+RL sandbox runner:
+ - Runs a scenario in simulation, collects trajectories, and runs policy offline in a sandboxed container/process
+ - Produces a signed evidence bundle when completed
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+SANDBOX_OUT = os.environ.get("RL_SANDBOX_OUT", "/tmp/rl_sandbox")
+os.makedirs(SANDBOX_OUT, exist_ok=True)
+
+def run_simulation(scenario_path, out_dir):
+    # invoke scenario runner (CARLA) and produce frames/logs
+    subprocess.check_call(["python","ops/sim/carla/run_scenario.py","--scenario",scenario_path,"--out",out_dir])
+    return out_dir
+
+def run_policy(policy_cmd, trace_dir, out_dir):
+    # execute policy in isolated process reading trace_dir
+    # policy_cmd is a shell command that should read trace_dir and write results in out_dir
+    env = os.environ.copy()
+    env["TRACE_DIR"] = trace_dir
+    env["OUT_DIR"] = out_dir
+    subprocess.check_call(policy_cmd, shell=True, env=env)
+    return out_dir
+
+def package_and_sign(out_files, name):
+    bundle = os.path.join(SANDBOX_OUT, f"{name}.json")
+    with open(bundle,"w") as fh:
+        json.dump({"files": out_files, "ts": datetime.utcnow().isoformat()+"Z"}, fh, indent=2)
+    # sign if backend configured
+    try:
+        if os.environ.get("EVIDENCE_SIGN_BACKEND"):
+            subprocess.check_call(["python","ops/evidence/sign_with_kms_or_vault.py", bundle])
+            return bundle + ".sig"
+    except Exception:
+        pass
+    return bundle
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--scenario", required=True)
+    p.add_argument("--policy-cmd", required=True)
+    args = p.parse_args()
+    out = run_simulation(args.scenario, SANDBOX_OUT)
+    pol_out = run_policy(args.policy_cmd, out, SANDBOX_OUT)
+    signed = package_and_sign([out,pol_out], "sandbox_result")
+    print("Sandbox done, evidence:", signed)
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/require_signed_evidence.py
+#!/usr/bin/env python3
+"""
+CI script: require that any artifact promoted to safety registry has signed evidence.
+ - Input: evidence_path(s) as args or via env PROMOTION_EVIDENCE
+ - Checks for existence of .sig file alongside the artifact (or validate via KMS/Vault public key)
+"""
+import os
+import sys
+
+def check_signed(path):
+    sig = path + ".sig"
+    return os.path.exists(sig)
+
+if __name__ == "__main__":
+    paths = sys.argv[1:] or (os.environ.get("PROMOTION_EVIDENCE") or "").split(",")
+    missing = []
+    for p in paths:
+        if not p:
+            continue
+        if not check_signed(p):
+            missing.append(p)
+    if missing:
+        print("Missing signatures for evidence:", missing)
+        raise SystemExit(2)
+    print("All evidence artifacts signed.")
+
*** End Patch
*** Begin Patch
*** Add File:ops/human/dashboard.py
+#!/usr/bin/env python3
+"""
+Operator dashboard (minimal) - shows human tasks, pending plans and decision logs.
+ - Integrates with human_validation_service, orchestrator_v2 decision log, and model registry
+ - Run as a small Flask app for operations team
+"""
+import os
+import json
+from flask import Flask, render_template_string
+import requests
+
+HUMAN_URL = os.environ.get("HUMAN_VALIDATION_URL", "http://127.0.0.1:8200")
+ORCH_URL = os.environ.get("ORCH_URL", "http://127.0.0.1:8600")
+DECISION_LOG = os.environ.get("DECISION_LOG", "/tmp/decision_log.jsonl")
+MODEL_REG = os.environ.get("MODEL_REGISTRY_PATH", "ops/models/model_registry.json")
+
+app = Flask(__name__)
+
+TEMPLATE = """
+<h1>Aegis Operations Dashboard</h1>
+<h2>Human Tasks</h2>
+<pre>{{tasks}}</pre>
+<h2>Pending Plans</h2>
+<pre>{{plans}}</pre>
+<h2>Decision Log (tail)</h2>
+<pre>{{decisions}}</pre>
+<h2>Models</h2>
+<pre>{{models}}</pre>
+"""
+
+@app.route("/")
+def index():
+    try:
+        t = requests.get(HUMAN_URL + "/tasks", timeout=2).json()
+    except Exception:
+        t = {"error":"unavailable"}
+    try:
+        p = requests.get(ORCH_URL + "/status", timeout=2).json() if ORCH_URL else {}
+    except Exception:
+        p = {"error":"unavailable"}
+    dec_lines = []
+    try:
+        if os.path.exists(DECISION_LOG):
+            with open(DECISION_LOG) as fh:
+                dec_lines = fh.readlines()[-50:]
+    except Exception:
+        dec_lines = []
+    models = {}
+    try:
+        if os.path.exists(MODEL_REG):
+            models = json.load(open(MODEL_REG))
+    except Exception:
+        models = {}
+    return render_template_string(TEMPLATE, tasks=json.dumps(t, indent=2), plans=json.dumps(p, indent=2), decisions="".join(dec_lines), models=json.dumps(models, indent=2))
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("DASH_PORT","8300")))
+
*** End Patch
*** End Patch
