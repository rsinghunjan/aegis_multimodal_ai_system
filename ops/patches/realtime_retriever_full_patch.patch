*** Begin Patch
*** Add File:services/realtime_retriever/config.py
+# Configuration for Real-Time Retriever service
+import os
+
+# Milvus
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "localhost")
+MILVUS_PORT = int(os.environ.get("MILVUS_PORT", 19530))
+MILVUS_COLLECTION = os.environ.get("MILVUS_COLLECTION", "aegis_retriever")
+
+# Snapshot / Evidence storage
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")  # if empty, write locally to SNAPSHOT_DIR
+SNAPSHOT_DIR = os.environ.get("SNAPSHOT_DIR", "/tmp/retriever_snapshots")
+
+# Embeddings
+EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
+EMBEDDING_BATCH = int(os.environ.get("EMBEDDING_BATCH", 16))
+
+# Retriever behavior
+DEFAULT_TOP_K = int(os.environ.get("RETRIEVER_TOP_K", 5))
+CHUNK_SIZE = int(os.environ.get("RETRIEVER_CHUNK_SIZE", 500))
+CHUNK_OVERLAP = int(os.environ.get("RETRIEVER_CHUNK_OVERLAP", 50))
+
+# Fetcher
+USER_AGENT = os.environ.get("RETRIEVER_USER_AGENT", "Aegis-Retriever/1.0 (+https://example.com)")
+
+# Provenance
+PROVENANCE_DIR = os.environ.get("PROVENANCE_DIR", "/tmp/retriever_provenance")
+
+# Simple allowlist (comma separated domains) - policy should be authoritative, this is runtime guard
+ALLOWED_DOMAINS = os.environ.get("RETRIEVER_ALLOWED_DOMAINS", "example.com,news.example.org").split(",")
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/embeddings.py
+"""
+Embeddings wrapper. Tries to use sentence-transformers locally; if not available,
+falls back to a simple hashing-based vector (not suitable for production).
+"""
+import os
+import numpy as np
+
+from .config import EMBEDDING_MODEL, EMBEDDING_BATCH
+
+try:
+    from sentence_transformers import SentenceTransformer
+    _MODEL = SentenceTransformer(EMBEDDING_MODEL)
+except Exception:
+    _MODEL = None
+
+def embed_texts(texts):
+    """
+    texts: list[str]
+    returns: np.ndarray shape (n, d)
+    """
+    if _MODEL:
+        vecs = _MODEL.encode(texts, show_progress_bar=False, convert_to_numpy=True, batch_size=EMBEDDING_BATCH)
+        return np.array(vecs).astype("float32")
+    # fallback: simple hashing -> low-dim deterministic vector
+    out = []
+    for t in texts:
+        h = abs(hash(t))
+        # create tiny deterministic vector
+        vec = [(h >> (i*8)) & 0xFF for i in range(32)]
+        vec = np.array(vec, dtype="float32")
+        vec = vec / (np.linalg.norm(vec) + 1e-9)
+        out.append(vec)
+    return np.vstack(out)
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/fetcher.py
+"""
+Fetcher + extractor:
+ - fetch_url respects robots.txt (best-effort), grabs HTML, extracts visible text via BeautifulSoup,
+ - snapshots raw HTML to local disk (or S3 if configured), returns cleaned text.
+"""
+import os
+import hashlib
+import requests
+from urllib.parse import urlparse
+from bs4 import BeautifulSoup
+import time
+import logging
+import json
+import urllib.robotparser as robotparser
+
+from .config import USER_AGENT, SNAPSHOT_DIR, EVIDENCE_BUCKET, ALLOWED_DOMAINS
+
+logger = logging.getLogger("realtime_retriever.fetcher")
+logger.setLevel(logging.INFO)
+
+def _allowed_by_policy(url):
+    # Basic allowlist guard
+    dom = urlparse(url).netloc
+    if ALLOWED_DOMAINS and any(d and d in dom for d in ALLOWED_DOMAINS):
+        return True
+    return False
+
+def _snapshot_path(url):
+    h = hashlib.sha256(url.encode()).hexdigest()
+    fn = f"snapshot_{h}.html"
+    return os.path.join(SNAPSHOT_DIR, fn)
+
+def _fetch(url, timeout=15):
+    headers = {"User-Agent": USER_AGENT}
+    r = requests.get(url, headers=headers, timeout=timeout)
+    return r
+
+def extract_text(html):
+    soup = BeautifulSoup(html, "html.parser")
+    # remove scripts/styles
+    for s in soup(["script", "style", "noscript", "iframe"]):
+        s.decompose()
+    texts = []
+    for el in soup.find_all(["p", "h1","h2","h3","li"]):
+        t = el.get_text(separator=" ", strip=True)
+        if t:
+            texts.append(t)
+    return "\n\n".join(texts)
+
+def fetch_url(url):
+    """
+    Returns a dict:
+    {
+      "url": url,
+      "status_code": int,
+      "fetched_at": iso,
+      "snapshot_path": local path or S3 uri,
+      "content_hash": sha256,
+      "raw_text": text,
+      "error": optional
+    }
+    """
+    if not _allowed_by_policy(url):
+        return {"url": url, "error": "domain_not_allowed"}
+    # robots.txt check (best effort)
+    parsed = urlparse(url)
+    rp = robotparser.RobotFileParser()
+    try:
+        rp.set_url(f"{parsed.scheme}://{parsed.netloc}/robots.txt")
+        rp.read()
+        if not rp.can_fetch(USER_AGENT, url):
+            return {"url": url, "error": "disallowed_by_robots"}
+    except Exception:
+        # ignore robots issues and continue
+        pass
+
+    try:
+        r = _fetch(url)
+    except Exception as e:
+        return {"url": url, "error": f"fetch_error: {e}"}
+    fetched_at = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
+    html = r.text
+    content_hash = hashlib.sha256(html.encode("utf-8")).hexdigest()
+    os.makedirs(SNAPSHOT_DIR, exist_ok=True)
+    local_path = _snapshot_path(url)
+    with open(local_path, "w", encoding="utf-8") as fh:
+        fh.write(html)
+    snapshot_ref = local_path
+    # optional upload to S3
+    if EVIDENCE_BUCKET:
+        try:
+            import boto3
+            s3 = boto3.client("s3")
+            key = f"realtime_snapshots/{os.path.basename(local_path)}"
+            s3.upload_file(local_path, EVIDENCE_BUCKET, key)
+            snapshot_ref = f"s3://{EVIDENCE_BUCKET}/{key}"
+        except Exception as e:
+            logger.warning("S3 upload failed: %s", e)
+
+    text = extract_text(html)
+    return {
+        "url": url,
+        "status_code": r.status_code,
+        "fetched_at": fetched_at,
+        "snapshot_ref": snapshot_ref,
+        "content_hash": content_hash,
+        "raw_text": text,
+        "http_headers": dict(r.headers)
+    }
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/provenance.py
+"""
+Provenance utilities: create and persist provenance records for each snapshot/indexed chunk.
+Provenance includes: url, fetch_ts, snapshot_ref, content_hash, extractor, chunk offset, chunk_id.
+"""
+import os
+import json
+import hashlib
+from datetime import datetime
+from .config import PROVENANCE_DIR, EVIDENCE_BUCKET
+
+os.makedirs(PROVENANCE_DIR, exist_ok=True)
+
+def _write_local(path, data):
+    with open(path, "w", encoding="utf-8") as fh:
+        json.dump(data, fh, indent=2)
+
+def persist_provenance(provenance_record):
+    """
+    provenance_record: dict
+    returns: provenance_id and location (local path or s3)
+    """
+    pid = hashlib.sha256((provenance_record.get("url","") + provenance_record.get("content_hash","") + provenance_record.get("chunk_id","")).encode()).hexdigest()
+    fname = f"prov_{pid}.json"
+    local_path = os.path.join(PROVENANCE_DIR, fname)
+    provenance_record["persisted_at"] = datetime.utcnow().isoformat() + "Z"
+    _write_local(local_path, provenance_record)
+    if EVIDENCE_BUCKET:
+        try:
+            import boto3
+            s3 = boto3.client("s3")
+            key = f"realtime_provenance/{fname}"
+            s3.upload_file(local_path, EVIDENCE_BUCKET, key)
+            return pid, f"s3://{EVIDENCE_BUCKET}/{key}"
+        except Exception:
+            pass
+    return pid, local_path
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/indexer.py
+"""
+Milvus indexer wrapper: create collection, insert vectors with metadata, search and return results.
+Metadata stored per vector includes: prov_id, url, chunk_text, chunk_offset.
+"""
+import time
+import json
+import logging
+from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
+
+from .config import MILVUS_HOST, MILVUS_PORT, MILVUS_COLLECTION
+
+logger = logging.getLogger("realtime_retriever.indexer")
+logger.setLevel(logging.INFO)
+
+def connect():
+    connections.connect(host=MILVUS_HOST, port=str(MILVUS_PORT))
+
+def ensure_collection(dim):
+    if not utility.has_collection(MILVUS_COLLECTION):
+        fields = [
+            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
+            FieldSchema(name="prov_id", dtype=DataType.VARCHAR, max_length=128),
+            FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=1024),
+            FieldSchema(name="chunk_offset", dtype=DataType.INT64),
+            FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=8192),
+        ]
+        schema = CollectionSchema(fields, description="Aegis real-time retriever collection")
+        coll = Collection(name=MILVUS_COLLECTION, schema=schema)
+        coll.create_index(field_name="embedding", index_params={"index_type":"IVF_FLAT", "metric_type":"L2", "params":{"nlist":128}})
+        coll.load()
+        return coll
+    else:
+        coll = Collection(name=MILVUS_COLLECTION)
+        return coll
+
+def insert_vectors(vectors, metadatas):
+    """
+    vectors: list of list[float]
+    metadatas: list of dicts containing prov_id, url, chunk_offset, chunk_text
+    """
+    connect()
+    dim = len(vectors[0])
+    coll = ensure_collection(dim)
+    # prepare column data
+    provs = [m["prov_id"] for m in metadatas]
+    urls = [m["url"] for m in metadatas]
+    offsets = [m.get("chunk_offset",0) for m in metadatas]
+    texts = [m.get("chunk_text","")[:8191] for m in metadatas]
+    mr = coll.insert([vectors, provs, urls, offsets, texts])
+    coll.flush()
+    return mr.primary_keys
+
+def search(query_vector, top_k=5):
+    connect()
+    coll = Collection(name=MILVUS_COLLECTION)
+    coll.load()
+    expr = None
+    results = coll.search([query_vector], anns_field="embedding", param={"metric_type":"L2","params":{"nprobe":10}}, limit=top_k)
+    out = []
+    for hit in results[0]:
+        meta = {
+            "prov_id": hit.entity.get("prov_id"),
+            "url": hit.entity.get("url"),
+            "chunk_offset": hit.entity.get("chunk_offset"),
+            "chunk_text": hit.entity.get("chunk_text"),
+            "distance": float(hit.distance)
+        }
+        out.append(meta)
+    return out
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/app.py
+"""
+FastAPI Real-Time Retriever service.
+Endpoints:
+ - POST /fetch_and_index { "url": "..."}  -> fetches, snapshots, chunks, embeds, indexes and returns snapshot/prov ids
+ - GET  /retrieve?q=...&top_k=5            -> returns top-K snippets with provenance and score
+"""
+from fastapi import FastAPI, HTTPException, Body, Query
+from pydantic import BaseModel
+from typing import List
+import os
+import math
+import logging
+import uuid
+
+from .config import DEFAULT_TOP_K, CHUNK_SIZE, CHUNK_OVERLAP, PROVENANCE_DIR
+from .fetcher import fetch_url
+from .embeddings import embed_texts
+from .indexer import insert_vectors, search, connect, ensure_collection
+from .provenance import persist_provenance
+
+logger = logging.getLogger("realtime_retriever.app")
+logger.setLevel(logging.INFO)
+
+app = FastAPI(title="Aegis Real-Time Retriever")
+
+class FetchRequest(BaseModel):
+    url: str
+    allow_index: bool = True
+    note: str = ""
+
+def _chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
+    words = text.split()
+    chunks = []
+    i = 0
+    n = len(words)
+    while i < n:
+        j = min(n, i + chunk_size)
+        chunk = " ".join(words[i:j])
+        chunks.append((i, chunk))
+        if j == n:
+            break
+        i = max(0, j - overlap)
+    return chunks
+
+@app.post("/fetch_and_index")
+def fetch_and_index(req: FetchRequest):
+    res = fetch_url(req.url)
+    if "error" in res:
+        raise HTTPException(status_code=400, detail=res["error"])
+    text = res.get("raw_text","")
+    if not text.strip():
+        raise HTTPException(status_code=400, detail="no_text_extracted")
+    # chunk
+    chunks = _chunk_text(text)
+    texts = [c[1] for c in chunks]
+    offsets = [c[0] for c in chunks]
+    # embeddings
+    vecs = embed_texts(texts)
+    # persist provenance and index
+    prov_ids = []
+    metadatas = []
+    for off, txt, vec in zip(offsets, texts, vecs):
+        prov = {
+            "url": req.url,
+            "content_hash": res.get("content_hash"),
+            "snapshot_ref": res.get("snapshot_ref"),
+            "chunk_offset": off,
+            "chunk_text": txt,
+            "fetch_ts": res.get("fetched_at"),
+            "source_headers": res.get("http_headers"),
+            "note": req.note
+        }
+        pid, location = persist_provenance(prov)
+        prov_ids.append(pid)
+        metadatas.append({"prov_id": pid, "url": req.url, "chunk_offset": off, "chunk_text": txt})
+    # insert into Milvus
+    try:
+        pks = insert_vectors(vecs.tolist() if hasattr(vecs, "tolist") else vecs, metadatas)
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=f"index_error: {e}")
+    return {"url": req.url, "snapshot_ref": res.get("snapshot_ref"), "prov_ids": prov_ids, "indexed_ids": pks}
+
+@app.get("/retrieve")
+def retrieve(q: str = Query(...), top_k: int = DEFAULT_TOP_K):
+    # embed query
+    qvec = embed_texts([q])[0]
+    results = search(qvec.tolist() if hasattr(qvec, "tolist") else qvec, top_k=top_k)
+    # decorate with provenance paths if available
+    out = []
+    for r in results:
+        prov_path = os.path.join(PROVENANCE_DIR, f"prov_{r['prov_id']}.json")
+        prov = {}
+        if os.path.exists(prov_path):
+            try:
+                prov = __import__("json").load(open(prov_path))
+            except Exception:
+                prov = {}
+        out.append({"score": r["distance"], "snippet": r["chunk_text"], "url": r["url"], "chunk_offset": r["chunk_offset"], "prov": prov})
+    return {"query": q, "results": out}
+
*** End Patch
*** Begin Patch
*** Add File:services/realtime_retriever/requirements.txt
+fastapi
+uvicorn[standard]
+requests
+beautifulsoup4
+pymilvus==2.2.11
+sentence-transformers
+boto3
+python-multipart
+
*** End Patch
*** Begin Patch
*** Add File:docs/policies/realtime_retriever_policy.md
+# Aegis Real‑Time Retriever Policy (recommended template)
+
+This policy governs safe, auditable use of real‑time web retrieval in the Aegis platform.
+
+## Purpose
+Provide fresh, grounded information to models and operators while preserving auditability, security, privacy and licensing constraints.
+
+## Scope
+Applies to the Real‑Time Retriever service (fetcher + snapshotting + provenance + index) and any downstream systems that consume retrieved content (LLM/RAG flows, verifiers, automation runners).
+
+## Policy Statements
+
+1. Allowlist / Denylist
+   - Only domains in the platform allowlist may be fetched for automation/attestation use cases.
+   - Non‑allowlisted domains may be used for low‑risk research experiments with prior approval and controlled retention.
+   - Operators must maintain the canonical allowlist in the platform config and Legal must approve any additions involving licensed content.
+
+2. Provenance & Snapshotting
+   - Every fetch must produce:
+     - raw snapshot (HTML) persisted to evidence store (S3 or secure object store) or local snapshot dir,
+     - provenance record (URL, fetch_ts, content_hash, snapshot_ref, HTTP headers, extractor version),
+     - extractor output (cleaned text) and per‑chunk provenance.
+   - Provenance records must be immutable and referenced in outputs.
+
+3. TTL / Caching
+   - Snapshot TTL is configurable per domain:
+     - Trusted, static docs: TTL = 30 days
+     - News/status pages: TTL = 5 minutes
+     - Provider status pages: TTL = 1 minute
+   - Cache must be consulted before re‑fetching; re‑fetch allowed if stale per TTL.
+
+4. Approval Gates
+   - Any action that affects production systems (OTA, attestation, finance, device actuation) that relies on web‑derived facts must:
+     1. Pass automatic verifier (cross‑source check / NLI),
+     2. Be reviewed by a human operator (authorised approver),
+     3. Be recorded and signed (HSM/cosign) when executed.
+   - Low‑risk informational queries may bypass human signoff but must still persist provenance.
+
+5. Data Retention & Privacy
+   - Snapshots containing PII are subject to retention rules and must be redacted or purged per data protection policy.
+   - Default snapshot retention: 90 days. Legal may require shorter retention for certain sources.
+   - Provide a mechanism for "right to be forgotten" where feasible (purge snapshots & provenance).
+
+6. Rate Limits & Quotas
+   - Query and fetch quotas per team must be enforced to control cost and detect anomalies.
+   - Define soft & hard quotas; exceedance triggers review and temporary suspension.
+
+7. Licensing & Terms of Use
+   - Scraping or indexing must respect source TOS.
+   - For licensed content (news, proprietary docs), ingest only with explicit license and record license metadata in provenance.
+
+8. Security & Sanitization
+   - All fetched HTML must be sanitized (strip scripts, forms, embedded iframes).
+   - Do not execute remote JavaScript.
+   - Remove secrets-looking tokens prior to indexing; log redaction incidents.
+
+9. Monitoring & Alerts
+   - Monitor: fetch success rate, verifier pass rate, cache hit rate, top domains, human approvals per day.
+   - Alert on unusual spikes, repeated source failures, or increasing verifier failures.
+
+10. Audit & Evidence
+   - All outputs that used real‑time retrieval must include a provenance reference ID and a snapshot hash.
+   - Evidence bundles for releases must include snapshot(s) & provenance for any web input used in decisioning.
+
+## Roles & Responsibilities
+- Platform Security: review and approve allowlist changes and security posture.
+- Legal/Compliance: approve sources, licensing, retention schedules.
+- Platform Ops: enforce quotas, run retriever service, ensure snapshot durability.
+- Owners/Teams: request allowlist additions with justification and acceptance of licensing terms.
+
+## Change Process
+- Allowlist updates require a pull request, approval from Security + Legal, and a staged rollout.
+- Policy updates require approval from Platform governance.
+
*** End Patch
*** Begin Patch
*** Add File:examples/rag_client.py
+"""
+Example RAG client showing how to call the Real-Time Retriever service,
+include provenance in prompts, and run a simple verifier step before accepting model output.
+
+This is an illustrative client — replace LLM call with your model endpoint.
+"""
+import requests
+import json
+import os
+from typing import List
+
+RETRIEVER_URL = os.environ.get("RETRIEVER_URL", "http://localhost:8000")
+LLM_API = os.environ.get("LLM_API", "http://localhost:9000/generate")  # placeholder
+
+def call_retriever(query: str, top_k: int = 5):
+    r = requests.get(f"{RETRIEVER_URL}/retrieve", params={"q": query, "top_k": top_k}, timeout=10)
+    r.raise_for_status()
+    return r.json()
+
+def format_prompt_with_provenance(query: str, retriever_results: dict):
+    """
+    Build a prompt that includes retrieved snippets and provenance notes.
+    Each snippet includes a prov reference that can be included in outputs.
+    """
+    header = f"User query: {query}\n\nUse the following retrieved evidence (include citation id) to answer.\n\n"
+    pieces = []
+    for i, res in enumerate(retriever_results.get("results", []), start=1):
+        snippet = res["snippet"]
+        url = res["url"]
+        prov = res.get("prov", {})
+        prov_id = prov.get("content_hash") or prov.get("snapshot_ref") or res.get("prov", {}).get("prov_id") or res.get("prov", {}).get("content_hash", "")[:8]
+        pieces.append(f"[CITE-{i}] ({prov_id}) {snippet}\nSource: {url}\n")
+    prompt = header + "\n\n".join(pieces) + "\n\nAnswer concisely and include citations like [CITE-1] where you used evidence.\n"
+    return prompt
+
+def call_llm(prompt: str):
+    """
+    Replace with your real LLM call. This example posts JSON {prompt: ...} and expects `text` in response.
+    """
+    try:
+        r = requests.post(LLM_API, json={"prompt": prompt, "max_tokens": 512}, timeout=30)
+        r.raise_for_status()
+        return r.json().get("text","")
+    except Exception as e:
+        # fallback simple echo
+        return "LLM unavailable: " + str(e)
+
+def simple_verifier(answer: str, retriever_results: dict) -> dict:
+    """
+    Very simple verifier: ensure that each citation referenced in answer matches the provided citations.
+    A real verifier would do NLI/fact-checks across sources.
+    """
+    cited = []
+    import re
+    for m in re.finditer(r"\[CITE-(\d+)\]", answer):
+        cited.append(int(m.group(1)))
+    issues = []
+    for c in cited:
+        if c < 1 or c > len(retriever_results.get("results", [])):
+            issues.append(f"citation {c} out of range")
+    return {"ok": len(issues) == 0, "issues": issues, "cited": cited}
+
+def rag_query_flow(query: str):
+    retr = call_retriever(query, top_k=5)
+    prompt = format_prompt_with_provenance(query, retr)
+    llm_output = call_llm(prompt)
+    verification = simple_verifier(llm_output, retr)
+    response = {
+        "query": query,
+        "llm_output": llm_output,
+        "verification": verification,
+        "retrieved": retr
+    }
+    return response
+
+if __name__ == "__main__":
+    q = "What is the current maintenance status for provider X?"
+    out = rag_query_flow(q)
+    print(json.dumps(out, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:docs/policies/realtime_retriever_policy_summary.txt
+A concise summary of the Real-Time Retriever policy (for README / ops).
+
+- Use a controlled retriever service (not free-form browsing).
+- Allowlist domains; require Legal approval for license-protected sources.
+- Snapshot raw HTML and persist provenance (url, fetch_ts, content_hash, snapshot_ref).
+- Cache with TTL rules per domain; use short TTL for status/news pages.
+- Require verifier + human approval for any action that affects production.
+- Enforce quotas and monitor costs; redact PII and adhere to retention rules.
+
*** End Patch
