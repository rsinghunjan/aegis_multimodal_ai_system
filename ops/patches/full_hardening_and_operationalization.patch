*** Begin Patch
*** Add File:k8s/hardened_services.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-retriever
+
+---
+# ServiceAccount, Role and RoleBinding for restricted access
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-retriever-sa
+  namespace: aegis-retriever
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: aegis-retriever-role
+  namespace: aegis-retriever
+rules:
+  - apiGroups: [""]
+    resources: ["pods", "services", "endpoints", "configmaps"]
+    verbs: ["get", "list", "watch", "patch", "update"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: aegis-retriever-rolebinding
+  namespace: aegis-retriever
+subjects:
+  - kind: ServiceAccount
+    name: aegis-retriever-sa
+    namespace: aegis-retriever
+roleRef:
+  kind: Role
+  name: aegis-retriever-role
+  apiGroup: rbac.authorization.k8s.io
+
+---
+# NetworkPolicy: restrict ingress to within namespace and accepted ports
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: allow-same-namespace
+  namespace: aegis-retriever
+spec:
+  podSelector: {}
+  policyTypes:
+    - Ingress
+  ingress:
+    - from:
+        - podSelector: {}
+      ports:
+        - protocol: TCP
+          port: 8000
+        - protocol: TCP
+          port: 8100
+        - protocol: TCP
+          port: 8110
+        - protocol: TCP
+          port: 8200
+
+---
+# Hardened calc service deployment (seccomp annotation, read-only rootfs, non-root)
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: calc-service-hardened
+  namespace: aegis-retriever
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: calc-service-hardened
+  template:
+    metadata:
+      labels:
+        app: calc-service-hardened
+      annotations:
+        seccomp.security.alpha.kubernetes.io/pod: "runtime/default"
+    spec:
+      serviceAccountName: aegis-retriever-sa
+      securityContext:
+        runAsNonRoot: true
+        runAsUser: 65534
+      containers:
+        - name: calc
+          image: aegis/calc-service:latest
+          ports:
+            - containerPort: 8100
+          resources:
+            requests:
+              cpu: "250m"
+              memory: "256Mi"
+            limits:
+              cpu: "1000m"
+              memory: "1Gi"
+          securityContext:
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            capabilities:
+              drop: ["ALL"]
+          volumeMounts:
+            - name: tmp
+              mountPath: /tmp
+      volumes:
+        - name: tmp
+          emptyDir: {}
+
+---
+# Calc pool (queue + workers) for controlled concurrency
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: calc-pool
+  namespace: aegis-retriever
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: calc-pool
+  template:
+    metadata:
+      labels:
+        app: calc-pool
+      annotations:
+        seccomp.security.alpha.kubernetes.io/pod: "runtime/default"
+    spec:
+      serviceAccountName: aegis-retriever-sa
+      containers:
+        - name: calc-pool
+          image: aegis/calc-pool:latest
+          ports:
+            - containerPort: 8110
+          env:
+            - name: CALC_POOL_WORKERS
+              value: "2"
+          securityContext:
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop: ["ALL"]
+
+---
+# Local NLI service - configured to prefer GPU nodes via tolerations/nodeSelector
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: nli-service
+  namespace: aegis-retriever
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: nli-service
+  template:
+    metadata:
+      labels:
+        app: nli-service
+      annotations:
+        seccomp.security.alpha.kubernetes.io/pod: "runtime/default"
+    spec:
+      serviceAccountName: aegis-retriever-sa
+      containers:
+        - name: nli
+          image: aegis/nli-service:latest
+          ports:
+            - containerPort: 8200
+          resources:
+            requests:
+              cpu: "500m"
+              memory: "2Gi"
+            limits:
+              cpu: "2000m"
+              memory: "8Gi"
+          securityContext:
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop: ["ALL"]
+      tolerations:
+        - key: "nvidia.com/gpu"
+          operator: "Exists"
+          effect: "NoSchedule"
+      nodeSelector:
+        nvidia.com/gpu.present: "true"
+
+---
+# Retriever deployment (vector retriever)
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: realtime-retriever
+  namespace: aegis-retriever
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: realtime-retriever
+  template:
+    metadata:
+      labels:
+        app: realtime-retriever
+      annotations:
+        seccomp.security.alpha.kubernetes.io/pod: "runtime/default"
+    spec:
+      serviceAccountName: aegis-retriever-sa
+      containers:
+        - name: retriever
+          image: aegis/retriever:latest
+          ports:
+            - containerPort: 8000
+          resources:
+            requests:
+              cpu: "250m"
+              memory: "512Mi"
+            limits:
+              cpu: "1000m"
+              memory: "2Gi"
+          securityContext:
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            capabilities:
+              drop: ["ALL"]
+
+---
+# HorizontalPodAutoscaler for NLI (scale on CPU or custom metrics e.g., GPU utilization)
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: nli-service-hpa
+  namespace: aegis-retriever
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: nli-service
+  minReplicas: 1
+  maxReplicas: 4
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 70
+
+---
+# HorizontalPodAutoscaler for calc-pool to scale workers if queue grows (assumes custom metric available)
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: calc-pool-hpa
+  namespace: aegis-retriever
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: calc-pool
+  minReplicas: 1
+  maxReplicas: 3
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/model_manager.py
+#!/usr/bin/env python3
+"""
+Model manager for local NLI service:
+ - Downloads and pins model versions (using transformers caching)
+ - Provides commands to warm pools and list available versions
+ - Provides a basic health check to be used by CI or readiness probes
+"""
+import os
+import sys
+import json
+import logging
+from pathlib import Path
+
+MODEL_DIR = os.environ.get("LOCAL_NLI_MODELS_DIR", "/var/models/nli")
+DEFAULT_MODEL = os.environ.get("LOCAL_NLI_MODEL", "ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli")
+LOG = logging.getLogger("aegis.nli.model_manager")
+
+def ensure_model(model_name=DEFAULT_MODEL):
+    """
+    Ensure model is downloaded into the transformers cache and a local marker (versioned) exists.
+    """
+    try:
+        from transformers import AutoModel, AutoTokenizer
+        LOG.info("Downloading model %s", model_name)
+        _ = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_DIR)
+        _ = AutoModel.from_pretrained(model_name, cache_dir=MODEL_DIR)
+        # write marker
+        os.makedirs(MODEL_DIR, exist_ok=True)
+        fn = os.path.join(MODEL_DIR, "models.json")
+        data = {}
+        if os.path.exists(fn):
+            data = json.load(open(fn))
+        data[model_name] = {"installed": True}
+        with open(fn, "w") as fh:
+            json.dump(data, fh, indent=2)
+        print("ok", model_name)
+        return 0
+    except Exception as e:
+        LOG.exception("model download failed: %s", e)
+        print("error", str(e))
+        return 2
+
+def list_models():
+    fn = os.path.join(MODEL_DIR, "models.json")
+    if os.path.exists(fn):
+        print(open(fn).read())
+    else:
+        print("{}")
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--ensure", action="store_true")
+    p.add_argument("--list", action="store_true")
+    p.add_argument("--model")
+    args = p.parse_args()
+    if args.ensure:
+        sys.exit(ensure_model(args.model or DEFAULT_MODEL))
+    if args.list:
+        list_models()
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/warm_pool.py
+#!/usr/bin/env python3
+"""
+Warm pool helper: call the local NLI service a few times to populate model caches and validate readiness.
+Intended to be run as a post-deploy Kubernetes Job or CI step.
+"""
+import os
+import requests
+from time import sleep
+
+NLI_URL = os.environ.get("NLI_URL", "http://nli-service.aegis-retriever.svc.cluster.local:8200/v1/entail")
+
+def warm(count=3):
+    for i in range(count):
+        try:
+            r = requests.post(NLI_URL, json={"premise":"Test premise","hypothesis":"Test hypothesis"}, timeout=30)
+            print("warm:", i, r.status_code, r.text[:200])
+        except Exception as e:
+            print("warm error:", e)
+        sleep(1)
+
+if __name__ == "__main__":
+    warm()
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/seccomp_test.py
+#!/usr/bin/env python3
+"""
+CI check that ensures Kubernetes manifests include seccomp/readOnlyRootFilesystem/readOnlyRootFilesystem set
+ - Scans k8s manifest files under k8s/ and fails if key hardening annotations/flags are missing
+"""
+import os
+import sys
+import glob
+import yaml
+
+MANIFEST_DIRS = ["k8s", "chart"]
+
+REQUIRED_ANNOTATION = "seccomp.security.alpha.kubernetes.io/pod"
+
+def check_file(path):
+    with open(path) as fh:
+        docs = list(yaml.safe_load_all(fh))
+    issues = []
+    for d in docs:
+        if not isinstance(d, dict):
+            continue
+        kind = d.get("kind","")
+        if kind not in ("Deployment", "StatefulSet", "DaemonSet"):
+            continue
+        meta = d.get("spec",{}).get("template",{}).get("metadata",{})
+        ann = meta.get("annotations",{})
+        if REQUIRED_ANNOTATION not in ann:
+            issues.append(f"{path}: missing seccomp annotation in {kind}")
+        # check containers securityContext readOnlyRootFilesystem
+        containers = d.get("spec",{}).get("template",{}).get("spec",{}).get("containers",[])
+        for c in containers:
+            sc = c.get("securityContext",{})
+            if not sc.get("readOnlyRootFilesystem", False):
+                issues.append(f"{path}: container {c.get('name')} missing readOnlyRootFilesystem")
+    return issues
+
+def main():
+    files = []
+    for d in MANIFEST_DIRS:
+        for p in glob.glob(os.path.join(d, "**", "*.yaml"), recursive=True):
+            files.append(p)
+    all_issues = []
+    for f in files:
+        try:
+            all_issues.extend(check_file(f))
+        except Exception:
+            pass
+    if all_issues:
+        print("Hardening issues found:")
+        for i in all_issues:
+            print(" -", i)
+        sys.exit(2)
+    print("All manifests include basic seccomp/readOnly checks.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hardening_and_bench.yml
+name: Hardening checks, model warm & benchmark shard
+on:
+  pull_request:
+    types: [opened, synchronize]
+  workflow_dispatch:
+
+jobs:
+  seccomp-check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run seccomp & hardening checks
+        run: |
+          python ops/ci/seccomp_test.py
+
+  ensure-nli-and-warm:
+    runs-on: ubuntu-latest
+    needs: seccomp-check
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure NLI model present (CI quick check)
+        run: |
+          python ops/nli/model_manager.py --ensure || true
+      - name: Warm local NLI pool (best-effort)
+        run: |
+          python ops/nli/warm_pool.py || true
+
+  benchmark-shard:
+    runs-on: ubuntu-latest
+    needs: ensure-nli-and-warm
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run representative math shard
+        env:
+          LLM_API: ${{ secrets.LLM_API }}
+        run: |
+          # run a quick shard so PRs have a cheap smoke-test
+          python ops/ci/math_bench_runner.py
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/ambiguity_detector.py
+#!/usr/bin/env python3
+"""
+Detect semantically ambiguous premises that are syntactically valid.
+ - Heuristics: unresolved pronouns, single-letter references without binding, implicit quantifiers like "they", "it".
+ - Flags premises requiring human review via human_review_queue.
+"""
+import re
+from ops.process.human_review_queue import enqueue_review
+
+PRONOUNS = {"it","they","them","this","that","he","she","we","you","their","its"}
+
+def detect_ambiguity(text: str) -> dict:
+    toks = re.findall(r"[A-Za-z]+", text.lower())
+    pronouns = [t for t in toks if t in PRONOUNS]
+    unresolved_vars = []
+    # detect single-letter variable usage like "x" without "let x" or "where x is"
+    if re.search(r"\b[a-z]\b", text):
+        if not re.search(r"\b(let|where|such that|where x is)\b", text, re.I):
+            unresolved_vars.append("single_letter_var")
+    issues = []
+    if pronouns:
+        issues.append({"type":"pronoun", "examples": pronouns[:3]})
+    if unresolved_vars:
+        issues.append({"type":"unresolved_vars", "examples": unresolved_vars})
+    return {"ambiguous": bool(issues), "issues": issues}
+
+def check_and_enqueue(text: str, metadata: dict):
+    res = detect_ambiguity(text)
+    if res["ambiguous"]:
+        enqueue_review({"text": text, "metadata": metadata, "issues": res["issues"]})
+    return res
+
+if __name__ == "__main__":
+    print(check_and_enqueue("If it is 2, then x equals 3", {"source":"demo"}))
+
*** End Patch
*** Begin Patch
*** Add File:ops/process/human_review_queue.py
+#!/usr/bin/env python3
+"""
+Simple human review queue backed by a JSON file (can be swapped to ConfigMap or Task system).
+ - enqueue_review(item), list_reviews(), pop_review()
+"""
+import os
+import json
+from datetime import datetime
+
+QUEUE_FILE = os.environ.get("HUMAN_REVIEW_QUEUE", "/tmp/human_review_queue.json")
+
+def _load():
+    if os.path.exists(QUEUE_FILE):
+        try:
+            return json.load(open(QUEUE_FILE))
+        except Exception:
+            return []
+    return []
+
+def _save(q):
+    with open(QUEUE_FILE, "w") as fh:
+        json.dump(q, fh, indent=2)
+
+def enqueue_review(item):
+    q = _load()
+    item["id"] = f"rev-{len(q)+1}"
+    item["enqueued_at"] = datetime.utcnow().isoformat()+"Z"
+    q.append(item)
+    _save(q)
+    return item["id"]
+
+def list_reviews():
+    return _load()
+
+def pop_review():
+    q = _load()
+    if not q:
+        return None
+    item = q.pop(0)
+    _save(q)
+    return item
+
+if __name__ == "__main__":
+    print("enqueue demo:", enqueue_review({"text":"Ambiguous premise", "metadata":{"src":"demo"}}))
+    print("reviews:", list_reviews())
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/fuzz_triage_enhanced.py
+#!/usr/bin/env python3
+"""
+Enhanced fuzz triage that files tasks to a simple CSV/JSON for domain experts and includes severity scoring and reproducibility hints.
+ - Reads continuous_fuzz results and outputs /tmp/fuzz_triage_enhanced.json with suggested proof strategies.
+"""
+import os
+import json
+from datetime import datetime
+
+IN = "/tmp/continuous_fuzz_counterexamples.jsonl"
+OUT = "/tmp/fuzz_triage_enhanced.json"
+
+def severity_from_mutant(mut):
+    s = 1
+    if mut.get("speed",0) > 10:
+        s += 2
+    if mut.get("lateral_accel",0) > 3:
+        s += 2
+    if mut.get("planner_ms",0) > 200:
+        s += 1
+    return min(5, s)
+
+def run():
+    tasks = []
+    if not os.path.exists(IN):
+        print("no fuzz results")
+        with open(OUT,"w") as fh:
+            json.dump([], fh)
+        return OUT
+    for line in open(IN):
+        rec = json.loads(line)
+        mut = rec.get("mutant",{})
+        sev = severity_from_mutant(mut)
+        task = {
+            "id": f"task-{len(tasks)+1}",
+            "summary": f"Fuzz invariant violation severity {sev}",
+            "mutant": mut,
+            "severity": sev,
+            "repro": rec,
+            "suggested_provers": ["Z3", "SMT-Numerical", "Lyapunov-check"],
+            "created_at": datetime.utcnow().isoformat()+"Z"
+        }
+        tasks.append(task)
+    with open(OUT,"w") as fh:
+        json.dump(tasks, fh, indent=2)
+    print("wrote", OUT)
+    return OUT
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/benchmark_config.json
+{
+  "shards": {
+    "pr_smoke": {
+      "datasets": ["gsm8k_sample"],
+      "limit": 5,
+      "cost_estimate": 0.1
+    },
+    "nightly_full": {
+      "datasets": ["gsm8k","mmlumath","aqua"],
+      "limit_per_dataset": 100,
+      "cost_estimate": 50
+    }
+  },
+  "budget": {
+    "pr_max_cost": 1,
+    "nightly_max_cost": 200
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/benchmark_runner_orchestrated.py
+#!/usr/bin/env python3
+"""
+Orchestrator that reads ops/ci/benchmark_config.json and enforces budget caps.
+ - Runs PR shard (cheap) for pull requests and full nightly suites under budget limits.
+ - Produces /tmp/bench_orch_run.json summarizing runs and costs.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+CFG = "ops/ci/benchmark_config.json"
+OUT = "/tmp/bench_orch_run.json"
+
+def run_shard(name):
+    cfg = json.load(open(CFG))
+    shard = cfg["shards"].get(name)
+    if not shard:
+        return {"error":"unknown_shard"}
+    est = shard.get("cost_estimate",0)
+    # cheap vs full: call existing math bench runners with environment to limit examples
+    if name == "pr_smoke":
+        subprocess.call(["python","ops/ci/math_bench_runner.py"])
+    else:
+        subprocess.call(["python","ops/ci/math_bench_expanded.py"])
+    return {"shard": name, "estimated_cost": est}
+
+def main(mode="pr_smoke"):
+    cfg = json.load(open(CFG))
+    budget = cfg["budget"]
+    shard = "pr_smoke" if mode=="pr" else "nightly_full"
+    rec = {"ts": datetime.utcnow().isoformat()+"Z", "mode": mode, "shard": shard}
+    if mode == "pr" and budget["pr_max_cost"] < cfg["shards"][shard]["cost_estimate"]:
+        rec["skipped"] = "over_budget"
+    else:
+        res = run_shard(shard)
+        rec["result"] = res
+    with open(OUT,"w") as fh:
+        json.dump(rec, fh, indent=2)
+    print("wrote", OUT)
+
+if __name__ == "__main__":
+    import sys
+    m = "pr" if (len(sys.argv)>1 and sys.argv[1]=="pr") else "nightly"
+    main(m)
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/ensure_nli_and_scale.py
+#!/usr/bin/env python3
+"""
+CI helper to ensure local NLI deployment health and scale policy:
+ - Calls the nli_service /health and reports status
+ - Optionally calls Kubernetes to scale the nli-service deployment if CPU is high (requires kubectl/KUBECONFIG)
+"""
+import os
+import requests
+import sys
+
+NLI_HEALTH = os.environ.get("NLI_HEALTH_URL", "http://nli-service.aegis-retriever.svc.cluster.local:8200/health")
+
+def check_health():
+    try:
+        r = requests.get(NLI_HEALTH, timeout=5)
+        if r.status_code == 200:
+            print("nli_health_ok")
+            return 0
+    except Exception as e:
+        print("nli_health_error", e)
+    return 2
+
+if __name__ == "__main__":
+    sys.exit(check_health())
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/deploy_hardened_services.md
+Deploy & harden calc_service, calc_pool, nli_service, retriever
+------------------------------------------------------------
+
+Steps (summary):
+1. Apply k8s manifest:
+   kubectl apply -f k8s/hardened_services.yaml
+
+2. Ensure NodePools:
+   - NLI benefits from GPU nodes. Label GPU nodes with nvidia.com/gpu.present=true
+
+3. Install Prometheus / metrics server and export metrics:
+   - Ensure calc_pool exposes Prometheus metrics (it does). Configure scraping.
+
+4. Run model manager & warm pool:
+   - python ops/nli/model_manager.py --ensure
+   - Run as job: kubectl create job --from=cronjob/warm-nli warm-nli-job (or run ops/nli/warm_pool.py)
+
+5. Seccomp & CIS checks:
+   - Run CI seccomp check (ops/ci/seccomp_test.py) and verify pods have seccomp annotation and readOnlyRootFilesystem.
+
+6. Configure HPA and autoscaling:
+   - Review HorizontalPodAutoscaler objects in k8s/hardened_services.yaml and adjust min/max replicas to your capacity.
+
+7. Add network policies & RBAC per manifest restrictions.
+
+8. Monitor:
+   - Add Prometheus alerts for a) calc queue length, b) CAS latency > SLO, c) NLI latency > SLO, d) verification gate blocks.
+
*** End Patch
*** End Patch
