*** Begin Patch
*** Add File:ops/serving/clip_triton_client.py
+#!/usr/bin/env python3
+"""
+Small client to send images to a Triton CLIP image-encoder model and receive embeddings.
+
+ - Handles preprocessing (PIL -> NCHW FP32) to match a typical Triton PyTorch/ONNX model
+ - Retries on transient errors and validates embedding dimensionality
+"""
+import os
+import time
+import json
+import base64
+import requests
+import numpy as np
+from PIL import Image
+
+TRITON_URL = os.environ.get("TRITON_URL", "http://triton:8000/v2/models")
+RETRY = int(os.environ.get("TRITON_CLIENT_RETRY","3"))
+
+def preprocess_image(pil_image, size=(224,224)):
+    img = pil_image.convert("RGB").resize(size)
+    arr = np.array(img).astype(np.float32) / 255.0
+    # normalize as ImageNet
+    mean = np.array([0.485,0.456,0.406], dtype=np.float32)
+    std = np.array([0.229,0.224,0.225], dtype=np.float32)
+    arr = (arr - mean) / std
+    # HWC -> CHW, add batch dim
+    arr = np.transpose(arr, (2,0,1))[None, ...]
+    return arr
+
+def infer_model(model_name, input_tensor):
+    url = f"{TRITON_URL}/{model_name}/infer"
+    # Triton v2 JSON protocol expects inputs/outputs spec
+    payload = {
+        "inputs": [
+            {"name":"INPUT__0", "shape": list(input_tensor.shape), "datatype":"FP32", "data": input_tensor.reshape(-1).tolist()}
+        ]
+    }
+    last_err = None
+    for attempt in range(RETRY):
+        try:
+            r = requests.post(url, json=payload, timeout=30)
+            r.raise_for_status()
+            out = r.json()
+            # expect output at outputs[0].data
+            emb = np.array(out["outputs"][0]["data"], dtype=np.float32)
+            return emb
+        except Exception as e:
+            last_err = e
+            time.sleep(1 + attempt)
+    raise RuntimeError("triton inference failed: %s" % last_err)
+
+def embed_image_file(model_name, image_path):
+    img = Image.open(image_path)
+    t = preprocess_image(img)
+    emb = infer_model(model_name, t)
+    return emb
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--model", default="clip-embed")
+    p.add_argument("--image", required=True)
+    args = p.parse_args()
+    emb = embed_image_file(args.model, args.image)
+    print("embedding length:", len(emb))
+
*** End Patch
*** Begin Patch
*** Add File:ops/serving/triton/deploy_clip_model.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Prepare Triton model repo layout for a CLIP-like image embedder with dynamic batching
+OUT=${1:-./models_repo/clip-embed}
+MODEL_NAME=${2:-clip-embed}
+MAX_BATCH=${3:-8}
+INSTANCES=${4:-1}
+DIM=${5:-1024}
+
+mkdir -p "${OUT}/1"
+cat > "${OUT}/config.pbtxt" <<EOF
+name: "${MODEL_NAME}"
+platform: "onnxruntime_onnx"
+max_batch_size: ${MAX_BATCH}
+input [
+  {
+    name: "INPUT__0"
+    data_type: TYPE_FP32
+    dims: [3, 224, 224]
+  }
+]
+output [
+  {
+    name: "OUTPUT__0"
+    data_type: TYPE_FP32
+    dims: [ ${DIM} ]
+  }
+]
+dynamic_batching {
+  preferred_batch_size: [ 1, 2, 4, ${MAX_BATCH} ]
+  max_queue_delay_microseconds: 1000
+}
+instance_group [
+  { kind: KIND_GPU, count: ${INSTANCES} }
+]
+optimize {
+  priority: PRIORITY_DEFAULT
+}
+EOF
+
+echo "Triton model config written to ${OUT}/config.pbtxt"
+echo "# Copy your model binary (ONNX/plan) into ${OUT}/1/"
+
*** End Patch
*** Begin Patch
*** Add File:k8s/triton/clip-triton-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-triton-clip
+  namespace: aegis-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-triton-clip
+  template:
+    metadata:
+      labels:
+        app: aegis-triton-clip
+    spec:
+      containers:
+        - name: triton
+          image: nvcr.io/nvidia/tritonserver:23.06-py3
+          args: ["tritonserver","--model-repository=/models","--exit-on-error=true"]
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "4"
+              memory: "12Gi"
+          env:
+            - name: TRITON_SERVER_ARGS
+              value: "--http-thread-count=4 --grpc-infer-allocation-pool-size=1"
+          volumeMounts:
+            - name: models
+              mountPath: /models
+      volumes:
+        - name: models
+          persistentVolumeClaim:
+            claimName: triton-models-pvc
+
+---
+# HPA placeholder - use KEDA or custom metrics for GPU queue length in production
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-triton-clip-hpa
+  namespace: aegis-system
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-triton-clip
+  minReplicas: 1
+  maxReplicas: 5
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:ops/memory/milvus_memory_service_v2.py
+#!/usr/bin/env python3
+"""
+Milvus-backed Agent Memory Service V2
+
+ - Stores episodic memories with timestamp, episode_id, type, text, metadata and embedding vector
+ - Provides retention policy endpoints, export to S3 before delete, and bulk upsert/search APIs
+ - Uses Milvus collection `aegis_agent_memory_v2`
+"""
+import os
+import json
+import time
+from datetime import datetime, timedelta
+from flask import Flask, request, jsonify
+import numpy as np
+
+try:
+    from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
+except Exception:
+    connections = None
+
+import boto3
+S3 = boto3.client("s3") if os.environ.get("EVIDENCE_BUCKET") else None
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET","")
+
+APP = Flask(__name__)
+COLLECTION = os.environ.get("MEMORY_COLLECTION","aegis_agent_memory_v2")
+
+def connect():
+    if connections:
+        connections.connect("default", host=os.environ.get("MILVUS_HOST","127.0.0.1"), port=os.environ.get("MILVUS_PORT","19530"))
+
+def ensure_collection(dim=1536):
+    if not connections:
+        raise RuntimeError("pymilvus not available")
+    if utility.has_collection(COLLECTION):
+        return Collection(COLLECTION)
+    fields = [
+        FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
+        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
+        FieldSchema(name="episode_id", dtype=DataType.VARCHAR, max_length=128),
+        FieldSchema(name="type", dtype=DataType.VARCHAR, max_length=64),
+        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=4096),
+        FieldSchema(name="meta", dtype=DataType.VARCHAR, max_length=2048),
+        FieldSchema(name="ts", dtype=DataType.INT64)
+    ]
+    schema = CollectionSchema(fields, description="Aegis agent episodic memory v2")
+    col = Collection(COLLECTION, schema)
+    return col
+
+@APP.route("/v1/memory/upsert", methods=["POST"])
+def upsert():
+    body = request.json or {}
+    emb = body.get("embedding")
+    if not emb:
+        return jsonify({"ok":False,"error":"no embedding"}), 400
+    connect()
+    col = ensure_collection(len(emb))
+    episode_id = body.get("episode_id","default")
+    mtype = body.get("type","misc")
+    text = body.get("text","")
+    meta = json.dumps(body.get("meta",{}))
+    ts = int(time.time() * 1000)
+    col.insert([[emb], [episode_id], [mtype], [text], [meta], [ts]])
+    col.flush()
+    return jsonify({"ok":True})
+
+@APP.route("/v1/memory/query", methods=["POST"])
+def query():
+    body = request.json or {}
+    emb = body.get("embedding")
+    topk = int(body.get("topk",10))
+    if not emb:
+        return jsonify({"ok":False,"error":"no embedding"}),400
+    connect()
+    col = ensure_collection(len(emb))
+    results = col.search([emb], "embedding", param={"anns_field":"embedding","topk":topk,"metric_type":"L2","params":{"nprobe":10}})
+    out=[]
+    for hits in results:
+        for h in hits:
+            ent = h.entity
+            out.append({"id": int(h.id), "distance": float(h.distance), "episode_id": ent.get("episode_id"), "type": ent.get("type"), "text": ent.get("text"), "meta": json.loads(ent.get("meta") or "{}"), "ts": int(ent.get("ts") or 0)})
+    return jsonify({"ok":True,"results":out})
+
+@APP.route("/v1/memory/set_retention", methods=["POST"])
+def set_retention():
+    body = request.json or {}
+    days = int(body.get("days",90))
+    # persist policy to local file or ConfigMap in k8s (simple file for now)
+    os.makedirs("/var/lib/aegis", exist_ok=True)
+    with open("/var/lib/aegis/memory_retention_policy.json","w") as fh:
+        json.dump({"days": days, "updated": datetime.utcnow().isoformat()+"Z"}, fh)
+    return jsonify({"ok":True,"days": days})
+
+@APP.route("/v1/memory/retention_run", methods=["POST"])
+def retention_run():
+    # export then delete older memory entries
+    policy = {}
+    try:
+        policy = json.load(open("/var/lib/aegis/memory_retention_policy.json"))
+    except Exception:
+        policy = {"days": 90}
+    cutoff_ts = int((datetime.utcnow() - timedelta(days=policy["days"])).timestamp() * 1000)
+    connect()
+    col = ensure_collection()
+    # Milvus deletion by expression (assumes ts field exists)
+    try:
+        expr = f"ts < {cutoff_ts}"
+        col.delete(expr)
+        col.flush()
+        return jsonify({"ok":True, "deleted_before": cutoff_ts})
+    except Exception as e:
+        return jsonify({"ok":False, "error": str(e)}), 500
+
+if __name__=="__main__":
+    APP.run(host="0.0.0.0", port=int(os.environ.get("MEMORY_PORT","8233")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/memory/memory_checkpoint_replay.py
+#!/usr/bin/env python3
+"""
+Memory checkpoint & replay helper
+
+ - Exports a snapshot of memory for a given episode_id to S3 (or local file)
+ - Can replay a snapshot back into the memory service (useful for plan resumption)
+"""
+import os
+import json
+import tempfile
+import boto3
+from ops.memory.milvus_memory_service_v2 import connect, ensure_collection
+
+S3 = boto3.client("s3") if os.environ.get("EVIDENCE_BUCKET") else None
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET","")
+COLLECTION = os.environ.get("MEMORY_COLLECTION","aegis_agent_memory_v2")
+
+def export_episode(episode_id, out_path=None):
+    connect()
+    col = ensure_collection()
+    # Milvus SDK read all entities via query (this is simplified - relies on JSON serialization)
+    res = col.query(expr=f"episode_id == \"{episode_id}\"","output_fields":["embedding","episode_id","type","text","meta","ts"])
+    out_path = out_path or f"/tmp/memory_snapshot_{episode_id}.json"
+    with open(out_path,"w") as fh:
+        json.dump(res, fh, default=str)
+    if S3:
+        key = f"memory_snapshots/{os.path.basename(out_path)}"
+        S3.upload_file(out_path, EVIDENCE_BUCKET, key)
+        return {"local": out_path, "s3": f"s3://{EVIDENCE_BUCKET}/{key}"}
+    return {"local": out_path}
+
+def replay_snapshot(snapshot_path):
+    data = json.load(open(snapshot_path))
+    connect()
+    col = ensure_collection()
+    # insert each as a row
+    for d in data:
+        emb = d.get("embedding")
+        col.insert([[emb], [d.get("episode_id")], [d.get("type","")], [d.get("text","")], [d.get("meta","{}")], [int(d.get("ts",0))]])
+    col.flush()
+    return {"ok": True}
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--export")
+    p.add_argument("--replay")
+    p.add_argument("--episode")
+    args = p.parse_args()
+    if args.export:
+        print(export_episode(args.episode, args.export))
+    if args.replay:
+        print(replay_snapshot(args.replay))
+
*** End Patch
*** Begin Patch
*** Add File:ops/sandbox/tools_matrix.json
+[
+  {"image":"python:3.11-slim","cmd":"python -c \"print('hello')\""},
+  {"image":"alpine:3.18","cmd":"sh -c \"echo hi\""},
+  {"image":"node:18","cmd":"node -e \"console.log('ok')\""},
+  {"image":"golang:1.20","cmd":"go version"}
+]
+
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/tune_verifier.py
+#!/usr/bin/env python3
+"""
+Verifier tuning utility
+
+ - Accepts a dataset of candidate answers with 'label' (correct/incorrect) and 'verifier_score'
+ - Sweeps thresholds to maximize a balanced metric (precision at threshold with min recall)
+ - Writes thresholds to a config file /var/lib/aegis/verifier_thresholds.json or prints suggested values
+ - Optionally auto-promote high-precision docs to golden KB using ops/rag/golden_kb_manager.tag_doc
+"""
+import os
+import json
+import numpy as np
+from sklearn.metrics import precision_recall_curve
+
+GOLDEN_PROMOTE = os.environ.get("GOLDEN_PROMOTE","0") == "1"
+
+def tune(scores, labels, min_recall=0.6):
+    # compute precision-recall curve and pick threshold with max precision subject to recall>=min_recall
+    prec, rec, thresh = precision_recall_curve(labels, scores)
+    choices = [(p,r,t) for p,r,t in zip(prec,rec, list(thresh)+[None]) if r >= min_recall]
+    if not choices:
+        # fallback: choose recall maximizing
+        idx = np.argmax(rec)
+        return thresh[idx] if idx < len(thresh) else (thresh[-1] if len(thresh) else 0.5)
+    best = max(choices, key=lambda x: x[0])
+    return float(best[2] if best[2] is not None else 0.5)
+
+def main():
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset", required=True, help="jsonl file with {'score':float,'label':0|1,'doc_id':...}")
+    p.add_argument("--min_recall", type=float, default=0.6)
+    args = p.parse_args()
+    items = [json.loads(l) for l in open(args.dataset)]
+    scores = [i["score"] for i in items]
+    labels = [i["label"] for i in items]
+    thresh = tune(scores, labels, args.min_recall)
+    out = {"threshold": thresh, "min_recall": args.min_recall}
+    os.makedirs("/var/lib/aegis", exist_ok=True)
+    with open("/var/lib/aegis/verifier_thresholds.json","w") as fh:
+        json.dump(out, fh)
+    print("wrote thresholds:", out)
+    if GOLDEN_PROMOTE:
+        try:
+            from ops.rag.golden_kb_manager import tag_doc
+            # promote docs with high score and label=1
+            for it in items:
+                if it.get("score",0) >= thresh and it.get("label") == 1 and it.get("doc_id"):
+                    tag_doc(it["doc_id"], reason="auto-promoted-by-tuning")
+        except Exception as e:
+            print("golden promote failed:", e)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/rag/golden_promote.py
+#!/usr/bin/env python3
+"""
+Golden KB promotion helper:
+
+ - Given retrieval evaluation outputs with doc_id -> relevance labels, compute per-doc precision@k and promote docs with high precision to golden KB
+ - Writes audit trail via ops/audit/attestation_audit_trail.append_audit_record
+"""
+import os
+import json
+from collections import defaultdict
+from ops.rag.golden_kb_manager import tag_doc
+from ops.audit.attestation_audit_trail import append_audit_record
+
+def compute_doc_scores(eval_file):
+    # eval_file: jsonl entries {"query":..,"retrieved":[{"doc_id":..,"relevant":0|1},...]}
+    docs = defaultdict(lambda: {"tp":0,"tpfp":0})
+    for line in open(eval_file):
+        j = json.loads(line)
+        for r in j.get("retrieved", []):
+            docs[r["doc_id"]]["tpfp"] += 1
+            if r.get("relevant",0):
+                docs[r["doc_id"]]["tp"] += 1
+    scores = {}
+    for d,stats in docs.items():
+        prec = stats["tp"] / stats["tpfp"] if stats["tpfp"]>0 else 0.0
+        scores[d] = {"tp":stats["tp"], "tpfp":stats["tpfp"], "precision":prec}
+    return scores
+
+def promote_high_precision(eval_file, precision_threshold=0.9, min_appearances=5):
+    scores = compute_doc_scores(eval_file)
+    promoted=[]
+    for doc,meta in scores.items():
+        if meta["precision"] >= precision_threshold and meta["tpfp"] >= min_appearances:
+            tag_doc(doc, reason=f"promoted_prec{precision_threshold}_min{min_appearances}")
+            append_audit_record({"attestation_id": f"golden-promote-{doc}", "actor":"golden_promote", "artifact":"golden_tag", "note": json.dumps(meta)})
+            promoted.append(doc)
+    return promoted
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--eval", required=True)
+    p.add_argument("--prec", type=float, default=0.9)
+    p.add_argument("--min", type=int, default=5)
+    args = p.parse_args()
+    print(promote_high_precision(args.eval, args.prec, args.min))
+
*** End Patch
*** Begin Patch
*** Add File:ops/infra/gpu_autoscaler_improved.py
+#!/usr/bin/env python3
+"""
+Improved GPU autoscaler:
+ - Uses Prometheus metrics for GPU utilization and Triton model queue length to decide desired nodepool size
+ - Writes desired size to a ConfigMap for operator review and optionally calls an ASG API
+"""
+import os
+import time
+import requests
+from kubernetes import client, config
+
+PROM_URL = os.environ.get("PROM_URL","http://prometheus:9090")
+MET_GPU_UTIL = os.environ.get("PROM_GPU_UTIL","aegis_gpu_utilization_percent")
+MET_TRITON_QUEUE = os.environ.get("PROM_TRITON_QUEUE","aegis_triton_queue_length")
+MIN = int(os.environ.get("GPU_MIN","1"))
+MAX = int(os.environ.get("GPU_MAX","20"))
+CM_NAME = "aegis-gpu-desired"
+CM_NS = "kube-system"
+
+def query_one(query):
+    try:
+        r = requests.get(f"{PROM_URL}/api/v1/query", params={"query": query}, timeout=5)
+        r.raise_for_status()
+        data = r.json().get("data",{}).get("result",[])
+        if not data: return None
+        return float(data[0]["value"][1])
+    except Exception:
+        return None
+
+def set_configmap(desired):
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=CM_NAME), data={"desired":str(desired)})
+    try:
+        core.replace_namespaced_config_map(CM_NAME, CM_NS, body)
+    except Exception:
+        core.create_namespaced_config_map(CM_NS, body)
+
+def main():
+    while True:
+        gpu = query_one(MET_GPU_UTIL) or 0.0
+        q = query_one(MET_TRITON_QUEUE) or 0.0
+        # simple heuristics: map combined signal to desired GPUs
+        desired = int(MIN + (q / 10.0) + (gpu / 100.0) * (MAX - MIN))
+        desired = max(MIN, min(MAX, desired))
+        print("signals: gpu_util", gpu, "triton_queue", q, "-> desired", desired)
+        set_configmap(desired)
+        time.sleep(int(os.environ.get("POLL_S","30")))
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/triton_model_deploy_ci.yml
+name: Triton Model Deploy & Latency CI
+on:
+  pull_request:
+    types: [opened, synchronize]
+
+jobs:
+  build-opt-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install numpy Pillow requests
+      - name: Generate Triton model config and run trtexec (local smoke if available)
+        run: |
+          mkdir -p artifacts/models/clip-embed/1
+          # generate config
+          bash ops/serving/triton/deploy_clip_model.sh artifacts/models/clip-embed clip-embed 8 1 1024
+          echo "If you have trtexec locally, place ONNX into artifacts/models/clip-embed/1/"
+      - name: Check latency script presence
+        run: |
+          ls -lah ops/serving/tensorrt || true
+
*** End Patch
*** End Patch
