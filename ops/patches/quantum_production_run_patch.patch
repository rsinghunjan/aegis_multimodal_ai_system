*** Begin Patch
*** Add File:ops/hsm/live_auditor_session_orchestrator.py
+#!/usr/bin/env python3
+"""
+Orchestrate a live CloudHSM auditor session end-to-end.
+
+Steps:
+ - Produce HSM audit bundle (ops/hsm/cloudhsm_kms_cert_playbook.py)
+ - Run a live signing check via in-cluster HSM signer
+ - Upload artifacts if EVIDENCE_BUCKET set
+ - Create signoff entry in ConfigMap and optionally request human approval
+ - Emit a final auditor_report.json for storage
+
+Operator: run this during the auditor meeting and follow manual steps for CloudHSM key ceremony.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+from kubernetes import client, config
+
+from ops.hsm.cloudhsm_kms_cert_playbook import produce_audit_bundle, live_sign_test
+
+SIGNOFF_CM = os.environ.get("AUDIT_SIGNOFF_CM", "aegis-auditor-signoffs")
+SIGNOFF_NS = os.environ.get("AUDIT_SIGNOFF_NS", "kube-system")
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def k8s_client():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+def create_signoff_record(bundle_ref, notes=""):
+    core = k8s_client()
+    try:
+        cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+        data = cm.data or {}
+    except Exception:
+        data = {}
+    rid = "auditor-" + str(int(datetime.utcnow().timestamp()))
+    entry = {"id": rid, "bundle": bundle_ref, "notes": notes, "state": "pending", "created": datetime.utcnow().isoformat()+"Z"}
+    data[rid] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=SIGNOFF_CM), data=data)
+    try:
+        core.replace_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS, body)
+    except Exception:
+        core.create_namespaced_config_map(SIGNOFF_NS, body)
+    return entry
+
+def run(sample_blob, key_id=None, cluster_id=None, notes=""):
+    print("Producing audit bundle...")
+    bundle = produce_audit_bundle(sample_blob, key_id, cluster_id)
+    print("Bundle produced:", bundle.get("bundle"))
+    print("Running live sign test...")
+    live = live_sign_test(sample_blob, key_label=None)
+    print("Live sign:", live)
+    bundle_ref = bundle.get("s3") or bundle.get("bundle")
+    signoff = create_signoff_record(bundle_ref, notes)
+    report = {"bundle": bundle, "live_sign": live, "signoff": signoff, "ts": datetime.utcnow().isoformat()+"Z"}
+    out = "/tmp/auditor_full_report.json"
+    with open(out, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote auditor report:", out)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--sample", default="ops/hsm/audit_sample.bin")
+    p.add_argument("--kms-key-id")
+    p.add_argument("--cluster-id")
+    p.add_argument("--notes", default="")
+    args = p.parse_args()
+    run(args.sample, args.kms_key_id, args.cluster_id, args.notes)
+
*** End Patch
*** Begin Patch
*** Add File:ops/validation/fullscale_validation_runner.py
+#!/usr/bin/env python3
+"""
+Execute 3x full-scale validation runs at production node counts, tune ASG/nodepool/Triton until p95/p99 pass,
+and capture reports for SRE/auditors.
+
+Uses:
+ - ops/validation/simulator_fleet_validation_orchestrator.py (for submission & tuning)
+ - ops/validation/scale_metrics_collector.py (for metric CSV collection)
+ - ops/release/release_packager.py (to package artifacts)
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+from ops.validation.simulator_fleet_validation_orchestrator import run_validation
+from ops.validation.scale_metrics_collector import run as collect_metrics
+
+REPEATS = int(os.environ.get("FULLSCALE_REPEATS", "3"))
+OUT_DIR = os.environ.get("FULLSCALE_OUT_DIR", "/tmp/fullscale")
+
+def ensure_outdir(d):
+    os.makedirs(d, exist_ok=True)
+    return d
+
+def run_all():
+    ensure_outdir(OUT_DIR)
+    all_runs = []
+    for i in range(REPEATS):
+        print(f"=== Full-scale run {i+1}/{REPEATS}")
+        res = run_validation()
+        metrics_csv = collect_metrics()
+        res_record = {"run_index": i+1, "validation_report": res, "metrics_csv": metrics_csv, "ts": datetime.utcnow().isoformat()+"Z"}
+        all_runs.append(res_record)
+        # persist per-run
+        path = os.path.join(OUT_DIR, f"fullscale_run_{i+1}.json")
+        with open(path, "w") as fh:
+            json.dump(res_record, fh, indent=2)
+    # summarize
+    summary = {"runs": all_runs, "summary_ts": datetime.utcnow().isoformat()+"Z"}
+    summary_path = os.path.join(OUT_DIR, "fullscale_summary.json")
+    with open(summary_path, "w") as fh:
+        json.dump(summary, fh, indent=2)
+    print("Wrote summary:", summary_path)
+    # package evidence using release_packager
+    try:
+        subprocess.check_call(["python", "ops/release/release_packager.py", "--artifacts", summary_path])
+    except Exception as e:
+        print("release_packager failed:", e)
+    return summary_path
+
+if __name__ == "__main__":
+    run_all()
+
*** End Patch
*** Begin Patch
*** Add File:ops/providers/sandbox_integration_test.py
+#!/usr/bin/env python3
+"""
+Run provider sandbox integration tests for Braket and IBMQ adapters and produce a pass/fail report.
+
+ - Runs in sandbox mode for both adapters
+ - Submits a small test job and checks status transitions
+ - Produces /tmp/provider_sandbox_report.json
+"""
+import json
+from datetime import datetime
+
+from ops.providers.braket_adapter_hardened import BraketHardened
+from ops.providers.ibmq_adapter_hardened import IBMQHardened
+
+def run_braket_sandbox():
+    b = BraketHardened(sandbox=True)
+    payload = {"circuit": "h; cx", "ops": ["h","cx"]}
+    try:
+        res = b.submit_task("arn:aws:braket:::device/mock", "s3://mock/out", payload, shots=100)
+        status = b.get_task_status(res.get("quantumTaskArn"))
+        return {"ok": True, "submit": res, "status": status}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def run_ibmq_sandbox():
+    i = IBMQHardened(sandbox=True)
+    try:
+        res = i.submit_job("ibmq_qasm_simulator", {"circuit":"h"}, shots=100)
+        status = i.get_status(res.get("job_id"))
+        return {"ok": True, "submit": res, "status": status}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def run_all():
+    out = {"ts": datetime.utcnow().isoformat()+"Z", "braket": run_braket_sandbox(), "ibmq": run_ibmq_sandbox()}
+    path = "/tmp/provider_sandbox_report.json"
+    with open(path, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote", path)
+    return path
+
+if __name__=="__main__":
+    run_all()
+
*** End Patch
*** Begin Patch
*** Add File:ops/finance/finance_rehearsal_runner.py
+#!/usr/bin/env python3
+"""
+Run finance rehearsal: produce reconciled CSVs across AWS/GCP/Azure and create a signoff request for Finance.
+Saves a report at /tmp/finance_rehearsal_report.json
+"""
+import json
+import subprocess
+from datetime import datetime
+
+from ops.finance.multi_provider_reconciler import merge_and_write
+from kubernetes import client, config
+
+SIGNOFF_CM = "aegis-finance-signoffs"
+SIGNOFF_NS = "kube-system"
+
+def create_signoff(csv_path, owner="finance@example.com"):
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    core = client.CoreV1Api()
+    try:
+        cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+        data = cm.data or {}
+    except Exception:
+        data = {}
+    rid = "finance-" + str(int(datetime.utcnow().timestamp()))
+    entry = {"id": rid, "csv": csv_path, "owner": owner, "state": "pending", "ts": datetime.utcnow().isoformat()+"Z"}
+    data[rid] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=SIGNOFF_CM), data=data)
+    try:
+        core.replace_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS, body)
+    except Exception:
+        core.create_namespaced_config_map(SIGNOFF_NS, body)
+    return entry
+
+def run():
+    csv = merge_and_write()
+    sign = create_signoff(csv)
+    report = {"csv": csv, "signoff": sign, "ts": datetime.utcnow().isoformat()+"Z"}
+    out = "/tmp/finance_rehearsal_report.json"
+    with open(out, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote", out)
+    return out
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:ops/quantum/hardware_calibration_ci.py
+#!/usr/bin/env python3
+"""
+Hardware calibration & mitigation CI runner.
+
+ - Runs calibration sequences (ops/quantum/calibration_and_mitigation.py)
+ - Stores calibration artifacts to /tmp and uploads to EVIDENCE_BUCKET if set
+ - Produces a calibration_report.json suitable for CI artifact upload
+"""
+import os
+import json
+import shutil
+from datetime import datetime
+from ops.quantum.calibration_and_mitigation import run_calibration_circuits, build_noise_model_from_calibration, apply_measurement_error_mitigation
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+
+def run_calibration(backend="sim", circuits=None, shots=1000):
+    circuits = circuits or ["cal0","cal1","cal2"]
+    calib = run_calibration_circuits(backend, circuits, shots=shots)
+    noise_model = build_noise_model_from_calibration(calib)
+    out = {"calibration": calib, "noise_model": noise_model, "ts": datetime.utcnow().isoformat()+"Z"}
+    path = "/tmp/calibration_report.json"
+    with open(path, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote calibration report:", path)
+    return path
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--backend", default="sim")
+    p.add_argument("--shots", type=int, default=1000)
+    args = p.parse_args()
+    run_calibration(args.backend, shots=args.shots)
+
*** End Patch
*** Begin Patch
*** Add File:k8s/monitoring/quantum_kpi_exporter_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-quantum-kpi-exporter
+  namespace: monitoring
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-quantum-kpi
+  template:
+    metadata:
+      labels:
+        app: aegis-quantum-kpi
+    spec:
+      containers:
+        - name: quantum-kpi-exporter
+          image: python:3.11-slim
+          command: ["python", "/opt/aegis/ops/metrics/quantum_kpi_exporter.py"]
+          ports:
+            - containerPort: 9105
+          volumeMounts:
+            - name: code
+              mountPath: /opt/aegis
+      volumes:
+        - name: code
+          hostPath:
+            path: ./ops
+            type: Directory
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-quantum-kpi-exporter
+  namespace: monitoring
+spec:
+  ports:
+    - port: 9105
+      targetPort: 9105
+  selector:
+    app: aegis-quantum-kpi
+
*** End Patch
*** Begin Patch
*** Add File:ops/metrics/deploy_grafana_dashboard.py
+#!/usr/bin/env python3
+"""
+Deploy Grafana dashboard via Grafana HTTP API.
+
+Requires GRAFANA_URL and GRAFANA_API_KEY env variables.
+"""
+import os
+import json
+import requests
+
+GRAFANA_URL = os.environ.get("GRAFANA_URL", "http://grafana:3000")
+GRAFANA_KEY = os.environ.get("GRAFANA_API_KEY", "")
+DASHBOARD_PATH = os.environ.get("QUANTUM_DASH_JSON", "ops/metrics/grafana/quantum_fidelity_dashboard.json")
+
+def push_dashboard():
+    if not GRAFANA_KEY:
+        raise RuntimeError("GRAFANA_API_KEY not set")
+    dash = json.load(open(DASHBOARD_PATH))
+    payload = {"dashboard": dash, "overwrite": True}
+    headers = {"Authorization": f"Bearer {GRAFANA_KEY}", "Content-Type": "application/json"}
+    r = requests.post(f"{GRAFANA_URL}/api/dashboards/db", headers=headers, json=payload, timeout=30)
+    r.raise_for_status()
+    return r.json()
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--push", action="store_true")
+    args = p.parse_args()
+    if args.push:
+        print(push_dashboard())
+    else:
+        print("Run with --push to upload dashboard to Grafana")
+
*** End Patch
*** Begin Patch
*** Add File:ops/release/run_release_and_signoff.py
+#!/usr/bin/env python3
+"""
+Run the release_packager end-to-end, collect artifacts, and mark signoff record ready for cross-team review.
+This script wraps ops/release/release_packager.py and then updates signoff ConfigMap with 'ready_for_review'.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+from kubernetes import client, config
+
+SIGNOFF_CM = os.environ.get("AUDIT_SIGNOFF_CM", "aegis-auditor-signoffs")
+SIGNOFF_NS = os.environ.get("AUDIT_SIGNOFF_NS", "kube-system")
+
+def k8s_client():
+    try:
+        config.load_incluster_config()
+    except Exception:
+        config.load_kube_config()
+    return client.CoreV1Api()
+
+def mark_ready(signoff_id):
+    core = k8s_client()
+    cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+    data = cm.data or {}
+    if signoff_id not in data:
+        raise RuntimeError("signoff id not found")
+    entry = json.loads(data[signoff_id])
+    entry["state"] = "ready_for_review"
+    entry["release_ready_ts"] = datetime.utcnow().isoformat()+"Z"
+    data[signoff_id] = json.dumps(entry)
+    body = client.V1ConfigMap(metadata=client.V1ObjectMeta(name=SIGNOFF_CM), data=data)
+    core.replace_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS, body)
+    return entry
+
+def run_release(artifacts=None, manifest="/tmp/manifest.json"):
+    cmd = ["python", "ops/release/release_packager.py", "--manifest", manifest]
+    if artifacts:
+        cmd += ["--artifacts"] + artifacts
+    subprocess.check_call(cmd)
+    # release_packager creates a signoff entry; read last entry
+    core = k8s_client()
+    cm = core.read_namespaced_config_map(SIGNOFF_CM, SIGNOFF_NS)
+    data = cm.data or {}
+    # find most recent entry
+    latest = None
+    for k,v in data.items():
+        latest = json.loads(v)
+    if latest:
+        return latest
+    return None
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", default="/tmp/manifest.json")
+    p.add_argument("--signoff-id", help="mark this signoff id ready for review")
+    args = p.parse_args()
+    entry = run_release(manifest=args.manifest)
+    print("release created signoff entry:", entry)
+    if args.signoff_id:
+        print("marking signoff", args.signoff_id, "ready")
+        print(mark_ready(args.signoff_id))
+
*** End Patch
*** End Patch
