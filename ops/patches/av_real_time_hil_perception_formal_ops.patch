*** Begin Patch
*** Add File:ops/rt/rt_scheduler_wrapper.py
+#!/usr/bin/env python3
+"""
+Run a control loop binary under a real-time scheduler and record timing metrics.
+ - Uses `chrt` to run under SCHED_FIFO if available (Linux).
+ - Measures per-iteration latency and writes Prometheus-style metrics to a local file or Pushgateway.
+ - Provides a WCET analyzer mode: run N iterations and report worst-case, p95/p99.
+
+Usage:
+  python ops/rt/rt_scheduler_wrapper.py --cmd "./control_loop_binary --config cfg.json" --iters 1000 --report /tmp/ctl_latency.prom
+"""
+import argparse
+import subprocess
+import time
+import shlex
+import os
+import statistics
+from datetime import datetime
+
+def run_control_loop(cmd, iters, sleep_between, realtime=False, rt_priority=80):
+    # Launch the control loop as a subprocess that itself iterates and logs per-step times.
+    # We expect the command to support a --iterations and --seed or similar; if not, we run it repeatedly.
+    latencies = []
+    for i in range(iters):
+        start = time.time()
+        try:
+            if realtime and shutil.which("chrt"):
+                full = f"chrt -f {rt_priority} {cmd}"
+            else:
+                full = cmd
+            subprocess.check_call(shlex.split(full))
+            dur = time.time() - start
+        except subprocess.CalledProcessError:
+            dur = time.time() - start
+        latencies.append(dur)
+        if sleep_between:
+            time.sleep(sleep_between)
+    return latencies
+
+def write_prom_file(path, metric_name, values):
+    # write a small prom exposition with p95/p99 and worst
+    p95 = statistics.quantiles(values, n=100)[94] if len(values) >= 100 else max(values)
+    p99 = statistics.quantiles(values, n=100)[98] if len(values) >= 100 else max(values)
+    worst = max(values) if values else 0.0
+    ts = int(time.time())
+    lines = [
+        f"# HELP {metric_name}_p95 P95 latency seconds",
+        f"# TYPE {metric_name}_p95 gauge",
+        f"{metric_name}_p95 {p95} {ts}",
+        f"# HELP {metric_name}_p99 P99 latency seconds",
+        f"# TYPE {metric_name}_p99 gauge",
+        f"{metric_name}_p99 {p99} {ts}",
+        f"# HELP {metric_name}_worst Worst-case latency seconds",
+        f"# TYPE {metric_name}_worst gauge",
+        f"{metric_name}_worst {worst} {ts}",
+    ]
+    with open(path, "w") as fh:
+        fh.write("\n".join(lines))
+
+if __name__ == "__main__":
+    import shutil
+    p = argparse.ArgumentParser()
+    p.add_argument("--cmd", required=True, help="Command to run the control iteration (should exit quickly)")
+    p.add_argument("--iters", type=int, default=10)
+    p.add_argument("--sleep-between", type=float, default=0.0)
+    p.add_argument("--realtime", action="store_true", help="Use chrt to set SCHED_FIFO scheduling")
+    p.add_argument("--rt-priority", type=int, default=80)
+    p.add_argument("--report", default="/tmp/control_loop_latency.prom")
+    args = p.parse_args()
+
+    lat = run_control_loop(args.cmd, args.iters, args.sleep_between, realtime=args.realtime, rt_priority=args.rt_priority)
+    write_prom_file(args.report, "control_loop_latency_seconds", lat)
+    print("Latency samples:", len(lat), "worst:", max(lat) if lat else None)
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/wcet_analyzer.py
+#!/usr/bin/env python3
+"""
+WCET analyzer helper
+ - Runs a provided binary (control loop) repeatedly under instrumentation to collect execution time samples.
+ - Produces a report with min, max, mean, median, p95, p99 and recommended guard margin.
+ - Optionally runs using `chrt` SCHED_FIFO to emulate real-time priority.
+"""
+import argparse
+import time
+import subprocess
+import shlex
+import statistics
+import json
+
+def sample_exec(cmd, n=100, rt=False, rt_priority=80):
+    import shutil
+    samples = []
+    for i in range(n):
+        start = time.time()
+        c = cmd
+        if rt and shutil.which("chrt"):
+            c = f"chrt -f {rt_priority} {cmd}"
+        try:
+            subprocess.check_call(shlex.split(c), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
+        except subprocess.CalledProcessError:
+            pass
+        samples.append(time.time()-start)
+    return samples
+
+def summarize(samples):
+    if not samples:
+        return {}
+    s = {
+        "n": len(samples),
+        "min": min(samples),
+        "max": max(samples),
+        "mean": statistics.mean(samples),
+        "median": statistics.median(samples),
+        "p95": statistics.quantiles(samples, n=100)[94] if len(samples) >= 100 else max(samples),
+        "p99": statistics.quantiles(samples, n=100)[98] if len(samples) >= 100 else max(samples),
+        "std": statistics.pstdev(samples),
+        "recommended_margin": max(samples) * 1.2
+    }
+    return s
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--cmd", required=True)
+    p.add_argument("--samples", type=int, default=200)
+    p.add_argument("--rt", action="store_true")
+    p.add_argument("--rt-priority", type=int, default=80)
+    p.add_argument("--out", default="/tmp/wcet_report.json")
+    args = p.parse_args()
+    samples = sample_exec(args.cmd, n=args.samples, rt=args.rt, rt_priority=args.rt_priority)
+    report = summarize(samples)
+    with open(args.out, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote WCET report to", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/firmware_manager.py
+#!/usr/bin/env python3
+"""
+Firmware manager for HIL racks / ECUs.
+ - Snapshot firmware via vendor API or local dump command and store metadata (version, checksum).
+ - Provide a small API to verify that HIL runs used the expected firmware snapshot.
+ - This is a plugin point: implement vendor-specific logic by overriding `snapshot_firmware` and `verify_firmware`.
+"""
+import os
+import json
+import subprocess
+import hashlib
+from datetime import datetime
+
+FW_DIR = os.environ.get("HIL_FW_DIR", "/tmp/hil_firmware")
+os.makedirs(FW_DIR, exist_ok=True)
+
+def sha256(data):
+    import hashlib
+    return hashlib.sha256(data).hexdigest()
+
+def snapshot_firmware(dump_cmd, out_name):
+    """
+    Run a dump command (vendor tool) and store the binary + metadata.
+    Example dump_cmd: "vendortool --dump /dev/ecu0"
+    """
+    out_path = os.path.join(FW_DIR, out_name)
+    try:
+        with open(out_path, "wb") as fh:
+            p = subprocess.Popen(dump_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+            out, err = p.communicate(timeout=120)
+            fh.write(out)
+        meta = {"path": out_path, "size": os.path.getsize(out_path), "sha256": sha256(open(out_path,"rb").read()), "ts": datetime.utcnow().isoformat()+"Z"}
+        meta_path = out_path + ".meta.json"
+        with open(meta_path, "w") as fh:
+            json.dump(meta, fh, indent=2)
+        return meta_path
+    except Exception as e:
+        raise
+
+def verify_firmware(meta_path, expected_sha256):
+    if not os.path.exists(meta_path):
+        return False
+    meta = json.load(open(meta_path))
+    return meta.get("sha256") == expected_sha256
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--dump-cmd", required=True)
+    p.add_argument("--name", default="fw_dump.bin")
+    args = p.parse_args()
+    print(snapshot_firmware(args.dump_cmd, args.name))
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/replay_verifier.py
+#!/usr/bin/env python3
+"""
+Verify deterministic replay for HIL runs:
+ - Compare input replay metadata (timestamps, seeds) to HIL run outputs and compute similarity / bitwise equality where applicable
+ - Produce a replay_verification.json with pass/fail and diagnostics (mismatched timestamps, unexpected events)
+"""
+import os
+import json
+from datetime import datetime
+
+def verify(replay_meta_path, hil_log_path, tolerance_ms=10):
+    if not os.path.exists(replay_meta_path) or not os.path.exists(hil_log_path):
+        return {"ok": False, "reason": "missing_files"}
+    replay_meta = json.load(open(replay_meta_path))
+    hil_log = json.load(open(hil_log_path))
+    # naive checks: seed equal, start time within tolerance, and status == pass
+    res = {"ok": True, "checks": {}}
+    seed_match = replay_meta.get("seed") == hil_log.get("seed")
+    res["checks"]["seed_match"] = seed_match
+    try:
+        replay_ts = datetime.fromisoformat(replay_meta.get("ts").replace("Z",""))
+        hil_ts = datetime.fromisoformat(hil_log.get("ts").replace("Z",""))
+        delta_ms = abs((replay_ts - hil_ts).total_seconds() * 1000.0)
+    except Exception:
+        delta_ms = None
+    res["checks"]["start_time_delta_ms"] = delta_ms
+    res["checks"]["status"] = hil_log.get("status")
+    if not seed_match or (delta_ms is not None and delta_ms > tolerance_ms) or hil_log.get("status") != "pass":
+        res["ok"] = False
+    return res
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--replay-meta", required=True)
+    p.add_argument("--hil-log", required=True)
+    args = p.parse_args()
+    print(json.dumps(verify(args.replay_meta, args.hil_log), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/scenario_augmenter.py
+#!/usr/bin/env python3
+"""
+Augment scenario-generated frames to produce occlusion, noise, brightness and sensor-dropout variants.
+ - Produces augmented datasets and writes metadata about the perturbation applied.
+"""
+import os
+import json
+from PIL import Image, ImageFilter, ImageEnhance
+import random
+from datetime import datetime
+
+def apply_occlusion(img_path, out_path, box):
+    im = Image.open(img_path).convert("RGB")
+    draw = Image.new("RGB", im.size, (0,0,0))
+    im.paste(draw.crop(box), box)
+    im.save(out_path)
+    return out_path
+
+def apply_blur(img_path, out_path, radius=3):
+    im = Image.open(img_path)
+    im2 = im.filter(ImageFilter.GaussianBlur(radius))
+    im2.save(out_path)
+    return out_path
+
+def adjust_brightness(img_path, out_path, factor=0.5):
+    im = Image.open(img_path)
+    enhancer = ImageEnhance.Brightness(im)
+    im2 = enhancer.enhance(factor)
+    im2.save(out_path)
+    return out_path
+
+def generate_variants(images_dir, out_dir, seed=0):
+    random.seed(seed)
+    os.makedirs(out_dir, exist_ok=True)
+    meta = {"generated": datetime.utcnow().isoformat()+"Z", "variants": []}
+    for fn in sorted([f for f in os.listdir(images_dir) if f.lower().endswith((".jpg",".png"))]):
+        src = os.path.join(images_dir, fn)
+        # choose random augmentation
+        vtype = random.choice(["occlude","blur","brightness","none"])
+        outp = os.path.join(out_dir, f"{os.path.splitext(fn)[0]}_{vtype}.jpg")
+        if vtype == "occlude":
+            w,h = Image.open(src).size
+            box = (int(w*0.4), int(h*0.4), int(w*0.6), int(h*0.6))
+            apply_occlusion(src, outp, box)
+            meta["variants"].append({"file": outp, "type": "occlude", "box": box})
+        elif vtype == "blur":
+            apply_blur(src, outp, radius=4)
+            meta["variants"].append({"file": outp, "type": "blur", "radius": 4})
+        elif vtype == "brightness":
+            factor = random.uniform(0.3, 1.5)
+            adjust_brightness(src, outp, factor)
+            meta["variants"].append({"file": outp, "type": "brightness", "factor": factor})
+        else:
+            # copy
+            from shutil import copyfile
+            copyfile(src, outp)
+            meta["variants"].append({"file": outp, "type": "none"})
+    meta_path = os.path.join(out_dir, "augmentation_metadata.json")
+    with open(meta_path, "w") as fh:
+        json.dump(meta, fh, indent=2)
+    return meta_path
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--images", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--seed", type=int, default=0)
+    args = p.parse_args()
+    print("Wrote", generate_variants(args.images, args.out, seed=args.seed))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/coverage_reporter.py
+#!/usr/bin/env python3
+"""
+Produce a simple coverage report over scenario variants:
+ - Counts variants per scenario, occlusion/blur/brightness distribution
+ - Computes basic coverage score (fraction of variants per scenario family)
+"""
+import os
+import json
+from glob import glob
+
+def report(aug_meta_dir):
+    meta_files = glob(os.path.join(aug_meta_dir, "**/augmentation_metadata.json"), recursive=True)
+    report = {"scenarios": []}
+    for mf in meta_files:
+        j = json.load(open(mf))
+        types = {}
+        for v in j.get("variants", []):
+            t = v.get("type","none")
+            types[t] = types.get(t, 0) + 1
+        report["scenarios"].append({"meta_file": mf, "counts": types, "total": sum(types.values())})
+    # simple coverage metric: number of scenarios with >0 augmentations
+    covered = sum(1 for s in report["scenarios"] if s["total"]>0)
+    report["coverage_score"] = covered / max(1, len(report["scenarios"]))
+    out = os.path.join(aug_meta_dir, "coverage_report.json")
+    with open(out, "w") as fh:
+        json.dump(report, fh, indent=2)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--aug-dir", required=True)
+    args = p.parse_args()
+    print("Wrote", report(args.aug_dir))
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/compositional_framework.py
+#!/usr/bin/env python3
+"""
+Compositional verification framework helper.
+ - Defines simple contract structure for modules: inputs, outputs, assumptions, guarantees.
+ - Validates that sub-module guarantees satisfy other modules' assumptions and emits composition obligations.
+ - Produces a short composition manifest and obligation list for human review / formalization.
+"""
+import os
+import json
+from datetime import datetime
+
+CONTRACT_REG = os.environ.get("CONTRACT_REG", "ops/formal/contracts.json")
+
+def load_contracts():
+    if os.path.exists(CONTRACT_REG):
+        return json.load(open(CONTRACT_REG))
+    return {}
+
+def save_contracts(c):
+    with open(CONTRACT_REG,"w") as fh:
+        json.dump(c, fh, indent=2)
+
+def register_contract(name, inputs, outputs, assumptions, guarantees):
+    c = load_contracts()
+    c[name] = {"name": name, "inputs": inputs, "outputs": outputs, "assumptions": assumptions, "guarantees": guarantees, "ts": datetime.utcnow().isoformat()+"Z"}
+    save_contracts(c)
+    return c[name]
+
+def compose(components):
+    """
+    components: list of contract names in topological order
+    Produces composition obligations: check for each consumer that a provider's guarantees satisfy assumptions.
+    """
+    c = load_contracts()
+    obligations = []
+    for idx, name in enumerate(components):
+        comp = c.get(name)
+        if not comp:
+            obligations.append({"component": name, "status": "missing_contract"})
+            continue
+        for i in comp.get("assumptions", []):
+            # search providers earlier in list
+            satisfied = False
+            for j in range(0, idx):
+                prov = c.get(components[j])
+                if not prov:
+                    continue
+                if i in prov.get("guarantees", []):
+                    satisfied = True
+                    break
+            obligations.append({"component": name, "assumption": i, "satisfied": satisfied})
+    out = {"ts": datetime.utcnow().isoformat()+"Z", "components": components, "obligations": obligations}
+    out_path = "/tmp/composition_obligations.json"
+    with open(out_path, "w") as fh:
+        json.dump(out, fh, indent=2)
+    return out_path
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--register", nargs=5, metavar=("name","inputs","outputs","assumptions","guarantees"), help="register a contract (JSON strings)")
+    p.add_argument("--compose", nargs="+", help="list of contract names in order")
+    args = p.parse_args()
+    if args.register:
+        name, inputs, outputs, assumptions, guarantees = args.register
+        print(register_contract(name, json.loads(inputs), json.loads(outputs), json.loads(assumptions), json.loads(guarantees)))
+    if args.compose:
+        print("Wrote composition obligations to", compose(args.compose))
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/enforce_safety_separation.py
+#!/usr/bin/env python3
+"""
+CI check to enforce safety architecture separation in Kubernetes manifests:
+ - Deployments in namespaces prefixed with 'safety-' must:
+    * run containers only from safety registry (image url contains 'safety-registry' or image annotation present)
+    * include the monitor_agent sidecar (container name 'monitor_agent')
+    * have securityContext.runAsNonRoot = false (may be required for RT priorities) OR documented justification
+ - Fails with non-zero exit if checks fail
+"""
+import os
+import sys
+import yaml
+from glob import glob
+
+def check_manifest(path):
+    try:
+        docs = list(yaml.safe_load_all(open(path)))
+    except Exception:
+        return False, "yaml_parse_error"
+    issues = []
+    for d in docs:
+        if not isinstance(d, dict):
+            continue
+        kind = d.get("kind","")
+        meta = d.get("metadata", {})
+        ns = meta.get("namespace","default")
+        if ns.startswith("safety-"):
+            spec = d.get("spec", {})
+            # find containers
+            template = spec.get("template", {})
+            podspec = template.get("spec", {}) if template else spec.get("jobTemplate", {}).get("spec", {}).get("template", {}).get("spec", {})
+            containers = podspec.get("containers", []) if podspec else []
+            # 1) image from safety registry
+            for c in containers:
+                img = c.get("image","")
+                if "safety-registry" not in img and not meta.get("annotations", {}).get("aegis/allow-non-safety-registry"):
+                    issues.append(f"{path}: container {c.get('name')} not from safety-registry")
+            # 2) monitor sidecar present
+            sidecars = podspec.get("containers", []) if podspec else []
+            if not any(c.get("name")=="monitor_agent" for c in sidecars):
+                issues.append(f"{path}: missing monitor_agent sidecar")
+            # 3) securityContext check - best effort
+            sc = podspec.get("securityContext", {})
+            run_as_non_root = sc.get("runAsNonRoot", None)
+            if run_as_non_root is None:
+                issues.append(f"{path}: missing securityContext.runAsNonRoot")
+    return (len(issues)==0), issues
+
+def main():
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifests", default="k8s/**/*.yaml")
+    args = p.parse_args()
+    files = glob(args.manifests, recursive=True)
+    all_issues = []
+    for f in files:
+        ok, issues = check_manifest(f)
+        if not ok:
+            all_issues.extend(issues)
+    if all_issues:
+        print("Safety separation checks failed:")
+        for i in all_issues:
+            print(i)
+        sys.exit(2)
+    print("Safety separation checks passed.")
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/safety/failover_controller.py
+#!/usr/bin/env python3
+"""
+Simple failover coordinator for redundant sensors/controllers.
+ - Uses kubectl to scale up backup pods and annotate the active controller.
+ - Provides API-style functions to promote a backup, demote a failed instance and record evidence.
+ - This is a coordinating script; integrate into safety_controller workflows for automated failover.
+"""
+import subprocess
+import json
+import os
+from datetime import datetime
+
+def kubectl(cmd):
+    subprocess.check_call(["kubectl"] + cmd.split())
+
+def promote_backup(namespace, deployment_name, backup_label="role=backup"):
+    # scale backup deployment to 1 and set annotation active
+    try:
+        # find backup deployment
+        out = subprocess.check_output(["kubectl","-n",namespace,"get","deploy","-l",backup_label,"-o","json"])
+        j = json.loads(out)
+        items = j.get("items",[])
+        if not items:
+            raise RuntimeError("no backup deployment found")
+        backup = items[0]["metadata"]["name"]
+        subprocess.check_call(["kubectl","-n",namespace,"scale","deployment",backup,"--replicas=1"])
+        # annotate to mark as active
+        subprocess.check_call(["kubectl","-n",namespace,"annotate","deployment",backup,"aegis/failover=promoted","--overwrite"])
+        ev = {"action":"promote_backup","backup":backup,"ts":datetime.utcnow().isoformat()+"Z"}
+        path = f"/tmp/failover_{backup}_{int(time.time())}.json"
+        open(path,"w").write(json.dumps(ev, indent=2))
+        return ev
+    except Exception as e:
+        raise
+
+def demote_failed(namespace, deployment_name):
+    # scale down failed deployment and annotate
+    try:
+        subprocess.check_call(["kubectl","-n",namespace,"scale","deployment",deployment_name,"--replicas=0"])
+        subprocess.check_call(["kubectl","-n",namespace,"annotate","deployment",deployment_name,"aegis/failover=demoted","--overwrite"])
+        ev = {"action":"demote_failed","deployment":deployment_name,"ts":datetime.utcnow().isoformat()+"Z"}
+        path = f"/tmp/failover_{deployment_name}_{int(time.time())}.json"
+        open(path,"w").write(json.dumps(ev, indent=2))
+        return ev
+    except Exception as e:
+        raise
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--promote", nargs=2, metavar=("namespace","deployment"))
+    p.add_argument("--demote", nargs=2, metavar=("namespace","deployment"))
+    args = p.parse_args()
+    if args.promote:
+        print(promote_backup(args.promote[0], args.promote[1]))
+    if args.demote:
+        print(demote_failed(args.demote[0], args.demote[1]))
+
*** End Patch
*** Begin Patch
*** Add File:ops/data/curation_tool.py
+#!/usr/bin/env python3
+"""
+Dataset curation and drift detector.
+ - Validates dataset schema, computes simple per-feature distributions, and compares to a baseline snapshot to detect drift.
+ - Produces a curation report (JSON) suitable for inclusion in evidence bundles prior to retraining.
+"""
+import os
+import json
+from collections import Counter, defaultdict
+import math
+
+def load_annotations(path):
+    # expect json: {"images": [{"file":"...","attrs": {"brightness":0.6,"weather":"clear"}, ... }]}
+    return json.load(open(path))
+
+def summarize(annotations):
+    stats = defaultdict(Counter)
+    for im in annotations.get("images", []):
+        for k,v in im.get("attrs", {}).items():
+            stats[k][str(v)] += 1
+    # normalize to frequencies
+    out = {}
+    for k,c in stats.items():
+        total = sum(c.values())
+        out[k] = {kk: vv/total for kk,vv in c.items() for kk,vv in [(kk,c[kk])] }
+    return out
+
+def jensen_shannon(p, q):
+    # p and q dicts of value->freq
+    import math
+    vals = set(p.keys()) | set(q.keys())
+    m = {}
+    for k in vals:
+        m[k] = 0.5 * p.get(k,0.0) + 0.5 * q.get(k,0.0)
+    def kld(a,b):
+        s = 0.0
+        for k in vals:
+            if a.get(k,0.0) > 0:
+                s += a.get(k,0.0) * math.log(a.get(k,0.0)/ (b.get(k,1e-12)))
+        return s
+    return 0.5 * kld(p,m) + 0.5 * kld(q,m)
+
+def compare_to_baseline(current_annotations_path, baseline_path, out_report):
+    curr = load_annotations(current_annotations_path)
+    base = load_annotations(baseline_path)
+    curr_s = summarize(curr)
+    base_s = summarize(base)
+    drift = {}
+    for k in curr_s.keys():
+        drift[k] = jensen_shannon(curr_s.get(k,{}), base_s.get(k,{}))
+    report = {"current": current_annotations_path, "baseline": baseline_path, "drift": drift}
+    with open(out_report, "w") as fh:
+        json.dump(report, fh, indent=2)
+    return out_report
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--current", required=True)
+    p.add_argument("--baseline", required=True)
+    p.add_argument("--out", default="/tmp/curation_report.json")
+    args = p.parse_args()
+    print("Wrote", compare_to_baseline(args.current, args.baseline, args.out))
+
*** End Patch
*** Begin Patch
*** Add File:ops/ml/retrain_pipeline.py
+#!/usr/bin/env python3
+"""
+Simple retraining pipeline that:
+ - Validates dataset via curation_tool
+ - Launches a training job (placeholder) and records training metadata
+ - Produces evidence (metadata.json, training_log, model hash) for safety track ingestion
+ - This is a scaffold: replace `train_model` with your training orchestration (Argo/cluster job)
+"""
+import os
+import json
+import hashlib
+import time
+from datetime import datetime
+from ops.data.curation_tool import compare_to_baseline
+
+EVIDENCE_DIR = os.environ.get("RETRAIN_EVIDENCE_DIR", "/tmp/retrain_evidence")
+os.makedirs(EVIDENCE_DIR, exist_ok=True)
+
+def train_model_stub(data_path, out_model_path):
+    # placeholder training - write a dummy model file with timestamp
+    with open(out_model_path, "w") as fh:
+        fh.write(f"dummy-model-for-{data_path}-{time.time()}")
+    return out_model_path
+
+def sha256_file(path):
+    import hashlib
+    h = hashlib.sha256()
+    with open(path,"rb") as fh:
+        for chunk in iter(lambda: fh.read(4096), b""):
+            h.update(chunk)
+    return h.hexdigest()
+
+def run_retrain(current_annotations, baseline_annotations, training_data_path):
+    # 1) curation / drift report
+    cur_report = os.path.join(EVIDENCE_DIR, "curation_report.json")
+    compare_to_baseline(current_annotations, baseline_annotations, cur_report)
+    # 2) run training (stub)
+    model_out = os.path.join(EVIDENCE_DIR, f"model_{int(time.time())}.bin")
+    train_model_stub(training_data_path, model_out)
+    model_hash = sha256_file(model_out)
+    # 3) record metadata
+    meta = {
+        "trained_at": datetime.utcnow().isoformat()+"Z",
+        "training_data": training_data_path,
+        "model_path": model_out,
+        "model_hash": model_hash,
+        "curation_report": cur_report
+    }
+    meta_path = os.path.join(EVIDENCE_DIR, "training_metadata.json")
+    with open(meta_path, "w") as fh:
+        json.dump(meta, fh, indent=2)
+    return meta_path
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--current-annotations", required=True)
+    p.add_argument("--baseline-annotations", required=True)
+    p.add_argument("--training-data", required=True)
+    args = p.parse_args()
+    print("Wrote retrain metadata:", run_retrain(args.current_annotations, args.baseline_annotations, args.training_data))
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/retrain_on_drift.yml
+name: Retrain on Drift (example)
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 3 * * *'  # daily drift check
+
+jobs:
+  retrain:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install requests sentence-transformers
+      - name: Run retrain pipeline (demo)
+        run: |
+          # In prod, current and baseline annotations would be S3 or DB paths; use samples here
+          python ops/ml/retrain_pipeline.py --current-annotations data/annotations_current.json --baseline-annotations data/annotations_baseline.json --training-data data/train
+      - name: Upload retrain evidence
+        uses: actions/upload-artifact@v4
+        with:
+          name: retrain-evidence
+          path: /tmp/retrain_evidence
+
*** End Patch
*** Begin Patch
*** Add File:docs/certification_mapping.md
+Certification mapping (ISO26262 / SOTIF / UL4600) — quick starter
+----------------------------------------------------------------
+This document maps common evidence types to representative certification artifacts.
+
+1) System Design & Architecture
+ - Artifact: system architecture diagrams, safety concept, component allocation
+ - Repo: docs/architecture/, docs/safety_governance.md
+
+2) Requirements & Traceability
+ - Artifact: requirement->test traceability matrix
+ - Repo: evidence/verification.json, tests/* with requirement IDs
+
+3) Implementation & Configuration Management
+ - Artifact: builder image digests, git commit shas, provenance.json
+ - Repo: provenance.json inside each evidence bundle
+
+4) Verification & Validation
+ - Artifact: SIL/HIL logs, scenario definitions, pass/fail matrices, proof certificates
+ - Repo: /tmp/safety_evidence/*, /tmp/verifier_logs/*, composed_certificate.json
+
+5) Runtime Monitoring & Field Data
+ - Artifact: monitor logs, incidents, telemetry aggregates
+ - Repo: monitor_agent logs, Prometheus snapshots, incident tickets (linked)
+
+6) Signing & Integrity
+ - Artifact: signed evidence bundle (.sig), public key material
+ - Repo: sign_with_kms_or_vault outputs and published public keys
+
+Use this as a starting checklist for auditor requests; expand with project- and regulator-specific mappings.
+
*** End Patch
*** Begin Patch
*** Add File:docs/release_board_template.md
+Release Board Template — Safety Release Approval
+---------------------------------------------
+Use this template to record approvals for safety releases.
+
+Release ID:
+Model:
+Version / Artifact Hash:
+Owner:
+Safety Owner:
+Verification Engineer:
+Release Approver:
+Evidence bundle path:
+Public Key ID:
+Proof certificate path:
+HIL rack / firmware ids:
+Scenario list & seeds:
+Key verification checklist:
+ - evidence bundle signed? (yes/no)
+ - public key published? (yes/no)
+ - reproducibility check passed? (yes/no)
+ - top invariants proved or monitored? (list)
+
+Approvals:
+- Safety Owner: name / timestamp / signature
+- Verification Engineer: name / timestamp / signature
+- Release Approver: name / timestamp / signature
+
+Notes / exceptions:
+
+Post-release actions:
+- schedule field monitoring:
+- audit retention location:
+
*** End Patch
*** End Patch
