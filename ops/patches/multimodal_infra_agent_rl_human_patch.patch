*** Begin Patch
*** Add File:ops/models/vision/clip_adapter.py
+#!/usr/bin/env python3
+"""
+Minimal CLIP adapter: returns image embeddings.
+ - If torch + torchvision + timm/transformers are available, uses a light model.
+ - Falls back to a deterministic stub embedding when libraries are missing (useful for CI).
+
+API:
+ - embed_image_bytes(image_bytes) -> list[float]
+"""
+import os
+import io
+import base64
+try:
+    from PIL import Image
+except Exception:
+    Image = None
+
+VECT_DIM = int(os.environ.get("CLIP_DIM", "512"))
+
+try:
+    import torch
+    from torchvision import transforms
+    # try to use torchvision's resnet18 as a lightweight visual embedder for demo
+    from torchvision.models import resnet18
+    _TORCH_AVAILABLE = True
+    _MODEL = resnet18(pretrained=True)
+    _MODEL.fc = torch.nn.Identity()  # produce embedding before final fc
+    _MODEL.eval()
+    _TRANSFORM = transforms.Compose([
+        transforms.Resize((224,224)),
+        transforms.ToTensor(),
+        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
+    ])
+except Exception:
+    _TORCH_AVAILABLE = False
+    _MODEL = None
+    _TRANSFORM = None
+
+def _normalize_vec(v):
+    import math
+    if not v:
+        return [0.0]*VECT_DIM
+    norm = math.sqrt(sum(float(x)*float(x) for x in v))
+    if norm == 0.0:
+        return [0.0]*len(v)
+    return [float(x)/norm for x in v]
+
+def embed_image_bytes(image_bytes):
+    """
+    Return a list[float] embedding for the provided image bytes.
+    """
+    if _TORCH_AVAILABLE and Image is not None:
+        try:
+            img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
+            x = _TRANSFORM(img).unsqueeze(0)
+            with torch.no_grad():
+                out = _MODEL(x).squeeze(0).cpu().numpy().tolist()
+            # truncate or pad to VECT_DIM
+            if len(out) >= VECT_DIM:
+                out = out[:VECT_DIM]
+            else:
+                out = out + [0.0]*(VECT_DIM - len(out))
+            return _normalize_vec(out)
+        except Exception:
+            pass
+    # fallback deterministic stub: hash
+    import hashlib, struct
+    h = hashlib.sha256(image_bytes).digest()
+    vec = []
+    for i in range(VECT_DIM):
+        # unpack bytes into float-ish deterministic numbers
+        b = h[(i*4) % len(h):((i*4)%len(h))+4]
+        val = struct.unpack("f", (b if len(b)==4 else (b + b'\x00'*(4-len(b)))))[0]
+        vec.append(float(val))
+    return _normalize_vec(vec)
+
+if __name__ == "__main__":
+    import sys, base64
+    p = sys.argv[1] if len(sys.argv)>1 else None
+    if not p:
+        print("usage: clip_adapter.py image.jpg")
+        raise SystemExit(2)
+    b = open(p,"rb").read()
+    v = embed_image_bytes(b)
+    print("len", len(v), "sample", v[:5])
+
*** End Patch
*** Begin Patch
*** Add File:ops/models/serve/model_server.py
+#!/usr/bin/env python3
+"""
+Lightweight multimodal model server.
+ - Endpoints:
+    POST /embed-image   multipart/form-data file -> {"embedding": [...]}
+    POST /generate      JSON {"text": "...", "image": base64?} -> {"output": "...", "latency_s": 0.12}
+    GET  /metrics       Prometheus metrics
+    GET  /health
+ - Uses CLIP adapter to embed images and optionally forwards text+embedding to configured LLM endpoint.
+ - Exposes simple latency metrics using prometheus_client.
+"""
+import os
+import time
+import base64
+import json
+from io import BytesIO
+from fastapi import FastAPI, File, UploadFile, HTTPException
+from fastapi.responses import JSONResponse, PlainTextResponse
+import uvicorn
+
+from prometheus_client import Summary, Counter, generate_latest, CONTENT_TYPE_LATEST
+from ops.models.vision.clip_adapter import embed_image_bytes
+
+LLM_API_URL = os.environ.get("LLM_API_URL", "")
+PORT = int(os.environ.get("MODEL_SERVER_PORT", "8080"))
+
+REQUEST_LATENCY = Summary("aegis_model_request_latency_seconds", "Latency of model server requests")
+REQUEST_COUNT = Counter("aegis_model_request_total", "Total model requests")
+
+app = FastAPI()
+
+@app.get("/health")
+def health():
+    return {"status":"ok"}
+
+@app.get("/metrics")
+def metrics():
+    return PlainTextResponse(generate_latest(), media_type=CONTENT_TYPE_LATEST)
+
+@app.post("/embed-image")
+@REQUEST_LATENCY.time()
+def embed_image(file: UploadFile = File(...)):
+    REQUEST_COUNT.inc()
+    try:
+        data = file.file.read()
+        emb = embed_image_bytes(data)
+        return {"embedding": emb}
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=str(e))
+
+@app.post("/generate")
+@REQUEST_LATENCY.time()
+def generate(payload: dict):
+    """
+    payload: {"text": "...", "image_b64": "...", "params": {...}}
+    If LLM_API_URL set, forwards a request {"text":..., "embedding": [...]}.
+    Otherwise returns a stub generation.
+    """
+    REQUEST_COUNT.inc()
+    text = payload.get("text","")
+    image_b64 = payload.get("image_b64")
+    emb = None
+    if image_b64:
+        try:
+            data = base64.b64decode(image_b64)
+            emb = embed_image_bytes(data)
+        except Exception as e:
+            raise HTTPException(status_code=400, detail="image decode failed:"+str(e))
+    start = time.time()
+    # forward to LLM if configured
+    if LLM_API_URL:
+        import requests
+        req = {"text": text, "embedding": emb}
+        try:
+            r = requests.post(LLM_API_URL, json=req, timeout=30)
+            r.raise_for_status()
+            resp = r.json()
+            lat = time.time() - start
+            return {"output": resp, "latency_s": lat}
+        except Exception as e:
+            # fallback to stub below
+            pass
+    # stub generation: echo plus len embedding
+    time.sleep(0.02)  # simulate small compute
+    out = {"text": text[::-1], "emb_len": len(emb) if emb else 0}
+    lat = time.time() - start
+    return {"output": out, "latency_s": lat}
+
+def serve():
+    uvicorn.run(app, host="0.0.0.0", port=PORT, log_level="info")
+
+if __name__ == "__main__":
+    serve()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/deployments/model-server-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-multimodal-server
+  namespace: aegis-models
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-multimodal-server
+  template:
+    metadata:
+      labels:
+        app: aegis-multimodal-server
+    spec:
+      containers:
+        - name: model-server
+          image: ${MODEL_SERVER_IMAGE:-aegis/model-server:latest}
+          imagePullPolicy: IfNotPresent
+          resources:
+            limits:
+              nvidia.com/gpu: 1
+              cpu: "2"
+              memory: "8Gi"
+            requests:
+              cpu: "1"
+              memory: "4Gi"
+          env:
+            - name: LLM_API_URL
+              value: ""
+            - name: MODEL_SERVER_PORT
+              value: "8080"
+          ports:
+            - containerPort: 8080
+---
+# Horizontal Pod Autoscaler
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-multimodal-hpa
+  namespace: aegis-models
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-multimodal-server
+  minReplicas: 1
+  maxReplicas: 4
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/dataset_builder.py
+#!/usr/bin/env python3
+"""
+Build multimodal JSONL datasets from scenario outputs and optional CLIP embeddings.
+ - Input: directory with images and annotation JSON files
+ - Output: dataset.jsonl with entries: {"image_path": "...", "text": "...", "embedding": [...], "meta": {...}}
+ - Uses clip_adapter to embed images when requested.
+"""
+import os
+import json
+from glob import glob
+from ops.models.vision.clip_adapter import embed_image_bytes
+
+def build_dataset(images_dir, annotations_json, out_path, embed=True):
+    annotations = json.load(open(annotations_json)) if os.path.exists(annotations_json) else {}
+    files = sorted(glob(os.path.join(images_dir, "*.*")))
+    with open(out_path, "w") as out:
+        for f in files:
+            fn = os.path.basename(f)
+            txt = annotations.get(fn, {}).get("caption", "")
+            meta = annotations.get(fn, {}).get("meta", {})
+            emb = None
+            if embed:
+                try:
+                    emb = embed_image_bytes(open(f,"rb").read())
+                except Exception:
+                    emb = None
+            entry = {"image_path": f, "text": txt, "embedding": emb, "meta": meta}
+            out.write(json.dumps(entry) + "\n")
+    print("Wrote dataset to", out_path)
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--images", required=True)
+    p.add_argument("--annotations", required=False, default="")
+    p.add_argument("--out", default="/tmp/multimodal_dataset.jsonl")
+    args = p.parse_args()
+    build_dataset(args.images, args.annotations, args.out)
+
*** End Patch
*** Begin Patch
*** Add File:ops/retrieval/vision_reranker_adapter.py
+#!/usr/bin/env python3
+"""
+Adapter to wrap CrossEncoder reranker to accept image embeddings + text candidates.
+ - Input: query_text, image_embedding, candidates: ["text1","text2",...]
+ - Produces scores by concatenating normalized vectors (text embedding optional) and ranking with CrossEncoder if available.
+ - Fallback: lexical overlap + dot product with image embedding stub.
+"""
+import os
+try:
+    from sentence_transformers import CrossEncoder
+    CROSS_AVAILABLE = True
+except Exception:
+    CROSS_AVAILABLE = False
+
+MODEL_PATH = os.environ.get("RERANKER_MODEL_PATH", "cross-encoder/ms-marco-MiniLM-L-6-v2")
+
+_model = None
+def load_model():
+    global _model
+    if _model is None and CROSS_AVAILABLE:
+        _model = CrossEncoder(MODEL_PATH)
+    return _model
+
+def score_candidates(query_text, image_embedding, candidates):
+    """
+    Returns list of {"text":c, "score":float}
+    """
+    if CROSS_AVAILABLE:
+        m = load_model()
+        pairs = [ ( (query_text + " ||| " + str(image_embedding) ), c) for c in candidates ]
+        scores = m.predict(pairs)
+        return [{"text":c,"score":float(s)} for c,s in zip(candidates, scores)]
+    # fallback simple scoring
+    qset = set(query_text.split())
+    res = []
+    for c in candidates:
+        overlap = len(qset.intersection(set(c.split())))
+        # simple image affinity: dot of embedding with constant vector
+        img_aff = 0.0
+        if image_embedding:
+            img_aff = sum(image_embedding)/len(image_embedding)
+        score = overlap + 0.01 * img_aff
+        res.append({"text":c,"score":score})
+    res.sort(key=lambda x: x["score"], reverse=True)
+    return res
+
+if __name__ == "__main__":
+    import json,sys
+    q="example"
+    emb=None
+    cands=["a b c","example text","other"]
+    print(score_candidates(q, emb, cands))
+
*** End Patch
*** Begin Patch
*** Add File:ops/agents/stateful_orchestrator.py
+#!/usr/bin/env python3
+"""
+Stateful agent orchestrator service.
+ - Stores plans in SQLite (persistent)
+ - Supports submit_plan, get_status, preempt_plan, complete_step
+ - Basic safety: requires plan to declare 'safe': true or requires pre-approval via human_validation_service
+ - Uses a simple worker thread to execute steps (simulated) and respects preemption.
+"""
+import os
+import sqlite3
+import threading
+import time
+import json
+from flask import Flask, request, jsonify
+from datetime import datetime
+
+DB_PATH = os.environ.get("AGENT_DB", "/tmp/aegis_agent.db")
+HUMAN_VALIDATION_URL = os.environ.get("HUMAN_VALIDATION_URL", "")
+
+app = Flask(__name__)
+_lock = threading.Lock()
+
+def init_db():
+    conn = sqlite3.connect(DB_PATH)
+    c = conn.cursor()
+    c.execute("CREATE TABLE IF NOT EXISTS plans (id TEXT PRIMARY KEY, owner TEXT, plan_json TEXT, state TEXT, created_at TEXT, updated_at TEXT)")
+    conn.commit()
+    conn.close()
+
+def save_plan(plan_id, owner, plan_json, state="pending"):
+    conn = sqlite3.connect(DB_PATH)
+    c = conn.cursor()
+    ts = datetime.utcnow().isoformat()+"Z"
+    c.execute("INSERT OR REPLACE INTO plans (id,owner,plan_json,state,created_at,updated_at) VALUES (?,?,?,?,?,?)",
+              (plan_id, owner, json.dumps(plan_json), state, ts, ts))
+    conn.commit()
+    conn.close()
+
+def load_plan(plan_id):
+    conn = sqlite3.connect(DB_PATH)
+    c = conn.cursor()
+    c.execute("SELECT id,owner,plan_json,state,created_at,updated_at FROM plans WHERE id=?", (plan_id,))
+    row = c.fetchone()
+    conn.close()
+    if not row:
+        return None
+    return {"id":row[0],"owner":row[1],"plan":json.loads(row[2]),"state":row[3],"created_at":row[4],"updated_at":row[5]}
+
+def worker_loop():
+    while True:
+        try:
+            conn = sqlite3.connect(DB_PATH)
+            c = conn.cursor()
+            c.execute("SELECT id, plan_json, state FROM plans WHERE state IN ('approved','running') LIMIT 1")
+            row = c.fetchone()
+            if row:
+                plan_id, plan_json_s, state = row
+                plan = json.loads(plan_json_s)
+                # mark running
+                c.execute("UPDATE plans SET state=?, updated_at=? WHERE id=?", ("running", datetime.utcnow().isoformat()+"Z", plan_id))
+                conn.commit()
+                # simulate executing steps, respecting preemption flag stored in state
+                steps = plan.get("steps", [])
+                for i, step in enumerate(steps):
+                    # check preemption
+                    c.execute("SELECT state FROM plans WHERE id=?", (plan_id,))
+                    st = c.fetchone()[0]
+                    if st == "preempted":
+                        break
+                    # execute step (simulate delay)
+                    time.sleep(step.get("duration", 0.1))
+                    # append step result to plan (naive approach)
+                # mark completed unless preempted
+                c.execute("SELECT state FROM plans WHERE id=?", (plan_id,))
+                final = c.fetchone()[0]
+                if final != "preempted":
+                    c.execute("UPDATE plans SET state=?, updated_at=? WHERE id=?", ("completed", datetime.utcnow().isoformat()+"Z", plan_id))
+                    conn.commit()
+            conn.close()
+        except Exception:
+            pass
+        time.sleep(1)
+
+@app.route("/submit_plan", methods=["POST"])
+def submit_plan():
+    body = request.get_json() or {}
+    plan_id = body.get("id")
+    owner = body.get("owner", "unknown")
+    plan = body.get("plan", {})
+    if not plan_id or not plan:
+        return jsonify({"error":"id and plan required"}), 400
+    # safety gating: require plan['safe']==True or human approval
+    if not plan.get("safe", False):
+        # create a human validation task and mark pending
+        if HUMAN_VALIDATION_URL:
+            import requests
+            tasks_resp = requests.post(HUMAN_VALIDATION_URL+"/tasks", json={"id":plan_id,"owner":owner,"plan":plan})
+        save_plan(plan_id, owner, plan, state="pending_approval")
+        return jsonify({"status":"pending_approval","plan_id":plan_id})
+    save_plan(plan_id, owner, plan, state="approved")
+    return jsonify({"status":"approved","plan_id":plan_id})
+
+@app.route("/approve/<plan_id>", methods=["POST"])
+def approve(plan_id):
+    p = load_plan(plan_id)
+    if not p:
+        return jsonify({"error":"not found"}), 404
+    save_plan(plan_id, p["owner"], p["plan"], state="approved")
+    return jsonify({"status":"approved"})
+
+@app.route("/preempt/<plan_id>", methods=["POST"])
+def preempt(plan_id):
+    p = load_plan(plan_id)
+    if not p:
+        return jsonify({"error":"not found"}), 404
+    save_plan(plan_id, p["owner"], p["plan"], state="preempted")
+    return jsonify({"status":"preempted"})
+
+@app.route("/status/<plan_id>")
+def status(plan_id):
+    p = load_plan(plan_id)
+    if not p:
+        return jsonify({"error":"not found"}), 404
+    return jsonify(p)
+
+if __name__ == "__main__":
+    init_db()
+    t = threading.Thread(target=worker_loop, daemon=True)
+    t.start()
+    app.run(host="0.0.0.0", port=int(os.environ.get("AGENT_PORT","8090")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/rl/online_trainer.py
+#!/usr/bin/env python3
+"""
+Online RL trainer scaffold for safe adaptation.
+ - Collects experiences via HTTP (append to buffer)
+ - When buffer reaches threshold, produces a retrain artifact and posts a human approval task
+ - Retrain only proceeds after approval; evidence is packaged and signed when retrained.
+"""
+import os
+import json
+import threading
+import time
+from flask import Flask, request, jsonify
+from datetime import datetime
+from ops.ml.retrain_pipeline import train_model_stub
+
+BUFFER_PATH = os.environ.get("RL_BUFFER", "/tmp/rl_buffer.jsonl")
+BUFFER_THRESHOLD = int(os.environ.get("RL_BUFFER_THRESHOLD", "100"))
+HUMAN_VALIDATION_URL = os.environ.get("HUMAN_VALIDATION_URL", "")
+EVIDENCE_DIR = os.environ.get("RETRAIN_EVIDENCE_DIR", "/tmp/retrain_evidence")
+os.makedirs(EVIDENCE_DIR, exist_ok=True)
+
+app = Flask(__name__)
+_lock = threading.Lock()
+
+def buffer_size():
+    if not os.path.exists(BUFFER_PATH):
+        return 0
+    return sum(1 for _ in open(BUFFER_PATH))
+
+def append_experience(exp):
+    with _lock:
+        with open(BUFFER_PATH, "a") as fh:
+            fh.write(json.dumps(exp) + "\n")
+
+def create_retrain_job():
+    # read buffer and write a dataset (simplified)
+    lines = []
+    with _lock:
+        if not os.path.exists(BUFFER_PATH):
+            return None
+        with open(BUFFER_PATH) as fh:
+            for l in fh:
+                lines.append(json.loads(l))
+        # rotate buffer
+        os.rename(BUFFER_PATH, BUFFER_PATH + ".processed")
+    dataset_path = os.path.join(EVIDENCE_DIR, f"rl_dataset_{int(time.time())}.jsonl")
+    with open(dataset_path,"w") as out:
+        for e in lines:
+            out.write(json.dumps(e) + "\n")
+    # produce retrain artifact via stub training
+    model_path = os.path.join(EVIDENCE_DIR, f"rl_model_{int(time.time())}.bin")
+    train_model_stub(dataset_path, model_path)
+    # create human approval task
+    task = {"id": os.path.basename(model_path), "type":"retrain", "model_path": model_path, "submit_ts": datetime.utcnow().isoformat()+"Z"}
+    if HUMAN_VALIDATION_URL:
+        import requests
+        try:
+            requests.post(HUMAN_VALIDATION_URL+"/tasks", json=task, timeout=5)
+        except Exception:
+            pass
+    return {"dataset": dataset_path, "model": model_path}
+
+@app.route("/experience", methods=["POST"])
+def experience():
+    body = request.get_json() or {}
+    append_experience(body)
+    if buffer_size() >= BUFFER_THRESHOLD:
+        job = create_retrain_job()
+        return jsonify({"status":"retrain_created","job":job})
+    return jsonify({"status":"buffered","size":buffer_size()})
+
+@app.route("/status")
+def status():
+    return jsonify({"buffer_size": buffer_size()})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("RL_PORT","8100")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/human/human_validation_service.py
+#!/usr/bin/env python3
+"""
+Minimal human validation service for manual approvals.
+ - Tasks are stored in a JSON file and exposed via endpoints for listing and approval.
+ - Intended to be integrated with orchestrator and RL pipeline for gating changes.
+"""
+import os
+import json
+from flask import Flask, request, jsonify
+from datetime import datetime
+
+TASK_DB = os.environ.get("HUMAN_TASK_DB", "/tmp/human_tasks.json")
+app = Flask(__name__)
+
+def load_tasks():
+    if os.path.exists(TASK_DB):
+        try:
+            return json.load(open(TASK_DB))
+        except Exception:
+            return {}
+    return {}
+
+def save_tasks(t):
+    with open(TASK_DB,"w") as fh:
+        json.dump(t, fh, indent=2)
+
+@app.route("/tasks", methods=["POST"])
+def create_task():
+    t = load_tasks()
+    body = request.get_json() or {}
+    tid = body.get("id") or f"task_{int(time.time()*1000)}"
+    t[tid] = {"id":tid, "payload": body, "state":"open", "created":datetime.utcnow().isoformat()+"Z"}
+    save_tasks(t)
+    return jsonify({"id":tid})
+
+@app.route("/tasks", methods=["GET"])
+def list_tasks():
+    return jsonify(load_tasks())
+
+@app.route("/task/<tid>/approve", methods=["POST"])
+def approve(tid):
+    t = load_tasks()
+    if tid not in t:
+        return jsonify({"error":"not found"}), 404
+    t[tid]["state"] = "approved"
+    t[tid]["approved_at"] = datetime.utcnow().isoformat()+"Z"
+    save_tasks(t)
+    return jsonify({"status":"approved"})
+
+@app.route("/task/<tid>/reject", methods=["POST"])
+def reject(tid):
+    t = load_tasks()
+    if tid not in t:
+        return jsonify({"error":"not found"}), 404
+    t[tid]["state"] = "rejected"
+    t[tid]["rejected_at"] = datetime.utcnow().isoformat()+"Z"
+    save_tasks(t)
+    return jsonify({"status":"rejected"})
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("HUMAN_PORT","8200")))
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/multimodal_inference_ci.yml
+name: Multimodal Inference CI
+on:
+  workflow_dispatch:
+
+jobs:
+  build-and-test:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps (light)
+        run: |
+          python -m pip install --upgrade pip
+          pip install fastapi uvicorn prometheus_client pillow requests
+      - name: Run model server and test endpoints
+        run: |
+          (python ops/models/serve/model_server.py &) ; sleep 2
+          # health
+          curl -f http://127.0.0.1:8080/health
+          # metrics
+          curl -f http://127.0.0.1:8080/metrics
+          # embed image: use a tiny generated image
+          python - <<'PY'
+from PIL import Image
+im=Image.new('RGB',(64,64),(128,128,128))
+im.save('/tmp/test.jpg')
+import base64,requests
+files={'file': open('/tmp/test.jpg','rb')}
+r=requests.post('http://127.0.0.1:8080/embed-image', files=files, timeout=10)
+print(r.status_code, r.json())
+PY
+
*** End Patch
*** End Patch
