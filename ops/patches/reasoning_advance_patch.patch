*** Begin Patch
*** Add File:ops/planning/domain_planner.py
+#!/usr/bin/env python3
+"""
+Domain-aware planner / decomposer.
+ - Combines heuristic decomposition (plan_decomposer) with optional few-shot LLM planning
+ - Allows domain templates to be registered so domain experts can provide decomposition patterns
+ - Produces an ordered plan of micro-tasks with explicit dataflow edges (outputs -> inputs)
+ - Intended to be used by chain_runner to perform verifiable micro-steps
+"""
+from typing import List, Dict, Callable, Optional
+import json
+import os
+
+from ops.reasoning.plan_decomposer import decompose_text_to_steps
+
+# Simple in-repo domain templates: mapping domain_name -> list of step templates
+DOMAIN_TEMPLATE_DIR = os.environ.get("DOMAIN_TEMPLATE_DIR", "ops/planning/domain_templates")
+
+def load_domain_templates() -> Dict[str, List[str]]:
+    templates = {}
+    if os.path.exists(DOMAIN_TEMPLATE_DIR):
+        for fn in os.listdir(DOMAIN_TEMPLATE_DIR):
+            path = os.path.join(DOMAIN_TEMPLATE_DIR, fn)
+            try:
+                templates[os.path.splitext(fn)[0]] = json.load(open(path))
+            except Exception:
+                continue
+    return templates
+
+def apply_template(domain: str, text: str) -> Optional[List[Dict]]:
+    """
+    If a domain template exists, produce micro-steps by filling template slots.
+    Template is list of strings with {input} or {context}. This is a best-effort helper.
+    """
+    templates = load_domain_templates()
+    if domain not in templates:
+        return None
+    out = []
+    for i, t in enumerate(templates[domain]):
+        step_text = t.format(input=text, context=text[:200])
+        out.append({"id": i+1, "text": step_text, "prereqs": [j+1 for j in range(i)]})
+    return out
+
+def few_shot_plan_via_llm(text: str, llm_call: Callable[[str], Dict], shots: List[str]=None) -> List[Dict]:
+    """
+    Use an LLM to propose a plan. llm_call(prompt)->{"answer":str}
+    shots: optional few-shot examples (strings) to include in prompt.
+    """
+    prompt = "Decompose the following task into ordered micro-steps with simple short descriptions.\n\n"
+    if shots:
+        prompt += "\n\n".join(shots) + "\n\n"
+    prompt += f"TASK: {text}\n\nAnswer with numbered steps."
+    res = llm_call(prompt)
+    # parse naive numbered list
+    lines = res.get("answer", "").splitlines()
+    steps = []
+    for idx, line in enumerate(lines):
+        line = line.strip()
+        if not line:
+            continue
+        # strip leading "1." or "1)"
+        import re
+        m = re.match(r"^\s*(\d+)[\.\)]\s*(.*)$", line)
+        if m:
+            steps.append({"id": int(m.group(1)), "text": m.group(2).strip(), "prereqs": [i for i in range(1,int(m.group(1)))]})
+        else:
+            # fallback append as continuing description
+            steps.append({"id": len(steps)+1, "text": line, "prereqs": [i for i in range(1,len(steps)+1)]})
+    return steps
+
+def plan(task_text: str, domain: Optional[str]=None, llm_call: Optional[Callable[[str], Dict]]=None, shots: Optional[List[str]]=None) -> List[Dict]:
+    # domain template preferred
+    if domain:
+        templated = apply_template(domain, task_text)
+        if templated:
+            return templated
+    # heuristic fallback
+    steps = decompose_text_to_steps(task_text)
+    if len(steps) <= 3 and llm_call:
+        # try to enhance with LLM planning
+        try:
+            llm_steps = few_shot_plan_via_llm(task_text, llm_call, shots=shots)
+            if llm_steps and len(llm_steps) > len(steps):
+                return llm_steps
+        except Exception:
+            pass
+    return steps
+
+if __name__ == "__main__":
+    import sys, json
+    txt = " ".join(sys.argv[1:]) or "Find the shortest safe route from A to B, estimate travel time, and recommend speed profile."
+    print(json.dumps(plan(txt, domain=None), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/causal/causal_reasoner.py
+#!/usr/bin/env python3
+"""
+Causal reasoning scaffold.
+ - Lightweight Structural Causal Model (SCM) representation and counterfactual stub.
+ - Intended as a composable service: domain experts can encode simple causal graphs and run interventions/counterfactuals.
+ - For production, integrate with DoWhy / EconML or a causal library.
+"""
+import json
+import os
+from typing import Dict, Any, List
+
+CAUSAL_DB = os.environ.get("CAUSAL_DB", "/etc/aegis/causal_models")
+os.makedirs(CAUSAL_DB, exist_ok=True)
+
+class SCM:
+    """
+    Very small SCM: nodes map to structural equations expressed as Python expressions.
+    nodes: {name: {"expr": "func(parents, noise)", "parents":[..]}}
+    """
+    def __init__(self, name: str, spec: Dict[str, Any]):
+        self.name = name
+        self.spec = spec
+
+    def save(self):
+        open(os.path.join(CAUSAL_DB, f"{self.name}.json"), "w").write(json.dumps(self.spec, indent=2))
+
+    @classmethod
+    def load(cls, name: str):
+        path = os.path.join(CAUSAL_DB, f"{name}.json")
+        if not os.path.exists(path):
+            raise FileNotFoundError(path)
+        spec = json.load(open(path))
+        return cls(name, spec)
+
+    def do_intervention(self, interventions: Dict[str, Any], samples: int =1) -> Dict:
+        """
+        Naive evaluator: apply interventions by overriding node values and evaluate nodes topologically.
+        This is a stub for domain experiments and counterfactual reasoning pipelines.
+        """
+        # order nodes by provided spec order
+        values = {}
+        for node, conf in self.spec.items():
+            if node in interventions:
+                values[node] = interventions[node]
+            else:
+                expr = conf.get("expr")
+                # UNSAFE eval used for demo only; replace with safe evaluator in prod
+                ctx = {"parents": values, "noise": 0}
+                try:
+                    values[node] = eval(expr, {}, ctx)
+                except Exception:
+                    values[node] = None
+        return values
+
+def register_scm(name: str, spec: Dict):
+    scm = SCM(name, spec)
+    scm.save()
+    return scm
+
+def counterfactual_query(model_name: str, factual: Dict, intervention: Dict):
+    scm = SCM.load(model_name)
+    # apply factual (not fully implemented)
+    baseline = scm.do_intervention({}, samples=1)
+    counter = scm.do_intervention(intervention, samples=1)
+    return {"baseline": baseline, "counterfactual": counter}
+
+if __name__ == "__main__":
+    # example
+    spec = {
+        "speed": {"expr": "50", "parents": []},
+        "stopping_dist": {"expr": "parents.get('speed',40)**2 / 20.0", "parents": ["speed"]}
+    }
+    register_scm("braking", spec)
+    print(counterfactual_query("braking", {}, {"speed": 60}))
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariant_authoring_tool.py
+#!/usr/bin/env python3
+"""
+Invariant authoring helper and SMT tactic generator.
+ - Adds new invariants with metadata into invariant registry and emits an SMT-LIB encoding scaffold for Z3/SMT solvers.
+ - Provides basic tactic hints (e.g., interval, induction, linearization) for targeted_prover to consume.
+"""
+import os
+import json
+from datetime import datetime
+
+REGISTRY = os.environ.get("INVARIANT_REGISTRY_PATH", "ops/formal/invariant_registry.json")
+SMT_OUT_DIR = os.environ.get("INVARIANT_SMT_DIR", "/tmp/invariant_smt")
+os.makedirs(SMT_OUT_DIR, exist_ok=True)
+
+def load_registry():
+    if os.path.exists(REGISTRY):
+        return json.load(open(REGISTRY))
+    return {}
+
+def save_registry(r):
+    os.makedirs(os.path.dirname(REGISTRY), exist_ok=True)
+    open(REGISTRY, "w").write(json.dumps(r, indent=2))
+
+def add_invariant(inv_id: str, spec: Dict, tactic_hint: str = "interval"):
+    r = load_registry()
+    r[inv_id] = {"spec": spec, "tactic": tactic_hint, "created_at": datetime.utcnow().isoformat()+"Z"}
+    save_registry(r)
+    # emit SMT stub
+    smt = f"; SMT stub for invariant {inv_id}\n; tactic: {tactic_hint}\n\n; TODO: replace with concrete encoding\n"
+    out = os.path.join(SMT_OUT_DIR, f"{inv_id}.smt2")
+    open(out, "w").write(smt)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--id", required=True)
+    p.add_argument("--expr", required=True, help="human-readable expression, e.g., speed <= 30")
+    p.add_argument("--tactic", default="interval")
+    args = p.parse_args()
+    smt_path = add_invariant(args.id, {"expr": args.expr}, tactic_hint=args.tactic)
+    print("Wrote SMT scaffold to", smt_path)
+
*** End Patch
*** Begin Patch
*** Add File:ops/retrieval/reranker.py
+#!/usr/bin/env python3
+"""
+Retrieval reranker interface (cross-encoder style).
+ - Provides a lightweight local reranker fallback and an API hook for a hosted cross-encoder.
+ - Combines textual relevance score with provenance_validator score to produce final ranking.
+ - In production integrate with a cross-encoder model (sentence-transformers / transformers).
+"""
+import os
+import math
+from typing import List, Dict
+
+PROV_VALIDATOR = "ops/retrieval/provenance_validator.py"
+
+def simple_overlap_score(query: str, text: str) -> float:
+    qset = set(query.lower().split())
+    tset = set(text.lower().split())
+    if not tset:
+        return 0.0
+    return len(qset.intersection(tset)) / len(qset.union(tset))
+
+def call_provenance_validator(items):
+    # import validate_retrieved_items
+    try:
+        from ops.retrieval.provenance_validator import validate_retrieved_items
+        return validate_retrieved_items(items)
+    except Exception:
+        # fallback: neutral scores
+        return [{"url": it.get("url",""), "score": 0.5, "title": it.get("title")} for it in items]
+
+def rerank(query: str, items: List[Dict], cross_encoder_fn=None) -> List[Dict]:
+    """
+    items: [{"url":..., "title":..., "text":..., "meta":{}}]
+    cross_encoder_fn: optional callable(query, text)->score float (0..1)
+    Returns items sorted with added "final_score"
+    """
+    prov = call_provenance_validator(items)
+    prov_map = {p["url"]: p for p in prov}
+    scored = []
+    for it in items:
+        text = it.get("text","")
+        base = cross_encoder_fn(query, text) if cross_encoder_fn else simple_overlap_score(query, text)
+        p = prov_map.get(it.get("url",""), {})
+        prov_score = p.get("score", 0.5)
+        # combine: weight provenance more for safety contexts
+        final = 0.6 * base + 0.4 * prov_score
+        scored.append({**it, "base_score": base, "prov_score": prov_score, "final_score": final})
+    scored.sort(key=lambda x: x["final_score"], reverse=True)
+    return scored
+
+if __name__ == "__main__":
+    q = "How fast should car brake from 60 km/h"
+    items = [{"url":"https://en.wikipedia.org/wiki/Braking_distance","title":"Braking distance","text":"Braking distance increases with speed squared...","meta":{"published":"2020-01-01T00:00:00Z"}}]
+    print(rerank(q, items))
+
*** End Patch
*** Begin Patch
*** Add File:ops/nli/recalibrate_ci.py
+#!/usr/bin/env python3
+"""
+Calibration CI job:
+ - Runs NLI calibration pipeline on stored validation JSON files
+ - Updates a calibration artifact (temperature value) consumed by inference runtime
+ - Emits a small JSON artifact to CARBON_ARCHIVE or CI artifacts for traceability
+"""
+import os
+import json
+from ops.nli.calibration import load_validation_json, find_temperature_grid
+
+VALIDATION_DIR = os.environ.get("NLI_VALIDATION_DIR", "/tmp/nli_validation")
+OUT_PATH = os.environ.get("NLI_CALIB_OUT", "/tmp/nli_temperature.json")
+
+def find_and_write():
+    # pick latest validation file
+    files = [f for f in os.listdir(VALIDATION_DIR)] if os.path.exists(VALIDATION_DIR) else []
+    if not files:
+        print("No validation files found in", VALIDATION_DIR)
+        return None
+    files = sorted(files)
+    path = os.path.join(VALIDATION_DIR, files[-1])
+    probs, labels = load_validation_json(path)
+    t, nll = find_temperature_grid(probs, labels)
+    out = {"temp": t, "nll": nll, "source_file": path}
+    open(OUT_PATH, "w").write(json.dumps(out, indent=2))
+    print("Wrote calibration to", OUT_PATH)
+    return out
+
+if __name__ == "__main__":
+    find_and_write()
+
*** End Patch
*** Begin Patch
*** Add File:ops/reasoning/compositional_verifier.py
+#!/usr/bin/env python3
+"""
+Compose micro-proof/verification results into a global verdict.
+ - Input: list of step verification results with dependency graph
+ - Performs transitive checks, confidence propagation and identifies weakest links
+ - Emits a combined verdict with provenance for audit and further prover focus
+"""
+import json
+from typing import List, Dict
+from math import prod
+
+def aggregate_steps(steps: List[Dict]) -> Dict:
+    """
+    steps: [{"id":int,"verify":{"verdict":"pass|fail","confidence":float},"prereqs":[ids], "evidence":...}, ...]
+    Return global verdict with aggregated confidence and list of failed/weak steps.
+    """
+    # map by id
+    m = {s["id"]: s for s in steps}
+    # find any explicit fails
+    failed = [s for s in steps if s.get("verify",{}).get("verdict") != "pass"]
+    if failed:
+        return {"verdict": "fail", "failed_steps": [s["id"] for s in failed], "confidence": 0.0}
+    # multiply confidences of steps as conservative estimator of end-to-end correctness
+    confidences = []
+    for s in steps:
+        c = s.get("verify",{}).get("confidence")
+        if c is None:
+            c = 0.5
+        confidences.append(max(0.0, min(1.0, float(c))))
+    # conservative composition: product (assume independence)
+    overall = prod(confidences) if confidences else 0.0
+    # identify weakest links
+    sorted_by_conf = sorted([(s["id"], s.get("verify",{}).get("confidence",0.5)) for s in steps], key=lambda x: x[1])
+    weakest = sorted_by_conf[:3]
+    return {"verdict":"pass" if overall > 0.5 else "warn", "confidence": overall, "weakest_steps": weakest}
+
+if __name__ == "__main__":
+    import sys
+    # demo usage
+    steps = [
+        {"id":1,"verify":{"verdict":"pass","confidence":0.98},"prereqs":[]},
+        {"id":2,"verify":{"verdict":"pass","confidence":0.9},"prereqs":[1]},
+        {"id":3,"verify":{"verdict":"pass","confidence":0.6},"prereqs":[1,2]},
+    ]
+    print(json.dumps(aggregate_steps(steps), indent=2))
+
*** End Patch
*** End Patch
