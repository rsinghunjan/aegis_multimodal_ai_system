*** Begin Patch
*** Add File:ops/rt/systemd_rt_unit_template.service
+[Unit]
+Description=Aegis real-time control loop service (template)
+After=network.target
+
+[Service]
+Type=simple
+# Example: ExecStart=/usr/local/bin/rt_scheduler_wrapper.py --cmd "/opt/aegis/bin/control_loop --once" --iters 1 --realtime
+ExecStart=/usr/local/bin/rt_scheduler_wrapper.py --cmd "/opt/aegis/bin/control_loop --once" --iters 1 --realtime --rt-priority 80 --report /var/log/aegis/control_loop_latency.prom
+Restart=on-failure
+RestartSec=2
+# Allow real-time scheduling if system allows it (CAP_SYS_NICE or root)
+Nice=0
+IOSchedulingClass=1
+
+[Install]
+WantedBy=multi-user.target
+
+# Notes:
+# - Copy or template this unit to /etc/systemd/system/control_loop.service and run:
+#   systemctl daemon-reload
+#   systemctl enable --now control_loop.service
+# - The underlying control binary should be instrumented to exit quickly when run with --once for measurement mode.
+
*** End Patch
*** Begin Patch
*** Add File:ops/infra/label_rt_nodes.py
+#!/usr/bin/env python3
+"""
+Label nodes as realtime-capable (ops helper).
+ - Scans nodes (by instance type, CPU features or annotation) and applies label aegis/rt=true
+ - Must be run by a user with cluster-admin privileges or appropriate RBAC.
+"""
+import subprocess
+import json
+import sys
+from typing import List
+
+def kubectl(args: List[str]):
+    return subprocess.check_output(["kubectl"] + args).decode()
+
+def list_nodes():
+    out = kubectl(["get","nodes","-o","json"])
+    j = json.loads(out)
+    return j.get("items",[])
+
+def label_node(node_name, key="aegis/rt", value="true"):
+    subprocess.check_call(["kubectl","label","node",node_name,f"{key}={value}","--overwrite"])
+
+def select_rt_candidates(nodes):
+    # heuristic: choose nodes with > 8 cpu and name contains rt or rtx (customize per cluster)
+    cand = []
+    for n in nodes:
+        name = n["metadata"]["name"]
+        cpu = int(n["status"]["capacity"].get("cpu","0"))
+        if cpu >= 8 or "rt" in name.lower() or "realtime" in name.lower():
+            cand.append(name)
+    return cand
+
+if __name__ == "__main__":
+    nodes = list_nodes()
+    cands = select_rt_candidates(nodes)
+    if not cands:
+        print("No candidates found")
+        sys.exit(0)
+    for n in cands:
+        print("Labeling node", n)
+        label_node(n)
+    print("Done")
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/hil_runner_server.py
+#!/usr/bin/env python3
+"""
+Simple HIL runner server for integration with ops/hil/run_hil.py
+ - Accepts run requests and replay-ready notifications
+ - Manages job lifecycle and stores logs/artifacts under HIL_SERVER_DIR
+ - Provides minimal firmware snapshot integration point
+
+Endpoints:
+ - POST /api/v1/replay_ready  {bag, outdir, seed, rate}
+ - POST /api/v1/run          {artifact, seed, firmware_version, replay_input}
+ - GET  /api/v1/status/<job_id>
+
+This is a reference implementation you can run on a HIL orchestration host and extend to call vendor-specific APIs.
+"""
+import os
+import json
+import uuid
+import time
+from flask import Flask, request, jsonify
+from datetime import datetime
+from ops.hil.firmware_manager import snapshot_firmware
+
+HIL_SERVER_DIR = os.environ.get("HIL_SERVER_DIR", "/var/lib/aegis/hil_server")
+os.makedirs(HIL_SERVER_DIR, exist_ok=True)
+
+app = Flask(__name__)
+JOBS = {}
+
+def write_job(job):
+    path = os.path.join(HIL_SERVER_DIR, f"{job['id']}.json")
+    open(path, "w").write(json.dumps(job, indent=2))
+
+@app.route("/api/v1/replay_ready", methods=["POST"])
+def replay_ready():
+    data = request.get_json() or {}
+    job_id = str(uuid.uuid4())
+    job = {"id": job_id, "type": "replay_ready", "payload": data, "state": "queued", "ts": datetime.utcnow().isoformat()+"Z"}
+    JOBS[job_id] = job
+    write_job(job)
+    return jsonify({"job_id": job_id, "status":"queued"})
+
+@app.route("/api/v1/run", methods=["POST"])
+def run_job():
+    data = request.get_json() or {}
+    job_id = str(uuid.uuid4())
+    job = {"id": job_id, "type": "run", "payload": data, "state": "running", "ts": datetime.utcnow().isoformat()+"Z"}
+    JOBS[job_id] = job
+    write_job(job)
+    # snapshot firmware if requested
+    try:
+        fw_meta = None
+        if data.get("firmware_dump_cmd"):
+            meta = snapshot_firmware(data["firmware_dump_cmd"], f"fw_{job_id}.bin")
+            fw_meta = meta
+        # emulate a run: in production, call vendor orchestration and poll real status
+        time.sleep(1)  # pretend work
+        # write fake device log and job result
+        result = {"status":"succeeded", "logs": {"out": "ok"}, "fw_meta": fw_meta, "ts": datetime.utcnow().isoformat()+"Z", "seed": data.get("seed")}
+        job["result"] = result
+        job["state"] = "succeeded"
+    except Exception as e:
+        job["state"] = "error"
+        job["error"] = str(e)
+    write_job(job)
+    return jsonify({"job_id": job_id, "state": job["state"]})
+
+@app.route("/api/v1/status/<job_id>", methods=["GET"])
+def status(job_id):
+    j = JOBS.get(job_id)
+    if not j:
+        # try load from disk
+        p = os.path.join(HIL_SERVER_DIR, f"{job_id}.json")
+        if os.path.exists(p):
+            j = json.load(open(p))
+        else:
+            return jsonify({"error":"not found"}), 404
+    return jsonify(j)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "5002")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/sim/carla/scenario_generator.py
+#!/usr/bin/env python3
+"""
+Generate parametric CARLA scenarios to expand coverage.
+ - Produces a family of scenarios varying weather, lighting, seeds, speeds and agent behaviors.
+ - Writes JSON scenario files into ops/sim/carla/scenario_library/generated_*.json
+"""
+import os
+import json
+import itertools
+from datetime import datetime
+
+OUT_DIR = os.environ.get("CARLA_SCENARIO_OUT", "ops/sim/carla/scenario_library")
+os.makedirs(OUT_DIR, exist_ok=True)
+
+WEATHERS = ["clear_day","rain","fog","night"]
+BEHAVIORS = ["cross","walk","run","hesitate"]
+SPEEDS = [1.0, 1.5, 2.0]
+SEEDS = [42, 1337, 2026]
+
+def generate():
+    idx = 0
+    for w,b,sd,sp in itertools.product(WEATHERS, BEHAVIORS, SEEDS, SPEEDS):
+        obj = {
+            "id": f"gen_{idx}",
+            "description": f"{w} with pedestrian {b} speed {sp} seed {sd}",
+            "seed": sd,
+            "ego_start": {"x":0,"y":0,"yaw":0},
+            "pedestrian": {"spawn_x": 20, "spawn_y": 1, "speed": sp, "behavior": b},
+            "weather": w,
+            "expected": {"min_clearance_m": 2}
+        }
+        fn = os.path.join(OUT_DIR, f"generated_{idx}.json")
+        with open(fn, "w") as fh:
+            json.dump(obj, fh, indent=2)
+        idx += 1
+    return idx
+
+if __name__ == "__main__":
+    n = generate()
+    print("Generated", n, "scenarios in", OUT_DIR)
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariant_enforcer_ci.py
+#!/usr/bin/env python3
+"""
+CI check: ensure prioritized invariants have either proof artifacts or documented runtime monitors.
+ - Reads ops/formal/invariant_registry.json and VERIFIER_LOG_DIR for *.proof.json
+ - For each invariant marked priority: high, require either proof artifact present or registry entry with "monitored": true
+ - Exit non-zero if any high-priority invariant lacks coverage
+"""
+import os
+import json
+from glob import glob
+
+REG = os.environ.get("INVARIANT_REGISTRY_PATH", "ops/formal/invariant_registry.json")
+VERIF_LOG_DIR = os.environ.get("VERIFIER_LOG_DIR", "/tmp/verifier_logs")
+
+def load_registry():
+    if not os.path.exists(REG):
+        return {}
+    return json.load(open(REG))
+
+def find_proof_ids():
+    ids = set()
+    for fn in glob(os.path.join(VERIF_LOG_DIR, "*.proof.json")):
+        try:
+            j = json.load(open(fn))
+            ids.add(j.get("id"))
+        except Exception:
+            continue
+    return ids
+
+def main():
+    reg = load_registry()
+    proofs = find_proof_ids()
+    missing = []
+    for k,v in reg.items():
+        if v.get("priority","low") == "high":
+            inv_id = v.get("id", k)
+            if inv_id in proofs:
+                continue
+            if v.get("monitored", False):
+                continue
+            missing.append(inv_id)
+    if missing:
+        print("High-priority invariants missing proof or monitor coverage:", missing)
+        raise SystemExit(2)
+    print("All high-priority invariants have proof or monitored coverage.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/safety/failover_hil_test.py
+#!/usr/bin/env python3
+"""
+Test failover behavior in HIL:
+ - Trigger a simulated controller failure
+ - Promote backup using failover_controller, run HIL scenario and verify continuity
+ - Record evidence in /tmp/failover_evidence
+"""
+import os
+import json
+import time
+from ops.safety.failover_controller import promote_backup, demote_failed
+from ops.hil.run_hil import run
+from ops.hil.replay_verifier import verify
+
+EVIDENCE_DIR = os.environ.get("FAILOVER_EVIDENCE_DIR", "/tmp/failover_evidence")
+os.makedirs(EVIDENCE_DIR, exist_ok=True)
+
+def simulate_failover(namespace, failed_deployment, hil_artifact, seed):
+    # demote failed component
+    demote_failed(namespace, failed_deployment)
+    # promote backup
+    ev = promote_backup(namespace, failed_deployment)
+    # run HIL with the promoted backup in place
+    res = run(hil_artifact, seed=seed)
+    # verify replay deterministic
+    # assume replay metadata exists at /tmp/hil_replay/ros_replay_metadata.json
+    replay_meta = "/tmp/hil_replay/ros_replay_metadata.json"
+    rpt = verify(replay_meta, res.get("hil_log"))
+    out = {"failover_event": ev, "hil_result": res, "replay_verification": rpt}
+    outp = os.path.join(EVIDENCE_DIR, f"failover_{int(time.time())}.json")
+    with open(outp, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote failover evidence to", outp)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--namespace", default="safety-vehicle")
+    p.add_argument("--failed-deployment", required=True)
+    p.add_argument("--hil-artifact", required=True)
+    p.add_argument("--seed", default="42")
+    args = p.parse_args()
+    simulate_failover(args.namespace, args.failed_deployment, args.hil_artifact, args.seed)
+
*** End Patch
*** Begin Patch
*** Add File:ops/ml/retrain_evidence_hook.py
+#!/usr/bin/env python3
+"""
+Hook to create an evidence bundle for a retrained model and optionally push to safety registry.
+ - Reads retrain metadata (ops/ml/retrain_pipeline produced) and packages evidence with evidence_packager
+ - Signs evidence if EVIDENCE_SIGN_BACKEND configured
+"""
+import os
+import json
+from ops.evidence.evidence_packager import package
+
+def make_retrain_evidence(meta_path):
+    meta = json.load(open(meta_path))
+    model_path = meta.get("model_path")
+    model_name = os.path.basename(model_path).split(".")[0]
+    provenance = {"trained_from": meta.get("training_data"), "curation": meta.get("curation_report")}
+    metadata = {"owner": os.environ.get("USER","retrain"), "approver": "", "notes": "autogenerated retrain evidence"}
+    evidence, sig = package(model_name, model_path, meta.get("curation_report"), provenance, metadata)
+    print("Produced retrain evidence:", evidence, "sig:", sig)
+    return evidence, sig
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--meta", required=True)
+    args = p.parse_args()
+    make_retrain_evidence(args.meta)
+
*** End Patch
*** Begin Patch
*** Add File:docs/ops/ops_certification_quickstart.md
+Ops Quickstart: Certification & governance checklist (practical)
+----------------------------------------------------------------
+Follow these concrete steps to move a candidate model through evidence & certification readiness.
+
+1) Assign roles
+ - Edit docs/safety_governance.md and record Safety Owner, Verification Engineer, Release Approver.
+
+2) Run reproducibility & timing checks
+ - Ensure images pinned and seeds used.
+ - Run WCET: python ops/rt/wcet_analyzer.py --cmd "./control_loop --once" --samples 200 --rt
+ - Run rt wrapper to collect latency: /usr/local/bin/rt_scheduler_wrapper.py ... (systemd unit provided)
+
+3) HIL integration
+ - Start example HIL runner server (for vendor integration): python ops/hil/hil_runner_server.py
+ - Notify HIL runner via ros_adapter replay_ready or call run endpoint:
+    curl -X POST http://hil-runner/api/v1/run -d '{"artifact":"s3://...", "seed":42}' -H "Content-Type: application/json"
+ - Use firmware_manager.snapshot_firmware(...) to capture firmware before running.
+
+4) Perception & scenario coverage
+ - Generate scenarios: python ops/sim/carla/scenario_generator.py
+ - Run scenario sweeps: argo workflow k8s/argo/workflows/carla_scenario_sweep.yaml or python ops/sim/carla/run_scenario.py ...
+ - Augment and validate perception: python ops/perception/scenario_augmenter.py ... && python ops/perception/validation_harness.py ...
+
+5) Formal invariants & proof obligations
+ - Register invariants and mark priorities in ops/formal/invariant_registry.json
+ - Run prover orchestrator: python ops/formal/prover_orchestrator.py
+ - Enforce CI check: ops/formal/invariant_enforcer_ci.py (fail PRs if high-priority invariants lack proofs/monitors)
+
+6) Retrain on drift
+ - Configure baseline and current annotations and run retrain workflow (retrain_on_drift.yml)
+ - Package retrain evidence: python ops/ml/retrain_evidence_hook.py --meta /tmp/retrain_evidence/training_metadata.json
+
+7) Release board signoff
+ - Fill release_board_template.md and collect signatures/approvals
+ - Ensure evidence bundle signed via KMS/Vault (use sign_with_kms_or_vault.py)
+
+8) Audit & retention
+ - Push evidence and proofs to immutable storage (S3 with object lock or equivalent)
+ - Keep a copy of public keys and provenance for auditors
+
*** End Patch
*** End Patch
