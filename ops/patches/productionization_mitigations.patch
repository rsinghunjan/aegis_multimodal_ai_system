*** Begin Patch
*** Add File:ops/hil/vendor_onboarding.md
+HIL Vendor Onboarding Checklist
+===============================
+
+Purpose
+-------
+Step-by-step checklist and minimal acceptance criteria to onboard a HIL rack vendor and reach repeatable HIL verification.
+
+Steps
+-----
+1. Procurement & access
+   - Obtain a HIL rack with documented API (SSH/REST/SDK) and account/keys.
+   - Provide network access and secure credentials (vault/KMS).
+
+2. Adapter mapping
+   - Implement ops/hil/vendor_adapter_template.py for the vendor (see template).
+   - Map vendor manifest fields (bag location, checksums, seed, expected env) to canonical fields.
+
+3. Health & smoke tests
+   - Run ops/hil/hardware_onboard.py --adapter <adapter> to perform health checks, list resources and a short run.
+   - Expected result: list of available slots/backends and successful health ping.
+
+4. Determinism baseline
+   - Run ops/hil/replay_verification_loop.py with repeats ≥ 10 on representative manifests.
+   - Acceptance: mean determinism score ≥ 80 and failure rate < 5% (tune for your domain).
+
+5. Firmware snapshot workflow
+   - Establish firmware_snapshot command and store snapshots in signed artifact store (see firmware_manager).
+   - Ensure snapshots before/after runs and compare hashes as part of deterministic checks.
+
+6. Evidence & signing
+   - Configure EVIDENCE_SIGN_BACKEND (KMS/Vault) for CI.
+   - Ensure HIL provenance and determinism reports are signed and uploaded to evidence store.
+
+7. Runbook & run cadence
+   - Document who can schedule runs, how to quarantine faulty racks, and retention of evidence.
+
+Acceptance criteria
+-------------------
+- Health check passes and vendor API calls succeed.
+- Determinism summary signed and mean score ≥ threshold for representative scenarios.
+- Firmware snapshots stored and accessible in evidence store.
+
+Notes
+-----
+- Use ops/hil/vendor_mapping.py to normalize vendor responses.
+- Keep secrets in Vault/KMS and use OIDC for CI short‑lived credentials.
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/vendor_adapter_template.py
+#!/usr/bin/env python3
+"""
+Vendor adapter template for HIL racks.
+ - Subclass VendorAdapter and implement prepare_replay, start_run, poll_status, fetch_artifacts, list_backends
+ - The adapter should normalize responses into the canonical shape used by run_hil_integrated.py
+"""
+from typing import Dict, Any, List
+import time
+
+class VendorAdapter:
+    def __init__(self, url: str = None, token: str = None):
+        self.url = url
+        self.token = token
+
+    def list_backends(self) -> List[Dict[str, Any]]:
+        """
+        Return list of backend slots with metadata: {"rack_id": "...", "slot": 1, "status": "idle"}
+        """
+        raise NotImplementedError
+
+    def prepare_replay(self, payload: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Optional: prepare the rack (mount bag, configure env). Return prep metadata.
+        """
+        return {}
+
+    def start_run(self, payload: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Start the replay. Return {"job_id": "..."} or vendor-specific object.
+        """
+        raise NotImplementedError
+
+    def poll_status(self, job_id: str) -> Dict[str, Any]:
+        """
+        Poll status for job_id. Return {"state": "running|succeeded|failed", ...}
+        """
+        raise NotImplementedError
+
+    def fetch_artifacts(self, job_id: str, out_dir: str) -> List[str]:
+        """
+        Fetch artifacts to out_dir and return list of file paths.
+        """
+        raise NotImplementedError
+
+    # Convenience utility for simple blocking runs
+    def run_blocking(self, payload: Dict[str, Any], poll_interval=2):
+        job = self.start_run(payload)
+        job_id = job.get("job_id") or job.get("id")
+        while True:
+            s = self.poll_status(job_id)
+            state = s.get("state") or s.get("status")
+            if state in ("succeeded","done","completed"):
+                return self.fetch_artifacts(job_id, "/tmp")
+            if state in ("failed","error"):
+                raise RuntimeError(f"Job {job_id} failed: {s}")
+            time.sleep(poll_interval)
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/hardware_onboard.py
+#!/usr/bin/env python3
+"""
+Minimal hardware onboarding CLI.
+ - Validates connectivity to vendor adapter
+ - Lists backends/slots and runs a short smoke replay if given a manifest
+ - Produces an onboarding report and optional signed evidence
+"""
+import os
+import json
+import importlib
+import subprocess
+from datetime import datetime
+
+def load_adapter(adapter_path, url=None, token=None):
+    modname, clsname = adapter_path.rsplit(".",1)
+    mod = importlib.import_module(modname)
+    cls = getattr(mod, clsname)
+    return cls(url, token)
+
+def sign_if_available(path):
+    if os.environ.get("EVIDENCE_SIGN_BACKEND"):
+        try:
+            subprocess.check_call(["python","ops/evidence/sign_with_kms_or_vault.py", path])
+            return path + ".sig"
+        except Exception:
+            return None
+    return None
+
+def onboard(adapter_path, manifest=None, firmware_cmd=None):
+    adapter = load_adapter(adapter_path, os.environ.get("HIL_VENDOR_URL"), os.environ.get("HIL_VENDOR_TOKEN"))
+    report = {"ts": datetime.utcnow().isoformat()+"Z", "adapter": adapter_path}
+    try:
+        backends = adapter.list_backends()
+        report["backends"] = backends
+    except Exception as e:
+        report["error"] = str(e)
+        with open("/tmp/hil_onboard_report.json","w") as fh:
+            json.dump(report, fh, indent=2)
+        print("Onboard failed:", e)
+        return
+    if manifest:
+        # run a short smoke run
+        try:
+            artifacts = adapter.run_blocking({"manifest": json.load(open(manifest)), "firmware_dump_cmd": firmware_cmd})
+            report["artifacts"] = artifacts
+        except Exception as e:
+            report["run_error"] = str(e)
+    path = "/tmp/hil_onboard_report.json"
+    with open(path,"w") as fh:
+        json.dump(report, fh, indent=2)
+    sig = sign_if_available(path)
+    print("Onboard report written:", path, "sig:", sig)
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--adapter", required=True)
+    p.add_argument("--manifest", default=None)
+    p.add_argument("--firmware-cmd", default=None)
+    args = p.parse_args()
+    onboard(args.adapter, manifest=args.manifest, firmware_cmd=args.firmware_cmd)
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/build_rt_image.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Build an RT-capable node image (demo wrapper).
+#
+IMAGE=${1:-aegis/rt-node:latest}
+DOCKERFILE=${2:-ops/rt/rt_node_image/Dockerfile}
+
+echo "Building RT node image: $IMAGE"
+docker build -t "$IMAGE" -f "$DOCKERFILE" .
+echo "Image built. Push to registry and use rollout script to label nodes and deploy."
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/wcet_orchestrator.py
+#!/usr/bin/env python3
+"""
+Orchestrate WCET analysis across many binaries and aggregate results.
+ - Accepts a JSON list of binaries and nodes
+ - Runs wcet_analyzer.py for each binary (optionally remote via SSH) and collects p99/p95
+ - Emits a signed aggregate report and exposes it for Prometheus reporter to push
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+def run_local(entry, samples=200):
+    cmd = ["python","ops/rt/wcet_analyzer.py","--cmd", entry["cmd"], "--samples", str(samples)]
+    subprocess.check_call(cmd)
+    if os.path.exists("/tmp/wcet_report.json"):
+        # move to unique name
+        out = f"/tmp/wcet_{entry['name']}.json"
+        os.rename("/tmp/wcet_report.json", out)
+        return out
+    return None
+
+def aggregate(reports, out="/tmp/wcet_aggregate.json"):
+    agg = {"ts": datetime.utcnow().isoformat()+"Z", "reports": []}
+    for r in reports:
+        j = json.load(open(r))
+        agg["reports"].append(j)
+    with open(out,"w") as fh:
+        json.dump(agg, fh, indent=2)
+    # sign if available
+    if os.environ.get("EVIDENCE_SIGN_BACKEND"):
+        try:
+            subprocess.check_call(["python","ops/evidence/sign_with_kms_or_vault.py", out])
+        except Exception:
+            pass
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--config", required=True, help="JSON config with entries: [{'name':..,'cmd':..}]")
+    args = p.parse_args()
+    cfg = json.load(open(args.config))
+    rep_files = []
+    for e in cfg.get("entries", []):
+        rep = run_local(e)
+        if rep:
+            rep_files.append(rep)
+    print(aggregate(rep_files))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/scenario_indexer.py
+#!/usr/bin/env python3
+"""
+Index scenario library into a single JSON index and shard scenarios for distributed runs.
+ - Scans ops/sim/carla/scenario_library for scenario_*.json and generated_exp_*.json
+ - Produces scenario_bank_index.json with metadata (id, file, weather/time tags)
+ - Optionally shards index into N files for distributed runners
+"""
+import os
+import json
+from glob import glob
+
+LIB_DIR = os.environ.get("SCENARIO_LIB", "ops/sim/carla/scenario_library")
+
+def index(out="/tmp/scenario_bank_index.json", shards=1):
+    files = sorted(glob(os.path.join(LIB_DIR, "*.json")))
+    scenarios = []
+    for f in files:
+        try:
+            j = json.load(open(f))
+        except Exception:
+            continue
+        scenarios.append({"id": j.get("id", os.path.basename(f)), "file": f, "meta": {"weather": j.get("weather"), "time_of_day": j.get("time_of_day"), "seed": j.get("seed")}})
+    idx = {"ts": __import__("datetime").datetime.utcnow().isoformat()+"Z", "count": len(scenarios), "scenarios": scenarios}
+    with open(out, "w") as fh:
+        json.dump(idx, fh, indent=2)
+    if shards and shards > 1:
+        per = max(1, len(scenarios)//shards)
+        for i in range(shards):
+            shard = scenarios[i*per:(i+1)*per]
+            with open(f"{out}.shard{i}.json","w") as fh:
+                json.dump({"shard": i, "scenarios": shard}, fh, indent=2)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--out", default="/tmp/scenario_bank_index.json")
+    p.add_argument("--shards", type=int, default=1)
+    args = p.parse_args()
+    print(index(out=args.out, shards=args.shards))
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/train_and_serve_pipeline.py
+#!/usr/bin/env python3
+"""
+End-to-end pipeline scaffold:
+ - Accept a curated dataset manifest, train detector, evaluate across scenario bank,
+   produce signed evaluation summary, and create a Kubernetes manifest for serving model.
+ - Training and serving steps are placeholders and should be replaced with your infra.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+def train(dataset_manifest, out_model_path):
+    # placeholder training step
+    subprocess.check_call(["python","ops/perception/train_detector_large.py","--dataset", dataset_manifest, "--scenario-index", "/tmp/scenario_bank_index.json"])
+    # expect train_detector_large to write model artifact in DETECTOR_OUT; copy to out_model_path
+    print("train placeholder completed")
+    return out_model_path
+
+def evaluate(model_path, scenario_index):
+    # placeholder: call run_sweep_and_aggregate which produces signed summary if configured
+    subprocess.check_call(["python","ops/perception/run_sweep_and_aggregate.py","--index", scenario_index])
+    return "/tmp/perception_sweep/sweep_summary.json" if os.path.exists("/tmp/perception_sweep/sweep_summary.json") else "/tmp/perception_sweep/sweep_summary.json"
+
+def generate_serving_manifest(model_path, name="detector-service"):
+    svc = {
+        "apiVersion": "apps/v1",
+        "kind": "Deployment",
+        "metadata": {"name": name},
+        "spec": {
+            "replicas": 1,
+            "template": {
+                "metadata": {"labels": {"app": name}},
+                "spec": {"containers": [{"name":"detector","image": model_path, "resources": {"limits":{"cpu":"2","memory":"4Gi"}}}]}
+            }
+        }
+    }
+    out = f"/tmp/{name}_deployment.json"
+    with open(out,"w") as fh:
+        json.dump(svc, fh, indent=2)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--dataset", required=True)
+    p.add_argument("--scenario-index", required=True)
+    p.add_argument("--out-model", default="registry/detector:latest")
+    args = p.parse_args()
+    model = train(args.dataset, args.out_model)
+    eval_summary = evaluate(model, args.scenario_index)
+    print("Eval summary:", eval_summary)
+    manifest = generate_serving_manifest(model)
+    print("Serving manifest:", manifest)
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/onboard_invariant_api.py
+#!/usr/bin/env python3
+"""
+API to onboard a new invariant into the registry.
+ - Writes entry into ops/formal/invariant_registry_high_priority.json (or a separate registry)
+ - Optionally create a PR template fragment for domain SME to fill SMT encoding
+"""
+import os
+import json
+from datetime import datetime
+
+REG = os.environ.get("INVARIANT_REG", "ops/formal/invariant_registry_high_priority.json")
+
+def load_reg():
+    if os.path.exists(REG):
+        return json.load(open(REG))
+    return {}
+
+def save_reg(r):
+    with open(REG,"w") as fh:
+        json.dump(r, fh, indent=2)
+
+def onboard(id, description, smt_file, priority="high", monitored=False, responsible="unassigned"):
+    reg = load_reg()
+    reg[id] = {"id": id, "description": description, "file": smt_file, "priority": priority, "monitored": monitored, "responsible": responsible, "onboarded_at": datetime.utcnow().isoformat()+"Z"}
+    save_reg(reg)
+    return reg[id]
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--id", required=True)
+    p.add_argument("--desc", required=True)
+    p.add_argument("--smt", required=True)
+    p.add_argument("--priority", default="high")
+    args = p.parse_args()
+    print(onboard(args.id, args.desc, args.smt, priority=args.priority))
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/kms_vault_check.py
+#!/usr/bin/env python3
+"""
+Check that KMS/Vault signing backend is reachable and that signing key exists.
+ - For AWS KMS: checks key id in AWS_KMS_KEY_ID
+ - For Vault: checks transit key in VAULT_TRANSIT_KEY
+"""
+import os
+import sys
+
+def check_kms():
+    if os.environ.get("EVIDENCE_SIGN_BACKEND") == "aws":
+        try:
+            import boto3
+            kms = boto3.client("kms", region_name=os.environ.get("AWS_REGION"))
+            kid = os.environ.get("AWS_KMS_KEY_ID")
+            if not kid:
+                print("AWS_KMS_KEY_ID not set"); return False
+            kms.describe_key(KeyId=kid)
+            return True
+        except Exception as e:
+            print("KMS check failed:", e); return False
+    if os.environ.get("EVIDENCE_SIGN_BACKEND") == "vault":
+        try:
+            import hvac
+            client = hvac.Client(url=os.environ.get("VAULT_ADDR"), token=os.environ.get("VAULT_TOKEN"))
+            key = os.environ.get("VAULT_TRANSIT_KEY")
+            if not client.is_authenticated():
+                print("Vault auth failed"); return False
+            r = client.secrets.transit.read_key(name=key)
+            return True
+        except Exception as e:
+            print("Vault check failed:", e); return False
+    print("No known EVIDENCE_SIGN_BACKEND configured; OK for demo")
+    return True
+
+if __name__ == "__main__":
+    ok = check_kms()
+    if not ok:
+        sys.exit(2)
+    print("KMS/Vault check OK")
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/cd_helm_pipeline.yml
+name: CI/CD - Build images & Publish Helm charts
+on:
+  workflow_dispatch:
+
+jobs:
+  build:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build model-server image
+        run: |
+          docker build -t ${{ secrets.REGISTRY }}/aegis/model-server:${{ github.sha }} -f ops/models/serve/Dockerfile .
+          echo "MODEL_SERVER_IMAGE=${{ secrets.REGISTRY }}/aegis/model-server:${{ github.sha }}" >> $GITHUB_ENV
+      - name: Push images
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.REGISTRY }}
+          username: ${{ secrets.DOCKER_USERNAME }}
+          password: ${{ secrets.DOCKER_PASSWORD }}
+      - name: Build & push
+        uses: docker/build-push-action@v4
+        with:
+          push: true
+          tags: ${{ env.MODEL_SERVER_IMAGE }}
+      - name: Package Helm charts
+        run: |
+          helm package charts/model-server
+          helm package charts/hil-runner
+      - name: Publish Helm artifacts
+        run: |
+          mkdir -p .artifacts
+          mv *.tgz .artifacts/ || true
+          ls -la .artifacts
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: helm-artifacts
+          path: .artifacts
+
*** End Patch
*** Begin Patch
*** Add File:ops/evidence/s3_lifecycle_config.json
+{
+  "Rules": [
+    {
+      "ID": "TransitionToIA",
+      "Prefix": "evidence/",
+      "Status": "Enabled",
+      "Transitions": [
+        {
+          "Days": 30,
+          "StorageClass": "STANDARD_IA"
+        },
+        {
+          "Days": 365,
+          "StorageClass": "GLACIER"
+        }
+      ],
+      "Expiration": {
+        "Days": 3650
+      },
+      "NoncurrentVersionExpiration": {
+        "NoncurrentDays": 3650
+      }
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/integration/require_no_stubs.py
+#!/usr/bin/env python3
+"""
+CI helper: fail if critical manifests or code reference 'stub' adapters or fallback images.
+ - Checks for common patterns that indicate demo stubs are being used in production manifests.
+ - Enforce in fast lane to prevent false green CI.
+"""
+import sys
+from glob import glob
+
+FILES = glob("k8s/**/*.yaml", recursive=True) + glob("ops/**/*.py", recursive=True)
+
+BAD_PATTERNS = ["adapter.*stub", "image:.*:latest", "PENNYLANE_AVAILABLE = False", "QISKIT_AVAILABLE = False"]
+
+issues = []
+for f in FILES:
+    try:
+        txt = open(f).read()
+    except Exception:
+        continue
+    for pat in BAD_PATTERNS:
+        if pat in txt:
+            issues.append(f"{f}: contains demo pattern {pat}")
+
+if issues:
+    print("Found stub/demo patterns that may mask production issues:")
+    for i in issues:
+        print(i)
+    sys.exit(2)
+print("No demo stub patterns detected.")
+
*** End Patch
*** End Patch
