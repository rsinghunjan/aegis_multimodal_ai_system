*** Begin Patch
*** Update File:ops/automation/verification_gate.py
@@
 import os
 import json
 from datetime import datetime
 from typing import Dict
 
 from ops.verifier.verifier_with_cas import verify_claim
 
 AUDIT_LOG = os.environ.get("VERIFICATION_AUDIT_LOG", "/tmp/verification_audit.log")
 
 def _record(a: Dict):
     a["ts"] = datetime.utcnow().isoformat() + "Z"
     with open(AUDIT_LOG, "a") as fh:
         fh.write(json.dumps(a) + "\n")
 
 class VerificationError(RuntimeError):
     pass
@@
 def require_verification_and_execute(action_name: str, claim: str, snippets: list, execute_fn, execute_args=None, execute_kwargs=None, threshold=0.7):
@@
     record = {"action": action_name, "claim": claim, "phase": "verify_start"}
     _record(record)
 
     vres = verify_claim(claim, snippets, threshold=threshold)
+    # instrumentation: record verification metrics (best-effort)
+    try:
+        from prometheus_client import Counter
+        VERIF_REQ = Counter("aegis_verification_requests_total", "Total verification requests")
+        VERIF_SUCCESS = Counter("aegis_verification_success_total", "Verification success count")
+        VERIF_FAILURE = Counter("aegis_verification_failure_total", "Verification failure count")
+        VERIF_BLOCKED = Counter("aegis_verification_blocked_total", "Verification blocked count")
+        VERIF_REQ.inc()
+        if vres.get("ok"):
+            VERIF_SUCCESS.inc()
+        else:
+            VERIF_FAILURE.inc()
+    except Exception:
+        # metrics are best-effort in case prometheus_client isn't available
+        pass
     _record({"action": action_name, "phase": "verify_result", "verification": vres})
     if not vres.get("ok"):
-        _record({"action": action_name, "phase": "blocked", "reason": "verification_failed", "verification": vres})
+        _record({"action": action_name, "phase": "blocked", "reason": "verification_failed", "verification": vres})
+        try:
+            from prometheus_client import Counter
+            VERIF_BLOCKED = Counter("aegis_verification_blocked_total", "Verification blocked count")
+            VERIF_BLOCKED.inc()
+        except Exception:
+            pass
         raise VerificationError(f"Verification failed for action {action_name}: {vres}")
 
     # permitted â€” execute action and record result
     _record({"action": action_name, "phase": "execute_start"})
*** End Patch
*** Begin Patch
*** Add File:ops/verifier/fail_closed_policy.py
+#!/usr/bin/env python3
+"""
+Fail-closed / health policy helpers for CAS and NLI services.
+ - Check health endpoints for nli_service and calc_pool/calc_service
+ - Provide helper `require_services_available_or_raise` to be used by high-risk callers
+ - If FAIL_CLOSED env var is set (default true), the helper will raise if services are unhealthy.
+"""
+import os
+import requests
+
+NLI_HEALTH = os.environ.get("NLI_HEALTH_URL", "http://nli-service.aegis-retriever.svc.cluster.local:8200/health")
+CALC_POOL_HEALTH = os.environ.get("CALC_POOL_HEALTH", "http://calc-pool.aegis-retriever.svc.cluster.local:8110/health")
+FAIL_CLOSED = os.environ.get("FAIL_CLOSED", "true").lower() in ("1","true","yes")
+
+def _check(url, timeout=3):
+    try:
+        r = requests.get(url, timeout=timeout)
+        return r.status_code == 200
+    except Exception:
+        return False
+
+def require_services_available_or_raise():
+    nli_ok = _check(NLI_HEALTH)
+    calc_ok = _check(CALC_POOL_HEALTH)
+    if nli_ok and calc_ok:
+        return True
+    if FAIL_CLOSED:
+        missing = []
+        if not nli_ok:
+            missing.append("nli_service")
+        if not calc_ok:
+            missing.append("calc_pool")
+        raise RuntimeError(f"Required services unavailable: {missing}")
+    # fail-open allowed: return False to allow higher-level logic to decide
+    return False
+
+if __name__=="__main__":
+    try:
+        ok = require_services_available_or_raise()
+        print("services_ok:", ok)
+    except Exception as e:
+        print("unavailable:", e)
+
*** End Patch
*** Begin Patch
*** Add File:ops/scaling/load_test.py
+#!/usr/bin/env python3
+"""
+Simple load test harness for calc_pool and nli_service.
+ - Sends concurrent requests and computes p50/p95 latencies and error rates.
+ - Produces /tmp/load_test_report.json
+"""
+import asyncio
+import aiohttp
+import time
+import json
+from statistics import median
+from datetime import datetime
+
+CALC_URL = "http://localhost:8110/v1/compute_sync"
+NLI_URL = "http://localhost:8200/v1/entail"
+OUT = "/tmp/load_test_report.json"
+
+async def call_calc(session, payload):
+    start = time.time()
+    try:
+        async with session.post(CALC_URL, json=payload, timeout=10) as r:
+            text = await r.text()
+            elapsed = time.time()-start
+            return {"ok": r.status==200, "status": r.status, "time": elapsed, "resp": text[:200]}
+    except Exception as e:
+        return {"ok": False, "error": str(e), "time": time.time()-start}
+
+async def call_nli(session, premise, hypothesis):
+    start = time.time()
+    try:
+        async with session.post(NLI_URL, json={"premise": premise, "hypothesis": hypothesis}, timeout=10) as r:
+            text = await r.text()
+            elapsed = time.time()-start
+            return {"ok": r.status==200, "status": r.status, "time": elapsed, "resp": text[:200]}
+    except Exception as e:
+        return {"ok": False, "error": str(e), "time": time.time()-start}
+
+async def run_load(calc_q=50, nli_q=20, concurrency=10):
+    calc_payload = {"mode":"eval", "expr":"2+2", "precision":50}
+    nli_prem = "Premise: 2+2 equals 4"
+    nli_hyp = "Hypothesis: 2+2=4"
+    calc_results = []
+    nli_results = []
+    sem = asyncio.Semaphore(concurrency)
+    async with aiohttp.ClientSession() as session:
+        async def do_calc(i):
+            async with sem:
+                r = await call_calc(session, calc_payload)
+                calc_results.append(r)
+        async def do_nli(i):
+            async with sem:
+                r = await call_nli(session, nli_prem, nli_hyp)
+                nli_results.append(r)
+        tasks = [do_calc(i) for i in range(calc_q)] + [do_nli(i) for i in range(nli_q)]
+        await asyncio.gather(*tasks)
+    def summarize(rs):
+        times = [r["time"] for r in rs if "time" in r]
+        oks = sum(1 for r in rs if r.get("ok"))
+        total = len(rs)
+        p50 = median(times) if times else None
+        p95 = sorted(times)[int(len(times)*0.95)-1] if times and len(times)>=1 else None
+        return {"count": total, "ok": oks, "p50": p50, "p95": p95}
+    out = {"ts": datetime.utcnow().isoformat()+"Z", "calc": summarize(calc_results), "nli": summarize(nli_results)}
+    with open(OUT, "w") as fh:
+        json.dump(out, fh, indent=2)
+    return out
+
+if __name__=="__main__":
+    r = asyncio.run(run_load())
+    print("load test result:", r)
+
*** End Patch
*** Begin Patch
*** Add File:ops/math/worker_fuzzer.py
+#!/usr/bin/env python3
+"""
+Runtime fuzzer for calc_worker_sandbox inputs.
+ - Generates mutated math-like strings and posts to /v1/compute_sync to check for crashes or anomalous outputs.
+ - Writes findings to /tmp/worker_fuzzer_findings.jsonl
+"""
+import random
+import requests
+import json
+from datetime import datetime
+
+CALC_SYNC = "http://localhost:8110/v1/compute_sync"
+OUT = "/tmp/worker_fuzzer_findings.jsonl"
+
+TOKENS = ["+", "-", "*", "/", "^", "(", ")", "sqrt", "sin", "cos", "log", "exp", "pi", "E", "x", "y", "2", "3", "10"]
+
+def mutate_expr(length=8):
+    return " ".join(random.choice(TOKENS) for _ in range(length))
+
+def run(iterations=200):
+    with open(OUT, "a") as fh:
+        for i in range(iterations):
+            expr = mutate_expr(random.randint(3,12))
+            payload = {"mode":"eval", "expr": expr, "precision": 20}
+            try:
+                r = requests.post(CALC_SYNC, json=payload, timeout=5)
+                try:
+                    j = r.json()
+                except Exception:
+                    j = {"text": r.text[:500]}
+                record = {"ts": datetime.utcnow().isoformat()+"Z", "expr": expr, "status": r.status_code, "resp": j}
+                if r.status_code != 200 or not j.get("ok"):
+                    fh.write(json.dumps(record) + "\n")
+            except Exception as e:
+                record = {"ts": datetime.utcnow().isoformat()+"Z", "expr": expr, "error": str(e)}
+                fh.write(json.dumps(record) + "\n")
+
+if __name__=="__main__":
+    run(500)
+
*** End Patch
*** Begin Patch
*** Add File:k8s/prometheus_rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-slo-rules
+  namespace: aegis-retriever
+spec:
+  groups:
+    - name: aegis-slo-alerts
+      rules:
+        - alert: CalcPoolHighPending
+          expr: aegis_calc_jobs_pending > 10
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Calc pool queue length high"
+            description: "Calc pool pending jobs > 10 for 5m"
+
+        - alert: CalcLatencyHigh
+          expr: histogram_quantile(0.95, sum(rate(aegis_calc_job_duration_seconds_bucket[5m])) by (le)) > 1.0
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "Calc service high p95 latency"
+            description: "Calc service p95 > 1.0s"
+
+        - alert: NLIHighLatency
+          expr: increase(aegis_nli_requests_total[5m]) > 0 and histogram_quantile(0.95, sum(rate(aegis_nli_request_duration_seconds_bucket[5m])) by (le)) > 0.5
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "NLI p95 latency high"
+            description: "NLI service p95 > 0.5s"
+
+        - alert: VerificationBlockedSpike
+          expr: increase(aegis_verification_blocked_total[5m]) > 5
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Verification gate blocking spike"
+            description: "Many verification blocks in short time - may indicate upstream issues"
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/invariant_task_poster.py
+#!/usr/bin/env python3
+"""
+Collect triage tasks and prepare a simple tasks file for domain experts.
+ - Reads /tmp/fuzz_triage_enhanced.json or /tmp/fuzz_triage_tasks.json and writes /tmp/invariant_tasks_to_review.json
+ - Adds reproducibility notes and suggested prover strategies
+"""
+import os
+import json
+from datetime import datetime
+
+IN_CANDIDATES = ["/tmp/fuzz_triage_enhanced.json", "/tmp/fuzz_triage_tasks.json"]
+OUT = "/tmp/invariant_tasks_to_review.json"
+
+def run():
+    tasks = []
+    for p in IN_CANDIDATES:
+        if os.path.exists(p):
+            try:
+                tasks.extend(json.load(open(p)))
+            except Exception:
+                pass
+    # normalize tasks
+    out = []
+    for t in tasks:
+        rec = {
+            "id": t.get("id") or t.get("summary","task")[:32],
+            "summary": t.get("summary",""),
+            "severity": t.get("severity", 3),
+            "mutant": t.get("mutant", t.get("mutant", {})),
+            "suggested_provers": t.get("suggested_provers", ["Z3","SMT-Numerical"]),
+            "repro": t.get("repro", {}),
+            "created_at": datetime.utcnow().isoformat()+"Z"
+        }
+        out.append(rec)
+    with open(OUT,"w") as fh:
+        json.dump(out, fh, indent=2)
+    print("wrote", OUT)
+    return OUT
+
+if __name__=="__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hardening_full.yml
+name: Hardening, Load & Fuzz CI
+on:
+  pull_request:
+    types: [opened, synchronize]
+  workflow_dispatch:
+
+jobs:
+  seccomp-check:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run seccomp & hardening checks
+        run: |
+          python ops/ci/seccomp_test.py
+
+  ensure-nli-and-warm:
+    runs-on: ubuntu-latest
+    needs: seccomp-check
+    steps:
+      - uses: actions/checkout@v4
+      - name: Ensure NLI model present (fast)
+        run: |
+          python ops/nli/model_manager.py --ensure || true
+      - name: Warm local NLI pool (best-effort)
+        run: |
+          python ops/nli/warm_pool.py || true
+
+  load-test:
+    runs-on: ubuntu-latest
+    needs: ensure-nli-and-warm
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          python -m pip install --upgrade pip
+          pip install aiohttp prometheus_client requests
+      - name: Run light load test
+        run: |
+          python ops/scaling/load_test.py
+      - name: Upload load report
+        uses: actions/upload-artifact@v4
+        with:
+          name: load-report
+          path: /tmp/load_test_report.json
+
+  worker-fuzz:
+    runs-on: ubuntu-latest
+    needs: load-test
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install requests
+        run: pip install requests
+      - name: Run worker fuzzer (short)
+        run: |
+          python ops/math/worker_fuzzer.py
+      - name: Upload fuzz findings
+        uses: actions/upload-artifact@v4
+        with:
+          name: worker-fuzz-findings
+          path: /tmp/worker_fuzzer_findings.jsonl
+
+  benchmark-shard:
+    runs-on: ubuntu-latest
+    needs: worker-fuzz
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run representative math shard
+        env:
+          LLM_API: ${{ secrets.LLM_API }}
+        run: |
+          python ops/ci/math_bench_runner.py
+      - name: Upload bench artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: math-bench-pr
+          path: /tmp/math_bench_results.json
+
*** End Patch
*** End Patch
