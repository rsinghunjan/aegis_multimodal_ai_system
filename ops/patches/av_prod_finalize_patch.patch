*** Begin Patch
*** Add File:ops/rt/image_builder.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Build a container image for real-time control nodes.
+# This script produces a docker image with an RT kernel userland helper and a pinned base image.
+#
+# Usage:
+#   ./ops/rt/image_builder.sh --tag myregistry/aegis-rt-node:1.0 --base ubuntu:22.04
+#
+TAG=""
+BASE="ubuntu:22.04"
+
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --tag) TAG="$2"; shift 2;;
+    --base) BASE="$2"; shift 2;;
+    *) echo "Unknown arg $1"; exit 2;;
+  esac
+done
+
+if [[ -z "$TAG" ]]; then
+  echo "Usage: $0 --tag <image:tag> [--base <base image>]"
+  exit 2
+fi
+
+TMPDIR=$(mktemp -d)
+cat > "$TMPDIR/Dockerfile" <<'DOCKER'
+FROM __BASE__
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y --no-install-recommends \\
+    build-essential systemd rsync iproute2 procps \\
+    python3 python3-pip sudo util-linux \\
+    && rm -rf /var/lib/apt/lists/*
+
+# Install chrt wrapper if missing (util-linux provides chrt)
+RUN pip3 install --no-cache-dir psutil
+
+# Copy runtime helpers
+COPY rt_scheduler_wrapper.py /usr/local/bin/rt_scheduler_wrapper.py
+RUN chmod +x /usr/local/bin/rt_scheduler_wrapper.py
+
+# Default entrypoint: a lightweight systemd-less supervisor is expected; CI will set systemd unit in host.
+CMD ["/bin/bash"]
+DOCKER
+
+sed "s|__BASE__|${BASE}|" "$TMPDIR/Dockerfile" > "$TMPDIR/Dockerfile.real"
+cp ops/rt/rt_scheduler_wrapper.py "$TMPDIR/rt_scheduler_wrapper.py"
+docker build -t "$TAG" -f "$TMPDIR/Dockerfile.real" "$TMPDIR"
+echo "Built image $TAG"
+rm -rf "$TMPDIR"
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/wcet_certify.py
+#!/usr/bin/env python3
+"""
+WCET certification helper
+ - Runs wcet_analyzer and produces a signed certificate JSON describing observed WCET statistics and recommended guard margin.
+ - If EVIDENCE_SIGN_BACKEND is configured and sign_with_kms_or_vault.py available, attempts to sign the certificate.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+REPORT_PATH = os.environ.get("WCET_REPORT_PATH", "/tmp/wcet_report.json")
+CERT_OUT = os.environ.get("WCET_CERT_OUT", "/tmp/wcet_certificate.json")
+
+def run_wcet(cmd, samples=200, rt=False):
+    subprocess.check_call(["python", "ops/rt/wcet_analyzer.py", "--cmd", cmd, "--samples", str(samples), "--rt" if rt else ""])
+
+def produce_cert(report_path, out_path):
+    j = json.load(open(report_path))
+    cert = {
+        "ts": datetime.utcnow().isoformat()+"Z",
+        "wcet_report": j,
+        "policy": {"guard_margin_factor": 1.2, "rationale": "20% margin over observed worst-case, subject to review"},
+    }
+    with open(out_path, "w") as fh:
+        json.dump(cert, fh, indent=2)
+    return out_path
+
+def sign_if_available(path):
+    backend = os.environ.get("EVIDENCE_SIGN_BACKEND", "stub")
+    if backend:
+        try:
+            subprocess.check_call(["python", "ops/evidence/sign_with_kms_or_vault.py", path])
+            sig = path + ".sig"
+            return sig
+        except Exception:
+            return None
+    return None
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--cmd", required=True)
+    p.add_argument("--samples", type=int, default=200)
+    p.add_argument("--rt", action="store_true")
+    p.add_argument("--out", default=CERT_OUT)
+    args = p.parse_args()
+    run_wcet(args.cmd, samples=args.samples, rt=args.rt)
+    cert_path = produce_cert(REPORT_PATH, args.out)
+    sig = sign_if_available(cert_path)
+    print("WCET certificate:", cert_path, "sig:", sig)
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/vendor_adapter.py
+#!/usr/bin/env python3
+"""
+HIL vendor adapter interface.
+ - Define a plugin API for vendor-specific HIL orchestration.
+ - Vendors should implement `VendorAdapter` with methods: prepare_replay, start_run, poll_status, fetch_artifacts.
+ - Example adapter is provided in vendor_impl_example.py
+"""
+from abc import ABC, abstractmethod
+
+class VendorAdapter(ABC):
+    @abstractmethod
+    def prepare_replay(self, replay_manifest: dict) -> dict:
+        """
+        Prepare HIL rack for deterministic replay.
+        Return dict with keys needed by run (e.g., replay_id, node_id)
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def start_run(self, run_spec: dict) -> dict:
+        """
+        Start a run on the HIL; return job metadata including job_id.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def poll_status(self, job_id: str) -> dict:
+        """
+        Poll job status and return a dict with state and artifacts when done.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def fetch_artifacts(self, job_id: str, dest_dir: str) -> list:
+        """
+        Fetch run artifacts into dest_dir and return list of files.
+        """
+        raise NotImplementedError
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/vendor_impl_example.py
+#!/usr/bin/env python3
+"""
+Example vendor adapter implementing VendorAdapter using a simple SSH/REST combination.
+This is a reference implementation; adapt for your HIL vendor API.
+"""
+from .vendor_adapter import VendorAdapter
+import requests
+import os
+import subprocess
+import time
+
+class ExampleVendor(VendorAdapter):
+    def __init__(self, base_url: str, api_token: str = None):
+        self.base = base_url.rstrip("/")
+        self.token = api_token
+
+    def _headers(self):
+        h = {"Content-Type": "application/json"}
+        if self.token:
+            h["Authorization"] = f"Bearer {self.token}"
+        return h
+
+    def prepare_replay(self, replay_manifest: dict) -> dict:
+        r = requests.post(self.base + "/api/v1/prepare_replay", json=replay_manifest, headers=self._headers(), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def start_run(self, run_spec: dict) -> dict:
+        r = requests.post(self.base + "/api/v1/run", json=run_spec, headers=self._headers(), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def poll_status(self, job_id: str) -> dict:
+        r = requests.get(self.base + f"/api/v1/status/{job_id}", headers=self._headers(), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def fetch_artifacts(self, job_id: str, dest_dir: str) -> list:
+        os.makedirs(dest_dir, exist_ok=True)
+        r = requests.get(self.base + f"/api/v1/artifacts/{job_id}", headers=self._headers(), timeout=30, stream=True)
+        if r.status_code == 200:
+            out = os.path.join(dest_dir, f"artifacts_{job_id}.tar.gz")
+            with open(out, "wb") as fh:
+                for chunk in r.iter_content(4096):
+                    fh.write(chunk)
+            # extract if tar
+            try:
+                subprocess.check_call(["tar","-xzf",out,"-C",dest_dir])
+            except Exception:
+                pass
+            return [out]
+        else:
+            return []
+
*** End Patch
*** Begin Patch
*** Add File:ops/hil/replay_manifest.py
+#!/usr/bin/env python3
+"""
+Replay manifest schema and validator.
+ - Standardize how deterministic replays are described and validated before HIL runs.
+ - Manifest example:
+   {
+     "bag": "/path/to/rosbag",
+     "seed": 42,
+     "start_time": "2026-01-01T00:00:00Z",
+     "topics": ["/camera/image_raw","/control_cmd"],
+     "checksums": {"bag": "sha256..."}
+   }
+"""
+import json
+import os
+from datetime import datetime
+
+REQUIRED = ["bag","seed","topics"]
+
+def validate(manifest_path):
+    j = json.load(open(manifest_path))
+    for r in REQUIRED:
+        if r not in j:
+            return False, f"missing {r}"
+    # basic timestamp parse
+    if "start_time" in j:
+        try:
+            datetime.fromisoformat(j["start_time"].replace("Z",""))
+        except Exception:
+            return False, "invalid start_time"
+    return True, ""
+
+def minimal_manifest(bag_path, seed=0, topics=None):
+    topics = topics or []
+    return {"bag": os.path.abspath(bag_path), "seed": int(seed), "topics": topics, "ts": datetime.utcnow().isoformat()+"Z"}
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    args = p.parse_args()
+    ok,msg = validate(args.manifest)
+    if not ok:
+        print("Invalid manifest:", msg)
+        raise SystemExit(2)
+    print("Manifest OK")
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/smt_examples/emergency_stop_invariant.smt2
+; Emergency stop invariant: if obstacle within stopping distance, emergency_brake must engage
+; Simple deterministic model: stopping_dist = v * reaction_time + brake_distance
+; Variables: v, reaction_time, brake_distance, obstacle_distance, emergency_brake
+(set-logic QF_NRA)
+(declare-fun v () Real)
+(declare-fun reaction_time () Real)
+(declare-fun brake_distance () Real)
+(declare-fun stopping_dist () Real)
+(declare-fun obstacle_distance () Real)
+(declare-fun emergency_brake () Bool)
+
;+ assumptions
+(assert (<= 0 v))
+(assert (<= v 60)) ; km/h units (toy)
+(assert (<= 0 reaction_time))
+(assert (<= reaction_time 2))
+(assert (<= 0 brake_distance))
+(assert (<= brake_distance 80))
+(assert (= stopping_dist (+ (* v reaction_time) brake_distance)))
+(assert (<= 0 obstacle_distance))
+
+; property: if obstacle_distance <= stopping_dist then emergency_brake must be true
+; negation: obstacle_distance <= stopping_dist AND (not emergency_brake) -> unsat if property holds
+(assert (<= obstacle_distance stopping_dist))
+(assert (not emergency_brake))
+
+(check-sat)
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/smt_examples/speed_limit_respect.smt2
+; Speed limit respect invariant: commanded_speed <= posted_speed_limit + tolerance
+(set-logic QF_NRA)
+(declare-fun commanded_speed () Real)
+(declare-fun posted_speed_limit () Real)
+(declare-fun tolerance () Real)
+
+(assert (<= 0 commanded_speed))
+(assert (<= 0 posted_speed_limit))
+(assert (<= 0 tolerance))
+(assert (<= tolerance 5))
+(assert (<= posted_speed_limit 120))
+
+; property: commanded_speed <= posted_speed_limit + tolerance
+; negation:
+(assert (> commanded_speed (+ posted_speed_limit tolerance)))
+
+(check-sat)
+
*** End Patch
*** Begin Patch
*** Add File:ops/sim/scenario_bank_builder.py
+#!/usr/bin/env python3
+"""
+Scenario bank builder
+ - Aggregates generated and curated scenarios into a versioned bank with metadata
+ - Produces an index file used by SIL sweeps and prioritized runs
+"""
+import os
+import json
+from glob import glob
+from datetime import datetime
+
+SCENARIO_DIR = os.environ.get("SCENARIO_DIR", "ops/sim/carla/scenario_library")
+INDEX_OUT = os.environ.get("SCENARIO_INDEX_OUT", "ops/sim/carla/scenario_bank_index.json")
+
+def build_index():
+    entries = []
+    for fn in sorted(glob(os.path.join(SCENARIO_DIR, "*.json"))):
+        try:
+            j = json.load(open(fn))
+            meta = {"file": fn, "id": j.get("id"), "seed": j.get("seed"), "description": j.get("description",""), "expected": j.get("expected",{})}
+            entries.append(meta)
+        except Exception:
+            continue
+    bank = {"ts": datetime.utcnow().isoformat()+"Z", "count": len(entries), "scenarios": entries}
+    with open(INDEX_OUT, "w") as fh:
+        json.dump(bank, fh, indent=2)
+    print("Wrote scenario bank index to", INDEX_OUT)
+    return INDEX_OUT
+
+if __name__ == "__main__":
+    build_index()
+
*** End Patch
*** Begin Patch
*** Add File:ops/retrieval/train_reranker_large.py
+#!/usr/bin/env python3
+"""
+Train a larger cross-encoder reranker with configurable training shards.
+ - Input: TSV or JSONL with query, positive, negative examples
+ - Output: saved model directory for production deployment
+ - This script is a simple wrapper and can be adapted to run on distributed training infra.
+"""
+import argparse
+import json
+import os
+from sentence_transformers import CrossEncoder, InputExample
+from torch.utils.data import DataLoader
+
+def load_examples(path, max_examples=None):
+    examples = []
+    with open(path) as fh:
+        for i,l in enumerate(fh):
+            if max_examples and i>=max_examples:
+                break
+            j = json.loads(l)
+            q = j["query"]
+            pos = j["pos"]
+            neg = j.get("neg")
+            examples.append(InputExample(texts=[q,pos], label=1.0))
+            if neg:
+                examples.append(InputExample(texts=[q,neg], label=0.0))
+    return examples
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--train", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--model", default="cross-encoder/ms-marco-MiniLM-L-6-v2")
+    p.add_argument("--epochs", type=int, default=3)
+    p.add_argument("--batch", type=int, default=32)
+    p.add_argument("--max-ex", type=int, default=None)
+    args = p.parse_args()
+    ex = load_examples(args.train, max_examples=args.max_ex)
+    model = CrossEncoder(args.model)
+    dl = DataLoader(ex, shuffle=True, batch_size=args.batch)
+    model.fit(train_dataloader=dl, epochs=args.epochs, output_path=args.out)
+    print("Saved reranker to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/argo/workflows/sil_sweep_prioritized.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: sil-sweep-priority-
+  namespace: aegis-retriever
+spec:
+  entrypoint: sweep
+  arguments:
+    parameters:
+      - name: scenario-index
+        value: ops/sim/carla/scenario_bank_index.json
+      - name: max-parallel
+        value: "4"
+  templates:
+    - name: sweep
+      inputs:
+        parameters:
+          - name: scenario-index
+          - name: max-parallel
+      steps:
+        - - name: index
+            template: read-index
+            arguments:
+              parameters:
+                - name: idx
+                  value: "{{inputs.parameters.scenario-index}}"
+        - - name: run-batch
+            template: run-scenario
+            arguments:
+              parameters:
+                - name: scenario
+                  value: "{{item}}"
+          withParam: "{{steps.index.outputs.result}}"
+
+    - name: read-index
+      inputs:
+        parameters:
+          - name: idx
+      script:
+        image: python:3.11
+        command: [python]
+        source: |
+          import json,sys
+          idx = sys.argv[1]
+          j = json.load(open(idx))
+          # sort by inferred priority: scenarios with 'fog' first (example heuristic)
+          s = sorted(j["scenarios"], key=lambda x: ("fog" in str(x.get("description","")), x.get("id")), reverse=True)
+          out = [sc["file"] for sc in s]
+          print(json.dumps(out))
+        args: ["{{inputs.parameters.idx}}"]
+      outputs:
+        result:
+          valueFrom:
+            path: /tmp/result.json
+
+    - name: run-scenario
+      inputs:
+        parameters:
+          - name: scenario
+      container:
+        image: aegis/carla-runner:latest
+        command: ["/bin/sh","-c"]
+        args:
+          - |
+            python ops/sim/carla/run_scenario.py --scenario "{{inputs.parameters.scenario}}" --out /tmp/carla_runs || exit 1
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/contract_ci_enforcer.py
+#!/usr/bin/env python3
+"""
+CI enforcer to require contracts to be satisfiable by proofs or runtime monitors.
+ - Reads ops/formal/contracts.json and ops/formal/invariant_registry.json
+ - For each contract marked 'critical', ensures dependencies either have proofs (VERIFIER_LOG_DIR) or the contract declares a monitor.
+ - Fails CI with missing obligations.
+"""
+import os
+import json
+from glob import glob
+
+CONTRACT_REG = os.environ.get("CONTRACT_REG", "ops/formal/contracts.json")
+INV_REG = os.environ.get("INVARIANT_REGISTRY_PATH", "ops/formal/invariant_registry.json")
+VERIF_LOG_DIR = os.environ.get("VERIFIER_LOG_DIR", "/tmp/verifier_logs")
+
+def load_json(path):
+    return json.load(open(path)) if os.path.exists(path) else {}
+
+def proof_ids():
+    ids = set()
+    for fn in glob(os.path.join(VERIF_LOG_DIR, "*.proof.json")):
+        try:
+            j = json.load(open(fn))
+            ids.add(j.get("id"))
+        except Exception:
+            continue
+    return ids
+
+def main():
+    contracts = load_json(CONTRACT_REG)
+    inv = load_json(INV_REG)
+    proofs = proof_ids()
+    missing = []
+    for name,c in contracts.items():
+        if c.get("critical", False):
+            for dep in c.get("assumptions", []):
+                # a dependency could be an invariant id
+                inv_entry = inv.get(dep)
+                if inv_entry:
+                    inv_id = inv_entry.get("id", dep)
+                    if inv_id in proofs:
+                        continue
+                    if inv_entry.get("monitored", False):
+                        continue
+                    missing.append({"contract": name, "missing_assumption": dep})
+    if missing:
+        print("Contract obligations missing proofs or monitors:", missing)
+        raise SystemExit(2)
+    print("All critical contracts satisfied by proofs or monitors.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/signing_enforce.yml
+name: Enforce Evidence Signing
+on:
+  workflow_dispatch:
+
+jobs:
+  enforce-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Validate signing backend present
+        env:
+          EVIDENCE_SIGN_BACKEND: ${{ secrets.EVIDENCE_SIGN_BACKEND }}
+        run: |
+          if [ -z "${EVIDENCE_SIGN_BACKEND:-}" ]; then
+            echo "EVIDENCE_SIGN_BACKEND not set. Failing enforcement."
+            exit 2
+          fi
+          echo "EVIDENCE_SIGN_BACKEND=${EVIDENCE_SIGN_BACKEND}"
+      - name: Attempt sign smoke test (requires artifact path in secrets)
+        env:
+          EVIDENCE_SIGN_BACKEND: ${{ secrets.EVIDENCE_SIGN_BACKEND }}
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
+          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
+          VAULT_TRANSIT_KEY: ${{ secrets.VAULT_TRANSIT_KEY }}
+        run: |
+          # Create a small artifact to sign
+          ART=/tmp/test_evidence.zip
+          echo "test" > /tmp/test.txt
+          zip -j "$ART" /tmp/test.txt
+          python ops/evidence/sign_with_kms_or_vault.py "$ART"
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/role_manager.py
+#!/usr/bin/env python3
+"""
+Simple role manager for Safety Owners, Verification Engineers and Release Approvers.
+ - Stores roles in ops/governance/roles.json
+ - Provides CLI to add/list/remove and to produce a release board entry from template
+"""
+import os
+import json
+from datetime import datetime
+
+ROLES_PATH = os.environ.get("ROLES_PATH", "ops/governance/roles.json")
+os.makedirs(os.path.dirname(ROLES_PATH), exist_ok=True) if os.path.dirname(ROLES_PATH) else None
+
+def load_roles():
+    if os.path.exists(ROLES_PATH):
+        return json.load(open(ROLES_PATH))
+    return {"safety_owners": [], "verification_engineers": [], "release_approvers": []}
+
+def save_roles(r):
+    with open(ROLES_PATH,"w") as fh:
+        json.dump(r, fh, indent=2)
+
+def add_role(kind, name, contact):
+    r = load_roles()
+    r.setdefault(kind, []).append({"name": name, "contact": contact, "added": datetime.utcnow().isoformat()+"Z"})
+    save_roles(r)
+    return r
+
+def list_roles():
+    return load_roles()
+
+def remove_role(kind, name):
+    r = load_roles()
+    if kind in r:
+        r[kind] = [x for x in r[kind] if x.get("name") != name]
+        save_roles(r)
+    return r
+
+def render_release_board(model, artifact_hash, owner, safety_owner, verifier, approver, evidence_path):
+    from pathlib import Path
+    tpl = Path("docs/release_board_template.md").read_text()
+    filled = tpl.replace("{{Model}}", model).replace("{{Version / Artifact Hash}}", artifact_hash).replace("{{Owner}}", owner)
+    out = f"ops/governance/release_board_{model}_{int(datetime.utcnow().timestamp())}.md"
+    with open(out,"w") as fh:
+        fh.write(filled)
+    return out
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--add", nargs=3, metavar=("kind","name","contact"))
+    p.add_argument("--list", action="store_true")
+    p.add_argument("--remove", nargs=2, metavar=("kind","name"))
+    p.add_argument("--render-release", nargs=6, metavar=("model","artifact_hash","owner","safety_owner","verifier","approver"))
+    args = p.parse_args()
+    if args.add:
+        print(add_role(*args.add))
+    if args.list:
+        print(json.dumps(list_roles(), indent=2))
+    if args.remove:
+        print(remove_role(*args.remove))
+    if args.render_release:
+        print("Wrote", render_release_board(*args.render_release))
+
*** End Patch
*** Begin Patch
*** Add File:docs/gov_processes.md
+Governance processes & retention (summary)
+-----------------------------------------
+
+Purpose: Operationalize certification & governance steps so evidence and artifacts satisfy auditor expectations.
+
+Retention policy
+- Safety evidence bundles: retain for at least 7 years.
+- Proof artifacts and verifier logs: retain for 7 years.
+- HIL logs & firmware snapshots: retain for 7 years.
+- Metadata and release board entries: retain for 10 years.
+
+Roles & escallation
+- Safety Owner: ultimate accountability; approves safety promotions.
+- Verification Engineer: triages verification failures and authors invariants.
+- Release Approver: final signatory for a safety release.
+- Incident Commander: leads incident response and postmortems.
+
+Approval flow (summary)
+1. Produce evidence bundle with SIL/HIL and verification artifacts.
+2. Verification Engineer prepares evidence and marks evidence path.
+3. Safety Owner reviews evidence and approves using Release Board template.
+4. Release Approver signs and triggers promote_to_safety flow.
+
+Incident response summary
+- Monitor alerts → safety_controller action → incident creation → evidence gather → triage → remediate → postmortem
+ - Postmortem documents root cause, tests added, invariants updated and a timeline.
+
*** End Patch
*** End Patch
