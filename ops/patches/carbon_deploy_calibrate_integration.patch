*** Begin Patch
*** Add File:k8s/node-power-service.yaml
+apiVersion: v1
+kind: Service
+metadata:
+  name: node-power-exporter
+  namespace: aegis-retriever
+  labels:
+    app: node-power-exporter
+  annotations:
+    prometheus.io/scrape: "true"
+    prometheus.io/port: "9203"
+    prometheus.io/path: "/metrics"
+spec:
+  selector:
+    app: node-power-exporter
+  ports:
+    - name: metrics
+      port: 9203
+      targetPort: 9202
+
*** End Patch
*** Begin Patch
*** Add File:k8s/servicemonitor-node-power.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: node-power-monitor
+  namespace: aegis-retriever
+spec:
+  selector:
+    matchLabels:
+      app: node-power-exporter
+  namespaceSelector:
+    matchNames:
+      - aegis-retriever
+  endpoints:
+    - port: metrics
+      interval: 15s
+      path: /metrics
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/enforce_job_wrapper.py
+#!/usr/bin/env python3
+"""
+CI check to ensure heavy-job templates and Argo/CI workflows call the job wrapper.
+ - Scans .github/workflows, k8s/argo templates and ops/ci job templates for direct calls to python or job manifests
+ - Flags runs that include `python ...` or `kubectl apply` for heavy jobs without using job_wrapper_improved.py
+ - This is heuristic-based; add exact patterns as you standardize templates.
+"""
+import os
+import sys
+import re
+from pathlib import Path
+
+ROOT = os.environ.get("GITHUB_WORKSPACE", os.getcwd())
+WORKFLOWS = list(Path(ROOT).glob(".github/workflows/**/*.yml")) + list(Path(ROOT).glob(".github/workflows/**/*.yaml"))
+ARGO_TEMPLATES = list(Path(ROOT).glob("k8s/argo/**/*.yaml")) + list(Path(ROOT).glob("k8s/argo/**/*.yml"))
+HEAVY_KEYWORDS = ["train", "math-bench", "benchmark", "eval", "long-run"]
+
+def file_contains_wrapper(path):
+    txt = path.read_text(encoding="utf-8", errors="ignore")
+    if "job_wrapper_improved.py" in txt or "job_wrapper.py" in txt:
+        return True
+    return False
+
+def heuristically_heavy(txt):
+    lower = txt.lower()
+    return any(k in lower for k in HEAVY_KEYWORDS)
+
+def scan_files(paths):
+    issues = []
+    for p in paths:
+        try:
+            txt = p.read_text(encoding="utf-8", errors="ignore")
+            if heuristically_heavy(txt) and not file_contains_wrapper(p):
+                issues.append(str(p))
+        except Exception:
+            continue
+    return issues
+
+def main():
+    files = WORKFLOWS + ARGO_TEMPLATES
+    issues = scan_files(files)
+    report = {"checked": len(files), "issues": issues}
+    print("Job wrapper enforcement report:", report)
+    if issues:
+        print("ERROR: Found heavy-job templates without job_wrapper usage. Files:")
+        for i in issues:
+            print(" -", i)
+        sys.exit(2)
+    print("OK: All scanned templates include job wrapper or no heavy-job heuristics found.")
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/enforce_job_wrapper.yml
+name: Enforce Job Wrapper
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  enforce-wrapper:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Run job wrapper enforcement
+        run: |
+          python ops/ci/enforce_job_wrapper.py
+
*** End Patch
*** Begin Patch
*** Add File:ops/carbon/calibration_orchestrator.py
+#!/usr/bin/env python3
+"""
+Calibration orchestrator:
+ - Runs calibration_runner on a set of representative instance types / nodes (can be executed as CI job or ad-hoc)
+ - Collects outputs and merges into CARBON_CALIB_PATH
+ - Example usage:
+     python ops/carbon/calibration_orchestrator.py --instances g4dn.xlarge,p3.2xlarge --nodes node-1,node-2
+"""
+import os
+import json
+import subprocess
+import argparse
+from datetime import datetime
+
+CALIB_PATH = os.environ.get("CARBON_CALIB_PATH", "/etc/aegis/carbon_calibration.json")
+
+def run_calibrate(instance, node):
+    try:
+        subprocess.check_call(["python", "ops/carbon/calibration_runner.py", "--instance", instance, "--node", node])
+        return True
+    except Exception as e:
+        print("calibration failed for", instance, node, e)
+        return False
+
+def collect_results():
+    if os.path.exists(CALIB_PATH):
+        return json.load(open(CALIB_PATH))
+    return {}
+
+def main(instances, nodes):
+    for inst, node in zip(instances, nodes):
+        run_calibrate(inst, node)
+    # after runs, print summary
+    data = collect_results()
+    print("Calibration DB keys:", list(data.keys()))
+    print("Saved to", CALIB_PATH)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--instances", required=True, help="comma-separated instance types")
+    p.add_argument("--nodes", required=True, help="comma-separated node names for calibration runs")
+    args = p.parse_args()
+    insts = args.instances.split(",")
+    nodes = args.nodes.split(",")
+    main(insts, nodes)
+
*** End Patch
*** Begin Patch
*** Add File:ops/carbon/electricitymap_config_validator.py
+#!/usr/bin/env python3
+"""
+Validate electricityMap configuration:
+ - Ensures ELECTRICITYMAP_API_KEY present and quick live query succeeds for configured zones
+ - Writes a short validation report to /tmp/electricitymap_validation.json
+"""
+import os
+import json
+import requests
+
+OUT = "/tmp/electricitymap_validation.json"
+
+def validate(zones):
+    key = os.environ.get("ELECTRICITYMAP_API_KEY", "")
+    url = os.environ.get("ELECTRICITYMAP_API_URL", "https://api.electricitymap.org/v3/carbon-intensity/latest")
+    res = {"ok": False, "zones": []}
+    if not key:
+        res["error"] = "missing_api_key"
+        open(OUT, "w").write(json.dumps(res, indent=2))
+        return res
+    headers = {"auth-token": key}
+    for z in zones:
+        try:
+            r = requests.get(url, params={"zone": z}, headers=headers, timeout=10)
+            r.raise_for_status()
+            j = r.json()
+            res["zones"].append({"zone": z, "status": "ok", "sample": j.get("data")})
+        except Exception as e:
+            res["zones"].append({"zone": z, "status": "error", "error": str(e)})
+    res["ok"] = all(z["status"]=="ok" for z in res["zones"])
+    open(OUT, "w").write(json.dumps(res, indent=2))
+    return res
+
+if __name__ == "__main__":
+    import sys
+    zones = sys.argv[1].split(",") if len(sys.argv)>1 else [os.environ.get("ELECTRICITYMAP_ZONE","")]
+    print(validate(zones))
+
*** End Patch
*** Begin Patch
*** Add File:ops/carbon/argo_integration_helper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+MANIFEST="$1"
+MAX_INTENSITY="${MAX_INTENSITY:-300}"
+DEFER_MINUTES="${DEFER_MINUTES:-60}"
+AUTO_OPT="${AUTO_OPT:-1}"
+ZONE="${ZONE:-}"
+echo "Argo hook calling carbon-aware submitter for $MANIFEST"
+python /opt/carbon/submit_job_carbon_aware.py --manifest "$MANIFEST" --max-intensity "$MAX_INTENSITY" --defer-minutes "$DEFER_MINUTES" $( [ "$AUTO_OPT" = "1" ] && echo "--auto-opt" ) $( [ -n "$ZONE" ] && echo "--zone $ZONE" )
+
*** End Patch
*** Begin Patch
*** Add File:ops/carbon/export_budget_sql_to_postgres.py
+#!/usr/bin/env python3
+"""
+Export budgets & consumption from SQLite (budget_sql) to Postgres for managed accounting.
+ - Reads SQLite DB and writes to Postgres tables budgets and consumption.
+ - Requires POSTGRES_DSN env var.
+"""
+import os
+import sqlite3
+import psycopg2
+
+SQLITE_DB = os.environ.get("CARBON_BUDGET_DB", "/var/lib/aegis/carbon_budget.db")
+PG_DSN = os.environ.get("POSTGRES_DSN", "")
+
+def read_sqlite():
+    conn = sqlite3.connect(SQLITE_DB)
+    cur = conn.cursor()
+    cur.execute("SELECT team, budget FROM teams")
+    teams = cur.fetchall()
+    cur.execute("SELECT team, kg, job_id, ts FROM consumption")
+    consum = cur.fetchall()
+    conn.close()
+    return teams, consum
+
+def write_postgres(teams, consum):
+    if not PG_DSN:
+        raise RuntimeError("POSTGRES_DSN not set")
+    conn = psycopg2.connect(PG_DSN)
+    cur = conn.cursor()
+    cur.execute("CREATE TABLE IF NOT EXISTS budgets(team TEXT PRIMARY KEY, budget REAL)")
+    cur.execute("CREATE TABLE IF NOT EXISTS consumption(team TEXT, kg REAL, job_id TEXT, ts TIMESTAMP)")
+    for t,b in teams:
+        cur.execute("INSERT INTO budgets(team,budget) VALUES (%s,%s) ON CONFLICT (team) DO UPDATE SET budget=EXCLUDED.budget", (t,b))
+    for team,kg,job_id,ts in consum:
+        cur.execute("INSERT INTO consumption(team,kg,job_id,ts) VALUES (%s,%s,%s,%s)", (team,kg,job_id,ts))
+    conn.commit()
+    conn.close()
+
+if __name__ == "__main__":
+    teams, consum = read_sqlite()
+    write_postgres(teams, consum)
+    print("Exported budgets to Postgres")
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/install_device_agent.sh
+#!/usr/bin/env bash
+set -euo pipefail
+AGENT_DIR=${1:-/opt/device_agent}
+DEVICE_ID=${2:-$(hostname -s)}
+mkdir -p "${AGENT_DIR}"
+cat > "${AGENT_DIR}/device_agent.py" <<'PY'
+#!/usr/bin/env python3
+from flask import Flask, jsonify
+import os,time
+app=Flask(__name__)
+DEVICE_ID=os.environ.get("DEVICE_ID","edge-unknown")
+def measure_power():
+    return float(os.environ.get("MOCK_DEVICE_POWER_W","2.5"))
+@app.route("/v1/power")
+def power():
+    return jsonify({"device_id": DEVICE_ID, "power_w": measure_power(), "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())})
+if __name__=="__main__":
+    app.run(host="0.0.0.0", port=8080)
+PY
+chmod +x "${AGENT_DIR}/device_agent.py"
+cat > /etc/systemd/system/device-agent.service <<SYS
+[Unit]
+Description=Aegis Device Agent
+After=network.target
+
+[Service]
+ExecStart=/usr/bin/env python3 ${AGENT_DIR}/device_agent.py
+Restart=always
+RestartSec=5
+
+[Install]
+WantedBy=multi-user.target
+SYS
+systemctl daemon-reload
+systemctl enable --now device-agent
+echo "Installed device agent at ${AGENT_DIR} and systemd service created (DEVICE_ID=${DEVICE_ID})"
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/run_validation_sweep.py
+#!/usr/bin/env python3
+"""
+Run validation_runner across discovered job metas and produce an aggregated validation summary.
+ - Scans AEGIS_JOB_DIR for job metas, runs validation_runner.py for each (best-effort), aggregates results, then packages audit.
+"""
+import os
+import json
+import subprocess
+from datetime import datetime
+
+JOB_DIR = os.environ.get("AEGIS_JOB_DIR", "/tmp/aegis_jobs")
+OUT = "/tmp/validation_sweep_summary.json"
+
+def discover_jobs(limit=50):
+    if not os.path.exists(JOB_DIR):
+        return []
+    files = [f for f in os.listdir(JOB_DIR) if f.endswith(".json")]
+    files = sorted(files, reverse=True)[:limit]
+    return [os.path.splitext(f)[0] for f in files]
+
+def run_validation(job_id):
+    try:
+        subprocess.check_call(["python", "ops/carbon/validation_runner.py", job_id])
+        return True
+    except Exception:
+        return False
+
+def main():
+    jobs = discover_jobs()
+    results = {"ts": datetime.utcnow().isoformat()+"Z", "jobs": []}
+    for j in jobs:
+        ok = run_validation(j)
+        results["jobs"].append({"job_id": j, "validated": ok})
+    with open(OUT, "w") as fh:
+        json.dump(results, fh, indent=2)
+    # produce audit package
+    try:
+        subprocess.check_call(["python", "ops/carbon/audit_packager.py", "validation-"+datetime.utcnow().strftime("%Y%m%d")])
+    except Exception:
+        pass
+    print("Wrote validation sweep summary to", OUT)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/carbon_deployment_runbook.md
+Carbon deployment & calibration runbook
+-------------------------------------
+
+Purpose:
+ - Deploy telemetry, ensure job instrumentation, run calibration sweep, integrate scheduler and operationalize budgets/audit.
+
+1. Deploy node power exporter DaemonSet and service monitors
+   - kubectl apply -f k8s/daemonsets/node_power_exporter_daemonset.yaml
+   - kubectl apply -f k8s/node-power-service.yaml
+   - kubectl apply -f k8s/servicemonitor-node-power.yaml
+   - Verify pods: kubectl get pods -n aegis-retriever -l app=node-power-exporter
+
+2. Ensure Prometheus scrapes node-power exporter
+   - Confirm ServiceMonitor is present in Prometheus configuration and metrics appear at /metrics endpoint
+
+3. Enforce job wrapper in CI/Argo
+   - Add job_wrapper_improved invocation to heavy job templates
+   - Validate PRs using .github/workflows/enforce_job_wrapper.yml
+
+4. Run calibration sweep
+   - Prepare representative nodes and list instance types
+   - python ops/carbon/calibration_orchestrator.py --instances g4dn.xlarge,p3.2xlarge --nodes node-1,node-2
+   - Verify /etc/aegis/carbon_calibration.json updated
+
+5. Validate mapping & electricityMap
+   - Set ELECTRICITYMAP_API_KEY and zones
+   - python ops/carbon/electricitymap_config_validator.py us-west-2,eu-central-1
+
+6. Integrate scheduler into Argo/CI
+   - Use k8s/argo/workflow-templates/carbon-aware-submit-workflow.yaml and ensure Argo pods have carbon-tools image with submit_job_carbon_aware.py available
+
+7. Promote budgets to managed DB
+   - Initialize Postgres and export via ops/carbon/export_budget_sql_to_postgres.py
+
+8. Device fleet rollout
+   - Use ops/edge/install_device_agent.sh to install device agent on edge devices
+   - Collect device samples and feed into aggregation pipeline
+
+9. Run validation sweep & audit
+   - python ops/ci/run_validation_sweep.py
+   - Review audit package in /tmp/release_reports and upload to evidence bucket if required
+
+10. Repeat
+   - Schedule weekly calibration & validation, monitor uncertainty and iterate calibration until target uncertainty achieved.
+
*** End Patch
*** End Patch
