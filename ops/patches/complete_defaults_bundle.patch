*** Begin Patch
*** Add File:ops/hil/adapters/acme_hil_adapter.py
+#!/usr/bin/env python3
+"""
+Acme HIL vendor adapter (improved)
+ - Robust mapping layer between Aegis canonical manifest and Acme REST vendor API.
+ - Produces normalized job objects expected by HIL scheduler and deterministic verification.
+ - Fallback to local simulator when ACME_HIL_URL not configured (useful for CI).
+
+Env:
+  ACME_HIL_URL, ACME_HIL_TOKEN
+"""
+import os
+import time
+import json
+import logging
+from typing import Dict, Any, List
+
+try:
+    import requests
+except Exception:
+    requests = None
+
+LOG = logging.getLogger("acme_hil_adapter")
+LOG.setLevel(logging.INFO)
+
+ACME_URL = os.environ.get("ACME_HIL_URL", "")
+ACME_TOKEN = os.environ.get("ACME_HIL_TOKEN", "")
+
+def _headers(token: str = None):
+    h = {"Content-Type": "application/json"}
+    if token:
+        h["Authorization"] = f"Bearer {token}"
+    return h
+
+def _normalize_backend(b: Dict[str,Any]) -> Dict[str,Any]:
+    # Map vendor fields to canonical fields: rack_id, slot, status
+    return {
+        "rack_id": b.get("id", b.get("rack_id", "unknown")),
+        "slot": b.get("slot", b.get("index", 1)),
+        "status": b.get("status", "idle")
+    }
+
+class AcmeHILAdapter:
+    def __init__(self, url: str = None, token: str = None):
+        self.url = url or ACME_URL
+        self.token = token or ACME_TOKEN
+
+    def list_backends(self) -> List[Dict[str,Any]]:
+        if not self.url or not requests:
+            LOG.info("No ACME URL configured — returning simulator backend")
+            return [{"rack_id": "acme-sim", "slot": 1, "status": "idle", "simulated": True}]
+        try:
+            r = requests.get(f"{self.url}/api/v1/racks", headers=_headers(self.token), timeout=10)
+            r.raise_for_status()
+            backends = r.json()
+            return [_normalize_backend(b) for b in backends]
+        except Exception as e:
+            LOG.exception("list_backends failed, falling back to simulator: %s", e)
+            return [{"rack_id":"acme-sim","slot":1,"status":"idle","simulated":True}]
+
+    def prepare_replay(self, payload: Dict[str,Any]) -> Dict[str,Any]:
+        manifest = payload.get("manifest", {})
+        # vendor expects bag_url, start_time, seed, env
+        vendor_payload = {
+            "bag_url": manifest.get("bag"),
+            "start_time": manifest.get("start_time"),
+            "seed": manifest.get("seed", 0),
+            "env": manifest.get("expected_env", {}),
+            "notes": manifest.get("notes", "")
+        }
+        if not self.url or not requests:
+            return {"mounted": vendor_payload["bag_url"], "ok": True, "simulated": True}
+        r = requests.post(f"{self.url}/api/v1/racks/prepare", headers=_headers(self.token), json=vendor_payload, timeout=30)
+        r.raise_for_status()
+        return r.json()
+
+    def start_run(self, payload: Dict[str,Any]) -> Dict[str,Any]:
+        manifest = payload.get("manifest", {})
+        vendor_payload = {
+            "bag_url": manifest.get("bag"),
+            "seed": manifest.get("seed", 0),
+            "start_time": manifest.get("start_time"),
+            "options": payload.get("options", {}),
+        }
+        if not self.url or not requests:
+            job_id = f"acme-sim-{int(time.time()*1000)}"
+            return {"job_id": job_id, "status": "submitted", "simulated": True}
+        r = requests.post(f"{self.url}/api/v1/runs", headers=_headers(self.token), json=vendor_payload, timeout=30)
+        r.raise_for_status()
+        return r.json()
+
+    def poll_status(self, job_id: str) -> Dict[str,Any]:
+        if not self.url or not requests:
+            return {"job_id": job_id, "state": "succeeded", "progress": 100}
+        r = requests.get(f"{self.url}/api/v1/runs/{job_id}/status", headers=_headers(self.token), timeout=10)
+        r.raise_for_status()
+        return r.json()
+
+    def fetch_artifacts(self, job_id: str, out_dir: str) -> List[str]:
+        os.makedirs(out_dir, exist_ok=True)
+        if not self.url or not requests:
+            path = os.path.join(out_dir, f"{job_id}_hil_log.json")
+            json.dump({"job_id": job_id, "status": "simulated_complete"}, open(path, "w"))
+            return [path]
+        r = requests.get(f"{self.url}/api/v1/runs/{job_id}/artifacts", headers=_headers(self.token), timeout=30)
+        r.raise_for_status()
+        arts = r.json().get("artifacts", [])
+        local = []
+        for a in arts:
+            try:
+                rr = requests.get(a, headers=_headers(self.token), timeout=60)
+                rr.raise_for_status()
+                fname = os.path.join(out_dir, a.split("/")[-1])
+                open(fname, "wb").write(rr.content)
+                local.append(fname)
+            except Exception:
+                LOG.exception("failed to fetch artifact %s", a)
+                continue
+        return local
+
+    # convenience CLI
+    def smoke(self):
+        print("Backends:", self.list_backends())
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--smoke", action="store_true")
+    args = p.parse_args()
+    a = AcmeHILAdapter()
+    if args.smoke:
+        a.smoke()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/hil_onboard_acme.yml
+name: HIL Onboard (Acme) & Certify
+on:
+  workflow_dispatch:
+
+jobs:
+  certify-acme:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: python -m pip install --upgrade pip requests boto3 || true
+      - name: Run certification
+        env:
+          ACME_HIL_URL: ${{ secrets.ACME_HIL_URL }}
+          ACME_HIL_TOKEN: ${{ secrets.ACME_HIL_TOKEN }}
+          EVIDENCE_SIGN_BACKEND: ${{ secrets.EVIDENCE_SIGN_BACKEND }}
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+        run: |
+          python ops/hil/certify_rack.py --adapter ops.hil.adapters.acme_hil_adapter.AcmeHILAdapter --manifest ops/hil/playbook/example_replay_manifest.json --name acme-rack-1 --repeats 12 --s3-bucket "${{ secrets.EVIDENCE_BUCKET }}"
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/aws_oidc_kms_setup.md
+AWS OIDC + KMS setup (quick guide)
+===================================
+
+This guide shows the minimal steps to configure GitHub Actions OIDC → AWS AssumeRole and an AWS KMS signing key used by CI.
+
+1) Create IAM Role Trust for GitHub OIDC
+   - Use ops/governance/create_oidc_role.sh OR follow AWS docs to create a role with:
+     Principal: token.actions.githubusercontent.com
+     Condition: token.actions.githubusercontent.com:aud == sts.amazonaws.com
+   - Limit allowed repositories in the trust condition for least privilege.
+
+2) Create KMS signing key
+   - Use CLI:
+       aws kms create-key --description "Aegis evidence signing key" --key-usage SIGN_VERIFY --output json
+     Save KeyId (e.g., alias/aegis-evidence).
+   - Create alias:
+       aws kms create-alias --alias-name alias/aegis-evidence --target-key-id <KeyId>
+
+3) Apply KMS key policy allowing role to Sign (least privilege)
+   - Sample policy snippet (apply via AWS Console or CLI). See ops/governance/oidc_kms_iam_policies.md for an example.
+
+4) Configure GitHub Secrets
+   - Add AEGIS_OIDC_ROLE_ARN (role ARN), AWS_REGION, AWS_KMS_KEY_ID, EVIDENCE_BUCKET, EVIDENCE_SIGN_BACKEND if needed.
+
+5) Test signing from GitHub Actions
+   - Use .github/workflows/oidc_kms_signing_prod.yml (provided) to exercise signing a small file in CI and store signature in artifact.
+
+Notes:
+ - Use short-lived credentials via OIDC in CI, avoid long-lived secrets.
+ - Restrict which repos/workflows can assume the role in the trust policy.
+ - Enable CloudTrail logging for KMS and role assumption events for audit.
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/oidc_kms_signing_prod.yml
+name: OIDC -> KMS Sign (production demo)
+on:
+  workflow_dispatch:
+
+permissions:
+  id-token: write
+  contents: read
+
+jobs:
+  sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Configure AWS credentials via OIDC
+        uses: aws-actions/configure-aws-credentials@v2
+        with:
+          role-to-assume: ${{ secrets.AEGIS_OIDC_ROLE_ARN }}
+          aws-region: ${{ secrets.AWS_REGION }}
+      - name: Create payload
+        run: |
+          echo "aegis-evidence-test-$(date -u +%s)" > /tmp/evidence_payload.txt
+      - name: Sign payload with KMS
+        env:
+          AWS_KMS_KEY_ID: ${{ secrets.AWS_KMS_KEY_ID }}
+        run: |
+          python ops/governance/kms_sign_helper.py --file /tmp/evidence_payload.txt --key-id "${AWS_KMS_KEY_ID}"
+      - name: Upload signature
+        uses: actions/upload-artifact@v4
+        with:
+          name: evidence-signature
+          path: /tmp/evidence_payload.txt.sig
+
*** End Patch
*** Begin Patch
*** Add File:ops/governance/kms_sign_helper.py
+#!/usr/bin/env python3
+"""
+Helper to sign a file with AWS KMS using boto3.
+Writes signature to <file>.sig (base64).
+"""
+import os
+import base64
+import argparse
+import hashlib
+try:
+    import boto3
+except Exception:
+    boto3 = None
+
+def sign_file(file_path, key_id):
+    if not boto3:
+        raise RuntimeError("boto3 required")
+    data = open(file_path, "rb").read()
+    # KMS signs the raw bytes; we compute digest when desired but here use RAW message
+    kms = boto3.client("kms", region_name=os.environ.get("AWS_REGION"))
+    resp = kms.sign(KeyId=key_id, Message=data, MessageType="RAW", SigningAlgorithm="RSASSA_PSS_SHA_256")
+    sig = resp["Signature"]
+    with open(file_path + ".sig", "wb") as fh:
+        fh.write(sig)
+    print("Wrote signature:", file_path + ".sig")
+    # also write base64 for portability
+    with open(file_path + ".sig.b64", "wb") as fh:
+        fh.write(base64.b64encode(sig))
+    print("Wrote base64 sig:", file_path + ".sig.b64")
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--file", required=True)
+    p.add_argument("--key-id", required=True)
+    args = p.parse_args()
+    sign_file(args.file, args.key_id)
+
*** End Patch
*** Begin Patch
*** Add File:ops/perception/scale_connector.py
+#!/usr/bin/env python3
+"""
+Scale-style annotation vendor connector with S3 fallback.
+ - submit_bundle(bundle_path) -> returns task_id
+ - poll_task(task_id) -> returns {"status": "done", "result": <s3_or_path>}
+
+Config:
+  ops/perception/annotation_vendor_config.example.json (vendor_url, api_token, s3_bucket_fallback)
+"""
+import os
+import json
+import time
+from glob import glob
+
+try:
+    import requests
+    import boto3
+except Exception:
+    requests = None
+    boto3 = None
+
+CONFIG = os.environ.get("ANNOTATION_VENDOR_CONFIG", "ops/perception/annotation_vendor_config.example.json")
+
+def _load_cfg():
+    if os.path.exists(CONFIG):
+        return json.load(open(CONFIG))
+    return {}
+
+def submit_bundle(bundle_path):
+    cfg = _load_cfg()
+    vendor = cfg.get("vendor_url")
+    token = cfg.get("api_token")
+    s3_bucket = cfg.get("s3_bucket_fallback")
+    if vendor and requests:
+        files = {"file": open(bundle_path, "rb")}
+        headers = {"Authorization": f"Bearer {token}"} if token else {}
+        r = requests.post(f"{vendor}/tasks", files=files, headers=headers, timeout=120)
+        r.raise_for_status()
+        return r.json().get("task_id")
+    # Fallback: upload to S3 (if configured) or local tasks dir
+    if s3_bucket and boto3:
+        s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION"))
+        key = f"annotation_bundles/{os.path.basename(bundle_path)}"
+        s3.upload_file(bundle_path, s3_bucket, key)
+        # create a simple local task file for human operator
+        os.makedirs("/tmp/annotation_tasks", exist_ok=True)
+        tid = f"manual-{int(time.time()*1000)}"
+        task = {"id": tid, "s3": f"s3://{s3_bucket}/{key}", "status":"queued"}
+        json.dump(task, open(f"/tmp/annotation_tasks/{tid}.json","w"))
+        return tid
+    # Final fallback: copy to /tmp for manual handling
+    os.makedirs("/tmp/annotation_tasks", exist_ok=True)
+    tid = f"manual-{int(time.time()*1000)}"
+    dst = f"/tmp/annotation_tasks/{os.path.basename(bundle_path)}"
+    import shutil
+    shutil.copy(bundle_path, dst)
+    task = {"id": tid, "path": dst, "status": "queued"}
+    json.dump(task, open(f"/tmp/annotation_tasks/{tid}.json","w"))
+    return tid
+
+def poll_task(task_id, timeout=86400, poll_interval=30):
+    cfg = _load_cfg()
+    vendor = cfg.get("vendor_url")
+    token = cfg.get("api_token")
+    if vendor and requests:
+        r = requests.get(f"{vendor}/tasks/{task_id}", headers={"Authorization": f"Bearer {token}"}, timeout=30)
+        r.raise_for_status()
+        return r.json()
+    # check local tasks dir
+    p = f"/tmp/annotation_tasks/{task_id}.json"
+    waited = 0
+    while waited < timeout:
+        if os.path.exists(p):
+            j = json.load(open(p))
+            if j.get("status") == "done":
+                return j
+        time.sleep(poll_interval); waited += poll_interval
+    return {"status": "timeout"}
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--submit")
+    p.add_argument("--poll")
+    args = p.parse_args()
+    if args.submit:
+        print(submit_bundle(args.submit))
+    if args.poll:
+        print(poll_task(args.poll))
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/perception_annotation_train.yml
+name: Perception Annotation -> Train (demo)
+on:
+  workflow_dispatch:
+
+jobs:
+  annotation-to-train:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: python -m pip install --upgrade pip requests boto3 || true
+      - name: Submit harvested failures to vendor (if any)
+        run: |
+          for b in /tmp/perception_failures/*; do
+            if [ -d "$b" ]; then
+              python ops/perception/scale_connector.py --submit "$b" || true
+            fi
+          done
+      - name: Ingest annotation results (poll local tasks)
+        run: |
+          python ops/perception/annotation_result_ingest.py || true
+      - name: Shard dataset and launch training (k8s)
+        run: |
+          python ops/perception/dataset_sharder.py --manifest /tmp/curated_dataset/manifest.json --shards 4 --out /tmp/dataset_shards
+          for s in /tmp/dataset_shards/manifest_shard_*.json; do
+            python ops/perception/train_launcher.py --shard "$s" --image "${{ secrets.TRAINER_IMAGE }}" || true
+          done
+      - name: (Optional) Run evaluation & produce signed eval summary
+        run: echo "Run evaluation and produce /tmp/perception_sweep/sweep_summary.json (out of band or separate job)"
+
*** End Patch
*** Begin Patch
*** Add File:ops/runbooks/production_automation_runbook.md
+Aegis Production Automation Runbook
+==================================
+
+Goal
+----
+Automate the remaining manual steps required to reach a certifiable production platform:
+ hardware onboarding, RT fleet validation, perception production ops, SME formalization, OIDC/KMS automation, SRE readiness and staged hardware CI.
+
+High-level sequence
+1. HIL vendor onboarding & certification
+   - Obtain vendor endpoint/credentials and set ACME_HIL_URL/ACME_HIL_TOKEN in Vault.
+   - Run .github/workflows/hil_onboard_acme.yml (CI) to execute certify_rack and produce signed determinism summary.
+   - Inspect /tmp/hil_replay_summary_*.json; verify mean score >= policy threshold; approve rack in ops/hil/hil_racks.json.
+
+2. RT fleet rollout & validation
+   - Build and publish RT image (ops/rt/build_rt_image.sh).
+   - Label candidate nodes using kubectl or ops/rt/label_nodes.sh.
+   - Deploy RT agent via Helm (charts/rt-node).
+   - Run ops/rt/wcet_k8s_job.yaml on labeled nodes and/or ops/rt/remote_wcet_runner.py for vehicle SSH hosts.
+   - Collect /tmp/wcet_aggregate_remote.json, sign and upload to evidence S3.
+
+3. Perception production ops
+   - Configure annotation vendor in ops/perception/annotation_vendor_config.example.json or use Scale-style connector.
+   - Run ops/perception/active_learning_worker to push bundles to vendor.
+   - Ingest results with ops/perception/annotation_result_ingest.py, shard dataset and launch jobs using ops/perception/train_launcher.py.
+   - Evaluate, sign evaluation_summary.json and run ops/perception/promote_model_ci.py to promote models.
+
+4. Formal invariant onboarding (SME flow)
+   - SMEs create PRs under ops/formal/onboard_submissions with SMT and meta.
+   - CI runs prover_batch_runner and uploads proof artifacts for review (ops/formal/sme_review_workflow.yml).
+   - Approved proofs are added to the invariant registry via ops/formal/onboard_invariant_api.py.
+
+5. Enterprise automation (OIDC, KMS, billing)
+   - Create OIDC role per ops/governance/create_oidc_role.sh.
+   - Create KMS key (ops/governance/create_kms_key.sh) and attach least-privilege policy for CI signing.
+   - Configure GitHub secrets: AEGIS_OIDC_ROLE_ARN, AWS_REGION, AWS_KMS_KEY_ID, EVIDENCE_BUCKET.
+   - Use .github/workflows/oidc_kms_signing_prod.yml to validate signing from CI.
+
+6. SRE readiness & staging
+   - Install Prometheus/Grafana dashboards (ops/sre/grafana_dashboard_rt.json, grafana_dashboard_hil.json).
+   - Run staging integration (.github/workflows/staging_integration.yml) and exercise load tests (ops/sre/loadtest.sh).
+   - Validate runbooks (ops/sre/runbook_hil_onboarding.md) and train oncall responders.
+
+7. CI gating & hardware integration
+   - Populate staging lab registry (ops/tests/staging/lab_manager.py) with test nodes.
+   - Enable hardware gates (.github/workflows/staging_hardware_gate.yml, .github/workflows/hardware_integration_gate.yml).
+
+Acceptance criteria (minimum)
+- At least one approved HIL rack with signed determinism summary and evidence stored in S3.
+- RT fleet p99 results validated & signed for control binaries across representative nodes.
+- Perception production pipeline produces signed evaluation summaries and promoted models are registered.
+- High-priority invariants have proofs or documented monitors in the registry and CI fails otherwise.
+- CI uses OIDC to sign artifacts via KMS; KMS usage and roleassume are audited.
+- Production-level dashboards and runbooks exist and oncall runbook steps are exercised.
+
*** End Patch
*** Begin Patch
*** Add File:ops/runbooks/prioritized_backlog.yml
+---
+backlog:
+  - id: 001
+    title: "Onboard first vendor HIL rack (Acme)"
+    owner: "hardware-team"
+    priority: high
+    tasks:
+      - "Acquire rack access and credentials"
+      - "Store creds in Vault and set ACME_HIL_URL/ACME_HIL_TOKEN for CI"
+      - "Run .github/workflows/hil_onboard_acme.yml"
+      - "Review determinism summary and approve rack in ops/hil/hil_racks.json"
+    estimate_days: 5
+
+  - id: 002
+    title: "Enable OIDC -> KMS signing in CI"
+    owner: "security-team"
+    priority: high
+    tasks:
+      - "Create OIDC role (ops/governance/create_oidc_role.sh)"
+      - "Create KMS key (ops/governance/create_kms_key.sh)"
+      - "Configure GitHub secrets"
+      - "Run .github/workflows/oidc_kms_signing_prod.yml"
+    estimate_days: 3
+
+  - id: 003
+    title: "RT fleet WCET validation"
+    owner: "systems-team"
+    priority: high
+    tasks:
+      - "Build RT image and push"
+      - "Label candidate RT nodes"
+      - "Run ops/rt/wcet_k8s_job.yaml and remote_wcet_runner.py"
+      - "Aggregate results and sign"
+    estimate_days: 7
+
+  - id: 004
+    title: "Perception production pipeline integration"
+    owner: "perception-team"
+    priority: high
+    tasks:
+      - "Configure annotation vendor or S3 fallback"
+      - "Wire active_learning_worker and scale_connector"
+      - "Ingest annotated results and shard dataset"
+      - "Launch distributed training via train_launcher"
+      - "Promote model via promote_model_ci"
+    estimate_days: 10
+
+  - id: 005
+    title: "SME SMT onboarding sprint"
+    owner: "formal-team"
+    priority: medium
+    tasks:
+      - "Run SME workshop to capture top-10 invariants"
+      - "Create PRs under ops/formal/onboard_submissions"
+      - "Iterate with prover artifacts until accepted"
+    estimate_days: 14
+
+  - id: 006
+    title: "SRE staging & runbook exercises"
+    owner: "sre-team"
+    priority: medium
+    tasks:
+      - "Install dashboards & alerts"
+      - "Run staging integration & load tests"
+      - "Exercise runbooks and oncall simulation"
+    estimate_days: 7
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/auto_promote_example.yml
+name: Auto-Promote Model (example gated flow)
+on:
+  workflow_dispatch:
+
+jobs:
+  evaluate-and-promote:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup
+        uses: actions/setup-python@v4
+        with:
+          python-version: 3.11
+      - name: Verify signed eval summary present
+        run: |
+          if [ ! -f /tmp/perception_sweep/sweep_summary.json ]; then echo "Missing eval summary"; exit 2; fi
+          if [ ! -f /tmp/perception_sweep/sweep_summary.json.sig ]; then echo "Eval summary not signed"; exit 2; fi
+      - name: Promote (example)
+        run: |
+          python ops/perception/promote_model_ci.py --model "registry/detector:latest" --eval /tmp/perception_sweep/sweep_summary.json
+
*** End Patch
*** End Patch
