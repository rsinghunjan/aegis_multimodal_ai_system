*** Begin Patch
*** Add File:docs/docling.yml
+# Docling configuration for Aegis
+required_files:
+  - docs/security_approval.txt
+  - docs/legal_approval.txt
+  - docs/runbooks/operator_cloudhsm_auditor_session.md
+  - docs/runbooks/deploy_hardened_services.md
+frontmatter_required:
+  - owner
+  - last_reviewed
+  - approver
+watch_paths:
+  - "services/realtime_retriever/"
+  - "k8s/"
+  - "ops/hsm/"
+  - "ops/math/"
+  - "chart/realtime-retriever/"
+ambiguity:
+  pronoun_threshold: 1
+link_check:
+  timeout_seconds: 3
+  max_links_check: 40
+  allowlist_domains:
+    - "example.com"
+    - "github.com"
+exemptions:
+  - path: "docs/security_approval.txt"
+    reason: "approval artifact"
+
*** End Patch
*** Begin Patch
*** Add File:ops/ci/doclint_runner.py
+#!/usr/bin/env python3
+"""
+Doclint runner for Aegis (basic Docling).
+ - Loads docs/docling.yml
+ - Checks presence of required files when critical paths change
+ - Verifies frontmatter for docs/*.md and docs/runbooks/*.md
+ - Performs simple link health checks on changed markdown files (bounded)
+ - Detects ambiguous text using existing ambiguity_detector and enqueues human reviews
+ - Writes /tmp/doclint_report.json (machine readable) and exits non-zero on hard failures
+"""
+import os
+import sys
+import yaml
+import json
+import glob
+import re
+import subprocess
+from urllib.parse import urlparse
+
+ROOT = os.environ.get("GITHUB_WORKSPACE", os.getcwd())
+CONFIG = os.path.join(ROOT, "docs", "docling.yml")
+REPORT = "/tmp/doclint_report.json"
+
+def load_config():
+    if not os.path.exists(CONFIG):
+        return {}
+    return yaml.safe_load(open(CONFIG))
+
+def changed_files():
+    # try to detect changed files in PR context
+    try:
+        # fetch main for comparison if available
+        subprocess.run(["git", "fetch", "origin", "main"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
+        out = subprocess.check_output(["git", "diff", "--name-only", "origin/main...HEAD"], cwd=ROOT).decode().splitlines()
+        return [p for p in out if p]
+    except Exception:
+        # fallback: list all docs files
+        return []
+
+def check_required_files(cfg, changed):
+    missing = []
+    required = cfg.get("required_files", [])
+    # Only enforce if changes occur under watch_paths (or always if no changed info)
+    watch = cfg.get("watch_paths", [])
+    enforce = False
+    if not changed:
+        enforce = True
+    else:
+        for ch in changed:
+            if any(ch.startswith(w) for w in watch):
+                enforce = True
+                break
+    if not enforce:
+        return {"enforced": False, "missing": []}
+    for f in required:
+        path = os.path.join(ROOT, f)
+        if not os.path.exists(path):
+            missing.append(f)
+    return {"enforced": True, "missing": missing}
+
+def find_markdown_files(changed):
+    # If changed list present, check changed md files; otherwise scan docs
+    files = []
+    if changed:
+        for p in changed:
+            if p.endswith(".md"):
+                files.append(os.path.join(ROOT, p))
+    if not files:
+        # scan docs
+        files = glob.glob(os.path.join(ROOT, "docs", "**", "*.md"), recursive=True)
+    return files
+
+YAML_FRONT_RE = re.compile(r"^\s*---\s*\n(.*?)\n---\s*\n", re.S)
+
+def check_frontmatter(files, cfg):
+    required = cfg.get("frontmatter_required", [])
+    missing = []
+    for f in files:
+        try:
+            txt = open(f, encoding="utf-8").read()
+            m = YAML_FRONT_RE.match(txt)
+            if not m:
+                missing.append({"file": f, "reason": "no_frontmatter"})
+                continue
+            block = m.group(1)
+            meta = yaml.safe_load(block) or {}
+            for r in required:
+                if r not in meta:
+                    missing.append({"file": f, "reason": f"missing_{r}"})
+        except Exception as e:
+            missing.append({"file": f, "reason": f"read_error:{e}"})
+    return missing
+
+LINK_RE = re.compile(r"\[.*?\]\((https?://[^\s\)]+)\)")
+
+def check_links(files, cfg):
+    timeout = cfg.get("link_check", {}).get("timeout_seconds", 3)
+    max_links = cfg.get("link_check", {}).get("max_links_check", 40)
+    allowlist = set(cfg.get("link_check", {}).get("allowlist_domains", []))
+    bad = []
+    cnt = 0
+    import requests
+    for f in files:
+        try:
+            txt = open(f, encoding="utf-8").read()
+        except Exception:
+            continue
+        for m in LINK_RE.finditer(txt):
+            url = m.group(1)
+            dom = urlparse(url).netloc
+            if any(a in dom for a in allowlist):
+                continue
+            cnt += 1
+            if cnt > max_links:
+                return {"skipped_rest": True, "bad": bad}
+            try:
+                r = requests.head(url, timeout=timeout, allow_redirects=True)
+                if r.status_code >= 400:
+                    bad.append({"file": f, "url": url, "status": r.status_code})
+            except Exception as e:
+                bad.append({"file": f, "url": url, "error": str(e)})
+    return {"skipped_rest": False, "bad": bad}
+
+def ambiguity_checks(files, cfg):
+    findings = []
+    try:
+        from ops.verifier.ambiguity_detector import detect_ambiguity, check_and_enqueue
+        from ops.process.human_review_queue import enqueue_review
+    except Exception:
+        # best-effort: implement a simple pronoun check
+        def detect_ambiguity_local(txt):
+            toks = re.findall(r"[A-Za-z]+", txt.lower())
+            pronouns = [t for t in toks if t in {"it","they","them","this","that","he","she","we","you","their","its"}]
+            return {"ambiguous": bool(pronouns), "issues": [{"type":"pronoun","examples": pronouns[:3]}] if pronouns else []}
+        detect_ambiguity = detect_ambiguity_local
+        enqueue_review = lambda item: None
+    threshold = cfg.get("ambiguity", {}).get("pronoun_threshold", 1)
+    for f in files:
+        try:
+            txt = open(f, encoding="utf-8").read()
+            res = detect_ambiguity(txt)
+            if res.get("ambiguous"):
+                # enqueue for human review with metadata
+                meta = {"source_file": f}
+                try:
+                    enqueue_review({"text": txt[:1000], "metadata": meta, "issues": res.get("issues")})
+                except Exception:
+                    pass
+                findings.append({"file": f, "issues": res.get("issues")})
+        except Exception:
+            continue
+    return findings
+
+def main():
+    cfg = load_config()
+    changed = changed_files()
+    rpt = {"changed_files": changed, "checks": {}, "status": "ok"}
+
+    # Required files
+    req = check_required_files(cfg, changed)
+    rpt["checks"]["required_files"] = req
+    if req.get("enforced") and req.get("missing"):
+        rpt["status"] = "fail"
+
+    # Frontmatter checks
+    md_files = find_markdown_files(changed)
+    fm_missing = check_frontmatter(md_files, cfg)
+    rpt["checks"]["frontmatter_missing"] = fm_missing
+    if fm_missing:
+        # treat as warning, not hard fail unless required fields very missing
+        rpt["status"] = rpt["status"] if rpt["status"] == "fail" else "warn"
+
+    # Link checks
+    link_res = check_links(md_files, cfg)
+    rpt["checks"]["link_check"] = link_res
+    if link_res.get("bad"):
+        rpt["status"] = rpt["status"] if rpt["status"] == "fail" else "warn"
+
+    # Ambiguity checks & human review enqueue
+    amb = ambiguity_checks(md_files, cfg)
+    rpt["checks"]["ambiguity_findings"] = amb
+    if amb:
+        rpt["status"] = rpt["status"] if rpt["status"] == "fail" else "warn"
+
+    # write report
+    with open(REPORT, "w") as fh:
+        json.dump(rpt, fh, indent=2)
+    print("Wrote doclint report to", REPORT)
+    # exit codes: 2 for hard fail (missing required files), 1 for warnings, 0 for ok
+    if rpt["status"] == "fail":
+        sys.exit(2)
+    if rpt["status"] == "warn":
+        sys.exit(1)
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/doclint.yml
+name: Doclint & Docs Policy Gate
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+  workflow_dispatch:
+
+jobs:
+  doclint:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install dependencies
+        run: |
+          python -m pip install --upgrade pip
+          pip install pyyaml requests
+      - name: Run doclint runner
+        run: |
+          python ops/ci/doclint_runner.py
+      - name: Run allowlist check (if relevant)
+        run: |
+          python ops/ci/allowlist_check.py || true
+      - name: Run seccomp hardening check (docs manifest)
+        run: |
+          python ops/ci/seccomp_test.py || true
+      - name: Upload doclint report
+        uses: actions/upload-artifact@v4
+        with:
+          name: doclint-report
+          path: /tmp/doclint_report.json
+
*** End Patch
*** End Patch
