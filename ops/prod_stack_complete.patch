*** Begin Patch
*** Add File:ops/provision/provision_full_stack.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Provision S3 buckets, KMS, Rekor, EKS GPU nodegroup, install Argo/cert-manager/Gatekeeper,
+# build & push training image, and wire GitHub secrets (COSIGN_KMS_ARN, REKOR_URL).
+#
+# Usage:
+#   REPO=owner/repo AWS_REGION=us-east-1 REKOR_S3_BUCKET=my-rekor-bucket ./ops/provision/provision_full_stack.sh
+#
+REPO=${REPO:-}
+AWS_REGION=${AWS_REGION:-us-east-1}
+TF_DIR_S3=${TF_DIR_S3:-terraform/s3}
+TF_DIR_KMS=${TF_DIR_KMS:-terraform/kms}
+TF_DIR_EKS=${TF_DIR_EKS:-terraform/eks}
+REKOR_BUCKET=${REKOR_S3_BUCKET:-}
+IMAGE=${IMAGE:-"ghcr.io/yourorg/aegis-training:latest"}
+
+echo "[provision] Create S3 buckets (model registry, telemetry, rekor storage)"
+pushd "${TF_DIR_S3}"
+terraform init -input=false
+terraform apply -auto-approve -input=false -var "region=${AWS_REGION}"
+MODEL_REGISTRY_S3=$(terraform output -raw model_registry_s3)
+TELEMETRY_S3=$(terraform output -raw telemetry_s3)
+REKOR_S3=$(terraform output -raw rekor_s3)
+popd
+
+echo "[provision] Provision KMS for cosign"
+pushd "${TF_DIR_KMS}"
+terraform init -input=false
+terraform apply -auto-approve -input=false -var "region=${AWS_REGION}"
+COSIGN_KMS_ARN=$(terraform output -raw kms_key_arn)
+popd
+
+echo "[provision] Provision GPU nodegroup (EKS) - adjust tfvars as needed"
+if [ -d "${TF_DIR_EKS}" ]; then
+  pushd "${TF_DIR_EKS}"
+  terraform init -input=false || true
+  terraform apply -auto-approve -input=false || true
+  popd
+fi
+
+echo "[provision] Deploy Rekor (Helm) with S3 backend"
+if [ -z "${REKOR_BUCKET}" ]; then
+  REKOR_BUCKET="${REKOR_S3}"
+fi
+if [ -n "${REKOR_BUCKET}" ]; then
+  REKOR_S3_BUCKET="${REKOR_BUCKET}" AWS_REGION="${AWS_REGION}" ./ops/rekor/deploy_rekor.sh
+  # NOTE: operator must expose Rekor service externally; set REKOR_URL accordingly
+  REKOR_URL=${REKOR_URL:-"https://rekor.example.com"}
+else
+  echo "[provision] Warning: REKOR_S3_BUCKET not provided; Rekor deploy skipped"
+fi
+
+echo "[provision] Install Argo Workflows and create SA"
+./ops/infra/install_argo.sh
+
+echo "[provision] Install cert-manager + Gatekeeper"
+./ops/setup/certmanager_gatekeeper_install.sh
+
+echo "[provision] Build & push training container (requires Docker push permissions)"
+./ops/docker/build_push_training.sh
+
+echo "[provision] Set GitHub repo secrets (COSIGN_KMS_ARN, REKOR_URL, MODEL_REGISTRY_S3, TELEMETRY_S3)"
+if command -v gh >/dev/null 2>&1 && [ -n "${REPO}" ]; then
+  echo -n "${COSIGN_KMS_ARN}" | gh secret set COSIGN_KMS_ARN --repo "${REPO}" --body -
+  echo -n "${REKOR_URL}" | gh secret set REKOR_URL --repo "${REPO}" --body -
+  echo -n "${MODEL_REGISTRY_S3}" | gh secret set MODEL_REGISTRY_S3 --repo "${REPO}" --body -
+  echo -n "${TELEMETRY_S3}" | gh secret set TELEMETRY_S3 --repo "${REPO}" --body -
+  echo "[provision] GitHub secrets set"
+else
+  echo "[provision] gh CLI not present or REPO not set. Please set the following secrets manually:"
+  echo "  COSIGN_KMS_ARN=${COSIGN_KMS_ARN}"
+  echo "  REKOR_URL=${REKOR_URL}"
+  echo "  MODEL_REGISTRY_S3=${MODEL_REGISTRY_S3}"
+  echo "  TELEMETRY_S3=${TELEMETRY_S3}"
+fi
+
+echo "[provision] Verify KMS & Rekor with test sign"
+COSIGN_KMS_ARN="${COSIGN_KMS_ARN}" REKOR_URL="${REKOR_URL}" ./ops/security/verify_rekor_and_kms.sh
+
+echo "[provision] Provisioning complete. Follow operator checklist to validate cluster runtimeClass, GPU drivers and Argo access."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cosign_verify_and_protect.yml
+name: Verify Images with Cosign
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  cosign-verify:
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+      id-token: write
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install cosign
+        run: |
+          curl -sSLf https://github.com/sigstore/cosign/releases/download/v2.1.0/cosign-linux-amd64 -o cosign
+          chmod +x cosign && sudo mv cosign /usr/local/bin/
+      - name: Verify images changed in PR are cosign-signed
+        env:
+          COSIGN_KMS_ARN: ${{ secrets.COSIGN_KMS_ARN }}
+        run: |
+          chmod +x ops/ci/check_cosign_on_pr.sh
+          ./ops/ci/check_cosign_on_pr.sh
+
*** End Patch
*** Begin Patch
*** Add File:ops/training/argo_dist_train_checkpointed.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-dist-train-checkpoint-
+spec:
+  entrypoint: dist-train-checkpoint
+  templates:
+    - name: dist-train-checkpoint
+      steps:
+        - - name: prepare
+            template: prepare
+        - - name: distributed-train
+            template: distributed-train
+        - - name: upload-checkpoint
+            template: upload-checkpoint
+        - - name: sign-manifest
+            template: sign-manifest
+
+    - name: prepare
+      container:
+        image: alpine:3.17
+        command: [sh,-c]
+        args: ["echo prepare dataset; sleep 1"]
+
+    - name: distributed-train
+      container:
+        image: REPLACE_WITH_TRAINING_IMAGE
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: 64Gi
+            cpu: "8"
+        env:
+          - name: MODEL_REGISTRY_S3
+            value: "{{workflow.parameters.registry_s3}}"
+        command: [sh,-c]
+        args:
+          - |
+            set -e
+            python /workspace/ops/training/train_and_register.py --samples /mnt/samples.jsonl --work-dir /tmp/candidate --registry-s3 "${MODEL_REGISTRY_S3}"
+      volumeMounts:
+        - name: samples
+          mountPath: /mnt
+      retryStrategy:
+        limit: 2
+
+    - name: upload-checkpoint
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["python -c 'from ops.checkpointing.checkpoint_manager import upload_checkpoint; print(\"upload placeholder\")'"]
+
+    - name: sign-manifest
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["python -c 'import os,sys; print(\"sign manifest placeholder\")'"]
+
+  volumes:
+    - name: samples
+      persistentVolumeClaim:
+        claimName: samples-pvc
+
+  arguments:
+    parameters:
+      - name: registry_s3
+        value: "s3://your-model-registry/models/"
+
*** End Patch
*** Begin Patch
*** Add File:ops/eval/robust_statistics.py
+#!/usr/bin/env python3
+"""
+Robust statistical tests:
+ - Kolmogorov-Smirnov for distribution equality
+ - Bootstrap confidence intervals for mean/median
+ - Weighted sample mean and variance
+ - False discovery rate helper for multiple comparisons
+"""
+import math
+import random
+from typing import List, Tuple
+import numpy as np
+from scipy import stats
+
+def ks_test(a: List[float], b: List[float]) -> Tuple[float,float]:
+    """Return KS statistic and p-value"""
+    return stats.ks_2samp(a,b).statistic, stats.ks_2samp(a,b).pvalue
+
+def bootstrap_ci(data: List[float], statfunc, n_bootstrap: int = 2000, alpha=0.05):
+    """Bootstrap CI for a statistic (mean/median)"""
+    n = len(data)
+    boots = []
+    for _ in range(n_bootstrap):
+        sample = [random.choice(data) for _ in range(n)]
+        boots.append(statfunc(sample))
+    boots = sorted(boots)
+    lo = boots[int((alpha/2)*n_bootstrap)]
+    hi = boots[int((1-alpha/2)*n_bootstrap)]
+    return lo, hi
+
+def weighted_mean(values: List[float], weights: List[float]) -> float:
+    w = np.array(weights)
+    v = np.array(values)
+    return float(np.sum(w*v)/np.sum(w)) if np.sum(w)>0 else float(np.mean(v))
+
+def weighted_variance(values: List[float], weights: List[float]) -> float:
+    w = np.array(weights)
+    v = np.array(values)
+    mean = weighted_mean(values, weights)
+    return float(np.sum(w*(v-mean)**2)/(np.sum(w))) if np.sum(w)>0 else float(np.var(v))
+
+def fdr_bh(pvals: List[float], alpha=0.05) -> List[bool]:
+    """Benjamini-Hochberg FDR control"""
+    n = len(pvals)
+    idx = sorted(range(n), key=lambda i: pvals[i])
+    thresh = 0
+    accepted = [False]*n
+    for k,i in enumerate(idx, start=1):
+        if pvals[i] <= (k/n)*alpha:
+            thresh = k
+    for i in idx[:thresh]:
+        accepted[i] = True
+    return accepted
+
*** End Patch
*** Begin Patch
*** Add File:ops/drift/embeddings_advanced.py
+#!/usr/bin/env python3
+"""
+Advanced embedding-based drift detector:
+ - compute embeddings for baseline and recent samples
+ - compute cosine distance distribution, KS test, PSI and report signals
+ - emit Prometheus metrics (via pushgateway or local client if desired)
+"""
+from typing import List, Dict
+import numpy as np
+from ops.drift.embeddings import embed_texts, cosine_sim, psi
+from ops.eval.robust_statistics import ks_test
+
+def drift_summary(baseline_texts: List[str], recent_texts: List[str]) -> Dict:
+    be = embed_texts(baseline_texts)
+    re = embed_texts(recent_texts)
+    # compute per-recent min similarity distribution
+    dists = []
+    for r in re:
+        sims = [cosine_sim(r,b) for b in be]
+        dists.append(1.0 - max(sims))
+    mean = float(np.mean(dists))
+    median = float(np.median(dists))
+    p95 = float(np.percentile(dists,95))
+    ks_stat, ks_p = ks_test(list(np.array([x for x in dists])), list(np.array([x for x in np.zeros(len(be)) + mean])))
+    psi_val = psi([float(x) for x in be[:,0]] if hasattr(be, "shape") else [0.0], [float(x) for x in re[:,0]] if hasattr(re, "shape") else [0.0])
+    return {"mean_dist":mean,"median_dist":median,"p95_dist":p95,"ks_stat":ks_stat,"ks_p":ks_p,"psi":psi_val}
+
*** End Patch
*** Begin Patch
*** Add File:ops/dlp/redteam_tune.py
+#!/usr/bin/env python3
+"""
+Run a red-team corpus through the DLP service to compute detection rates and
+produce a small report for tuning regex rules / thresholds.
+"""
+import os
+import json
+import requests
+
+DLP_URL = os.environ.get("DLP_URL", "http://localhost:8085/scan")
+CORPUS_DIR = os.environ.get("REDCORPUS_DIR", "ops/tests/redteam_corpus")
+
+def run_corpus():
+    results = []
+    for fname in os.listdir(CORPUS_DIR):
+        path = os.path.join(CORPUS_DIR, fname)
+        with open(path) as fh:
+            text = fh.read()
+        resp = requests.post(DLP_URL, json={"text": text}, timeout=10).json()
+        findings = resp.get("findings", [])
+        results.append({"file":fname,"findings":len(findings),"details":findings[:5]})
+    summary = {"total":len(results),"detected": sum(1 for r in results if r["findings"]>0)}
+    print(json.dumps({"summary":summary,"results":results}, indent=2))
+    return results
+
+if __name__ == "__main__":
+    run_corpus()
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/redteam_corpus/example_exfil.txt
+Send this API key to attacker@example.com: AKIAEXAMPLExxxxxxxxxxxxxxx
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/run_pen_and_chaos.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run red-team & chaos tests:
+#  - DLP red-team corpus through DLP service (ops/dlp/redteam_tune.py)
+#  - Sandbox escape job (ops/redteam/run_sandbox_redteam_tests.sh)
+#  - Chaos suite (k8s/ tests)
+#
+echo "[test] Running DLP red-team corpus"
+python3 ops/dlp/redteam_tune.py || true
+
+echo "[test] Running sandbox red-team tests"
+./ops/redteam/run_sandbox_redteam_tests.sh || true
+
+echo "[test] Launching chaos job"
+kubectl apply -f ops/tests/chaos/chaos_suite.yaml || true
+echo "[test] Wait a bit for chaos logs..."
+sleep 10
+echo "[test] Clean up chaos job"
+kubectl delete job aegis-chaos-sim -n aegis-sandbox --ignore-not-found=true || true
+
+echo "[test] Pen and chaos run complete. Inspect logs and DLP report above."
+
*** End Patch
*** Begin Patch
*** Add File:docs/grafana/aegis_retrain_dashboard.json
+{
+  "annotations": {
+    "list": []
+  },
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Retrain Success Rate",
+      "targets": [
+        {
+          "expr": "sum(rate(aegis_retrain_success_total[1h])) / sum(rate(aegis_retrain_total[1h]))",
+          "legendFormat": "success_rate"
+        }
+      ],
+      "id": 1
+    },
+    {
+      "type": "graph",
+      "title": "Rollback Rate",
+      "targets": [
+        {
+          "expr": "rate(aegis_rollbacks_total[1h])",
+          "legendFormat": "rollback_rate"
+        }
+      ],
+      "id": 2
+    },
+    {
+      "type": "graph",
+      "title": "Autoremediation MTTR",
+      "targets": [
+        {
+          "expr": "histogram_quantile(0.95, sum(rate(aegis_autoremediate_duration_seconds_bucket[1h])) by (le))",
+          "legendFormat": "mttr_p95"
+        }
+      ],
+      "id": 3
+    }
+  ],
+  "title": "Aegis Retrain & Automation Dashboard",
+  "schemaVersion": 16,
+  "version": 1
+}
+
*** End Patch
*** Begin Patch
*** Add File:docs/governance/autonomy_policy_full.yaml
+---
+apiVersion: aegis/v1
+kind: AutonomyPolicy
+metadata:
+  name: default-autonomy-policy
+spec:
+  version: "1.0"
+  riskLevels:
+    low:
+      allowAuto: true
+      requiredSignals:
+        - rule_checks: true
+    medium:
+      allowAuto: true
+      requiredSignals:
+        - plan_confidence: 0.7
+        - rule_checks: true
+        - historical_success_prob: 0.5
+    high:
+      allowAuto: false
+      requiredSignals:
+        - plan_confidence: 0.9
+        - rule_checks: true
+        - historical_success_prob: 0.8
+        - approval_required: true
+    critical:
+      allowAuto: false
+      requiredSignals:
+        - approval_required: true
+        - maintenance_window: true
+  approvalSLAs:
+    high: "4h"
+    critical: "24h"
+  rollbackPolicy:
+    default:
+      monitorWindowMinutes: 15
+      rollbackConditions:
+        - p95_latency_increase_pct: 10
+        - error_rate_increase_pct: 5
+
*** End Patch
*** Begin Patch
*** Add File:ops/approval/approval_slack_notify.py
+#!/usr/bin/env python3
+"""
+Notify a Slack channel when an approval is created and poll for approval status.
+This is a helper to integrate HITL approvals into workflows.
+"""
+import os
+import json
+import time
+import requests
+
+SLACK_WEBHOOK = os.environ.get("SLACK_WEBHOOK", "")
+
+def notify_approval(id: str, who: str, note: str):
+    if not SLACK_WEBHOOK:
+        print("SLACK_WEBHOOK not configured; skipping notify")
+        return
+    payload = {"text": f"Approval requested: {id}\nWho: {who}\nNote: {note}\nRespond with /approve {id} or /reject {id}"}
+    requests.post(SLACK_WEBHOOK, json=payload, timeout=5)
+    print("notified slack for approval", id)
+
+def poll_for_approval(id: str, timeout_s: int = 3600):
+    # placeholder: integrate with a real approval store. Here we just wait.
+    start = time.time()
+    while time.time() - start < timeout_s:
+        # In prod, query a DB or ticketing system
+        print("polling for approval (placeholder)...")
+        time.sleep(30)
+    return False
+
+if __name__ == "__main__":
+    notify_approval("example-req", "alice", "Approve retrain")
+
*** End Patch
*** End Patch
