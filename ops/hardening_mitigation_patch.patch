*** Begin Patch
*** Add File:terraform/kms/kms_key.tf
+/*
+Terraform module: KMS key for cosign + restricted usage
+
+Usage:
+  - populate variables (principal_arns: list of ARNs allowed to use the key, e.g. GitHub OIDC role, k8s nodes' role ARNs)
+  - run `terraform init && terraform apply`
+
+Notes:
+  - This key policy is intentionally restrictive: only the account root and specified principals can use Sign/Verify.
+  - Replace placeholders before applying.
+*/
+variable "name" {
+  type    = string
+  default = "aegis-cosign-key"
+}
+variable "principal_arns" {
+  type = list(string)
+  description = "List of IAM principal ARNs allowed to use this KMS key (e.g. GitHub Actions role, IRSA roles)"
+  default = []
+}
+variable "description" {
+  type = string
+  default = "KMS key for cosign signatures for Aegis artifacts"
+}
+provider "aws" {
+  region = var.region != "" ? var.region : "us-east-1"
+}
+resource "aws_kms_key" "cosign" {
+  description             = var.description
+  deletion_window_in_days = 7
+  enable_key_rotation     = true
+
+  policy = jsonencode({
+    Version = "2012-10-17"
+    Id      = "key-policy-aegis-cosign"
+    Statement = [
+      {
+        Sid       = "Allow administration by account root"
+        Effect    = "Allow"
+        Principal = { AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root" }
+        Action    = "kms:*"
+        Resource  = "*"
+      },
+      {
+        Sid    = "Allow use for cosign signing"
+        Effect = "Allow"
+        Principal = { AWS = var.principal_arns }
+        Action = [
+          "kms:Sign",
+          "kms:GetPublicKey",
+          "kms:Verify",
+          "kms:DescribeKey"
+        ]
+        Resource = "*"
+      }
+    ]
+  })
+}
+
+resource "aws_kms_alias" "cosign_alias" {
+  name          = "alias/${var.name}"
+  target_key_id = aws_kms_key.cosign.key_id
+}
+
+data "aws_caller_identity" "current" {}
+
+output "kms_key_id" {
+  value = aws_kms_key.cosign.key_id
+}
+output "kms_key_arn" {
+  value = aws_kms_key.cosign.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/iam/github_oidc_role.tf
+/*
+Terraform module: Create a least-privilege IAM role for GitHub Actions OIDC
+
+Notes:
+ - This module creates an OIDC-assumable role and attaches a narrowly scoped inline policy.
+ - Set variables (ecr_registry_arn, cosign_kms_arn, evidence_bucket_arn, secrets_prefix) before apply.
+*/
+variable "role_name" {
+  type    = string
+  default = "aegis-github-actions-role"
+}
+variable "ecr_registry_arn" {
+  type = string
+}
+variable "cosign_kms_arn" {
+  type = string
+}
+variable "evidence_bucket_arn" {
+  type = string
+}
+variable "secrets_prefix" {
+  type    = string
+  default = "arn:aws:secretsmanager:${var.region}:${data.aws_caller_identity.current.account_id}:secret:aegis/*"
+}
+variable "region" {
+  type    = string
+  default = "us-east-1"
+}
+
+data "aws_caller_identity" "current" {}
+
+data "aws_iam_policy_document" "trust" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = ["arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/token.actions.githubusercontent.com"]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "token.actions.githubusercontent.com:aud"
+      values   = ["sts.amazonaws.com"]
+    }
+  }
+}
+
+resource "aws_iam_role" "github_oidc" {
+  name               = var.role_name
+  assume_role_policy = data.aws_iam_policy_document.trust.json
+}
+
+data "aws_iam_policy_document" "inline" {
+  statement {
+    effect = "Allow"
+    actions = [
+      "ecr:GetAuthorizationToken"
+    ]
+    resources = ["*"]
+  }
+  statement {
+    effect = "Allow"
+    actions = [
+      "ecr:BatchCheckLayerAvailability",
+      "ecr:GetDownloadUrlForLayer",
+      "ecr:PutImage",
+      "ecr:InitiateLayerUpload",
+      "ecr:UploadLayerPart",
+      "ecr:CompleteLayerUpload"
+    ]
+    resources = [var.ecr_registry_arn]
+  }
+  statement {
+    effect = "Allow"
+    actions = [
+      "kms:Sign",
+      "kms:DescribeKey",
+      "kms:GetPublicKey"
+    ]
+    resources = [var.cosign_kms_arn]
+  }
+  statement {
+    effect = "Allow"
+    actions = [
+      "s3:PutObject",
+      "s3:GetObject",
+      "s3:ListBucket"
+    ]
+    resources = [var.evidence_bucket_arn, "${var.evidence_bucket_arn}/*"]
+  }
+  statement {
+    effect = "Allow"
+    actions = [
+      "secretsmanager:GetSecretValue",
+      "secretsmanager:DescribeSecret",
+      "secretsmanager:ListSecretVersionIds"
+    ]
+    resources = [var.secrets_prefix]
+  }
+}
+
+resource "aws_iam_role_policy" "github_oidc_inline" {
+  name   = "${var.role_name}-inline"
+  role   = aws_iam_role.github_oidc.id
+  policy = data.aws_iam_policy_document.inline.json
+}
+
+output "role_arn" {
+  value = aws_iam_role.github_oidc.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/iam/validate_iam_policies.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Validate IAM role permissions using aws iam simulate-principal-policy and Access Analyzer (if available).
+#
+# Usage:
+#   ./ops/iam/validate_iam_policies.sh <role-arn> [--expected-actions-file expected_actions.txt]
+#
+ROLE_ARN=${1:-}
+EXPECTED_FILE=${2:-}
+
+if [ -z "${ROLE_ARN}" ]; then
+  echo "Usage: $0 <role-arn> [expected_actions_file]"
+  exit 2
+fi
+
+echo "[iam-validate] Simulating principal policy for role: ${ROLE_ARN}"
+
+ACTIONS_TO_CHECK=(
+  "ecr:PutImage"
+  "ecr:InitiateLayerUpload"
+  "kms:Sign"
+  "s3:PutObject"
+  "secretsmanager:GetSecretValue"
+)
+
+for a in "${ACTIONS_TO_CHECK[@]}"; do
+  echo "Simulating action: $a"
+  aws iam simulate-principal-policy --policy-source-arn "${ROLE_ARN}" --action-names "${a}" --output json | jq -r '.EvaluationResults[] | "\(.EvalActionName) -> \(.EvalDecision)"'
+done
+
+if [ -n "${EXPECTED_FILE}" ] && [ -f "${EXPECTED_FILE}" ]; then
+  echo "[iam-validate] Checking for unexpected allowed actions (heuristic)"
+  # This is a simple heuristic: list a wide action set and flag any that are allowed but not in expected list.
+  WIDE_ACTIONS=("iam:*" "s3:*" "kms:*" "secretsmanager:*" "ecr:*")
+  for wa in "${WIDE_ACTIONS[@]}"; do
+    echo "Simulating wildcard group: ${wa}"
+    aws iam simulate-principal-policy --policy-source-arn "${ROLE_ARN}" --action-names "${wa}" --output json | jq -r '.EvaluationResults[] | "\(.EvalActionName) -> \(.EvalDecision)"' | sed -n '1,5p' || true
+  done
+  echo "[iam-validate] Manual review recommended: check role policies for over-broad resources or * permissions."
+fi
+
+echo "[iam-validate] If you have AWS Access Analyzer enabled, run a principal access review in console (or use accessanalyzer APIs)."
+
*** End Patch
*** Begin Patch
*** Add File:ops/rekor/deploy_rekor.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy Rekor (Sigstore) via Helm with S3 storage backend.
+#
+# Prereqs:
+#  - helm installed
+#  - S3 bucket exists for rekor storage and env AWS creds configured
+#  - set REKOR_RELEASE_NAME, REKOR_NAMESPACE, REKOR_S3_BUCKET
+#
+RELEASE=${REKOR_RELEASE_NAME:-rekor}
+NS=${REKOR_NAMESPACE:-rekor}
+BUCKET=${REKOR_S3_BUCKET:-REPLACE_REKOR_S3_BUCKET}
+REGION=${AWS_REGION:-us-east-1}
+
+if [ "${BUCKET}" = "REPLACE_REKOR_S3_BUCKET" ]; then
+  echo "Set REKOR_S3_BUCKET env to a real bucket"
+  exit 2
+fi
+
+helm repo add sigstore https://sigstore.github.io/helm-charts
+helm repo update
+
+kubectl create ns "${NS}" || true
+
+cat > /tmp/rekor-values.yaml <<EOF
+replicaCount: 1
+server:
+  storage:
+    type: s3
+    s3:
+      bucket: ${BUCKET}
+      region: ${REGION}
+      prefix: rekor
+EOF
+
+helm upgrade --install "${RELEASE}" sigstore/rekor -n "${NS}" -f /tmp/rekor-values.yaml
+
+echo "Rekor deployed in namespace ${NS}. Configure REKOR_URL to point at the service (ClusterIP/Ingress as appropriate)."
+
*** End Patch
*** Begin Patch
*** Add File:ops/security/verify_rekor_and_kms.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Verify cosign can sign a test blob with KMS and that Rekor receives an entry.
+#
+TEST_FILE=${1:-/tmp/rekor_kms_test.txt}
+TMPSIG=${TEST_FILE}.sig
+
+if [ -z "${COSIGN_KMS_ARN:-}" ]; then
+  echo "Set COSIGN_KMS_ARN in env"
+  exit 2
+fi
+if [ -z "${REKOR_URL:-}" ]; then
+  echo "Set REKOR_URL in env"
+  exit 2
+fi
+
+echo "hello aegis test" > "${TEST_FILE}"
+
+echo "[verify] Signing test blob with cosign (KMS)"
+COSIGN_EXPERIMENTAL=1 REKOR_URL="${REKOR_URL}" cosign sign-blob --kms "${COSIGN_KMS_ARN}" --output-signature "${TMPSIG}" "${TEST_FILE}"
+
+echo "[verify] Verifying signature locally"
+cosign verify-blob --key k8s://"${COSIGN_KMS_ARN}" --signature "${TMPSIG}" "${TEST_FILE}"
+
+echo "[verify] Asking Rekor for recent entries (may require rekorctl)"
+if command -v rekor-cli >/dev/null 2>&1; then
+  rekor-cli get --log-index 0 || true
+else
+  echo "rekor-cli not installed. Use rekor UI/API to inspect entries for the artifact signed."
+fi
+
+echo "[verify] Completed. If cosign verify succeeded and Rekor shows an entry, KMS/Rekor integration works."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constrainttemplate_no_plain_env.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: k8snoPlainEnv
+spec:
+  crd:
+    spec:
+      names:
+        kind: K8sNoPlainEnv
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package k8snoPlainEnv
+
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Pod"
+          containers := input.review.object.spec.containers
+          some i
+          c := containers[i]
+          env := c.env
+          some j
+          e := env[j]
+          # reject any literal env value that contains sensitive patterns
+          contains_sensitive(e.value)
+          msg := sprintf("container %v has plain-text env var '%v' that appears sensitive; use secrets instead", [c.name, e.name])
+        }
+
+        contains_sensitive(val) {
+          not is_null(val)
+          re_match("(?i).*((passwd|password|secret|token|key|api[-_]?key)).*", val)
+        }
+
+        is_null(x) {
+          x == null
+        }
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_no_plain_env.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sNoPlainEnv
+metadata:
+  name: disallow-plain-env-secrets
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["!kube-system", "!kube-public"]
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constrainttemplate_runtimeclass.yaml
+apiVersion: templates.gatekeeper.sh/v1beta1
+kind: ConstraintTemplate
+metadata:
+  name: restrictRuntimeClassUsage
+spec:
+  crd:
+    spec:
+      names:
+        kind: RestrictRuntimeClass
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package restrictRuntimeClass
+
+        violation[{"msg": msg}] {
+          input.review.kind.kind == "Pod"
+          rc := input.review.object.spec.runtimeClassName
+          rc != ""
+          not allowed_namespace(input.review.object.metadata.namespace)
+          msg := sprintf("runtimeClass '%v' is restricted to specific namespaces", [rc])
+        }
+
+        allowed_namespace(ns) {
+          ns == "aegis-sandbox"
+        }
+
*** End Patch
*** Begin Patch
*** Add File:k8s/gatekeeper/constraint_runtimeclass.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RestrictRuntimeClass
+metadata:
+  name: restrict-runtimeclass-to-sandbox
+spec:
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces: ["!kube-system"]
+
*** End Patch
*** Begin Patch
*** Add File:k8s/policy/cert-manager-clusterissuer.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: aegis-org-ca
+spec:
+  ca:
+    secretName: org-ca
+
+# NOTE: Create the 'org-ca' secret in 'cert-manager' namespace with your organization's CA cert/key before using.
+
*** End Patch
*** Begin Patch
*** Add File:ops/deploy/admission_with_certmanager.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Deploy a Certificate for the admission webhook using cert-manager and patch the webhook's caBundle.
+#
+# Usage:
+#  ./ops/deploy/admission_with_certmanager.sh <webhook-service-name> <webhook-namespace> <webhook-secret-name>
+#
+WEBHOOK_SVC=${1:-aegis-admission-webhook}
+WEBHOOK_NS=${2:-aegis}
+WEBHOOK_SECRET=${3:-aegis-admission-tls}
+
+if ! command -v kubectl >/dev/null 2>&1; then
+  echo "kubectl required"
+  exit 2
+fi
+if ! command -v jq >/dev/null 2>&1; then
+  echo "jq required"
+  exit 2
+fi
+
+cat > /tmp/webhook-cert.yaml <<EOF
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: ${WEBHOOK_SECRET}
+  namespace: ${WEBHOOK_NS}
+spec:
+  secretName: ${WEBHOOK_SECRET}
+  commonName: ${WEBHOOK_SVC}.${WEBHOOK_NS}.svc
+  dnsNames:
+    - ${WEBHOOK_SVC}.${WEBHOOK_NS}.svc
+  issuerRef:
+    name: aegis-org-ca
+    kind: ClusterIssuer
+EOF
+
+kubectl apply -f /tmp/webhook-cert.yaml
+echo "Waiting for certificate to be ready..."
+kubectl -n "${WEBHOOK_NS}" wait --for=condition=ready certificate/"${WEBHOOK_SECRET}" --timeout=120s
+
+CA_BUNDLE=$(kubectl -n "${WEBHOOK_NS}" get secret "${WEBHOOK_SECRET}" -o jsonpath='{.data.ca\.crt}')
+if [ -z "${CA_BUNDLE}" ]; then
+  echo "Failed to read CA bundle from secret ${WEBHOOK_SECRET}"
+  exit 2
+fi
+
+echo "Patching ValidatingWebhookConfiguration aegis-carbon-webhook (example). Replace name as needed."
+WEBHOOK_NAME=${VALIDATING_WEBHOOK_NAME:-aegis-carbon-webhook}
+kubectl patch validatingwebhookconfiguration "${WEBHOOK_NAME}" --type='json' -p "[{\"op\":\"replace\",\"path\":\"/webhooks/0/clientConfig/caBundle\",\"value\":\"${CA_BUNDLE}\"}]" || true
+
+echo "Patched webhook ${WEBHOOK_NAME} with cert-manager CA bundle (verify configuration manually)."
+
*** End Patch
*** Begin Patch
*** Add File:ops/dlp/dlp_service.py
+#!/usr/bin/env python3
+"""
+Pluggable DLP service.
+
+- Provides a simple HTTP endpoint to scan text with regex rules and optional integrations:
+  - regex scanning (default)
+  - optional GCP DLP integration (if GOOGLE_CREDENTIALS env set)
+  - optional placeholder for AWS Macie (operator must implement S3-based scan)
+
+Usage:
+  python ops/dlp/dlp_service.py
+
+POST /scan { "text": "..." } -> JSON { "findings": [...] }
+"""
+import os
+import re
+import json
+from http.server import BaseHTTPRequestHandler, HTTPServer
+
+PATTERNS = {
+    "email": re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+"),
+    "ssn": re.compile(r"\b\d{3}-\d{2}-\d{4}\b"),
+    "credit_card": re.compile(r"\b(?:\d[ -]*?){13,16}\b"),
+    "aws_access_key": re.compile(r"AKIA[0-9A-Z]{16}"),
+}
+
+GCP_DLP_PROJECT = os.environ.get("GCP_DLP_PROJECT", "")
+
+class SimpleHandler(BaseHTTPRequestHandler):
+    def _send(self, code, data):
+        self.send_response(code)
+        self.send_header("Content-Type", "application/json")
+        self.end_headers()
+        self.wfile.write(json.dumps(data).encode())
+
+    def do_POST(self):
+        if self.path != "/scan":
+            return self._send(404, {"error":"not found"})
+        length = int(self.headers.get("content-length", "0"))
+        body = self.rfile.read(length)
+        payload = json.loads(body)
+        text = payload.get("text","")
+        findings = []
+        for name,pat in PATTERNS.items():
+            for m in pat.finditer(text):
+                findings.append({"type":name,"match":m.group(0),"start":m.start(),"end":m.end()})
+        # Optional: plug into GCP DLP if configured
+        if GCP_DLP_PROJECT:
+            try:
+                from google.cloud import dlp_v2
+                dlp = dlp_v2.DlpServiceClient()
+                inspect_config = {"info_types": [{"name": k} for k in []], "min_likelihood": "POSSIBLE"}
+                item = {"value": text}
+                parent = f"projects/{GCP_DLP_PROJECT}"
+                resp = dlp.inspect_content(request={"parent":parent,"inspect_config":inspect_config,"item":item})
+                for r in resp.result.findings:
+                    findings.append({"type":"gcp_dlp","infoType":r.info_type.name,"likelihood":r.likelihood.name,"quote":r.quote})
+            except Exception:
+                pass
+        return self._send(200, {"findings": findings})
+
+def run(port=8085):
+    server = HTTPServer(("0.0.0.0", port), SimpleHandler)
+    print(f"DLP service running on :{port}")
+    server.serve_forever()
+
+if __name__ == "__main__":
+    run(int(os.environ.get("DLP_PORT", "8085")))
+
*** End Patch
*** Begin Patch
*** Add File:docs/hardening/mitigation_summary.md
+# Risk mitigation summary â€” Aegis
+
+This patch implements infrastructure, policy and runtime mitigations for five principal risks:
+
+1) Keys & trust
+  - Terraform: kms_key.tf to create a KMS key with a restrictive key policy and outputs for COSIGN_KMS_ARN.
+  - Rekor: ops/rekor/deploy_rekor.sh to deploy Rekor (helm) with S3 storage backend, and ops/security/verify_rekor_and_kms.sh to validate sign->rekor behavior.
+
+2) Privilege creep
+  - Terraform: github_oidc_role.tf creates a narrowly-scoped OIDC role for GitHub Actions (ECR push, kms:Sign on specified key, limited S3 and SecretsManager access).
+  - Validation: ops/iam/validate_iam_policies.sh to simulate/principal-policy checks and flag overly-broad permissions.
+
+3) Sandbox escape
+  - RuntimeClass & sandbox namespace manifests (runtimeclass-sandbox.yaml added earlier).
+  - Gatekeeper constraint to restrict runtimeClass usage to aegis-sandbox (k8s/gatekeeper/*).
+  - k8s security artifacts: seccomp and AppArmor profiles included earlier in k8s/security/.
+  - Launcher: ops/agents/k8s_job_launcher.py (in previous patch) runs tool steps as restricted pods in sandbox namespace; sandbox job template uses small resource limits and RuntimeClass.
+
+4) Data leakage / DLP
+  - DLP service: ops/dlp/dlp_service.py provides regex-based scanning and optional GCP DLP integration. Use this as a gating check for prompts/outputs before logging or storage.
+  - Gatekeeper constraint to forbid literal sensitive env variables in pod manifests (k8s/gatekeeper/constrainttemplate_no_plain_env.yaml).
+
+5) Policy enforcement & PKI
+  - cert-manager ClusterIssuer manifest (k8s/policy/cert-manager-clusterissuer.yaml) and script ops/deploy/admission_with_certmanager.sh to provision a Certificate for the admission webhook and inject the CA bundle into the webhook.
+  - This avoids ad-hoc self-signed certs and enables integration with organization PKI (create secret 'org-ca' with CA cert/key).
+
+How to use
+ - Provision KMS via Terraform: terraform -chdir=terraform/kms init && terraform -chdir=terraform/kms apply -var='principal_arns=["arn:aws:iam::...:role/..."]'
+ - Create OIDC role with least-privilege using terraform/iam/github_oidc_role.tf (supply ECR/KMS/S3 ARNs).
+ - Run ops/rekor/deploy_rekor.sh to deploy Rekor (configure S3).
+ - Configure GitHub secrets: COSIGN_KMS_ARN, REKOR_URL, EVIDENCE_BUCKET; run ops/security/verify_rekor_and_kms.sh
+ - Deploy Gatekeeper constraint templates & constraints (apply YAMLs in k8s/gatekeeper/). Gatekeeper must be installed in cluster.
+ - Deploy cert-manager and create 'org-ca' secret; then run ops/deploy/admission_with_certmanager.sh to issue a webhook certificate and patch webhook config.
+ - Deploy the DLP service and call /scan on prompts and outputs as part of pre-send/post-receive processing.
+
+Next improvements
+ - Automate IAM policy verification in CI for any role changes; integrate with Access Analyzer.
+ - Add tests that attempt sandbox escape primitives in a controlled red-team job and verify enforcement.
+ - Extend DLP service to integrate with enterprise Macie or on-prem DLP solutions and add rate limiting & quota for scanning.
+ - Provide a Helm chart to package the DLP service and Rekor deployment values for reproducible installs.
+
*** End Patch
*** End Patch
