*** Begin Patch
*** Add File:ops/security/cosign_rekor_verify.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Verify cosign/KMS + Rekor integration for an image or a blob.
+# Usage:
+#   COSIGN_KMS_ARN=arn:aws:kms:... REKOR_URL=https://rekor.example.com ./ops/security/cosign_rekor_verify.sh <image-or-blob>
+IMAGE_OR_BLOB=${1:-}
+if [ -z "${IMAGE_OR_BLOB}" ]; then
+  echo "Usage: $0 <image-ref-or-blob-file>"
+  exit 2
+fi
+
+if ! command -v cosign >/dev/null 2>&1; then
+  echo "cosign not installed; please install cosign"
+  exit 2
+fi
+
+if [ -n "${COSIGN_KMS_ARN:-}" ]; then
+  echo "[cosign] Using KMS: ${COSIGN_KMS_ARN}"
+fi
+if [ -n "${REKOR_URL:-}" ]; then
+  echo "[rekor] Using Rekor URL: ${REKOR_URL}"
+fi
+
+echo "[verify] Running cosign verify (may use Rekor)."
+if [[ "${IMAGE_OR_BLOB}" == *.tar || -f "${IMAGE_OR_BLOB}" ]]; then
+  cosign verify-blob --key k8s://${COSIGN_KMS_ARN} --signature ${IMAGE_OR_BLOB}.sig "${IMAGE_OR_BLOB}" || true
+else
+  cosign verify --key k8s://${COSIGN_KMS_ARN} "${IMAGE_OR_BLOB}" || true
+fi
+
+echo "[verify] If cosign verify indicated Rekor entries, they are recorded by Rekor. Use rekor-cli or Rekor UI for deeper inspection."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/external-secrets/externalsecret-production.yaml
+apiVersion: external-secrets.io/v1beta1
+kind: ExternalSecret
+metadata:
+  name: aegis-runtime-secrets
+  namespace: aegis
+spec:
+  refreshInterval: "1m"
+  secretStoreRef:
+    name: aws-secrets
+    kind: SecretStore
+  target:
+    name: aegis-runtime-secrets
+    creationPolicy: Owner
+  data:
+    - secretKey: OPENAI_API_KEY
+      remoteRef:
+        key: aegis/keys/openai
+    - secretKey: HF_API_TOKEN
+      remoteRef:
+        key: aegis/keys/hf
+    - secretKey: COSIGN_KMS_ARN
+      remoteRef:
+        key: aegis/cosign
+    - secretKey: EVIDENCE_BUCKET
+      remoteRef:
+        key: aegis/s3/evidence
+
*** End Patch
*** Begin Patch
*** Add File:ops/secrets/rotate_all_secrets.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate and audit multiple secrets and ensure ExternalSecrets is refreshed.
+# This script is a template: operator must supply secure values and not commit secrets.
+#
+if ! command -v aws >/dev/null 2>&1; then
+  echo "aws CLI required"
+  exit 2
+fi
+if [ -z "${EVIDENCE_BUCKET:-}" ]; then
+  echo "Set EVIDENCE_BUCKET env"
+  exit 2
+fi
+
+SECRETS_PREFIX=${SECRETS_PREFIX:-aegis/}
+declare -A SECRETS
+SECRETS["${SECRETS_PREFIX}keys/openai"]="${OPENAI_API_KEY:-REPLACE_OPENAI}"
+SECRETS["${SECRETS_PREFIX}keys/hf"]="${HF_API_TOKEN:-REPLACE_HF}"
+SECRETS["${SECRETS_PREFIX}cosign"]="${COSIGN_KMS_ARN:-REPLACE_COSIGN}"
+SECRETS["${SECRETS_PREFIX}s3/evidence"]="${EVIDENCE_BUCKET}"
+
+for key in "${!SECRETS[@]}"; do
+  val=${SECRETS[$key]}
+  if [ -z "${val}" ]; then
+    echo "Skipping ${key} (no value set)"
+    continue
+  fi
+  echo "[secrets] Putting secret ${key}"
+  aws secretsmanager create-secret --name "${key}" --secret-string "${val}" >/dev/null 2>&1 || aws secretsmanager put-secret-value --secret-id "${key}" --secret-string "${val}" >/dev/null
+  # Audit record
+  ts=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
+  jq -n --arg name "${key}" --arg ts "${ts}" --arg user "$(whoami)" '{secret:$name,rotated_at:$ts,rotated_by:$user}' > /tmp/secret-audit-${key//\//_}.json
+  aws s3 cp /tmp/secret-audit-${key//\//_}.json "s3://${EVIDENCE_BUCKET}/secrets-rotation/" || true
+done
+
+echo "[secrets] Triggering ExternalSecrets refresh (best-effort)"
+kubectl -n aegis delete pod -l app.kubernetes.io/name=kubernetes-external-secrets --ignore-not-found || true
+echo "[secrets] Done. Verify k8s secret aegis-runtime-secrets appears or ExternalSecrets logs show sync."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/aegis-seccomp.json
+{
+  "defaultAction": "SCMP_ACT_ERRNO",
+  "architectures": ["SCMP_ARCH_X86_64"],
+  "syscalls": [
+    {
+      "names": [
+        "execve",
+        "ptrace",
+        "kexec_load"
+      ],
+      "action": "SCMP_ACT_ERRNO"
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/aegis-apparmor.txt
+# AppArmor profile for sandbox runner
+profile aegis-sandbox flags=(attach_disconnected,mediate_deleted) {
+  file,
+  network,
+  capability,
+  deny /proc/sys/** w,
+  deny /sys/** w,
+  /bin/** rix,
+  /usr/** rix,
+  /etc/** r,
+  /tmp/** rw,
+}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/runtimeclass-sandbox.yaml
+apiVersion: node.k8s.io/v1
+kind: RuntimeClass
+metadata:
+  name: aegis-sandbox-rc
+handler: kata-qemu
+---
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-sandbox
+  labels:
+    pod-security.kubernetes.io/enforce: restricted
+    pod-security.kubernetes.io/audit: restricted
+    pod-security.kubernetes.io/warn: baseline
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: sandbox-runner
+  namespace: aegis-sandbox
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: sandbox-runner-role
+  namespace: aegis-sandbox
+rules:
+  - apiGroups: [""]
+    resources: ["pods","pods/log","jobs","secrets"]
+    verbs: ["create","get","list","watch","delete","patch"]
+  - apiGroups: ["batch"]
+    resources: ["jobs"]
+    verbs: ["create","get","list","watch","delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: sandbox-runner-rb
+  namespace: aegis-sandbox
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: sandbox-runner-role
+subjects:
+  - kind: ServiceAccount
+    name: sandbox-runner
+    namespace: aegis-sandbox
+
+---
+apiVersion: v1
+kind: ResourceQuota
+metadata:
+  name: sandbox-quota
+  namespace: aegis-sandbox
+spec:
+  hard:
+    requests.cpu: "4"
+    requests.memory: 16Gi
+    limits.cpu: "8"
+    limits.memory: 32Gi
+
+---
+apiVersion: v1
+kind: LimitRange
+metadata:
+  name: sandbox-limits
+  namespace: aegis-sandbox
+spec:
+  limits:
+    - default:
+        cpu: "500m"
+        memory: "512Mi"
+      defaultRequest:
+        cpu: "250m"
+        memory: "256Mi"
+      type: Container
+
*** End Patch
*** Begin Patch
*** Add File:k8s/security/sandbox-tool-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: sandbox-tool-run-{{JOB_ID}}
+  namespace: aegis-sandbox
+spec:
+  backoffLimit: 0
+  template:
+    metadata:
+      labels:
+        app: sandbox-tool-run
+    spec:
+      runtimeClassName: aegis-sandbox-rc
+      serviceAccountName: sandbox-runner
+      securityContext:
+        runAsNonRoot: true
+        runAsUser: 1000
+        readOnlyRootFilesystem: true
+      containers:
+        - name: runner
+          image: REPLACE_IMAGE_REGISTRY/aegis-tool-runner:latest
+          command: ["sh","-c","/runner/run_tool.sh"]
+          env:
+            - name: TOOL_PARAMS_JSON
+              valueFrom:
+                secretKeyRef:
+                  name: sandbox-tool-params
+                  key: params.json
+          resources:
+            requests:
+              cpu: "250m"
+              memory: "256Mi"
+            limits:
+              cpu: "1"
+              memory: "1Gi"
+      restartPolicy: Never
+
*** End Patch
*** Begin Patch
*** Add File:ops/agents/k8s_job_launcher.py
+#!/usr/bin/env python3
+"""
+Create and monitor a Kubernetes Job that runs a tool step inside the sandbox namespace.
+This moves tool execution out of the control plane process and into a restricted pod.
+
+Requirements: kubernetes Python client configured (KUBECONFIG) and appropriate RBAC.
+"""
+import os
+import json
+import time
+from kubernetes import client, config
+
+SANDBOX_NS = os.environ.get("SANDBOX_NAMESPACE", "aegis-sandbox")
+JOB_TEMPLATE = "k8s/security/sandbox-tool-job.yaml"
+
+def load_job_template():
+    with open(JOB_TEMPLATE) as fh:
+        return fh.read()
+
+def create_job(job_id: str, params: dict, image_registry: str):
+    config.load_kube_config()
+    batch = client.BatchV1Api()
+    core = client.CoreV1Api()
+    # create secret with params
+    params_secret = client.V1Secret(
+        metadata=client.V1ObjectMeta(name=f"sandbox-tool-params-{job_id}", namespace=SANDBOX_NS),
+        string_data={"params.json": json.dumps(params)}
+    )
+    try:
+        core.create_namespaced_secret(SANDBOX_NS, params_secret)
+    except client.exceptions.ApiException:
+        core.replace_namespaced_secret(params_secret.metadata.name, SANDBOX_NS, params_secret)
+
+    # render job manifest
+    tmpl = load_job_template()
+    manifest = tmpl.replace("{{JOB_ID}}", job_id).replace("REPLACE_IMAGE_REGISTRY", image_registry)
+    job_obj = client.ApiClient().deserialize(client.ApiClient().sanitize_for_serialization(client.ApiClient()._ApiClient__deserialize_model(manifest, None)), object)
+    # Simpler: use CoreV1Api to create from yaml via utils
+    from kubernetes.utils import create_from_yaml
+    # create_from_yaml expects a file, write to tmp
+    import tempfile
+    f = tempfile.NamedTemporaryFile("w", delete=False)
+    f.write(manifest)
+    f.flush()
+    f.close()
+    create_from_yaml(client.ApiClient(), f.name, namespace=SANDBOX_NS)
+    os.unlink(f.name)
+    return f"sandbox-tool-run-{job_id}"
+
+def wait_for_job(job_name: str, timeout: int = 120):
+    config.load_kube_config()
+    batch = client.BatchV1Api()
+    start = time.time()
+    while time.time() - start < timeout:
+        resp = batch.read_namespaced_job_status(job_name, SANDBOX_NS)
+        if resp.status.succeeded and resp.status.succeeded >= 1:
+            return True
+        if resp.status.failed and resp.status.failed > 0:
+            return False
+        time.sleep(2)
+    return False
+
+if __name__ == "__main__":
+    import sys, uuid
+    job_id = uuid.uuid4().hex[:8]
+    params = {"tool":"echo","params":{"message":"hello from k8s job"}}
+    image_registry = os.environ.get("IMAGE_REGISTRY", "ghcr.io/rsinghunjan")
+    job_name = create_job(job_id, params, image_registry)
+    ok = wait_for_job(job_name)
+    print("job done:", ok)
+
*** End Patch
*** Begin Patch
*** Add File:ops/observability/model_metrics_exporter.py
+#!/usr/bin/env python3
+"""
+Lightweight model inference metrics exporter.
+- Wraps inference calls and exposes Prometheus metrics:
+  - aegis_inference_latency_seconds (histogram)
+  - aegis_inference_request_total (counter)
+  - aegis_model_output_size_bytes (histogram)
+  - aegis_model_quality_similarity (gauge) [optional: when baseline present]
+
+Usage: run alongside inference adapter and call instrumented_request(url, payload)
+"""
+from prometheus_client import start_http_server, Histogram, Counter, Gauge
+import time
+import requests
+import os
+import json
+
+LATENCY = Histogram("aegis_inference_latency_seconds", "Inference request latency seconds", ["model"])
+REQS = Counter("aegis_inference_request_total", "Inference requests total", ["model","status"])
+OUTSIZE = Histogram("aegis_model_output_size_bytes", "Model output size in bytes", ["model"])
+SIMIL = Gauge("aegis_model_quality_similarity", "Similarity between baseline and candidate", ["model","baseline"])
+
+BASELINE_URL = os.environ.get("BASELINE_ADAPTER_URL", "")
+
+def instrumented_request(model: str, url: str, payload: dict):
+    start = time.time()
+    try:
+        r = requests.post(f"{url}/v1/complete", json=payload, timeout=30)
+        latency = time.time() - start
+        LATENCY.labels(model=model).observe(latency)
+        status = "ok" if r.status_code == 200 else "error"
+        REQS.labels(model=model, status=status).inc()
+        out = r.text
+        OUTSIZE.labels(model=model).observe(len(out.encode("utf-8")))
+        # optional baseline similarity
+        if BASELINE_URL:
+            b = requests.post(f"{BASELINE_URL}/v1/complete", json={"model": payload.get("baseline_model", ""), "prompt": payload.get("prompt","")}, timeout=30)
+            try:
+                s = similarity_score(b.text, out)
+                SIMIL.labels(model=model, baseline="baseline").set(s)
+            except Exception:
+                pass
+        return r
+    except Exception as e:
+        REQS.labels(model=model, status="error").inc()
+        raise
+
+def similarity_score(a: str, b: str) -> float:
+    # naive sequence matcher fallback; replace with embedding-based similarity in production
+    from difflib import SequenceMatcher
+    return SequenceMatcher(None, a, b).ratio()
+
+if __name__ == "__main__":
+    PORT = int(os.environ.get("METRICS_PORT", "9000"))
+    start_http_server(PORT)
+    print(f"metrics exporter running on :{PORT}")
+    # run a simple loop if needed (not recommended). The inference adapter should call instrumented_request.
+    while True:
+        time.sleep(60)
+
*** End Patch
*** Begin Patch
*** Add File:k8s/monitoring/prometheusrules-aegis.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-inference-alerts
+  namespace: monitoring
+spec:
+  groups:
+    - name: aegis.inference.rules
+      rules:
+        - alert: AegisHighInferenceLatency
+          expr: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le, model)) > 2
+          for: 5m
+          labels:
+            severity: page
+          annotations:
+            summary: "High inference tail latency for model {{ $labels.model }}"
+            description: "95th percentile latency > 2s for model {{ $labels.model }}"
+        - alert: AegisQualityDrift
+          expr: aegis_model_quality_similarity{baseline="baseline"} < 0.75
+          for: 30m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Model quality drift detected"
+            description: "Similarity to baseline fell below 0.75. Investigate drift and consider retraining/rollback."
+        - alert: AegisAgentToolFailures
+          expr: increase(agent_tool_executions_total{success="false"}[5m]) > 5
+          for: 2m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Agent tool failures rate high"
+            description: "Several agent tool executions failed recently; check sandbox logs."
+
*** End Patch
*** Begin Patch
*** Add File:ops/drift/drift_detector.py
+#!/usr/bin/env python3
+"""
+Basic drift detector that computes simple embedding-like vectors (placeholder)
+and emits a Prometheus gauge `aegis_drift_score` with labels model/version.
+
+In production: replace vectorization with a proper embedding model and a historical baseline store.
+"""
+import time
+import os
+from prometheus_client import start_http_server, Gauge
+import hashlib
+
+DRIFT = Gauge("aegis_drift_score", "Drift score (0..1) lower means more drift", ["model","version"])
+SAMPLE_DIR = os.environ.get("DRIFT_SAMPLE_DIR", "/data/drift_samples")
+POLL = int(os.environ.get("DRIFT_POLL_SECONDS", "300"))
+
+def pseudo_embed(text: str) -> int:
+    # placeholder: uses hash to create a pseudo-vector scalar
+    h = hashlib.sha256(text.encode()).hexdigest()
+    return int(h[:8], 16) % 1000
+
+def compute_drift(model: str, version: str):
+    # read baseline and new samples (operator must populate)
+    base_file = os.path.join(SAMPLE_DIR, f"{model}_{version}_baseline.txt")
+    new_file = os.path.join(SAMPLE_DIR, f"{model}_{version}_recent.txt")
+    if not os.path.exists(base_file) or not os.path.exists(new_file):
+        return None
+    with open(base_file) as f:
+        base = [line.strip() for line in f if line.strip()]
+    with open(new_file) as f:
+        recent = [line.strip() for line in f if line.strip()]
+    if not base or not recent:
+        return None
+    # compute average pseudo distance
+    diffs = []
+    for a,b in zip(base, recent):
+        diffs.append(abs(pseudo_embed(a)-pseudo_embed(b))/1000.0)
+    score = max(0.0, 1.0 - (sum(diffs)/len(diffs)))
+    return score
+
+if __name__ == "__main__":
+    PORT = int(os.environ.get("DRIFT_METRICS_PORT", "9100"))
+    start_http_server(PORT)
+    print(f"drift detector metrics on :{PORT}")
+    while True:
+        # operator should list models to check; example reads from env
+        models = os.environ.get("DRIFT_MODELS", "")
+        for m in (models.split(",") if models else []):
+            if ":" in m:
+                model,ver = m.split(":",1)
+            else:
+                model,ver = m,"v1"
+            s = compute_drift(model,ver)
+            if s is not None:
+                DRIFT.labels(model=model, version=ver).set(s)
+        time.sleep(POLL)
+
*** End Patch
*** Begin Patch
*** Add File:ops/automation/approval_service.py
+#!/usr/bin/env python3
+"""
+Signed approval service:
+- Create approval requests and sign them using cosign (KMS) or HMAC fallback.
+- Upload approvals to S3 (EVIDENCE_BUCKET) under a well-known prefix.
+- Verification step downloads approval and verifies signature.
+
+Usage:
+  ops/automation/approval_service.py create --id <id> --who alice --note "Approve rollout"
+  ops/automation/approval_service.py verify --id <id>
+"""
+import argparse
+import json
+import os
+import tempfile
+import subprocess
+import time
+
+try:
+    import boto3
+except Exception:
+    boto3 = None
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET", "")
+COSIGN_KMS_ARN = os.environ.get("COSIGN_KMS_ARN", "")
+HMAC_KEY = os.environ.get("PROVENANCE_HMAC_KEY", "")
+
+def create_approval(id: str, who: str, note: str):
+    payload = {"id": id, "who": who, "note": note, "ts": int(time.time())}
+    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
+    tmp.write(json.dumps(payload).encode())
+    tmp.flush()
+    tmp.close()
+    sig_info = {}
+    if COSIGN_KMS_ARN:
+        sigfile = tmp.name + ".sig"
+        cmd = ["cosign", "sign-blob", "--kms", COSIGN_KMS_ARN, "-key", COSIGN_KMS_ARN, "--output-signature", sigfile, tmp.name]
+        subprocess.check_call(cmd)
+        sig_info = {"method":"cosign-kms","signature_file":os.path.basename(sigfile)}
+    else:
+        # HMAC fallback
+        import hmac, hashlib
+        if not HMAC_KEY:
+            raise RuntimeError("No COSIGN_KMS_ARN or PROVENANCE_HMAC_KEY configured")
+        data = open(tmp.name,"rb").read()
+        sig = hmac.new(HMAC_KEY.encode(), data, hashlib.sha256).hexdigest()
+        sig_info = {"method":"hmac","signature_hex":sig}
+    # upload
+    if boto3 and EVIDENCE_BUCKET:
+        s3 = boto3.client("s3")
+        key = f"approvals/{id}.json"
+        s3.upload_file(tmp.name, EVIDENCE_BUCKET, key)
+        if "signature_file" in sig_info:
+            s3.upload_file(sigfile, EVIDENCE_BUCKET, f"{key}.sig")
+        else:
+            s3.put_object(Bucket=EVIDENCE_BUCKET, Key=f"{key}.sig.json", Body=json.dumps(sig_info))
+    print({"payload":payload,"signature":sig_info})
+
+def verify_approval(id: str):
+    if not boto3 or not EVIDENCE_BUCKET:
+        raise RuntimeError("boto3 and EVIDENCE_BUCKET required for verify")
+    s3 = boto3.client("s3")
+    key = f"approvals/{id}.json"
+    local = f"/tmp/{id}.json"
+    s3.download_file(EVIDENCE_BUCKET, key, local)
+    # try cosign verify blob
+    if COSIGN_KMS_ARN and os.path.exists(local + ".sig"):
+        subprocess.check_call(["cosign","verify-blob","--key","k8s://"+COSIGN_KMS_ARN,"--signature", local + ".sig", local])
+        print("cosign verification succeeded")
+        return True
+    else:
+        # hmac verify
+        import hmac, hashlib
+        obj = s3.get_object(Bucket=EVIDENCE_BUCKET, Key=f"{key}.sig.json")
+        siginfo = json.loads(obj['Body'].read())
+        if siginfo.get("method") == "hmac":
+            expected = siginfo["signature_hex"]
+            data = open(local,"rb").read()
+            actual = hmac.new(HMAC_KEY.encode(), data, hashlib.sha256).hexdigest()
+            if actual == expected:
+                print("hmac verification succeeded")
+                return True
+        print("no valid verification available")
+        return False
+
+def main():
+    p = argparse.ArgumentParser()
+    sp = p.add_subparsers(dest="cmd")
+    c = sp.add_parser("create")
+    c.add_argument("--id", required=True)
+    c.add_argument("--who", required=True)
+    c.add_argument("--note", default="")
+    v = sp.add_parser("verify")
+    v.add_argument("--id", required=True)
+    args = p.parse_args()
+    if args.cmd == "create":
+        create_approval(args.id, args.who, args.note)
+    elif args.cmd == "verify":
+        ok = verify_approval(args.id)
+        print("verified:", ok)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/customer_onboarding.md
+# Customer Onboarding Runbook (Aegis)
+
+This runbook provides a checklist and templates for onboarding customers for consulting engagements and managed AIOps.
+
+Steps:
+1. NDA & TOS
+   - Execute NDA and customer-specific TOS (data residency & liability).
+2. Scoping call
+   - Use the PoC template to agree on acceptance criteria (latency, accuracy, cost).
+3. Environment prep
+   - Ensure AWS/GCP/Azure accounts, K8s clusters and network access.
+   - Create a staging namespace; assign platform service account for onboarding.
+4. Security & Compliance
+   - Arrange key management: COSIGN_KMS_ARN, Rekor endpoint, EVIDENCE_BUCKET, PROVENANCE_HMAC_KEY.
+   - Confirm data residency and retention with customer legal.
+5. Secrets & IRSA
+   - Provision secrets in AWS Secrets Manager and grant ExternalSecrets access.
+   - Run ops/secrets/rotate_all_secrets.sh with customer values.
+6. PoC deployment
+   - Deploy model(s) to staging, run quant & A/B validation, and present results.
+7. Handover
+   - Deliver runbooks, SLOs, dashboards, and signed evidence storage info.
+
+Deliverables:
+- SOW (use SOW_TEMPLATE.md)
+- Staging & production manifests
+- Evidence retention policy & bucket info
+- Acceptance test checklist and results
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/compliance_playbook.md
+# Compliance & Evidence Playbook (Aegis)
+
+Core principles:
+- All production promotion actions must be signed and recorded.
+- Secrets must not be stored in git; use ExternalSecrets -> SecretsManager.
+- Evidence (approvals, step records, artifact signatures) stored in EVIDENCE_BUCKET with lifecycle rules.
+
+Playbook:
+1. Promotion: ensure image is cosign-signed and Rekor entry exists. CI must assert this.
+2. Approval: require an approval created with ops/automation/approval_service.py create --id <id> --who <user> --note "..."
+3. Evidence: executor signs step pre/post and uploads to S3 using step_provenance.
+4. Retention: run ops/provenance/retention_policy.sh quarterly to archive evidence older than configured retention days.
+5. Audit: sample verify signatures (ops/security/cosign_rekor_verify.sh) and approvals (ops/automation/approval_service.py verify).
+
+Legal & Data Residency:
+- Maintain mapping of EVIDENCE_BUCKET → region and ensure it aligns with customer residency constraints.
+- Document deletion/retention requests and tie to legal approvals.
+
*** End Patch
*** Begin Patch
*** Add File:docs/SOW_TEMPLATE.md
+# Statement of Work (SOW) Template — Aegis Consulting
+
+Project: <Project Name>
+Customer: <Customer Name>
+Duration: <weeks>
+
+Scope:
+- Deliver PoC for <use-case> using Aegis platform (staging).
+- Build model artifact, quantization, A/B validation, infra deployment & monitoring.
+- Provide runbooks and handover.
+
+Acceptance Criteria:
+- Model latency < X ms p50, p95 < Y ms.
+- Similarity/AUC >= threshold on validation tests.
+- Signed artifacts in evidence bucket and Rekor/cosign verification pass.
+
+Deliverables:
+- Staging deployment manifests
+- CI pipeline configured (cosign/KMS)
+- Operator runbook and compliance playbook
+- Final report and knowledge transfer session
+
+Pricing & Payment:
+- Retainer: $...
+- Milestone payments: ...
+
+Change control, liability & termination: see contract annex.
+
*** End Patch
*** End Patch
