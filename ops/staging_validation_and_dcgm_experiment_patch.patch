*** Begin Patch
*** Add File:ops/validation/stage_validation_runner.py
+#!/usr/bin/env python3
+"""
+Stage validation runner
+
+Runs a sequence of smoke and validation checks for a staging environment and
+emits a consolidated JSON report. Checks:
+ - Carbon attribution validation (ops/carbon/dcgm_validation.py)
+ - Milvus load test (k8s/milvus/load_test_job -> ops/vector/milvus/milvus_load_test.py)
+ - RAG + LLM smoke (ops/validation/assertion_runner.py rag)
+ - Agentic shadow workflow existence (assertion_runner.py agentic)
+
+This script assumes kubectl and (optionally) argo CLI are available in PATH,
+and that the repo files are mounted or available on the runner filesystem.
+It can be run locally with kubeconfig or in-cluster.
+"""
+import os
+import sys
+import subprocess
+import json
+import time
+from datetime import datetime
+
+ROOT = os.path.join(os.path.dirname(__file__), "..")
+ROOT = os.path.abspath(ROOT)
+
+def run_cmd(cmd, timeout=600):
+    print(f"[run] {cmd}")
+    try:
+        res = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
+        return {"rc": res.returncode, "stdout": res.stdout, "stderr": res.stderr}
+    except subprocess.TimeoutExpired as e:
+        return {"rc": 124, "stdout": "", "stderr": f"timeout after {timeout}s"}
+
+def step_carbon():
+    # run dcgm_validation.py
+    script = os.path.join(ROOT, "carbon", "dcgm_validation.py")
+    if not os.path.exists(script):
+        return {"ok": False, "error": "dcgm_validation.py not found"}
+    res = run_cmd(f"python {script} --prom ${'PROM_URL' if 'PROM_URL' in os.environ else ''} --telemetry ${'TELEMETRY_DIR' if 'TELEMETRY_DIR' in os.environ else ''}", timeout=120)
+    ok = res["rc"]==0
+    return {"ok": ok, "output": res}
+
+def step_milvus():
+    # create load test job and wait for completion
+    job_yaml = os.path.join(ROOT, "vector", "milvus", "load_test_job.yaml")
+    job_manifest = job_yaml if os.path.exists(job_yaml) else None
+    if not job_manifest:
+        return {"ok": False, "error": "milvus load test manifest not found"}
+    apply = run_cmd(f"kubectl apply -f {job_manifest}")
+    if apply["rc"] != 0:
+        return {"ok": False, "error": "kubectl apply failed", "apply": apply}
+    # wait for job to complete (poll)
+    job_name = "milvus-load-test"
+    timeout = 600
+    start = time.time()
+    while time.time() - start < timeout:
+        status = run_cmd(f"kubectl get job {job_name} -o json || true")
+        if status["rc"] == 0 and '"succeeded":' in status["stdout"]:
+            if '"succeeded": 1' in status["stdout"]:
+                logs = run_cmd(f"kubectl logs job/{job_name} --tail=200")
+                cleanup = run_cmd(f"kubectl delete job {job_name} --ignore-not-found")
+                return {"ok": True, "logs": logs, "apply": apply}
+        time.sleep(5)
+    return {"ok": False, "error": "timed out waiting for milvus job", "apply": apply}
+
+def step_rag_llm():
+    script = os.path.join(ROOT, "validation", "assertion_runner.py")
+    if not os.path.exists(script):
+        return {"ok": False, "error": "assertion_runner.py not found"}
+    res = run_cmd(f"python {script} rag", timeout=60)
+    ok = res["rc"]==0
+    return {"ok": ok, "output": res}
+
+def step_agentic_shadow():
+    # apply shadow workflow then run assertion_runner agentic check
+    shadow_wf = os.path.join(ROOT, "agentic", "shadow_canary.yaml")
+    if os.path.exists(shadow_wf):
+        apply = run_cmd(f"kubectl -n argo apply -f {shadow_wf}")
+    else:
+        apply = {"rc":0,"stdout":"no-shadow-file","stderr":""}
+    script = os.path.join(ROOT, "validation", "assertion_runner.py")
+    res = run_cmd(f"python {script} agentic", timeout=30)
+    ok = res["rc"]==0
+    return {"ok": ok, "apply": apply, "output": res}
+
+def main():
+    report = {"ts": datetime.utcnow().isoformat()+"Z", "results": {}}
+    # Carbon
+    report["results"]["carbon"] = step_carbon()
+    # Milvus
+    report["results"]["milvus"] = step_milvus()
+    # RAG + LLM
+    report["results"]["rag_llm"] = step_rag_llm()
+    # Agentic shadow
+    report["results"]["agentic_shadow"] = step_agentic_shadow()
+
+    out = os.environ.get("STAGE_VALIDATION_OUT","/tmp/aegis_stage_validation_report.json")
+    with open(out, "w") as fh:
+        json.dump(report, fh, indent=2)
+    print("Wrote report to", out)
+    # exit non-zero if any critical check failed
+    critical_fail = any(not report["results"][k].get("ok", False) for k in ["carbon","milvus","rag_llm","agentic_shadow"])
+    if critical_fail:
+        print("One or more checks failed. See report.")
+        sys.exit(2)
+    print("All checks passed")
+    sys.exit(0)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/staging_validation_runbook.md
+Stage Validation Runbook
+=======================
+
+Purpose
+-------
+Run a staged validation that exercises carbon attribution, Milvus availability, RAG+LLM smoke tests and agentic shadow checks. This runbook documents commands, expected outputs and troubleshooting steps.
+
+Prereqs
+--------
+- kubectl configured to target the staging cluster.
+- (Optional) argo CLI configured for the Argo server if you want to inspect workflows.
+- Python 3.11 and basic utilities installed on the runner.
+- Repo checked out or mounted on the runner so scripts in ops/ are available.
+
+Files produced
+--------------
+- Consolidated JSON report: /tmp/aegis_stage_validation_report.json (modifiable via STAGE_VALIDATION_OUT env)
+
+Steps
+-----
+1) Run the stage validation runner (recommended)
+   - Command:
+     python ops/validation/stage_validation_runner.py
+   - Expected result:
+     - Script prints progress for each step and writes a JSON report at /tmp/aegis_stage_validation_report.json.
+     - Exits with code 0 when all checks pass; non-zero otherwise.
+   - Troubleshooting:
+     - If carbon check fails: ensure Prometheus (PROM_URL) and TELEMETRY_DIR are reachable and node reporters are running.
+     - If milvus job times out: inspect job logs: kubectl logs job/milvus-load-test
+
+2) Carbon attribution check (manual)
+   - Ensure DCGM exporter + pid_mapper sidecars are deployed (see k8s manifests).
+   - Run:
+     python ops/carbon/dcgm_validation.py --prom http://prometheus:9090 --telemetry /var/lib/aegis/telemetry
+   - Expected output: JSON with pod attribute counts and sample mismatches (if any).
+   - Troubleshooting:
+     - Verify node-side pid_map files exist: ls /var/lib/aegis/telemetry/pid_map_*
+     - Check dcgm exporter metrics in Prometheus: http://prometheus:9090/graph?g0.expr=dcgm_gpu_power_watts
+
+3) Milvus load test (manual)
+   - Install Milvus (prod/dev depending on staging):
+     ./ops/vector/milvus/install_milvus.sh
+   - Run the load test job:
+     kubectl apply -f ops/vector/milvus/load_test_job.yaml
+   - Monitor job:
+     kubectl logs job/milvus-load-test --follow
+   - Expected: job completes and prints insert/search timing. Check p95 targets in Grafana.
+   - Troubleshooting:
+     - If job fails, inspect events: kubectl describe job/milvus-load-test
+
+4) RAG + LLM smoke
+   - Ensure RETRIEVER_URL and LLM_ADAPTER endpoints are reachable (set via envs or Service DNS).
+   - Run smoke test:
+     python ops/validation/assertion_runner.py rag
+   - Expected: JSON printed with ok:true.
+   - Troubleshooting:
+     - If retriever fails, confirm Milvus service and embeddings service are available.
+     - If LLM adapter fails, check llm adapter deployment logs.
+
+5) Agentic shadow check
+   - Apply the shadow canary workflow if not present:
+     kubectl -n argo apply -f ops/agentic/shadow_canary.yaml
+   - Run quick check:
+     python ops/validation/assertion_runner.py agentic
+   - Expected: ok:true (Argo server reachable)
+   - Troubleshooting:
+     - If Argo unreachable, check argo-server pod and port-forward or expose API.
+
+6) Review consolidated report
+   - Location: /tmp/aegis_stage_validation_report.json
+   - Example fields:
+     {
+       "ts": "...",
+       "results": {
+         "carbon": {"ok": true, "output": {...}},
+         "milvus": {"ok": true, "logs": "..."},
+         "rag_llm": {"ok": true, "output": {...}},
+         "agentic_shadow": {"ok": true}
+       }
+     }
+
+Cleanup
+-------
+- Delete test jobs if needed:
+  kubectl delete job milvus-load-test --ignore-not-found
+- Remove any shadow workflows if undesired:
+  kubectl -n argo delete -f ops/agentic/shadow_canary.yaml --ignore-not-found
+
+Notes & safety
+--------------
+- Run the validation in staging only; avoid running Milvus load tests on production.
+- Ensure token logs and telemetry are handled according to privacy policy.
+
*** End Patch
*** Begin Patch
*** Add File:k8s/carbon/dcgm_test_jobs.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: gpu-burn-1
+  namespace: default
+spec:
+  template:
+    spec:
+      containers:
+        - name: burn
+          image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
+          command: ["sh","-c"]
+          args:
+            - |
+              python - <<'PY'
+import torch, time
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    dev = torch.device("cuda")
    for i in range(60):
        x = torch.randn(2000,2000, device=dev)
+        y = x.matmul(x)
+        time.sleep(0.5)
print("done")
+PY
+      restartPolicy: Never
+  backoffLimit: 0
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: gpu-burn-2
+  namespace: default
+spec:
+  template:
+    spec:
+      containers:
+        - name: burn
+          image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
+          command: ["sh","-c"]
+          args:
+            - |
+              python - <<'PY'
+import torch, time
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
+    dev = torch.device("cuda")
+    for i in range(60):
+        x = torch.randn(1500,1500, device=dev)
+        y = x.matmul(x)
+        time.sleep(0.6)
+print("done")
+PY
+      restartPolicy: Never
+  backoffLimit: 0
+
*** End Patch
*** Begin Patch
*** Add File:ops/dcgm/run_attribution_experiment.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Orchestrate a DCGM attribution experiment in staging:
+#  - Deploy DCGM exporter + pid_mapper DaemonSet (ensure ops/dcgm/device_plugin_sidecar_daemonset.yaml applied)
+#  - Launch test GPU jobs (k8s/carbon/dcgm_test_jobs.yaml)
+#  - Wait for a short period, gather telemetry & pid maps, run dcgm_validation and write report
+#
+PROM_URL=${PROM_URL:-http://prometheus:9090}
+TELEMETRY_DIR=${TELEMETRY_DIR:-/var/lib/aegis/telemetry}
+ATTR_OUT=${ATTR_OUT:-/var/lib/aegis/carbon_by_workflow}
+
+echo "[experiment] Ensure DCGM sidecar and pid-mapper are deployed (apply manifest if not already)"
+kubectl apply -f ops/dcgm/device_plugin_sidecar_daemonset.yaml
+echo "[experiment] Deploy test GPU jobs"
+kubectl apply -f k8s/carbon/dcgm_test_jobs.yaml
+echo "[experiment] Wait 30s for jobs to schedule and produce GPU load..."
+sleep 30
+echo "[experiment] Wait additional 90s to collect metrics"
+sleep 90
+echo "[experiment] Run DCGM validation"
+python ops/carbon/dcgm_validation.py --prom "${PROM_URL}" --telemetry "${TELEMETRY_DIR}" > /tmp/dcgm_validation_output.json || true
+echo "[experiment] Run attribution aggregator (if available)"
+if [ -f ops/carbon/dcgm_attributor.py ]; then
+  python ops/carbon/dcgm_attributor.py > /tmp/dcgm_attributor_out.json || true
+fi
+echo "[experiment] Copy results to ${ATTR_OUT}"
+mkdir -p "${ATTR_OUT}"
+cp /tmp/dcgm_validation_output.json "${ATTR_OUT}/dcgm_validation_$(date +%s).json" || true
+cp /tmp/dcgm_attributor_out.json "${ATTR_OUT}/dcgm_attributor_$(date +%s).json" || true
+echo "[experiment] Cleanup test jobs"
+kubectl delete -f k8s/carbon/dcgm_test_jobs.yaml --ignore-not-found
+echo "[experiment] Done. Reports placed in ${ATTR_OUT}"
+
*** End Patch
*** End Patch
