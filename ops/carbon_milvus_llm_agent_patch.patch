*** Begin Patch
*** Add File:ops/carbon/node_gpu_reporter.py
+#!/usr/bin/env python3
+"""
+Node GPU reporter (daemon) for attributing GPU devices -> pods on the same node.
+Runs as a DaemonSet on GPU nodes. It:
+ - polls nvidia-smi to get per-GPU UUID/power
+ - queries the kube-apiserver for pods scheduled onto this node
+ - attributes GPU usage heuristically to pods that request GPUs
+ - writes a JSONL status report to TELEMETRY_DIR for central aggregator (attrib.py) to consume
+
+This is intentionally conservative (heuristic attribution) — improves attribution accuracy vs node-level only.
+"""
+import os
+import time
+import json
+import subprocess
+from pathlib import Path
+
+TELEMETRY_DIR = os.environ.get("TELEMETRY_DIR", "/var/lib/aegis/telemetry")
+POLL_S = int(os.environ.get("NODE_REPORT_POLL_S", "15"))
+
+try:
+    from kubernetes import client, config
+    config.load_incluster_config()
+    k8s = client.CoreV1Api()
+except Exception:
+    k8s = None
+
+def run_nvidia_smi():
+    try:
+        out = subprocess.check_output(
+            ["nvidia-smi", "--query-gpu=index,uuid,power.draw,utilization.gpu", "--format=csv,noheader,nounits"],
+            text=True,
+            stderr=subprocess.DEVNULL,
+            timeout=5
+        )
+    except Exception:
+        return []
+    res=[]
+    for line in out.splitlines():
+        parts = [p.strip() for p in line.split(",")]
+        if len(parts) >= 4:
+            res.append({"index": parts[0], "uuid": parts[1], "power_w": float(parts[2] or 0.0), "util_pct": float(parts[3] or 0.0)})
+    return res
+
+def list_pods_on_node(node_name):
+    if k8s is None:
+        return []
+    try:
+        pods = k8s.list_pod_for_all_namespaces(field_selector=f"spec.nodeName={node_name}").items
+        out=[]
+        for p in pods:
+            # check if pod requests GPUs or has NVIDIA_VISIBLE_DEVICES env var
+            gpu_req = 0
+            for c in (p.spec.containers or []):
+                resources = c.resources
+                if resources and resources.requests and resources.requests.get("nvidia.com/gpu"):
+                    try:
+                        gpu_req += int(resources.requests.get("nvidia.com/gpu"))
+                    except Exception:
+                        gpu_req += 1
+            out.append({"name": p.metadata.name, "namespace": p.metadata.namespace, "gpu_req": gpu_req, "labels": p.metadata.labels or {}})
+        return out
+    except Exception:
+        return []
+
+def write_report(node_name, gpus, pods):
+    Path(TELEMETRY_DIR).mkdir(parents=True, exist_ok=True)
+    rec = {"ts": int(time.time()), "node": node_name, "gpus": gpus, "pods": pods}
+    fname = os.path.join(TELEMETRY_DIR, f"node_gpu_report_{node_name}_{int(time.time())}.jsonl")
+    with open(fname, "a") as fh:
+        fh.write(json.dumps(rec) + "\n")
+
+def get_node_name():
+    # Kubernetes downward API should set NODE_NAME env in the DaemonSet
+    return os.environ.get("NODE_NAME") or os.uname().nodename
+
+def main():
+    node = get_node_name()
+    while True:
+        gpus = run_nvidia_smi()
+        pods = list_pods_on_node(node)
+        write_report(node, gpus, pods)
+        time.sleep(POLL_S)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/carbon/node-reporter-daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: aegis-node-gpu-reporter
+  namespace: aegis-system
+spec:
+  selector:
+    matchLabels:
+      app: aegis-node-gpu-reporter
+  template:
+    metadata:
+      labels:
+        app: aegis-node-gpu-reporter
+    spec:
+      serviceAccountName: aegis-node-reporter-sa
+      containers:
+        - name: node-gpu-reporter
+          image: python:3.11-slim
+          command: ["sh","-c","pip install kubernetes && python /opt/aegis/node_gpu_reporter.py"]
+          env:
+            - name: NODE_NAME
+              valueFrom:
+                fieldRef:
+                  fieldPath: spec.nodeName
+            - name: TELEMETRY_DIR
+              value: /var/lib/aegis/telemetry
+          volumeMounts:
+            - name: telemetry
+              mountPath: /var/lib/aegis/telemetry
+            - name: dev
+              mountPath: /dev
+          securityContext:
+            runAsUser: 0
+          resources:
+            requests:
+              cpu: "50m"
+              memory: "64Mi"
+      volumes:
+        - name: telemetry
+          hostPath:
+            path: /var/lib/aegis/telemetry
+            type: DirectoryOrCreate
+        - name: dev
+          hostPath:
+            path: /dev
+            type: Directory
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: aegis-node-reporter-sa
+  namespace: aegis-system
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: aegis-node-reporter-role
+rules:
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["get","list","watch"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: aegis-node-reporter-crb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: aegis-node-reporter-role
+subjects:
+  - kind: ServiceAccount
+    name: aegis-node-reporter-sa
+    namespace: aegis-system
+
*** End Patch
*** Begin Patch
*** Add File:ops/vector/milvus/values.yaml
+# Minimal Milvus Helm values for small test cluster. Operators should tune for production (replicas/volume/HA).
+image:
+  tag: "2.2.10"
+persistence:
+  enabled: false
+proxy:
+  service:
+    type: ClusterIP
+standalone:
+  enabled: true
+  persistence:
+    enabled: false
+
*** End Patch
*** Begin Patch
*** Add File:k8s/milvus/install_milvus.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Installs Milvus via Helm using the provided values file.
+#
+NAMESPACE=${NAMESPACE:-milvus}
+helm repo add milvus https://milvus-io.github.io/milvus-helm/
+helm repo update
+kubectl create ns "${NAMESPACE}" || true
+helm upgrade --install milvus milvus/milvus -n "${NAMESPACE}" -f ops/vector/milvus/values.yaml
+echo "Milvus installed in namespace ${NAMESPACE}"
+
*** End Patch
*** Begin Patch
*** Add File:ops/vector/milvus/test_ingest.py
+#!/usr/bin/env python3
+"""
+Test ingestion to Milvus: connects and inserts random vectors then queries.
+Requires pymilvus installed in the environment where this job runs.
+"""
+import os
+import numpy as np
+from pymilvus import connections, CollectionSchema, FieldSchema, DataType, Collection
+
+MILVUS_HOST = os.environ.get("MILVUS_HOST", "milvus.milvus.svc.cluster.local")
+MILVUS_PORT = os.environ.get("MILVUS_PORT", "19530")
+COLLECTION_NAME = os.environ.get("MILVUS_COLLECTION", "aegis_test_vectors")
+
+def main():
+    connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)
+    dim = 64
+    fields = [
+        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
+        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
+        FieldSchema(name="meta", dtype=DataType.VARCHAR, max_length=1024)
+    ]
+    schema = CollectionSchema(fields, description="test")
+    if COLLECTION_NAME in [c.name for c in Collection.list_collections()]:
+        col = Collection(COLLECTION_NAME)
+    else:
+        col = Collection(COLLECTION_NAME, schema)
+    vecs = np.random.rand(100, dim).astype(np.float32).tolist()
+    metas = [f"doc-{i}" for i in range(100)]
+    col.insert([vecs, metas])
+    col.flush()
+    print("inserted 100 vectors")
+    # test search
+    q = np.random.rand(1, dim).astype(np.float32).tolist()
+    res = col.search(q, "embedding", param={"metric_type":"IP","params":{"nprobe":10}}, limit=5, output_fields=["meta"])
+    print("search results example:", [[h.meta for h in hits] for hits in res])
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:k8s/milvus/test-ingest-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: milvus-test-ingest
+  namespace: default
+spec:
+  template:
+    spec:
+      containers:
+        - name: test-ingest
+          image: python:3.11-slim
+          command: ["sh","-c","pip install pymilvus numpy && python /workdir/test_ingest.py"]
+          volumeMounts:
+            - name: workdir
+              mountPath: /workdir
+      restartPolicy: Never
+      volumes:
+        - name: workdir
+          hostPath:
+            path: /tmp/milvus-test
+            type: DirectoryOrCreate
+  backoffLimit: 0
+
*** End Patch
*** Begin Patch
*** Add File:ops/generative/llm_adapter.py
+#!/usr/bin/env python3
+"""
+LLM adapter supporting pluggable providers and streaming.
+ - Providers: local_http (default), openai (if OPENAI_API_KEY present)
+ - Emits token-level logs for auditing
+ - Provides blocking generate() and streaming generate() interfaces
+ - Integrates with DLP pre/post hooks
+"""
+import os
+import json
+import time
+import requests
+from typing import Iterable
+
+PROVIDER = os.environ.get("LLM_PROVIDER", "local_http")
+LOCAL_URL = os.environ.get("LLM_URL", "http://localhost:8080/generate")
+LOCAL_STREAM = os.environ.get("LLM_STREAM_URL", "http://localhost:8080/stream")
+OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
+TOKEN_LOG_DIR = os.environ.get("LLM_TOKEN_LOG_DIR", "/var/lib/aegis/llm_tokens")
+os.makedirs(TOKEN_LOG_DIR, exist_ok=True)
+DLP_URL = os.environ.get("DLP_URL", "http://localhost:8085/scan")
+
+def dlp_scan(text: str):
+    try:
+        r = requests.post(DLP_URL, json={"text": text}, timeout=3)
+        r.raise_for_status()
+        return r.json().get("findings", [])
+    except Exception:
+        # on failure, assume no findings to avoid blocking core workload — operators may change
+        return []
+
+def _log_tokens(entry: dict):
+    fname = os.path.join(TOKEN_LOG_DIR, f"tokens_{int(time.time()*1000)}.jsonl")
+    with open(fname, "a") as fh:
+        fh.write(json.dumps(entry) + "\n")
+
+def generate(prompt: str, params: dict = None) -> str:
+    params = params or {}
+    pre = dlp_scan(prompt)
+    if pre:
+        return json.dumps({"error":"prompt_blocked","findings":pre})
+    if PROVIDER == "openai" and OPENAI_API_KEY:
+        # Example: call OpenAI completion (operator must install openai lib/config)
+        import openai
+        openai.api_key = OPENAI_API_KEY
+        resp = openai.ChatCompletion.create(model=params.get("model","gpt-4o-mini"), messages=[{"role":"user","content": prompt}], temperature=params.get("temperature",0.0))
+        out = resp.choices[0].message.content
+        _log_tokens({"prompt": prompt, "output": out, "provider": "openai", "ts": int(time.time())})
+        post = dlp_scan(out)
+        if post:
+            return json.dumps({"error":"output_blocked","findings":post})
+        return out
+    # default: local http
+    r = requests.post(LOCAL_URL, json={"prompt": prompt, "params": params}, timeout=60)
+    r.raise_for_status()
+    out = r.text
+    _log_tokens({"prompt": prompt, "output": out, "provider": "local_http", "ts": int(time.time())})
+    post = dlp_scan(out)
+    if post:
+        return json.dumps({"error":"output_blocked","findings":post})
+    return out
+
+def stream_generate(prompt: str, params: dict = None) -> Iterable[str]:
+    params = params or {}
+    pre = dlp_scan(prompt)
+    if pre:
+        yield json.dumps({"error":"prompt_blocked","findings":pre})
+        return
+    # stream from local LLM stream endpoint
+    with requests.post(LOCAL_STREAM, json={"prompt": prompt, "params": params}, stream=True) as r:
+        r.raise_for_status()
+        for chunk in r.iter_content(chunk_size=None):
+            if not chunk:
+                continue
+            s = chunk.decode(errors="ignore")
+            # token-level DLP
+            findings = dlp_scan(s)
+            if findings:
+                _log_tokens({"prompt": prompt, "partial": "[REDACTED]", "provider": "local_stream", "ts": int(time.time())})
+                yield "[REDACTED_DUE_TO_DLP]"
+                break
+            _log_tokens({"prompt": prompt, "partial": s, "provider": "local_stream", "ts": int(time.time())})
+            yield s
+
*** End Patch
*** Begin Patch
*** Add File:k8s/generative/llm-adapter-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-llm-adapter
+  namespace: aegis-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-llm-adapter
+  template:
+    metadata:
+      labels:
+        app: aegis-llm-adapter
+    spec:
+      containers:
+        - name: llm-adapter
+          image: python:3.11-slim
+          command: ["sh","-c","pip install requests openai && python /opt/aegis/llm_adapter.py"]
+          env:
+            - name: LLM_PROVIDER
+              value: "local_http"
+            - name: LOCAL_URL
+              value: "http://localhost:8080/generate"
+            - name: LOCAL_STREAM
+              value: "http://localhost:8080/stream"
+            - name: DLP_URL
+              value: "http://dlp-service:8085/scan"
+          volumeMounts:
+            - name: code
+              mountPath: /opt/aegis
+      volumes:
+        - name: code
+          hostPath:
+            path: ./ops/generative
+            type: Directory
+
*** End Patch
*** Begin Patch
*** Add File:ops/agentic/sandbox_executor.py
+#!/usr/bin/env python3
+"""
+Sandbox executor for agentic tools:
+ - Runs arbitrary commands in a constrained subprocess with timeout, resource limits and user mapping.
+ - Logs command invocation and result for auditing.
+ - In production, this should be replaced by an isolated container runtime (K8s Job with strict RBAC and seccomp).
+"""
+import os
+import subprocess
+import shlex
+import json
+import time
+from typing import Dict, Any
+
+AUDIT_DIR = os.environ.get("AGENT_AUDIT_DIR", "/var/lib/aegis/agent_audit")
+os.makedirs(AUDIT_DIR, exist_ok=True)
+
+def run_command(cmd: str, timeout: int = 30, capture_output: bool = True) -> Dict[str, Any]:
+    ts = int(time.time())
+    rec = {"cmd": cmd, "ts": ts}
+    try:
+        # shlex for safe split
+        args = cmd if isinstance(cmd, list) else shlex.split(cmd)
+        p = subprocess.run(args, stdout=subprocess.PIPE if capture_output else None, stderr=subprocess.PIPE if capture_output else None, timeout=timeout, check=False, text=True)
+        rec.update({"returncode": p.returncode, "stdout": p.stdout if capture_output else "", "stderr": p.stderr if capture_output else ""})
+    except subprocess.TimeoutExpired as e:
+        rec.update({"error": "timeout", "timeout": timeout})
+    except Exception as e:
+        rec.update({"error": str(e)})
+    # write audit
+    fname = os.path.join(AUDIT_DIR, f"sandbox_{ts}.json")
+    with open(fname, "w") as fh:
+        fh.write(json.dumps(rec))
+    return rec
+
+if __name__ == "__main__":
+    import sys
+    if len(sys.argv) < 2:
+        print("usage: sandbox_executor.py <cmd>")
+        exit(2)
+    cmd = " ".join(sys.argv[1:])
+    print(run_command(cmd))
+
*** End Patch
*** Begin Patch
*** Add File:k8s/agentic/sandbox-runner-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-sandbox-runner
+  namespace: aegis-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-sandbox-runner
+  template:
+    metadata:
+      labels:
+        app: aegis-sandbox-runner
+    spec:
+      containers:
+        - name: sandbox-runner
+          image: python:3.11-slim
+          command: ["sh","-c","pip install && python /opt/aegis/sandbox_executor.py"]
+          securityContext:
+            runAsNonRoot: false
+            allowPrivilegeEscalation: false
+          resources:
+            limits:
+              cpu: "500m"
+              memory: "256Mi"
+          volumeMounts:
+            - name: code
+              mountPath: /opt/aegis
+      volumes:
+        - name: code
+          hostPath:
+            path: ./ops/agentic
+            type: Directory
+
*** End Patch
*** Begin Patch
*** Add File:ops/agentic/plan_verifier_enhanced.py
+#!/usr/bin/env python3
+"""
+Enhanced plan verifier:
+ - Uses static checks (deny-list of dangerous kubectl verbs), dry-run for kubectl,
+ - Estimates simple risk & cost for plan and returns structured verdict for the multi_signal_verifier.
+"""
+import re
+import subprocess
+from typing import Dict, Any, List
+
+DENY_VERBS = ["delete", "exec", "attach", "port-forward"]
+
+def static_check(step: Dict[str,Any]) -> List[str]:
+    errs=[]
+    if step.get("tool") == "kubectl":
+        cmd = step.get("params",{}).get("cmd","")
+        # deny verbs
+        for v in DENY_VERBS:
+            if re.search(r"\b"+v+r"\b", cmd):
+                errs.append(f"deny-verb:{v}")
+    return errs
+
+def dryrun_kubectl(cmd: str) -> bool:
+    try:
+        dry = cmd + " --dry-run=client"
+        subprocess.check_call(dry, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
+        return True
+    except Exception:
+        return False
+
+def verify_plan(plan: List[Dict[str,Any]]) -> Dict[str,Any]:
+    total_cost=0.0
+    issues=[]
+    ok=True
+    for step in plan:
+        issues += static_check(step)
+        if step.get("tool") == "kubectl":
+            cmd = step.get("params",{}).get("cmd","")
+            if cmd and not dryrun_kubectl(cmd):
+                issues.append("kubectl-dryrun-failed")
+                ok=False
+            total_cost += 0.05
+    score = 0.95 if ok and not issues else 0.2
+    return {"ok": ok, "score": score, "cost": total_cost, "issues": issues}
+
*** End Patch
*** Begin Patch
*** Add File:ops/agentic/shadow_canary.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-agent-shadow-canary-
+spec:
+  entrypoint: shadow-canary
+  templates:
+    - name: shadow-canary
+      steps:
+        - - name: run-policy-in-shadow
+            template: run-shadow
+        - - name: compare-with-ground-truth
+            template: compare
+
+    - name: run-shadow
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args:
+          - |
+            pip install requests
+            python - <<'PY'
+import json,requests
+print("running policy in shadow mode (no actuations).")
+PY
+
+    - name: compare
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["echo compare placeholder; sleep 1"]
+
*** End Patch
*** End Patch
