*** Begin Patch
*** Add File:ops/ros2/ros2_ingest_bridge.py
+#!/usr/bin/env python3
+"""
+ROS2 ingestion & replay bridge
+
+ - Subscribe to a list of ROS2 topics and record to a rosbag2 file, upload to S3 (or local store)
+ - Replay a bag into a ROS2/Gazebo scenario (invoked by Argo workflow)
+ - Note: requires ROS2 (foxy/galactic/rolling) environment where this runs (CI container mount or dedicated runner)
+"""
+import os
+import sys
+import json
+import time
+import subprocess
+import tempfile
+import boto3
+
+S3 = boto3.client("s3") if os.environ.get("EVIDENCE_BUCKET") else None
+BUCKET = os.environ.get("EVIDENCE_BUCKET","")
+
+def record_bag(output_path, duration_s=30, topics=None):
+    topics_args = []
+    if topics:
+        for t in topics:
+            topics_args += ["-t", t]
+    cmd = ["ros2", "bag", "record", "-o", output_path] + topics_args
+    print("recording bag with cmd:", " ".join(cmd), "duration:", duration_s)
+    p = subprocess.Popen(cmd)
+    try:
+        time.sleep(duration_s)
+    finally:
+        p.terminate()
+        p.wait()
+    # ros2 bag produces directory; create tar of bag dir
+    tar = f"{output_path}.tar.gz"
+    subprocess.check_call(["tar","-czf", tar, "-C", os.path.dirname(output_path), os.path.basename(output_path)])
+    return tar
+
+def upload_to_s3(local_path, s3_key_prefix="ros2_bags"):
+    if not S3:
+        return {"local": local_path}
+    key = f"{s3_key_prefix}/{os.path.basename(local_path)}"
+    S3.upload_file(local_path, BUCKET, key)
+    return {"s3": f"s3://{BUCKET}/{key}"}
+
+def replay_bag(tar_path):
+    # extract and ros2 bag play
+    tmpdir = tempfile.mkdtemp(prefix="bagplay_")
+    subprocess.check_call(["tar","-xzf", tar_path, "-C", tmpdir])
+    # find bag directory
+    items = os.listdir(tmpdir)
+    if not items:
+        raise RuntimeError("no bag extracted")
+    bagdir = os.path.join(tmpdir, items[0])
+    cmd = ["ros2", "bag", "play", bagdir, "--clock"]
+    print("replaying bag:", cmd)
+    p = subprocess.Popen(cmd)
+    return p
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--record", action="store_true")
+    p.add_argument("--replay", help="local tar path to replay")
+    p.add_argument("--out", default="/tmp/rosbag")
+    p.add_argument("--duration", type=int, default=30)
+    p.add_argument("--topics", nargs="*", default=[])
+    args = p.parse_args()
+    if args.record:
+        tar = record_bag(args.out, args.duration, args.topics)
+        print("recorded", tar)
+        print(upload_to_s3(tar))
+    elif args.replay:
+        p = replay_bag(args.replay)
+        print("replay pid", p.pid)
+    else:
+        print("specify --record or --replay")
+
*** End Patch
*** Begin Patch
*** Add File:ops/rt/ptp_sync_check.py
+#!/usr/bin/env python3
+"""
+PTP / time sync checker
+
+ - Checks system clock and PTP offset using phc_ctl/ptp4l/pmc if available or via simple ntpstat fallback.
+ - Produce JSON result for CI/HIL to verify deterministic timing requirements.
+"""
+import os
+import json
+import subprocess
+import shutil
+
+def run_cmd(cmd):
+    try:
+        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode()
+        return {"ok": True, "out": out}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def check_ptp():
+    # try pmc (ptp4l companion)
+    if shutil.which("pmc"):
+        return run_cmd(["pmc","-u","-b","0","GET","CLOCK_REALTIME"])
+    if shutil.which("ptp4l"):
+        return run_cmd(["ptp4l","-v","-m","-i","eth0"])
+    if shutil.which("timedatectl"):
+        return run_cmd(["timedatectl","status"])
+    return {"ok": False, "error": "no_ptp_tools"}
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--out","-o", default="/tmp/ptp_check.json")
+    args = p.parse_args()
+    res = check_ptp()
+    with open(args.out,"w") as fh:
+        json.dump(res, fh, indent=2)
+    print("wrote", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:ops/serving/tensorrt/optimize_to_tensorrt.py
+#!/usr/bin/env python3
+"""
+Optimize model to TensorRT engine (supports ONNX/trtexec fallback).
+
+ - Accepts ONNX or TorchScript model and produces a TensorRT engine (fp16 or int8)
+ - For INT8, uses a calibration dataset path (images or arrays) and trtexec --int8 --calib
+ - Produces a Triton artifact layout optionally
+"""
+import os
+import subprocess
+import json
+import shutil
+
+def run_trtexec(onnx_path, output_engine, fp16=False, int8=False, calib_data=None, max_batch=8):
+    cmd = ["trtexec", "--onnx="+onnx_path, f"--saveEngine={output_engine}", f"--maxBatch={max_batch}"]
+    if fp16:
+        cmd.append("--fp16")
+    if int8:
+        cmd.append("--int8")
+        if calib_data:
+            cmd += ["--calib=" + calib_data]
+    print("running:", " ".join(cmd))
+    subprocess.check_call(cmd)
+
+def make_triton_layout(model_name, engine_path, out_repo):
+    model_dir = os.path.join(out_repo, model_name)
+    ver_dir = os.path.join(model_dir, "1")
+    os.makedirs(ver_dir, exist_ok=True)
+    shutil.copy(engine_path, os.path.join(ver_dir, os.path.basename(engine_path)))
+    # generate minimal config via generator if available
+    print("Placed engine in", ver_dir)
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--onnx", required=True)
+    p.add_argument("--out", required=True)
+    p.add_argument("--mode", choices=["fp32","fp16","int8"], default="fp16")
+    p.add_argument("--calib", default=None)
+    p.add_argument("--model-name", required=True)
+    args = p.parse_args()
+    out_engine = args.out
+    run_trtexec(args.onnx, out_engine, fp16=(args.mode=="fp16"), int8=(args.mode=="int8"), calib_data=args.calib)
+    print("optimized engine at", out_engine)
+    # Optionally layout for Triton
+    repo = os.environ.get("TRITON_MODEL_REPO","/models")
+    make_triton_layout(args.model_name, out_engine, repo)
+
*** End Patch
*** Begin Patch
*** Add File:ops/serving/tensorrt/latency_budget_checker.py
+#!/usr/bin/env python3
+"""
+Simple latency budget checker for a Triton model endpoint.
+
+ - Sends N requests and computes p50/p95/p99; compares against budget thresholds supplied in JSON config.
+ - Uses HTTP inference API (Triton) or local client
+"""
+import os
+import time
+import json
+import requests
+import numpy as np
+
+TRITON_URL = os.environ.get("TRITON_URL","http://triton:8000/v2/models")
+
+def infer(model_name, input_payload):
+    url = f"{TRITON_URL}/{model_name}/infer"
+    r = requests.post(url, json=input_payload, timeout=30)
+    r.raise_for_status()
+    return r.json()
+
+def measure(model_name, payload, iterations=100):
+    lat = []
+    for i in range(iterations):
+        t0 = time.time()
+        try:
+            infer(model_name, payload)
+            lat.append((time.time()-t0))
+        except Exception:
+            lat.append(999.0)
+    return np.percentile(lat, [50,95,99]).tolist(), lat
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--model", required=True)
+    p.add_argument("--payload-file", required=True)
+    p.add_argument("--out", default="/tmp/latency_report.json")
+    args = p.parse_args()
+    payload = json.load(open(args.payload_file))
+    stats, lat = measure(args.model, payload, iterations=50)
+    report = {"model": args.model, "p50": stats[0], "p95": stats[1], "p99": stats[2], "samples": len(lat)}
+    with open(args.out,"w") as fh:
+        json.dump(report, fh, indent=2)
+    print("wrote", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:k8s/hil/ros2_gazebo_hil_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-ros2-gazebo-hil-
+spec:
+  entrypoint: ros2-gazebo-hil
+  templates:
+    - name: ros2-gazebo-hil
+      steps:
+        - - name: start-gazebo
+            template: start-gazebo
+        - - name: start-robot-node
+            template: start-robot-node
+        - - name: run-scenario
+            template: run-scenario
+        - - name: collect-attestation
+            template: collect-attestation
+
+    - name: start-gazebo
+      container:
+        image: osrf/ros:foxy-desktop
+        command: [sh, -c]
+        args:
+          - apt-get update && apt-get install -y gazebo && gzserver & sleep 3
+        resources:
+          limits:
+            cpu: "2"
+            memory: "4Gi"
+
+    - name: start-robot-node
+      container:
+        image: ros:foxy
+        command: [sh, -c]
+        args:
+          - ros2 run my_robot robot_node --ros-args & sleep 2
+
+    - name: run-scenario
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - pip install boto3 requests || true
+            # record rosbag and run scenario (uses ros2_ingest_bridge)
+            python /opt/aegis/ops/ros2/ros2_ingest_bridge.py --record --out /tmp/hil_bag --duration 60 --topics /camera/image_raw /cmd_vel || true
+            # run rollout_simulator checks and formal checks
+            python /opt/aegis/ops/planner/rollout_simulator.py --plan-file /opt/aegis/hil/plan.json || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+    - name: collect-attestation
+      container:
+        image: python:3.11-slim
+        command: [sh, -c]
+        args:
+          - python /opt/aegis/ops/audit/attestation_audit_trail.py --attestation-id hil-$(date +%s) || true
+        volumeMounts:
+          - name: code
+            mountPath: /opt/aegis
+
+  volumes:
+    - name: code
+      hostPath:
+        path: ./ops
+        type: Directory
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/robot_invariants.json
+{
+  "max_lateral_accel": {"type": "numeric_limit", "variable": "lateral_accel", "op": "le", "value": 4.0},
+  "max_longitudinal_decel": {"type": "numeric_limit", "variable": "long_decel", "op": "ge", "value": -8.0},
+  "min_stopping_distance": {"type": "stopping_distance", "params": {"min_margin_m": 0.5}},
+  "forbidden_zones": {"type": "forbidden_area", "areas": [{"id":"zone-warehouse","polygon": [[0,0],[10,0],[10,10],[0,10]]}]}
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/formal/robot_formal_checker.py
+#!/usr/bin/env python3
+"""
+Domain-specific formal checker for robotic invariants (dynamics and geo-fences).
+ - Uses z3 for numeric checks where applicable.
+ - Supports stopping distance check (simple conservative model).
+"""
+try:
+    from z3 import Real, Solver, Not, sat
+except Exception:
+    Solver = None
+
+import math
+
+def check_numeric_limit(var_value, op, value):
+    if op == "le":
+        return var_value <= value
+    if op == "ge":
+        return var_value >= value
+    if op == "lt":
+        return var_value < value
+    if op == "gt":
+        return var_value > value
+    return False
+
+def stopping_distance_ok(speed_m_s, decel_m_s2, obstacle_distance_m, min_margin=0.5):
+    # conservative stopping distance: d = v^2 / (2 * a) where a is abs(decel)
+    if decel_m_s2 >= 0:
+        return False
+    a = abs(decel_m_s2)
+    stopping = (speed_m_s**2) / (2.0 * a) if a > 0 else float("inf")
+    return (stopping + min_margin) <= obstacle_distance_m
+
+def point_in_polygon(point, polygon):
+    # ray casting
+    x, y = point
+    inside = False
+    n = len(polygon)
+    for i in range(n):
+        xi, yi = polygon[i]
+        xj, yj = polygon[(i+1)%n]
+        intersect = ((yi > y) != (yj > y)) and (x < (xj - xi) * (y - yi) / (yj - yi + 1e-12) + xi)
+        if intersect:
+            inside = not inside
+    return inside
+
+def check_plan(plan_state, invariants):
+    """
+    plan_state: dict containing runtime sensors / planned actions, e.g. {"speed":1.2,"long_decel":-5.0,"obstacle_dist":10.0,"lateral_accel":2.0,"position":[x,y]}
+    invariants: dict loaded from robot_invariants.json
+    """
+    res = {"ok": True, "details": {}}
+    # numeric limits
+    for name, spec in invariants.items():
+        if spec.get("type") == "numeric_limit":
+            var = spec["variable"]
+            op = spec["op"]
+            val = spec["value"]
+            cur = plan_state.get(var, None)
+            ok = False
+            if cur is not None:
+                ok = check_numeric_limit(cur, op, val)
+            res["details"][name] = {"ok": ok, "value": cur, "expr": f"{var} {op} {val}"}
+            res["ok"] = res["ok"] and ok
+        elif spec.get("type") == "stopping_distance":
+            min_margin = spec.get("params",{}).get("min_margin_m",0.5)
+            ok = stopping_distance_ok(plan_state.get("speed",0.0), plan_state.get("long_decel",-1.0), plan_state.get("obstacle_dist",0.0), min_margin)
+            res["details"][name] = {"ok": ok}
+            res["ok"] = res["ok"] and ok
+        elif spec.get("type") == "forbidden_area":
+            pos = plan_state.get("position")
+            if pos is None:
+                res["details"][name] = {"ok": True, "note":"no position"}
+                continue
+            found = False
+            for area in spec.get("areas", []):
+                if point_in_polygon(pos, area.get("polygon",[])):
+                    found = True
+                    res["details"][name] = {"ok": False, "area": area.get("id")}
+                    break
+            if not found:
+                res["details"][name] = {"ok": True}
+            res["ok"] = res["ok"] and (not found)
+        else:
+            res["details"][name] = {"ok": False, "reason":"unsupported"}
+            res["ok"] = False
+    return res
+
+if __name__=="__main__":
+    import argparse, json
+    p = argparse.ArgumentParser()
+    p.add_argument("--state", required=True)
+    p.add_argument("--invariants", default="ops/formal/robot_invariants.json")
+    args = p.parse_args()
+    state = json.load(open(args.state))
+    inv = json.load(open(args.invariants))
+    print(json.dumps(check_plan(state, inv), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/planner/real_time_planner.py
+#!/usr/bin/env python3
+"""
+Real-time planner wrapper
+
+ - Launches planner process with a hard timeout and resource limits
+ - Ensures planner returns within bounded time or returns a safe fallback plan
+ - Safe fallback plan is a short stop/hold command
+"""
+import os
+import json
+import subprocess
+import signal
+import tempfile
+import time
+
+FALLBACK_PLAN = {"steps":[{"type":"stop","reason":"timeout_or_error"}]}
+
+def run_planner_cmd(cmd, timeout_s=1.0):
+    try:
+        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+        try:
+            out, err = p.communicate(timeout=timeout_s)
+            if p.returncode != 0:
+                return {"ok": False, "error": err.decode()}
+            return {"ok": True, "plan": json.loads(out.decode())}
+        except subprocess.TimeoutExpired:
+            p.kill()
+            return {"ok": False, "error": "timeout"}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def get_plan(planner_bin, input_state, timeout_s=1.0):
+    # write input to tmp
+    tmp = tempfile.NamedTemporaryFile(delete=False, mode="w")
+    tmp.write(json.dumps(input_state))
+    tmp.flush()
+    tmp.close()
+    cmd = [planner_bin, "--state", tmp.name]
+    res = run_planner_cmd(cmd, timeout_s=timeout_s)
+    if not res.get("ok"):
+        return FALLBACK_PLAN, res
+    return res.get("plan"), res
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--planner-bin", required=True)
+    p.add_argument("--state-file", required=True)
+    p.add_argument("--timeout", type=float, default=1.0)
+    args = p.parse_args()
+    state = json.load(open(args.state_file))
+    plan, meta = get_plan(args.planner_bin, state, args.timeout)
+    print(json.dumps({"plan": plan, "meta": meta}, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:ops/safety/watchdog_service.py
+#!/usr/bin/env python3
+"""
+Watchdog / emergency-stop service for actuators
+
+ - Receives heartbeats from robots (HTTP POST) keyed by robot_id
+ - If heartbeat missing beyond TTL, triggers emergency-stop via configured actuator endpoint or publishes to ROS2 topic via ros2 topic pub
+ - Writes audit records for any triggered emergency actions
+"""
+from flask import Flask, request, jsonify
+import time
+import threading
+import os
+import requests
+from ops.audit.attestation_audit_trail import append_audit_record
+
+app = Flask(__name__)
+HEARTBEAT_TTL = int(os.environ.get("WD_TTL_S","5"))
+CHECK_INTERVAL = int(os.environ.get("WD_CHECK_S","2"))
+actuator_map = {}  # robot_id -> actuator endpoint or ros2 topic config
+heartbeats = {}
+
+def monitor_loop():
+    while True:
+        now = time.time()
+        for rid, ts in list(heartbeats.items()):
+            if now - ts > HEARTBEAT_TTL:
+                print("missing heartbeat for", rid, "-> triggering emergency stop")
+                trigger_emergency_stop(rid)
+                heartbeats.pop(rid, None)
+        time.sleep(CHECK_INTERVAL)
+
+def trigger_emergency_stop(robot_id):
+    cfg = actuator_map.get(robot_id, {})
+    endpoint = cfg.get("http_endpoint")
+    if endpoint:
+        try:
+            requests.post(endpoint, json={"cmd":"emergency_stop","ts":int(time.time())}, timeout=5)
+        except Exception as e:
+            print("emergency_stop http failed", e)
+    else:
+        # fallback: write audit record
+        append_audit_record({"attestation_id": f"emergency-{robot_id}-{int(time.time())}", "actor":"watchdog", "artifact":"emergency_stop", "note": "no actuator endpoint configured"})
+
+@app.route("/v1/hb/<robot_id>", methods=["POST"])
+def heartbeat(robot_id):
+    heartbeats[robot_id] = time.time()
+    return jsonify({"ok": True})
+
+@app.route("/v1/register", methods=["POST"])
+def register():
+    body = request.json or {}
+    rid = body.get("robot_id")
+    actuator_map[rid] = body.get("actuator", {})
+    return jsonify({"ok": True})
+
+if __name__=="__main__":
+    t = threading.Thread(target=monitor_loop, daemon=True)
+    t.start()
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT","8086")))
+
*** End Patch
*** Begin Patch
*** Add File:ops/ota/ota_sign_manifest.py
+#!/usr/bin/env python3
+"""
+Create an OTA manifest and sign it using cosign KMS URI or local hsm_signer service.
+
+ - Produces manifest JSON with metadata and signature
+ - Preference: call cosign with KMS URI (awskms:///alias/...) if cosign configured
+ - Fallback: call in-cluster HSM signing service (ops/signing/hsm_signing_service.py)
+"""
+import os
+import json
+import subprocess
+import tempfile
+import requests
+
+COSIGN_KMS = os.environ.get("COSIGN_KMS","")
+HSM_SIGNER = os.environ.get("HSM_SIGNER_URL","http://aegis-hsm-signer.aegis-system.svc.cluster.local:8085")
+
+def sign_with_cosign_kms(manifest_path, kms_uri):
+    cmd = ["cosign", "sign-blob", "--kms", kms_uri, "--output-signature", f"{manifest_path}.sig", manifest_path]
+    subprocess.check_call(cmd)
+    sig = open(f"{manifest_path}.sig","rb").read().hex()
+    return sig
+
+def sign_with_hsm_service(manifest_path):
+    blob = open(manifest_path,"rb").read()
+    import base64
+    b64 = base64.b64encode(blob).decode()
+    r = requests.post(HSM_SIGNER + "/v1/sign", json={"blob_b64": b64, "actor":"ota"}, timeout=30)
+    return r.json()
+
+def make_manifest(image_s3, version, notes=""):
+    m = {"image_s3": image_s3, "version": version, "ts": int(__import__("time").time()), "notes": notes}
+    return m
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--image-s3", required=True)
+    p.add_argument("--version", required=True)
+    p.add_argument("--notes", default="")
+    p.add_argument("--out", default="/tmp/ota_manifest.json")
+    args = p.parse_args()
+    manifest = make_manifest(args.image_s3, args.version, args.notes)
+    with open(args.out,"w") as fh:
+        json.dump(manifest, fh, indent=2)
+    sig = None
+    if COSIGN_KMS:
+        try:
+            sig = sign_with_cosign_kms(args.out, COSIGN_KMS)
+        except Exception as e:
+            print("cosign kms sign failed:", e)
+    if not sig:
+        print("falling back to HSM signer service")
+        res = sign_with_hsm_service(args.out)
+        sig = res.get("signature") or res
+    print("manifest signed; signature:", str(sig)[:80])
+    print("manifest path:", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:ops/ota/device_verify.py
+#!/usr/bin/env python3
+"""
+Device-side OTA verifier
+
+ - Downloads manifest and signature, verifies via cosign public key (if available) or via KMS-backed public key endpoint
+ - Placeholder for device verification: in production, use vendor SDK, cosign-verify-blob with KMS pub, or direct KMS Verify APIs
+"""
+import os
+import subprocess
+import requests
+import json
+
+COSIGN_PUB = os.environ.get("COSIGN_PUB","")  # path or KMS pub reference
+HSM_PUB_ENDPOINT = os.environ.get("HSM_PUB_ENDPOINT","")  # optional endpoint exposing public key for verification
+
+def verify_with_cosign(manifest_path, sig_path, pub):
+    cmd = ["cosign","verify-blob","-key", pub, manifest_path]
+    try:
+        subprocess.check_call(cmd)
+        return True
+    except Exception:
+        return False
+
+def verify(manifest_path, signature_hex):
+    # write sig to tmp file for cosign verify-blob if using pub key
+    if COSIGN_PUB:
+        sig_file = manifest_path + ".sig"
+        open(sig_file,"wb").write(bytes.fromhex(signature_hex))
+        ok = verify_with_cosign(manifest_path, sig_file, COSIGN_PUB)
+        return ok
+    # fallback: ask HSM pub endpoint to verify signature
+    if HSM_PUB_ENDPOINT:
+        r = requests.post(HSM_PUB_ENDPOINT + "/v1/verify", json={"manifest": open(manifest_path).read(), "signature": signature_hex}, timeout=10)
+        return r.json().get("ok", False)
+    return False
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--manifest", required=True)
+    p.add_argument("--signature-hex", required=True)
+    args = p.parse_args()
+    ok = verify(args.manifest, args.signature_hex)
+    print("verification ok:", ok)
+    if not ok:
+        raise SystemExit(2)
+
*** End Patch
*** Begin Patch
*** Add File:ops/edge/quantize_model.py
+#!/usr/bin/env python3
+"""
+Simple quantization pipeline for PyTorch -> ONNX -> INT8 via ONNX Runtime quantization or TorchQuantization
+
+ - Exports a PyTorch model to ONNX (user provides export script or model)
+ - Runs ONNX quantization with calibration dataset (images folder or numpy)
+ - Produces an optimized model ready for TensorRT or Triton conversion
+"""
+import os
+import subprocess
+import json
+
+def export_torch_to_onnx(export_script, out_onnx, args=""):
+    # export_script is a python script path that writes ONNX to out_onnx given env/args
+    cmd = ["python", export_script, "--out", out_onnx] + (args.split() if args else [])
+    subprocess.check_call(cmd)
+
+def run_onnx_quantize(onnx_path, out_path, calib_dataset_dir=None):
+    # try onnxruntime quantization tool
+    if not os.path.exists(onnx_path):
+        raise RuntimeError("onnx not found")
+    cmd = ["python","-m","onnxruntime_tools.optimizer_cli", "-i", onnx_path, "-o", out_path, "--optimization-level", "qlinear"]
+    if calib_dataset_dir:
+        cmd += ["--calibration", calib_dataset_dir]
+    print("running:", " ".join(cmd))
+    subprocess.check_call(cmd)
+
+if __name__=="__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--export-script", help="script to run to export onnx")
+    p.add_argument("--onnx", help="path to existing onnx")
+    p.add_argument("--out", required=True)
+    p.add_argument("--calib-dir", default=None)
+    args = p.parse_args()
+    if args.export_script:
+        export_torch_to_onnx(args.export_script, args.out + ".onnx")
+        run_onnx_quantize(args.out + ".onnx", args.out, args.calib_dir)
+    elif args.onnx:
+        run_onnx_quantize(args.onnx, args.out, args.calib_dir)
+    else:
+        print("provide --export-script or --onnx")
+
*** End Patch
*** Begin Patch
*** Add File:docs/runbooks/robotics_certification.md
+Robotics Certification & Integration Runbook
+=========================================
+
+Purpose
+-------
Guidance for bringing Aegis to robotics readiness: real‑time integration, low‑latency inference, closed‑loop HIL, formal verification, actuator safety, certified OTA, and edge quantization.
+
+Key Steps (high level)
+1. Real‑time & ROS2
+   - Deploy ros2_ingest_bridge on dedicated runners with pinned ROS2 distro.
+   - Validate PTP/time sync across HIL hosts using ptp_sync_check; ensure <1ms offset for critical topics.
+
+2. Low‑latency inference
+   - Convert models to TensorRT using optimize_to_tensorrt.py; generate Triton model layouts.
+   - Run latency_budget_checker per-model and set SLOs in configmap aegis-model-slos.
+   - Use non-batching or micro-batching policies for real-time inference.
+
+3. Closed‑loop simulation & HIL
+   - Use ros2_gazebo_hil_workflow as baseline Argo job for closed-loop tests.
+   - Collect artifacts (rosbags, traces, attestations) and feed to formal checker and KPI tracker.
+
+4. Formal V&V
+   - Author robot_invariants.json for the domain; run robot_formal_checker in HIL.
+   - Add hil_formal_check into PR HIL for any plan that issues actuator commands.
+
+5. Deterministic planners & watchdogs
+   - Wrap planners with real_time_planner to bound execution; fallback to stop plan on timeout.
+   - Deploy watchdog_service to monitor heartbeats and trigger emergency stop if required.
+
+6. OTA & Attestation
+   - Sign OTA manifests with ota_sign_manifest.py using COSIGN_KMS or HSM signer.
+   - Deploy device_verify on devices; ensure verification runs before applying image.
+
+7. Edge quantization & tuning
+   - Use quantize_model.py to produce INT8 artifacts and validate through Triton & latency checker.
+   - Document per-robot hardware profiles and memory/thermal budgets.
+
+Certification evidence
+ - Rosbags & closed-loop traces
+ - HSM-signed OTA manifests and audit bundles
+ - Formal check reports (hil_formal_result.json)
+ - Latency & throughput SLO reports
+ - HIL run attestations and repair/rollback logs
+
+Operator notes
+ - Hardware & network preconditions (PTP, NICs, GPU drivers) must be validated and recorded.
+ - Safety‑critical actuations: never enable autonomous actuation without human supervised shadow mode and emergency cutover.
+ - Legal & standards: map evidence to ISO 26262 / SOTIF artifacts during certification.
+
*** End Patch
*** End Patch
