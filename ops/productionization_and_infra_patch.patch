*** Begin Patch
*** Add File:terraform/s3/s3_buckets.tf
+/*
+Terraform: create S3 buckets for model-registry, telemetry/data-lake and Rekor storage.
+Usage:
+  terraform -chdir=terraform/s3 init
+  terraform -chdir=terraform/s3 apply -var="prefix=aegis" -var="region=us-east-1"
+*/
+variable "prefix" {
+  type    = string
+  default = "aegis"
+}
+variable "region" {
+  type    = string
+  default = "us-east-1"
+}
+provider "aws" {
+  region = var.region
+}
+
+resource "aws_s3_bucket" "model_registry" {
+  bucket = "${var.prefix}-model-registry-${data.aws_caller_identity.current.account_id}"
+  acl    = "private"
+  force_destroy = false
+  versioning { enabled = true }
+  lifecycle_rule {
+    id      = "archive-old"
+    enabled = true
+    transition {
+      days          = 30
+      storage_class = "STANDARD_IA"
+    }
+  }
+}
+
+resource "aws_s3_bucket" "telemetry" {
+  bucket = "${var.prefix}-telemetry-${data.aws_caller_identity.current.account_id}"
+  acl    = "private"
+  versioning { enabled = true }
+}
+
+resource "aws_s3_bucket" "rekor_storage" {
+  bucket = "${var.prefix}-rekor-${data.aws_caller_identity.current.account_id}"
+  acl    = "private"
+  versioning { enabled = true }
+}
+
+output "model_registry_s3" {
+  value = aws_s3_bucket.model_registry.bucket
+}
+output "telemetry_s3" {
+  value = aws_s3_bucket.telemetry.bucket
+}
+output "rekor_s3" {
+  value = aws_s3_bucket.rekor_storage.bucket
+}
+
+data "aws_caller_identity" "current" {}
+
*** End Patch
*** Begin Patch
*** Add File:terraform/eks/gpu_nodegroup.tf
+/*
+Terraform example to create an EKS managed node group with GPU instances (AWS).
+Adapt to your environment/provider. This is a reference module.
+*/
+variable "cluster_name" { type = string }
+variable "node_group_name" { type = string, default = "aegis-gpu-ng" }
+variable "instance_type" { type = string, default = "g4dn.xlarge" }
+variable "desired_capacity" { type = number, default = 1 }
+variable "min_size" { type = number, default = 0 }
+variable "max_size" { type = number, default = 3 }
+provider "aws" { region = var.region != "" ? var.region : "us-east-1" }
+
+resource "aws_iam_role" "eks_node_role" {
+  name = "${var.node_group_name}-role"
+  assume_role_policy = data.aws_iam_policy_document.eks_node_assume_role.json
+}
+
+resource "aws_eks_node_group" "gpu" {
+  cluster_name    = var.cluster_name
+  node_group_name = var.node_group_name
+  node_role_arn   = aws_iam_role.eks_node_role.arn
+  scaling_config {
+    desired_size = var.desired_capacity
+    min_size     = var.min_size
+    max_size     = var.max_size
+  }
+  instance_types = [var.instance_type]
+  ami_type = "AL2_x86_64_GPU"
+}
+
+data "aws_iam_policy_document" "eks_node_assume_role" {
+  statement {
+    actions = ["sts:AssumeRole"]
+    principals { type = "Service" ; identifiers = ["ec2.amazonaws.com"] }
+  }
+}
+
+output "nodegroup_name" { value = aws_eks_node_group.gpu.node_group_name }
+output "node_role_arn" { value = aws_iam_role.eks_node_role.arn }
+
*** End Patch
*** Begin Patch
*** Add File:ops/infra/install_argo.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Install Argo Workflows + server with Helm and create a service account with cluster role binding.
+#
+NAMESPACE=${NAMESPACE:-argo}
+RELEASE_NAME=${RELEASE_NAME:-argo}
+
+helm repo add argo https://argoproj.github.io/argo-helm || true
+helm repo update
+kubectl create ns "${NAMESPACE}" || true
+helm upgrade --install "${RELEASE_NAME}" argo/argo-workflows -n "${NAMESPACE}"
+
+kubectl apply -f - <<EOF
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: argo-workflows-sa
+  namespace: ${NAMESPACE}
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: argo-workflows-crb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: cluster-admin
+subjects:
+- kind: ServiceAccount
+  name: argo-workflows-sa
+  namespace: ${NAMESPACE}
+EOF
+
+echo "Argo installed in namespace ${NAMESPACE}. Ensure Argo server ingress or port-forward is configured for access."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/argo/argo-serviceaccount.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: argo-workflows-sa
+  namespace: argo
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: argo-workflows-crb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: cluster-admin
+subjects:
+  - kind: ServiceAccount
+    name: argo-workflows-sa
+    namespace: argo
+
*** End Patch
*** Begin Patch
*** Add File:docker/training/Dockerfile
+FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y --no-install-recommends \
+    build-essential git curl ca-certificates libsndfile1 && \
+    rm -rf /var/lib/apt/lists/*
+
+RUN python3 -m pip install --upgrade pip
+RUN pip install \
+    torch --extra-index-url https://download.pytorch.org/whl/cu118 || true
+RUN pip install \
+    pyarrow sentence-transformers sacrebleu rouge-score boto3 prometheus_client \
+    transformers accelerate bitsandbytes datasets==2.12.0
+
+WORKDIR /workspace
+COPY ops/ /workspace/ops/
+ENV PATH="/workspace:${PATH}"
+ENTRYPOINT ["python","-u"]
+CMD ["-c","print('training container ready')"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/docker/build_push_training.sh
+#!/usr/bin/env bash
+set -euo pipefail
+IMAGE=${IMAGE:-"ghcr.io/yourorg/aegis-training:latest"}
+REGISTRY=${REGISTRY:-}
+
+docker build -t "${IMAGE}" docker/training
+echo "Built ${IMAGE}"
+if [ -n "${REGISTRY}" ]; then
+  docker push "${IMAGE}"
+  echo "Pushed ${IMAGE}"
+else
+  echo "No REGISTRY specified; push manually if desired"
+fi
+
*** End Patch
*** Begin Patch
*** Add File:ops/training/distributed_training_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-dist-train-
+spec:
+  entrypoint: dist-train
+  templates:
+    - name: dist-train
+      steps:
+        - - name: prepare
+            template: prepare
+        - - name: train
+            template: train
+        - - name: checkpoint-upload
+            template: upload
+        - - name: register
+            template: register
+
+    - name: prepare
+      container:
+        image: alpine:3.17
+        command: [sh,-c]
+        args: ["echo prepare datasets; sleep 1"]
+
+    - name: train
+      container:
+        image: REPLACE_WITH_TRAINING_IMAGE
+        resources:
+          limits:
+            nvidia.com/gpu: 1
+            memory: 64Gi
+            cpu: "8"
+        command: [sh,-c]
+        args:
+          - |
+            set -e
+            python /workspace/ops/training/train_and_register.py --samples /mnt/samples.jsonl --work-dir /tmp/candidate
+      retryStrategy:
+        limit: 2
+        retryPolicy: "Always"
+      volumeMounts:
+        - name: samples
+          mountPath: /mnt
+
+    - name: upload
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["python -c 'print(\"upload placeholder (checkpoint to S3)\")'"]
+
+    - name: register
+      container:
+        image: alpine:3.17
+        command: [sh,-c]
+        args: ["echo register model (placeholder)"]
+
+  volumes:
+    - name: samples
+      persistentVolumeClaim:
+        claimName: samples-pvc
+
*** End Patch
*** Begin Patch
*** Add File:ops/checkpointing/checkpoint_manager.py
+#!/usr/bin/env python3
+"""
+Checkpoint manager: upload and fetch checkpoints to/from S3 and keep latest pointer.
+"""
+import os
+import boto3
+from typing import Optional
+
+S3_BUCKET = os.environ.get("MODEL_REGISTRY_S3", "")
+S3_PREFIX = os.environ.get("MODEL_REGISTRY_PREFIX", "models")
+
+def upload_checkpoint(local_path: str, model_name: str, version: str) -> Optional[str]:
+    if not S3_BUCKET:
+        print("No MODEL_REGISTRY_S3 configured")
+        return None
+    s3 = boto3.client("s3")
+    key = f"{S3_PREFIX}/{model_name}/{version}/{os.path.basename(local_path)}"
+    s3.upload_file(local_path, S3_BUCKET, key)
+    # write latest pointer
+    latest_key = f"{S3_PREFIX}/{model_name}/latest"
+    s3.put_object(Bucket=S3_BUCKET, Key=latest_key, Body=version.encode())
+    return f"s3://{S3_BUCKET}/{key}"
+
+def get_latest_version(model_name: str) -> Optional[str]:
+    if not S3_BUCKET:
+        return None
+    s3 = boto3.client("s3")
+    latest_key = f"{S3_PREFIX}/{model_name}/latest"
+    try:
+        obj = s3.get_object(Bucket=S3_BUCKET, Key=latest_key)
+        return obj['Body'].read().decode()
+    except Exception:
+        return None
+
*** End Patch
*** Begin Patch
*** Add File:ops/canary/canary_workflow_with_kubectl.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-canary-k8s-
+spec:
+  entrypoint: canary
+  templates:
+    - name: canary
+      steps:
+        - - name: deploy-canary
+            template: deploy
+        - - name: wait-and-monitor
+            template: monitor
+        - - name: decide
+            template: decide
+
+    - name: deploy
+      container:
+        image: bitnami/kubectl:1.26
+        command: [sh,-c]
+        args: ["kubectl -n inference apply -f canary/deployment.yaml && echo deployed; sleep 2"]
+
+    - name: monitor
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["python - <<'PY'\nimport time, sys\nprint('monitor placeholder'); time.sleep(30)\nPY"]
+
+    - name: decide
+      container:
+        image: python:3.11-slim
+        command: [sh,-c]
+        args: ["python - <<'PY'\nfrom ops.experiments.manager import Experiment\nprint('decision placeholder')\nPY"]
+
*** End Patch
*** Begin Patch
*** Add File:ops/dlp/macie_integration.py
+#!/usr/bin/env python3
+"""
+Simple integration to run AWS Macie S3 object classification for DLP.
+Note: Macie must be enabled in the account and proper permissions granted.
+This file demonstrates a pattern: list objects, request classification, and interpret results.
+"""
+import os
+import boto3
+
+S3_BUCKET = os.environ.get("DLP_S3_BUCKET", "")
+
+def inspect_s3_object(bucket: str, key: str):
+    macie = boto3.client("macie2")
+    s3 = boto3.client("s3")
+    # Get a pre-signed URL (expires short)
+    presigned = s3.generate_presigned_url('get_object', Params={'Bucket': bucket, 'Key': key}, ExpiresIn=300)
+    # Macie API doesn't accept presigned directly; in production you'd use Macie S3 classification jobs.
+    print("Presigned URL (for manual review):", presigned)
+    # For automated DLP use Macie classification jobs (not implemented here).
+
+if __name__ == "__main__":
+    print("macie integration placeholder - implement classification jobs per AWS docs")
+
*** End Patch
*** Begin Patch
*** Add File:ops/tests/chaos/chaos_suite.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: aegis-chaos-sim
+  namespace: aegis-sandbox
+spec:
+  template:
+    spec:
+      containers:
+        - name: chaos
+          image: litmuschaos/chaos-runner:latest
+          command: ["sh","-c","echo chaos placeholder; sleep 10"]
+      restartPolicy: Never
+  backoffLimit: 0
+
*** End Patch
*** Begin Patch
*** Add File:docs/governance/risk_taxonomy.md
+# Aegis Risk Taxonomy (starter)
+
+Risk levels:
+- low: read-only, non-impactful operations (logs, metrics collection).
+- medium: non-destructive infra changes (scale replicas, restart non-prod pods).
+- high: changes impacting production data or persistent state (promote model to production, modify DB).
+- critical: destructive operations or data exfiltration risk (delete data, change IAM, access secrets).
+
+Action examples mapped to risk:
+- scale -> medium
+- swap-to-quant -> medium/high depending on service
+- retrain-and-promote -> high
+- production DB migration -> critical
+
+Autonomy rules:
+- low/medium: allow automated execution with basic checks.
+- high: require multi-signal verification and signed human approval.
+- critical: require manual approval and scheduled maintenance window.
+
*** End Patch
*** Begin Patch
*** Add File:docs/governance/autonomy_policy_template.md
+# Autonomy Policy Template
+
+Fields:
+- action_id: unique id
+- risk_level: low|medium|high|critical
+- required_signals:
+  - plan_confidence: >= 0.80
+  - rule_checks: true
+  - historical_success_prob: >= 0.6
+  - cost_limit_usd: <= 5.0
+- approval_required: boolean
+- can_auto_promote: boolean
+- rollback_policy:
+  - monitor_window_minutes: 15
+  - rollback_condition: p95_latency_increase > 10% or error_rate_increase > 5%
+
+Use this template to codify which playbooks may run automatically.
+
*** End Patch
*** Begin Patch
*** Add File:docs/legal/sow_data_retention.md
+# SOW Template: Data Retention & Usage (example)
+
+Retention policy:
+- Telemetry: retain raw telemetry 90 days in searchable store, archive to cold storage (S3 Glacier) after 180 days.
+- Evidence (signed approvals, traces): retain 7 years unless customer requests deletion (subject to legal).
+- Model artifacts: retain promoted production artifacts until superseded + 1 year (or customer requirement).
+
+Data usage:
+- Customer data used for retraining only with explicit consent documented in SOW.
+- PII is redacted prior to storage or flagged and quarantined per DLP policies.
+
+Legal:
+- Provide deletion workflows and audit trails for data subject requests.
+- Liability and SLAs defined in master services agreement.
+
*** End Patch
*** End Patch
