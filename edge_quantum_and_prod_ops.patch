*** Begin Patch
*** Add File: quantum/adapters/base_adapter.py
+"""
+Quantum provider adapter base class.
+Defines a common interface for provider-specific adapters (Qiskit, Braket, Azure).
+"""
+from abc import ABC, abstractmethod
+from typing import Dict, Any
+
+
+class QuantumAdapter(ABC):
+    @abstractmethod
+    def submit(self, circuit: Any, shots: int = 1024, params: Dict[str, Any] = None) -> Dict[str, Any]:
+        """
+        Submit a circuit for execution. Returns a dict with provider_job_id and metadata.
+        """
+        pass
+
+    @abstractmethod
+    def status(self, job_id: str) -> Dict[str, Any]:
+        """
+        Return job status and provider metadata.
+        """
+        pass
+
+    @abstractmethod
+    def result(self, job_id: str) -> Dict[str, Any]:
+        """
+        Fetch raw result (counts, shots) and provider diagnostics/calibration snapshot.
+        """
+        pass
+
*** End Patch
*** Begin Patch
*** Add File: quantum/adapters/qiskit_adapter.py
+"""
+Qiskit provider adapter (IBM Q + Aer simulator).
+Requires qiskit installed on controller or in adapter runtime.
+"""
+import os
+from .base_adapter import QuantumAdapter
+from qiskit import transpile, assemble
+from qiskit.providers import ProviderV1 as Provider
+from qiskit import IBMQ, Aer, execute
+import tempfile
+
+
+class QiskitAdapter(QuantumAdapter):
+    def __init__(self, provider_name=None, use_simulator=False):
+        self.use_simulator = use_simulator
+        self.backend = None
+        if use_simulator:
+            self.backend = Aer.get_backend("aer_simulator")
+        else:
+            # expects IBMQ API token in env: IBMQ_API_TOKEN
+            token = os.environ.get("IBMQ_API_TOKEN")
+            if token:
+                IBMQ.enable_account(token)
+                providers = IBMQ.providers()
+                # choose provider_name or default
+                self.provider = providers[0]
+                self.backend = self.provider.backends()[0]
+            else:
+                raise RuntimeError("IBMQ_API_TOKEN not set for QiskitAdapter non-simulator mode")
+
+    def submit(self, circuit, shots: int = 1024, params: dict = None):
+        # transpile/assemble and submit
+        circ = transpile(circuit, self.backend)
+        qobj = assemble(circ, backend=self.backend, shots=shots)
+        job = self.backend.run(qobj) if hasattr(self.backend, "run") else self.backend.run(qobj)
+        return {"provider_job_id": job.job_id(), "backend_name": self.backend.name()}
+
+    def status(self, job_id: str):
+        job = self.backend.retrieve_job(job_id)
+        return {"status": job.status().name, "id": job.job_id()}
+
+    def result(self, job_id: str):
+        job = self.backend.retrieve_job(job_id)
+        res = job.result()
+        # try to capture calibration snapshot / metadata if available
+        metadata = {}
+        try:
+            metadata = job.backend().properties().to_dict() if job.backend() else {}
+        except Exception:
+            pass
+        return {"counts": res.get_counts(), "raw_data": None, "metadata": metadata}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/adapters/braket_adapter.py
+"""
+AWS Braket adapter scaffold.
+Requires AWS credentials with access to Braket, and amazon-braket-sdk installed.
+"""
+import os
+from .base_adapter import QuantumAdapter
+try:
+    from braket.aws import AwsDevice, AwsQuantumTask
+    from braket.circuits import Circuit
+except Exception:
+    AwsDevice = None
+
+
+class BraketAdapter(QuantumAdapter):
+    def __init__(self, device_arn=None):
+        if AwsDevice is None:
+            raise RuntimeError("amazon-braket-sdk not installed")
+        self.device_arn = device_arn or os.environ.get("BRAKET_DEVICE_ARN")
+        self.device = AwsDevice(self.device_arn)
+
+    def submit(self, circuit, shots: int = 1000, params: dict = None):
+        # expects circuit convertible to Braket Circuit
+        qtask = self.device.run(circuit, shots=shots)
+        return {"provider_job_id": qtask.id, "backend_name": self.device_arn}
+
+    def status(self, job_id: str):
+        task = AwsQuantumTask.get(job_id)
+        return {"status": task.metadata.status, "id": job_id}
+
+    def result(self, job_id: str):
+        task = AwsQuantumTask.get(job_id)
+        result = task.result()
+        # braket result contains measurement_counts
+        return {"counts": result.measurement_counts, "raw_data": None, "metadata": {"device_arn": self.device_arn}}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/adapters/azure_adapter.py
+"""
+Azure Quantum adapter scaffold.
+Requires azure-quantum SDK and provider token.
+"""
+import os
+from .base_adapter import QuantumAdapter
+try:
+    from azure.quantum import Workspace
+    from azure.quantum.job import Job
+except Exception:
+    Workspace = None
+
+
+class AzureAdapter(QuantumAdapter):
+    def __init__(self, target=None):
+        if Workspace is None:
+            raise RuntimeError("azure-quantum not installed")
+        # expects AZURE_ environment vars for workspace
+        self.ws = Workspace(
+            subscription_id=os.environ.get("AZ_SUB"),
+            resource_group=os.environ.get("AZ_RG"),
+            workspace_name=os.environ.get("AZ_WS"),
+            location=os.environ.get("AZ_LOCATION", "eastus")
+        )
+        self.target = target or os.environ.get("AZ_TARGET")
+
+    def submit(self, circuit, shots: int = 1000, params: dict = None):
+        # placeholder: assume circuit is QIR or QASM
+        job = self.ws.create_job(target=self.target, name=f"aegis-job-{int(os.times()[4])}", input_data=circuit, job_type="qir")
+        return {"provider_job_id": job.id, "backend_name": self.target}
+
+    def status(self, job_id: str):
+        job = self.ws.get_job(job_id)
+        return {"status": job.status, "id": job_id}
+
+    def result(self, job_id: str):
+        job = self.ws.get_job(job_id)
+        return {"counts": None, "raw_data": job.output_uri, "metadata": {"id": job_id, "target": self.target}}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/scheduler/scheduler.py
+"""
+Simple QuantumJob scheduler & queue.
+- Uses Postgres (SQLAlchemy) to persist jobs & quota accounting.
+- Background worker picks runnable jobs, routes to chosen provider adapter, records provider job id and provenance.
+"""
+import os
+import time
+import json
+import threading
+from sqlalchemy import create_engine, Column, Integer, String, Text, Float, Boolean, JSON
+from sqlalchemy.ext.declarative import declarative_base
+from sqlalchemy.orm import sessionmaker
+from sqlalchemy.exc import OperationalError
+
+DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@postgres:5432/aegis")
+engine = create_engine(DATABASE_URL, pool_size=10, max_overflow=20)
+Session = sessionmaker(bind=engine)
+Base = declarative_base()
+
+
+class QuantumJob(Base):
+    __tablename__ = "quantum_jobs"
+    id = Column(Integer, primary_key=True)
+    job_id = Column(String, unique=True, index=True)
+    circuit = Column(Text)  # serialized (QASM/QISKIT)
+    provider = Column(String)
+    target = Column(String)
+    shots = Column(Integer, default=1024)
+    status = Column(String, default="queued")
+    provider_job_id = Column(String, nullable=True)
+    created_at = Column(Float)
+    updated_at = Column(Float)
+    provenance = Column(JSON, nullable=True)
+    owner = Column(String, nullable=True)  # team/user
+
+
+def init_db():
+    try:
+        Base.metadata.create_all(bind=engine)
+    except OperationalError as e:
+        print("DB not ready", e)
+
+
+class Scheduler:
+    def __init__(self, adapters: dict):
+        self.adapters = adapters
+        self._stop = False
+
+    def start(self):
+        t = threading.Thread(target=self.loop, daemon=True)
+        t.start()
+
+    def stop(self):
+        self._stop = True
+
+    def loop(self):
+        init_db()
+        while not self._stop:
+            try:
+                s = Session()
+                job = s.query(QuantumJob).filter(QuantumJob.status == "queued").order_by(QuantumJob.created_at).first()
+                if job:
+                    # simple quota/billing check (placeholder)
+                    if not self._check_quota(job.owner):
+                        job.status = "blocked_quota"
+                        s.commit()
+                        s.close()
+                        time.sleep(1)
+                        continue
+                    adapter = self.adapters.get(job.provider)
+                    if not adapter:
+                        job.status = "failed"
+                        job.provenance = {"error": "no_adapter"}
+                        s.commit()
+                        s.close()
+                        continue
+                    # submit
+                    circ = job.circuit  # serialized; adapter may expect specific format
+                    res = adapter.submit(circ, shots=job.shots)
+                    job.provider_job_id = res.get("provider_job_id")
+                    job.status = "submitted"
+                    job.provenance = {"submitted_at": time.time(), "backend": res.get("backend_name")}
+                    s.commit()
+                s.close()
+            except Exception as e:
+                print("scheduler error", e)
+            time.sleep(1)
+
+    def _check_quota(self, owner):
+        # Placeholder: query quotas table; allow all for now
+        return True
+
*** End Patch
*** Begin Patch
*** Add File: quantum/controller/quantum_app.py
+"""
+QuantumJob API integrated into fleet controller.
+Provides endpoints to create QuantumJob, query status, cancel, and promote from simulator to QPU.
+"""
+import time
+import uuid
+import json
+import os
+from fastapi import FastAPI, HTTPException, Body, Depends
+from pydantic import BaseModel
+from sqlalchemy import create_engine
+from scheduler.scheduler import init_db, QuantumJob, Session, init_db as init_scheduler_db
+
+from quantum.adapters.qiskit_adapter import QiskitAdapter
+from quantum.adapters.braket_adapter import BraketAdapter
+from quantum.adapters.azure_adapter import AzureAdapter
+
+app = FastAPI(title="Aegis Quantum Controller")
+
+# instantiate adapters
+adapters = {
+    "qiskit_sim": QiskitAdapter(use_simulator=True),
+    "qiskit_ibm": None,
+    "braket": None,
+    "azure": None
+}
+try:
+    adapters["braket"] = BraketAdapter()
+except Exception:
+    pass
+try:
+    adapters["azure"] = AzureAdapter()
+except Exception:
+    pass
+
+sched = None
+
+class CreateQuantumJobReq(BaseModel):
+    circuit: str  # QASM or Qiskit serialized
+    provider: str  # e.g., qiskit_sim, braket, azure
+    target: str = None
+    shots: int = 1024
+    owner: str = "unknown"
+
+
+@app.on_event("startup")
+def startup():
+    global sched
+    init_scheduler_db()
+    from quantum.scheduler.scheduler import Scheduler
+    sched = Scheduler(adapters)
+    sched.start()
+
+
+@app.post("/api/v1/quantum_jobs")
+def create_job(req: CreateQuantumJobReq):
+    s = Session()
+    job_id = str(uuid.uuid4())
+    qj = QuantumJob(job_id=job_id, circuit=req.circuit, provider=req.provider, target=req.target, shots=req.shots, status="queued", created_at=time.time(), updated_at=time.time(), owner=req.owner)
+    s.add(qj)
+    s.commit()
+    s.close()
+    return {"job_id": job_id}
+
+
+@app.get("/api/v1/quantum_jobs/{job_id}")
+def get_job(job_id: str):
+    s = Session()
+    job = s.query(QuantumJob).filter(QuantumJob.job_id == job_id).first()
+    if not job:
+        raise HTTPException(status_code=404, detail="not found")
+    return {"job": {"job_id": job.job_id, "status": job.status, "provider_job_id": job.provider_job_id, "provenance": job.provenance}}
+
+
+@app.post("/api/v1/quantum_jobs/{job_id}/promote")
+def promote_to_qpu(job_id: str, provider: str = Body(..., embed=True)):
+    """
+    Promote a job that ran on simulator to a real QPU by creating a new submission to target provider.
+    CI must have run simulator validation and this endpoint should be guarded by auth/approval in production.
+    """
+    s = Session()
+    job = s.query(QuantumJob).filter(QuantumJob.job_id == job_id).first()
+    if not job:
+        raise HTTPException(status_code=404, detail="not found")
+    # check job ran on simulator and passed validation (provenance can include a flag)
+    if not job.provenance or not job.provenance.get("sim_passed"):
+        raise HTTPException(status_code=400, detail="simulator validation not passed")
+    adapter = adapters.get(provider)
+    if not adapter:
+        raise HTTPException(status_code=400, detail="provider adapter missing")
+    # create new job entry targeting provider
+    new_job_id = str(uuid.uuid4())
+    new = QuantumJob(job_id=new_job_id, circuit=job.circuit, provider=provider, target=None, shots=job.shots, status="queued", created_at=time.time(), updated_at=time.time(), owner=job.owner)
+    s.add(new)
+    s.commit()
+    s.close()
+    return {"new_job_id": new_job_id}
+
*** End Patch
*** Begin Patch
*** Add File: quantum/transpile/noise_aware.py
+"""
+Noise-aware transpiler wrapper (Qiskit).
+Given a target backend (Qiskit backend or device properties), runs transpile with appropriate basis_gates,
+optimization_level and optionally applies simple error mitigation postprocessing (readout mitigation).
+"""
+import os
+from qiskit import transpile
+from qiskit.providers.aer.noise import NoiseModel
+from qiskit.ignis.mitigation.measurement import complete_meas_cal, CompleteMeasFitter
+from qiskit import Aer, execute
+import tempfile
+import json
+
+
+def transpile_for_device(circuit, backend, optimization_level=3):
+    # backend may expose basis_gates and coupling_map
+    transpiled = transpile(circuit, backend=backend, optimization_level=optimization_level)
+    return transpiled
+
+
+def apply_readout_mitigation(backend, circuits, shots=1024):
+    # generate calibration circuits and run them on backend (simulator or hardware)
+    meas_calibs, state_labels = complete_meas_cal(qubit_list=range(backend.configuration().n_qubits), circlabel='mcal')
+    job = execute(meas_calibs, backend=backend, shots=shots)
+    cal_results = job.result()
+    meas_fitter = CompleteMeasFitter(cal_results, state_labels)
+    fitter = meas_fitter
+    return fitter
+
*** End Patch
*** Begin Patch
*** Add File: quantum/provenance/record.py
+"""
+QPU provenance recorder:
+- Save transpiled circuit, calibration snapshot, provider job id, raw shots, and attach to MLflow.
+- Optionally submit a Rekor attestation for the manifest describing the QPU run.
+"""
+import os
+import json
+import mlflow
+import subprocess
+from pathlib import Path
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
+REKOR_CLI = os.environ.get("REKOR_CLI", "rekor-cli")
+USE_MLFLOW = bool(MLFLOW_URI)
+
+
+def record_provenance(run_id: str, job_meta: dict, transpiled_circuit: str, calibration: dict, raw_shots: dict):
+    if USE_MLFLOW:
+        mlflow.set_tracking_uri(MLFLOW_URI)
+        with mlflow.start_run(run_id=run_id):
+            Path("/tmp/qpu_prov").mkdir(parents=True, exist_ok=True)
+            Path("/tmp/qpu_prov/transpiled.qasm").write_text(transpiled_circuit)
+            Path("/tmp/qpu_prov/calibration.json").write_text(json.dumps(calibration))
+            Path("/tmp/qpu_prov/raw_shots.json").write_text(json.dumps(raw_shots))
+            mlflow.log_artifact("/tmp/qpu_prov", artifact_path="qpu_provenance")
+    # build manifest for Rekor attestation
+    manifest = {"run_id": run_id, "job_meta": job_meta}
+    manifest_path = f"/tmp/qpu_prov/manifest_{run_id}.json"
+    with open(manifest_path, "w") as fh:
+        json.dump(manifest, fh)
+    sig = f"{manifest_path}.sig"
+    # sign manifest via cosign/KMS if available (cosign must be configured)
+    try:
+        if os.environ.get("COSIGN_KMS_KEY"):
+            subprocess.check_call(["cosign", "sign", "--key", os.environ.get("COSIGN_KMS_KEY"), manifest_path])
+        else:
+            # fallback to local cosign key
+            subprocess.check_call(["cosign", "sign", manifest_path])
+        # Rekor submission is generally handled by cosign; optionally call rekor-cli to verify
+    except Exception as e:
+        print("cosign signing failed", e)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/billing/quotas.py
+"""
+Simple billing and quota accounting.
+- Tracks used QPU shots per team and enforces monthly quotas.
+"""
+import os
+import time
+from sqlalchemy import create_engine, Column, Integer, String, Float
+from sqlalchemy.ext.declarative import declarative_base
+from sqlalchemy.orm import sessionmaker
+
+DATABASE_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@postgres:5432/aegis")
+engine = create_engine(DATABASE_URL)
+Session = sessionmaker(bind=engine)
+Base = declarative_base()
+
+
+class Quota(Base):
+    __tablename__ = "qpu_quotas"
+    owner = Column(String, primary_key=True)
+    monthly_shots = Column(Integer, default=100000)
+    used_shots = Column(Integer, default=0)
+    updated_at = Column(Float)
+
+
+def init_db():
+    Base.metadata.create_all(bind=engine)
+
+
+def check_and_consume(owner: str, shots: int) -> bool:
+    s = Session()
+    q = s.query(Quota).get(owner)
+    if not q:
+        q = Quota(owner=owner, monthly_shots=int(os.environ.get("DEFAULT_QPU_SHOTS", "100000")), used_shots=0, updated_at=time.time())
+        s.add(q)
+        s.commit()
+    if q.used_shots + shots > q.monthly_shots:
+        s.close()
+        return False
+    q.used_shots += shots
+    q.updated_at = time.time()
+    s.commit()
+    s.close()
+    return True
+
*** End Patch
*** Begin Patch
*** Add File: quantum/monitoring/quantum_prom_rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-quantum-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: quantum.rules
+      rules:
+        - alert: QuantumJobQueueHigh
+          expr: increase(quantum_jobs_queued_total[5m]) > 10
+          for: 5m
+          labels:
+            severity: warning
+          annotations:
+            summary: "Quantum job queue growing"
+            description: "More than 10 quantum jobs queued in 5m"
+        - alert: QPUErrorRateHigh
+          expr: rate(quantum_job_failures_total[5m]) / rate(quantum_job_submissions_total[5m]) > 0.2
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "High QPU error rate"
+            description: "More than 20% of QPU jobs failing"
+
*** End Patch
*** Begin Patch
*** Add File: quantum/simulator/simulate_and_promote.py
+"""
+Simulator integration and promote path:
+- Run circuit on local simulator (Qiskit Aer), run validation tests (e.g., fidelity or accuracy),
+  and if pass, call controller to promote to QPU (requires controller approval token).
+"""
+import os
+import json
+import requests
+from qiskit import QuantumCircuit, Aer, execute
+import time
+
+SIM_ENDPOINT = os.environ.get("SIM_ENDPOINT", "local")
+CONTROLLER_URL = os.environ.get("CONTROLLER_URL", "http://localhost:8080")
+API_KEY = os.environ.get("CONTROLLER_API_KEY", "")
+
+def run_simulation(circuit_qasm: str, shots=1024):
+    backend = Aer.get_backend("aer_simulator")
+    circuit = QuantumCircuit.from_qasm_str(circuit_qasm)
+    job = execute(circuit, backend=backend, shots=shots)
+    res = job.result()
+    counts = res.get_counts()
+    # simple validation: non-empty counts
+    passed = len(counts) > 0
+    return {"counts": counts, "passed": passed}
+
+def promote_sim_job(sim_job_id: str, provider: str):
+    headers = {"X-API-Key": API_KEY, "Content-Type": "application/json"}
+    r = requests.post(f"{CONTROLLER_URL}/api/v1/quantum_jobs/{sim_job_id}/promote", json={"provider": provider}, headers=headers, timeout=15)
+    return r.status_code, r.text
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--circuit-file", required=True)
+    p.add_argument("--shots", type=int, default=1024)
+    p.add_argument("--promote-to", default="braket")
+    args = p.parse_args()
+    qasm = open(args.circuit_file).read()
+    sim = run_simulation(qasm, shots=args.shots)
+    print("sim result:", sim)
+    if sim["passed"]:
+        # create a simulator job in controller first; for demo we assume a job_id equals timestamp
+        job_id = f"sim-{int(time.time())}"
+        # in real flow we'd call create_job api; here we directly attempt promote
+        code, text = promote_sim_job(job_id, args.promote_to)
+        print("promote:", code, text)
+    else:
+        print("simulation failed, abort promote")
+
*** End Patch
*** Begin Patch
*** Add File: rekor/ci_cosign_rekor.yml
+name: CI: cosign + Rekor sign & attest
+on:
+  workflow_dispatch:
+
+jobs:
+  sign-manifest:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install cosign
+        run: |
+          curl -LO https://github.com/sigstore/cosign/releases/download/v1.13.0/cosign-linux-amd64
+          chmod +x cosign-linux-amd64
+          sudo mv cosign-linux-amd64 /usr/local/bin/cosign
+      - name: Sign manifest with KMS (if configured) or local key
+        env:
+          COSIGN_KMS_KEY: ${{ secrets.COSIGN_KMS_KEY }} # e.g., gcpkms://...
+          COSIGN_PASSWORD: ${{ secrets.COSIGN_PASSWORD }}
+        run: |
+          if [ -n "${COSIGN_KMS_KEY}" ]; then
+            cosign sign --key ${COSIGN_KMS_KEY} manifest.json
+          else
+            echo "${{ secrets.COSIGN_PRIVATE_KEY }}" > cosign.key
+            cosign sign --key cosign.key manifest.json
+          cosign verify manifest.json || true
+      - name: Rekor verify via cosign (cosign submits to Rekor)
+        run: |
+          cosign triangulate --rekor-server https://rekor.sigstore.dev manifest.json || true
+
*** End Patch
*** Begin Patch
*** Add File: ci/hardware_runners/provision_secure_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Provision a self-hosted GitHub Actions runner on Jetson/ARM with secure defaults
+REPO="$1"    # owner/repo
+TOKEN="$2"   # runner registration token
+NAME="${3:-jetson-runner}"
+
+ARCH=$(uname -m)
+mkdir -p actions-runner && cd actions-runner
+curl -O -L https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-linux-${ARCH}-2.308.0.tar.gz
+tar xzf actions-runner-linux-${ARCH}-2.308.0.tar.gz
+./config.sh --url https://github.com/${REPO} --token "${TOKEN}" --name "${NAME}" --labels "self-hosted,jetson,${ARCH}" --unattended
+# Create systemd unit for runner
+sudo tee /etc/systemd/system/github-runner.service >/dev/null <<EOF
+[Unit]
+Description=GitHub Actions Runner
+After=network.target
+
+[Service]
+Type=simple
+User=$(whoami)
+WorkingDirectory=$(pwd)
+ExecStart=$(pwd)/run.sh
+Restart=always
+RestartSec=5
+
+[Install]
+WantedBy=multi-user.target
+EOF
+sudo systemctl daemon-reload
+sudo systemctl enable --now github-runner.service
+echo "Runner installed and running as systemd service"
+
*** End Patch
*** Begin Patch
*** Add File: ci/hardware_runners/validate_tensorrt_on_board.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Validate TensorRT artifact on Jetson/ARM board
+MODEL_PLAN="$1"
+TEST_INPUT="$2"  # path to test input numpy or sample
+PYTHON_BIN="${3:-python3}"
+# run a simple inference script (assumes runtime/edge_runtime has TRT support)
+${PYTHON_BIN} - <<PY
+import tensorrt as trt, numpy as np, sys
+print("loading plan", "${MODEL_PLAN}")
+# Placeholder: add proper TRT engine load and inference flow
+print("Assuming inference OK (placeholder).")
+PY
+echo "Tensorrt artifact validation placeholder completed"
+
*** End Patch
*** Begin Patch
*** Add File: hardening/pki/crl_and_rotation.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate device certs via Vault PKI, generate CRL, and optionally push to MDM endpoint.
+ROLE="${1:-aegis-device-role}"
+MDM_PUSH_URL="${2:-}"
+
+# Issue new certs for devices list (assumes device list in devices.txt)
+while read -r dev; do
+  CN="${dev}.devices.aegis"
+  echo "Issuing cert for $dev"
+  cert_json=$(vault write -format=json pki_int/issue/${ROLE} common_name="${CN}" ttl="720h")
+  echo "$cert_json" | jq -r '.data.certificate' > /tmp/${dev}_cert.pem
+  echo "$cert_json" | jq -r '.data.private_key' > /tmp/${dev}_key.pem
+  # Store into vault kv for device consumption
+  vault kv put secret/aegis/devices/${dev} cert=@/tmp/${dev}_cert.pem key=@/tmp/${dev}_key.pem
+done < devices.txt
+
+# Generate CRL from intermediate CA (if supported)
+echo "Generating CRL (placeholder - depends on Vault PKI configuration)."
+# If MDM_PUSH_URL provided, POST certs/metadata
+if [ -n "${MDM_PUSH_URL}" ]; then
+  for f in /tmp/*_cert.pem; do
+    dev=$(basename $f | cut -d'_' -f1)
+    curl -X POST -H "Content-Type: application/x-pem-file" --data-binary "@${f}" "${MDM_PUSH_URL}/api/v1/devices/${dev}/cert" || true
+  done
+fi
+echo "Rotation & CRL generation placeholder complete"
+
*** End Patch
*** Begin Patch
*** Add File: ci/image_hardening/image_sign_and_scan.yml
+name: Image sign & vulnerability scan
+on:
+  workflow_dispatch:
+
+jobs:
+  scan-and-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Build image
+        run: |
+          docker build -t aegis/edge-runtime:latest runtime/edge_runtime
+      - name: Scan image with Trivy
+        uses: aquasecurity/trivy-action@v0.1.0
+        with:
+          image-ref: aegis/edge-runtime:latest
+      - name: Sign image with cosign
+        env:
+          COSIGN_KMS_KEY: ${{ secrets.COSIGN_KMS_KEY }}
+        run: |
+          if [ -n "${COSIGN_KMS_KEY}" ]; then
+            cosign sign --key ${COSIGN_KMS_KEY} aegis/edge-runtime:latest
+          else
+            echo "${{ secrets.COSIGN_PRIVATE_KEY }}" > cosign.key
+            cosign sign --key cosign.key aegis/edge-runtime:latest
+          cosign verify aegis/edge-runtime:latest || true
+
*** End Patch
*** Begin Patch
*** Add File: edge/controller/ha/helm/values.yaml
+replicaCount: 3
+image:
+  repository: aegis/fleet-controller
+  tag: latest
+resources:
+  limits:
+    cpu: "2"
+    memory: "2Gi"
+  requests:
+    cpu: "500m"
+    memory: "512Mi"
+postgresql:
+  enabled: false
+  host: postgres.aegis.svc.cluster.local
+  user: aegis
+  password: aegis
+  database: aegis
+ingress:
+  enabled: true
+  tls:
+    enabled: true
+  annotations:
+    nginx.ingress.kubernetes.io/auth-tls-verify-client: "on"
+    nginx.ingress.kubernetes.io/auth-tls-secret: "aegis/ca-secret"
+    nginx.ingress.kubernetes.io/auth-tls-verify-depth: "2"
+
*** End Patch
*** Begin Patch
*** Add File: edge/controller/load_test/locustfile.py
+from locust import HttpUser, task, between
+
+class QuantumUser(HttpUser):
+    wait_time = between(1, 2)
+
+    @task(3)
+    def poll_bundle(self):
+        self.client.get("/api/v1/devices/device-123/bundle")
+
+    @task(1)
+    def submit_telemetry(self):
+        self.client.post("/api/v1/devices/device-123/telemetry", json={"status":"ok"})
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/quantum_sre.md
+# Quantum SRE Runbook (starter)
+
+1) Alert: QuantumJobQueueHigh or QPUErrorRateHigh
+   - Check quantum controller health: kubectl logs deploy/aegis-quantum-controller
+   - Inspect job queue: SELECT count(*) FROM quantum_jobs WHERE status IN ('queued','submitted');
+2) If QueueHigh:
+   - Identify backlog root cause: provider quota, provider outage, or scheduler stuck.
+   - Scale scheduler workers (increase replicas / worker threads).
+3) If QPUErrorRateHigh:
+   - Fetch sample failed job ids and inspect provenance artifacts in MLflow (qpu_provenance).
+   - Check provider status dashboards (IBM/Braket/Azure) for known incidents.
+   - If cause is calibration, wait for provider calibration refresh or re-transpile with lower optimization.
+4) Mitigation:
+   - Pause promotions: disable new rollouts to QPU until resolved.
+   - Rollback to previous run or route jobs to simulator.
+5) Postmortem:
+   - Gather job manifests, provider job ids, calibration snapshots, failure rates, and timeline.
+   - Update device catalog or transpiler heuristics to avoid recurrence.
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/edge_scale.md
+# Edge Scale & HA Runbook (starter)
+
+1) Scaling controller replicas:
+   - Ensure Postgres is highly available (primary/replica) and connection pool sizes are tuned.
+   - Increase controller replicaCount in Helm values and run helm upgrade.
+2) Load testing:
+   - Use locust (edge/controller/load_test/locustfile.py) to simulate thousands of device polls.
+   - Tune database indices and add read replicas for telemetry ingestion (sharding/partitioning).
+3) Rolling updates & zero-downtime:
+   - Use PodDisruptionBudgets and rollingUpdate strategy with maxUnavailable=1.
+4) Runbook for incident:
+   - If DB CPU high, scale RDS/Postgres and offload telemetry aggregation to background workers.
+   - If controller high CPU, scale replicas and consider sharding device assignments across controller instances.
+
*** End Patch
*** End Patch
