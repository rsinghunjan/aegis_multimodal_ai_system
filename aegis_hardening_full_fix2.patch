*** Begin Patch
*** Add File:deploy/vault/ha-values-raft.yaml
+# Vault Helm values for HA using Raft storage with KMS auto-unseal and TLS via cert-manager
+server:
+  ha:
+    enabled: true
+  dataStorage:
+    enabled: false
+  standalone:
+    enabled: false
+  extraEnvironmentVars:
+    - name: VAULT_LOG_LEVEL
+      value: "info"
+unsealer:
+  aws:
+    enabled: true
+    region: "us-west-2"                 # << REPLACE
+    kmsKeyId: "arn:aws:kms:...:key/..." # << REPLACE KMS key ARN for auto-unseal
+s3:
+  enabled: false
+# Raft storage (in-cluster) - recommended for Vault HA with raft
+server:
+  raft:
+    enabled: true
+    # raft.migrate is optional for upgrades; configure storage class if needed
+tls:
+  enabled: true
+  # Operator must provision a cert-manager Certificate bound to the vault service DNS.
+  certManager:
+    enabled: true
+    issuerRef:
+      name: vault-ca-issuer
+      kind: ClusterIssuer
+resources:
+  requests:
+    cpu: 250m
+    memory: 512Mi
+  limits:
+    cpu: 1
+    memory: 2Gi
+
+# NOTES:
+# - Install cert-manager and create a CA ClusterIssuer/vault-ca-issuer (see cert-manager manifests file).
+# - Ensure the KMS key policy allows the Vault node IAM role to Decrypt/GenerateDataKey.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/cert-manager-issuer-and-ca.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: cert-manager
+---
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: vault-ca-issuer
+spec:
+  selfSigned: {}
+
+---
+apiVersion: cert-manager.io/v1
+kind: Certificate
+metadata:
+  name: vault-server-tls
+  namespace: aegis
+spec:
+  secretName: vault-server-tls
+  subject:
+    organizations: ["aegis"]
+  commonName: "vault.aegis.svc"
+  dnsNames:
+    - "vault.aegis.svc"
+  issuerRef:
+    name: vault-ca-issuer
+    kind: ClusterIssuer
+  duration: 2160h
+  renewBefore: 360h
+
+# Operator: Install cert-manager (CRDs) before applying and consider using a real CA for production.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/vault/init-vault-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: vault-init
+  namespace: aegis
+spec:
+  template:
+    spec:
+      serviceAccountName: vault
+      restartPolicy: OnFailure
+      containers:
+        - name: init
+          image: hashicorp/vault:1.14.0
+          env:
+            - name: VAULT_ADDR
+              value: "https://vault.aegis.svc:8200"
+          command:
+            - sh
+            - -c
+            - |
+              set -e
+              if vault status >/dev/null 2>&1; then
+                echo "Vault already initialized/unsealed or unreachable"
+              else
+                vault operator init -key-shares=1 -key-threshold=1 -format=json > /tmp/vault-init.json
+                kubectl create secret generic vault-init --from-file=/tmp/vault-init.json -n aegis || true
+                # Example policy
+                cat > /tmp/aegis-policy.hcl <<'HCL'
+path "secret/data/aegis/*" {
+  capabilities = ["read","list"]
+}
+HCL
+                vault login $(jq -r '.root_token' /tmp/vault-init.json)
+                vault policy write aegis-policy /tmp/aegis-policy.hcl
+                # enable k8s auth (operator must configure service account token reviewer)
+                vault auth enable kubernetes || true
+                # configure kubernetes auth with placeholder values (operator replace)
+                vault write auth/kubernetes/config \
+                  token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
+                  kubernetes_host="https://kubernetes.default.svc" \
+                  kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt || true
+                vault write auth/kubernetes/role/aegis-role \
+                  bound_service_account_names="*" \
+                  bound_service_account_namespaces="aegis" \
+                  policies="aegis-policy" \
+                  ttl="24h" || true
+              fi
+      restartPolicy: OnFailure
+
+# NOTE: This job is an operator convenience. In production take care with root token handling and key storage.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/rekoar/rekor-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          ports:
+            - containerPort: 3000
+          env:
+            - name: REKOR_STORAGE_TYPE
+              value: "sqlite" # operator: replace with postgres for production
+            - name: REKOR_API_PORT
+              value: "3000"
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor
+  namespace: aegis
+spec:
+  selector:
+    app: rekor
+  ports:
+    - name: http
+      port: 3000
+      targetPort: 3000
+
+# For production: deploy Postgres and configure REKOR_STORAGE_TYPE=postgres with connection string and TLS.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/cosign/cosign-readme.md
+Cosign production notes (KMS-backed signing)
+
+- To sign images with AWS KMS:
+  - Create a KMS key and grant usage to the CI identity.
+  - Use cosign with the KMS key URI: cosign sign --key aws-kms://arn:aws:kms:... <image>
+  - Rekor should be reachable; pass --rekor-url https://rekor.aegis.svc:3000
+
+- Example GitHub Action step:
+  - name: Cosign sign (AWS KMS)
+    env:
+      COSIGN_EXPERIMENTAL: "1"
+    run: |
+      cosign sign --key "awskms://arn:aws:kms:us-west-2:123456789012:key/EXAMPLE" --rekor-url "${{ secrets.REKOR_URL }}" $IMAGE
+
+Replace AWS key ARN and ensure GitHub runner has AWS credentials.
+
*** End Patch
*** Begin Patch
*** Add File:deploy/signing-proxy/mtls-secrets-example.yaml
+# Example: create mTLS secrets (operator must replace with real certs or use cert-manager)
+apiVersion: v1
+kind: Secret
+metadata:
+  name: signing-proxy-tls
+  namespace: aegis
+type: kubernetes.io/tls
+data:
+  tls.crt: "BASE64_CERT"    # << REPLACE (base64)
+  tls.key: "BASE64_KEY"     # << REPLACE
+
+---
+apiVersion: v1
+kind: Secret
+metadata:
+  name: signing-proxy-client-ca
+  namespace: aegis
+type: Opaque
+data:
+  ca.crt: "BASE64_CA_CERT"  # << REPLACE
+
+# In production you should provision server cert via cert-manager and distribute client CA to CI runners securely.
+
*** End Patch
*** Begin Patch
*** Add File:ops/signing-proxy/hardened-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: signing-proxy
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: signing-proxy
+  template:
+    metadata:
+      labels:
+        app: signing-proxy
+    spec:
+      serviceAccountName: signing-proxy-sa
+      containers:
+        - name: signing-proxy
+          image: registry.example.com/aegis/signing-proxy:latest
+          args:
+            - "--port=8443"
+            - "--tls-cert=/tls/tls.crt"
+            - "--tls-key=/tls/tls.key"
+            - "--client-ca=/tls/ca.crt"
+          ports:
+            - containerPort: 8443
+          volumeMounts:
+            - name: tls
+              mountPath: /tls
+              readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: signing-proxy-tls
+
+# The signing-proxy must validate client certs (mTLS) and issue ephemeral signing tokens via Vault.
+# Implementation notes:
+# - The proxy should accept a client cert and map to a Vault role; it should request an ephemeral signing token signed by Vault.
+# - All signing operations must be logged to an audit sink (EVIDENCE_BUCKET) and sent to Rekor.
+
*** End Patch
*** Begin Patch
*** Add File:ops/network/networkpolicy-vault.yaml
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: vault-restrict
+  namespace: aegis
+spec:
+  podSelector:
+    matchLabels:
+      app.kubernetes.io/name: vault
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              name: aegis
+        - podSelector:
+            matchLabels:
+              app: signing-proxy
+      ports:
+        - protocol: TCP
+          port: 8200
+  policyTypes:
+    - Ingress
+
+# This limits Vault access to the aegis namespace and signing-proxy. Extend selectors as needed.
+
*** End Patch
*** Begin Patch
*** Add File:ops/s3/create_buckets_and_kms.sh
+#!/usr/bin/env bash
+set -euo pipefail
+AWS_REGION=${AWS_REGION:-us-west-2}
+EVIDENCE_BUCKET=${EVIDENCE_BUCKET:-aegis-evidence-bucket}
+MODEL_BUCKET=${MODEL_ARTIFACT_BUCKET:-aegis-model-bucket}
+KMS_ALIAS=${KMS_ALIAS:-alias/aegis-vault-unseal}
+
+echo "Creating S3 buckets and KMS key (placeholders) - update names before running in production!"
+aws s3api create-bucket --bucket "${EVIDENCE_BUCKET}" --region "${AWS_REGION}" --create-bucket-configuration LocationConstraint="${AWS_REGION}"
+aws s3api create-bucket --bucket "${MODEL_BUCKET}" --region "${AWS_REGION}" --create-bucket-configuration LocationConstraint="${AWS_REGION}"
+
+echo "Creating KMS key for Vault auto-unseal"
+KMS_KEY_ID=$(aws kms create-key --description "Aegis Vault auto-unseal key" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name "${KMS_ALIAS}" --target-key-id "${KMS_KEY_ID}"
+echo "KMS KeyId: ${KMS_KEY_ID}"
+
+echo "Enable default encryption on S3 buckets"
+aws s3api put-bucket-encryption --bucket "${EVIDENCE_BUCKET}" --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"aws:kms","KMSMasterKeyID":"'"${KMS_KEY_ID}"'"}}]}'
+aws s3api put-bucket-encryption --bucket "${MODEL_BUCKET}" --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"aws:kms","KMSMasterKeyID":"'"${KMS_KEY_ID}"'"}}]}'
+
+echo "NOTE: For object-lock (WORM) you must enable during bucket creation (not supported by this script)."
+echo "Record KMS KeyId to use as VAULT auto-unseal KMS ARN: arn:aws:kms:${AWS_REGION}:$(aws sts get-caller-identity --query Account --output text):key/${KMS_KEY_ID}"
+
*** End Patch
*** Begin Patch
*** Add File:deploy/velero/velero-values-aws.yaml
+configuration:
+  provider: aws
+  backupStorageLocation:
+    name: default
+    bucket: "aegis-velero-backups"   # << REPLACE
+    config:
+      region: "us-west-2"
+  volumeSnapshotLocation:
+    name: default
+    config:
+      region: "us-west-2"
+initContainers: []
+credentials:
+  secretContents:
+    cloud: |
+      [default]
+      aws_access_key_id = YOUR_AWS_ACCESS_KEY_ID
+      aws_secret_access_key = YOUR_AWS_SECRET_ACCESS_KEY
+
+# Operator: Use velero CLI to install with this configuration or helm chart. To use PV snapshots enable provider plugin.
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/staging-enforcement.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: aegis-staging
+  labels:
+    env: staging
+
+---
+# ConstraintTemplate already defined earlier; here we create a Constraint in dry-run for staging
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: RequireCosignAttestation
+metadata:
+  name: cosign-required-staging
+spec:
+  enforcementAction: dryrun
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+    namespaces:
+      - "aegis-staging"
+
+---
+# Allow privileged pods only in namespaces labeled hil=approved for production gating
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: allowprivilegedinlabel
+spec:
+  crd:
+    spec:
+      names:
+        kind: AllowPrivilegedInLabel
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package allowprivilegedinlabel
+        violation[{"msg": msg}] {
+          input.review.object.spec.containers[_].securityContext.privileged == true
+          not input.review.object.metadata.namespace
+          msg := "Privileged pods not allowed"
+        }
+
*** End Patch
*** Begin Patch
*** Add File:falco/outputs/webhook-config.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: falco-outputs
+  namespace: kube-system
+data:
+  falco_rules.yaml: |
+    # Configure webhook output for SIEM integration (Elastic or Splunk)
+    # In Falco, use falco-sidekick or external forwarder to send to SIEM.
+    # This is a stub: operator must deploy a sidecar or falco-sidekick to forward events.
+    # Example forwarder settings are left to operator.
+
*** End Patch
*** Begin Patch
*** Add File:logging/fluentbit/output-elastic.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluent-bit-elastic-output
+  namespace: logging
+data:
+  elastic.conf: |
+    [OUTPUT]
+        Name  es
+        Match *
+        Host  elasticsearch.logging.svc
+        Port  9200
+        HTTP_User  elastic
+        HTTP_Passwd  ${ELASTIC_PASSWORD}
+        tls   On
+        tls.verify   Off
+
+# Operator: set ELASTIC_PASSWORD via environment in Fluent Bit daemonset/helm values
+
*** End Patch
*** Begin Patch
*** Add File:hil/guidance/privileged-hil-policy.md
+# HIL Hardware-in-the-Loop (HIL) Privileged Pod Policy & Process
+
+1. Policy:
+   - Privileged pods are NOT allowed by default.
+   - To run HIL privileged pods, create a namespace and label it: kubectl create ns hil-test && kubectl label ns hil-test hil=approved
+   - Gatekeeper should have constraint to only allow privileged pods in namespaces with label hil=approved.
+
+2. Process:
+   - Operator requests approval by creating a signed ticket in the ticketing system.
+   - Admin applies label to namespace and deploys the privileged HIL pod manifest.
+   - After testing, admin removes label and deletes privileged pods.
+
+3. Example privileged manifest (apply only after approval)
+   kubectl apply -f adapters/hil-privileged-pod.yaml
+
+4. Safety:
+   - Ensure hardware is powered in safe mode and actuators disabled. Have a kill switch and human-in-loop.
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ops/run_restore_drill.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NAMESPACE=${1:-aegis}
+BACKUP_NAME="drill-$(date +%s)"
+echo "Creating Velero backup ${BACKUP_NAME} for namespace ${NAMESPACE}"
+velero backup create "${BACKUP_NAME}" --include-namespaces "${NAMESPACE}"
+echo "Waiting for backup to complete..."
+velero backup wait "${BACKUP_NAME}" --for=complete --timeout 10m || { echo "Backup did not complete"; exit 2; }
+echo "Creating temporary restore namespace: ${NAMESPACE}-restore"
+kubectl create ns "${NAMESPACE}-restore" || true
+RESTORE_NAME="${BACKUP_NAME}-restore"
+velero restore create --from-backup "${BACKUP_NAME}" --namespace-mappings "${NAMESPACE}:${NAMESPACE}-restore" "${RESTORE_NAME}"
+echo "Waiting for restore to complete..."
+velero restore wait "${RESTORE_NAME}" --for=complete --timeout 15m || { echo "Restore did not complete"; exit 2; }
+echo "Restore completed. Run smoke tests against ${NAMESPACE}-restore and then delete it."
+echo "Cleanup: velero restore delete ${RESTORE_NAME} ; kubectl delete ns ${NAMESPACE}-restore ; velero backup delete ${BACKUP_NAME}"
+
*** End Patch
*** Begin Patch
*** Add File:scripts/ops/vault_policy_loader.sh
+#!/usr/bin/env bash
+set -euo pipefail
+VAULT_ADDR=${VAULT_ADDR:-https://vault.aegis.svc:8200}
+POLICY_FILE=${1:-deploy/vault/policies/aegis-policy.hcl}
+if [ ! -f "${POLICY_FILE}" ]; then
+  echo "Policy file not found: ${POLICY_FILE}"
+  exit 2
+fi
+if ! command -v vault >/dev/null 2>&1; then
+  echo "vault cli required"
+  exit 2
+fi
+export VAULT_ADDR
+echo "Writing policy ${POLICY_FILE} to Vault"
+vault policy write aegis-policy "${POLICY_FILE}"
+echo "Ensuring Kubernetes auth role 'aegis-role' exists (operator: verify token reviewer and CA config manually)"
+vault write auth/kubernetes/role/aegis-role bound_service_account_names="*" bound_service_account_namespaces="aegis" policies="aegis-policy" ttl="24h" || true
+echo "Policy loader complete"
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/cosign_kms_sign.yml
+name: Cosign KMS sign example
+on:
+  workflow_dispatch:
+
+jobs:
+  cosign-sign:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Install cosign
+        run: |
+          curl -sSfL https://raw.githubusercontent.com/sigstore/cosign/main/install.sh | sh -s -- -b /usr/local/bin
+      - name: Sign image with AWS KMS
+        env:
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          IMAGE: ghcr.io/${{ github.repository_owner }}/aegis-sample:${{ github.sha }}
+        run: |
+          cosign sign --key "awskms://arn:aws:kms:${AWS_REGION}:${{ secrets.AWS_ACCOUNT_ID }}:key/${{ secrets.COSIGN_KMS_KEY_ID }}" --rekor-url "${{ secrets.REKOR_URL }}" "$IMAGE"
+
+# Requires: COSIGN_KMS_KEY_ID set in repo secrets and proper AWS credentials in the runner
+
*** End Patch
*** Begin Patch
*** Add File:gatekeeper/tuning/allowlist-namespace-labels.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: hil-approved
+  labels:
+    hil: "approved"
+
+---
+# Gatekeeper constraint to only allow privileged pods in namespaces labeled hil=approved
+apiVersion: templates.gatekeeper.sh/v1
+kind: ConstraintTemplate
+metadata:
+  name: privileged-in-approved
+spec:
+  crd:
+    spec:
+      names:
+        kind: PrivilegedInApprovedNamespace
+  targets:
+    - target: admission.k8s.gatekeeper.sh
+      rego: |
+        package privilegednamespace
+        violation[{"msg": msg}] {
+          some i
+          input.review.object.spec.containers[i].securityContext.privileged == true
+          not input.review.object.metadata.namespace
+          msg := "Privileged containers are restricted to approved namespaces"
+        }
+
+---
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: PrivilegedInApprovedNamespace
+metadata:
+  name: allow-priv-in-approved
+spec:
+  enforcementAction: dryrun
+  match:
+    kinds:
+      - apiGroups: [""]
+        kinds: ["Pod"]
+
+# Start in dryrun and migrate to enforce after testing.
+
*** End Patch
*** Begin Patch
*** Add File:falco/tuning/suppressions.yaml
+# Falco suppressions to reduce common false positives (example)
+suppressions:
+  - rule: Container shell
+    # Suppress for known CI runner images by container image name
+    imageRepository: "ghcr.io/aegis/ci-runner"
+  - rule: Falco write to sensitive mount
+    # Suppress in namespace 'kube-system' where some system components write secrets
+    namespace: "kube-system"
+
+# Operator: Expand suppression list based on observed false positives.
+
*** End Patch
*** Begin Patch
*** Add File:hil/adapters/README.md
+# HIL Vendor SDK Integration Guidance
+
+This directory contains instructions and placeholders for integrating vendor SDKs for MIL-STD-1553 / SpaceWire / other avionics bus interfaces.
+
+Steps:
+1) Obtain vendor SDK and install into a secure image to be run on HIL nodes.
+2) Modify adapters/milstd1553_adapter_v2.py and adapters/spacewire_adapter_v2.py to call vendor SDK functions instead of dry-run prints.
+3) Build an HIL runner image and host it on a private registry.
+4) Use adapters/hil-privileged-pod.yaml to deploy the HIL runner on nodes with PCI/USB devices passed through (requires cluster admin).
+
+Security notes:
+- Only run privileged HIL pods in approved namespaces (see gatekeeper/tuning/allowlist-namespace-labels.yaml).
+- Ensure hardware interlocks and manual kill switches exist.
+
*** End Patch
*** End Patch
