*** Begin Patch
*** Add File: provider/legal/gha_baa_gate.yml
+name: BAA Enforcement & Notification
+on:
+  workflow_dispatch:
+
+jobs:
+  check-baa-and-notify:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Check BAA for provider
+        env:
+          PROVIDER: ${{ inputs.provider || 'braket' }}
+        run: |
+          python provider/legal/check_baa.py $PROVIDER || (echo "BAA missing for $PROVIDER; sending notification" && python provider/legal/notify_baa_needed.py $PROVIDER && exit 2)
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/notify_baa_needed.py
+#!/usr/bin/env python3
+import sys, csv, smtplib
+from email.message import EmailMessage
+def load_contact(provider):
+    with open("provider/legal/baas_tracker.csv") as fh:
+        for r in csv.DictReader(fh):
+            if r["provider"]==provider:
+                return r.get("contact","")
+    return ""
+
+def send_alert(to, provider):
+    if not to:
+        print("No contact for provider", provider)
+        return
+    msg = EmailMessage()
+    msg["Subject"] = f"BAA required for provider {provider}"
+    msg["From"] = "ops@example.com"
+    msg["To"] = to
+    msg.set_content(f"Please sign the BAA for provider {provider} so regulated workloads may be sent.")
+    print("SEND EMAIL (stub) to", to)
+    # Integrate with real SMTP or PagerDuty here.
+
+if __name__=="__main__":
+    prov = sys.argv[1] if len(sys.argv)>1 else "braket"
+    contact = load_contact(prov)
+    send_alert(contact, prov)
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/baa_request_template.md
+BAA Request Template
+
+Provider: <provider name>
+Contact: <legal contact name / email>
+Requested by: <team/contact>
+Scope: Describe data types (PHI/PII), purpose (research/production), retention & deletion requirements.
+Security controls requested: encryption, KMS/HSM keys, audit logs (CloudTrail), attestation (Rekor), contract terms for sub-processors.
+SLA requirements: queue latency, availability, error budgets.
+Sign and return: attach signed PDF and run provider/legal/upload_baa.py to register.
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/cloudhsm_postprovision_playbook.yml
+- name: Initialize AWS CloudHSM cluster & create Crypto User (CU)
+  hosts: cloudhsm_admin
+  become: true
+  tasks:
+    - name: Ensure CloudHSM client tools present
+      apt:
+        name:
+          - libcloudhsm-client
+          - pkcs11-tool
+        state: present
+      ignore_errors: yes
+
+    - name: Initialize cluster (manual steps may be required)
+      debug:
+        msg: "Run AWS CloudHSM initialization steps per AWS docs. This playbook provides hooks to run initialization commands."
+
+    - name: Create Crypto User (CU) - placeholder
+      shell: |
+        echo "Create CloudHSM user via hsm_client or AWS CLI; then import or wrap keys for PKCS11 access."
+      changed_when: false
+
+    - name: Deploy PKCS11 config for cosign
+      copy:
+        dest: /etc/cosign/pkcs11.conf
+        content: |
+          module_path = /usr/lib/libcloudhsm_pkcs11.so
+          token_label = cosign-key
+      mode: "0600"
+
+    - name: Place cosign test script
+      copy:
+        src: hsm/pkcs11/cosign_pkcs11_test.sh
+        dest: /usr/local/bin/cosign_pkcs11_test.sh
+        mode: "0755"
+
*** End Patch
*** Begin Patch
*** Add File: hsm/init_cloudhsm.sh
+#!/usr/bin/env bash
+set -euo pipefail
+echo "CloudHSM initialization helper (scaffold)."
+echo "Follow AWS CloudHSM documentation to: create cluster, initialize, claim HSMs, create CU and token, configure client."
+echo "After initialization, run cosign pkcs11 signing test: ./hsm/pkcs11/cosign_pkcs11_test.sh manifest.json"
+exit 0
+
*** End Patch
*** Begin Patch
*** Add File: hsm/rotation/rotate_cosign_alias.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Rotate KMS alias or PKCS11 key pointer for cosign and update CI secrets.
+# For AWS KMS: create new key, point alias to new key; for CloudHSM PKCS11 you will update key label mapping.
+if [ "${CLOUD_HSM:-}" = "true" ]; then
+  echo "Rotate CloudHSM key: create new key in HSM and update COSIGN_PKCS11_LABEL mapping in CI"
+  # manual steps required: create key via HSM admin tools & update CI secret
+else
+  NEW_KEY_ID=$(aws kms create-key --description "Rotated cosign key" --query KeyMetadata.KeyId --output text)
+  aws kms create-alias --alias-name alias/aegis-cosign --target-key-id "$NEW_KEY_ID"
+  echo "New key created $NEW_KEY_ID. Update CI secret COSIGN_KMS_KEY to awskms:///$NEW_KEY_ID"
+  if command -v gh >/dev/null 2>&1; then
+    REPO="${1:-owner/repo}"
+    gh secret set COSIGN_KMS_KEY --repo "$REPO" --body "awskms:///$NEW_KEY_ID"
+    echo "Updated GitHub secret COSIGN_KMS_KEY"
+  fi
+fi
+echo "Rotate done. Audit CloudTrail events for kms:Sign usage."
+
*** End Patch
*** Begin Patch
*** Add File: hsm/cloudtrail/generate_audit_report.py
+#!/usr/bin/env python3
+"""
+Fetch and summarize CloudTrail KMS/CloudHSM events related to cosign signing usage.
+Outputs JSON summary for compliance review.
+"""
+import boto3, json, time, os
+from datetime import datetime, timedelta
+
+def lookup_events(key_id, minutes=60):
+    client = boto3.client("cloudtrail")
+    start = datetime.utcnow() - timedelta(minutes=minutes)
+    resp = client.lookup_events(LookupAttributes=[{"AttributeKey":"ResourceName","AttributeValue":key_id}], StartTime=start)
+    return resp.get("Events", [])
+
+if __name__=="__main__":
+    KEY_ID = os.environ.get("COSIGN_KEY_ID","")
+    if not KEY_ID:
+        print("COSIGN_KEY_ID env required")
+        raise SystemExit(2)
+    ev = lookup_events(KEY_ID, minutes=1440)
+    print(json.dumps(ev, default=str, indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/site_hardening_site.yml
+- name: Site hardening for Jetson runner site
+  hosts: runners
+  become: true
+  vars:
+    admin_allowed_cidrs: ["10.0.0.0/8"]
+  tasks:
+    - name: Ensure OS packages up to date
+      apt:
+        upgrade: dist
+        update_cache: yes
+
+    - name: Configure UFW default deny
+      ufw:
+        state: enabled
+        policy: deny
+
+    - name: Allow SSH only from bastion
+      ufw:
+        rule: allow
+        proto: tcp
+        port: 22
+        from_ip: "{{ hostvars['bastion-01'].ansible_host | default('10.0.0.10') }}"
+
+    - name: Install Fail2Ban
+      apt:
+        name: fail2ban
+        state: present
+
+    - name: Ensure unattended-upgrades present
+      apt:
+        name: unattended-upgrades
+        state: present
+
+    - name: Ensure disk encryption suggestion note
+      debug:
+        msg: "Ensure device disk encryption (LUKS) is enabled in procurement & provisioning pipeline."
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/cloud_init_runner.yaml
+#cloud-config
+users:
+  - name: aegis-runner
+    groups: docker
+    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
+    shell: /bin/bash
+runcmd:
+  - [ apt-get, update ]
+  - [ apt-get, install, -y, docker.io, python3-pip ]
+  - [ mkdir, -p, /home/aegis-runner/actions-runner ]
+  - [ wget, -O, /tmp/runner.tar.gz, "https://github.com/actions/runner/releases/download/v2.308.0/actions-runner-linux-arm64-2.308.0.tar.gz" ]
+  - [ tar, xzf, /tmp/runner.tar.gz, -C, /home/aegis-runner/actions-runner ]
+  - [ systemctl, enable, --now, node_exporter ]
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/runner_alerts.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-runner-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: runners.rules
+      rules:
+        - alert: RunnerDownPercentageHigh
+          expr: (sum(up{job="node_exporter",instance=~"runner-.*"} == 0) / count(up{job="node_exporter",instance=~"runner-.*"})) > 0.2
+          for: 5m
+          labels:
+            severity: critical
+          annotations:
+            summary: "More than 20% of runners are down"
+
*** End Patch
*** Begin Patch
*** Add File: runners/ops/auto_reprovision_orchestrator.py
+#!/usr/bin/env python3
+"""
+Orchestrator that monitors runner health via Prometheus and triggers Ansible reprovision for unhealthy nodes.
+"""
+import requests, subprocess, os, time
+PROM_URL = os.environ.get("PROM_URL","http://prometheus.monitoring.svc:9090")
+ANSIBLE_PLAYBOOK = "ansible/provision/reprovision_runners.yml"
+
+def query_unhealthy():
+    q = 'sum(up{job="node_exporter",instance=~"runner-.*"} == 0)'
+    r = requests.get(f"{PROM_URL}/api/v1/query", params={"query":q})
+    r.raise_for_status()
+    data = r.json()
+    val = int(data["data"]["result"][0]["value"][1]) if data["data"]["result"] else 0
+    return val
+
+def reprovision(node):
+    print("Reprovisioning", node)
+    subprocess.run(["ansible-playbook", ANSIBLE_PLAYBOOK, "--limit", node], check=False)
+
+if __name__=="__main__":
+    while True:
+        try:
+            bad = query_unhealthy()
+            print("unhealthy nodes:", bad)
+            if bad>0:
+                # For demo: run playbook for all runners
+                reprovision("runners")
+        except Exception as e:
+            print("error:",e)
+        time.sleep(300)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/run_sla_harness_loop.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Run provider SLA harness repeatedly and aggregate reports for tuning.
+OUTDIR=${1:-/tmp/qpu_sla_reports}
+mkdir -p "$OUTDIR"
+for i in $(seq 1 10); do
+  ts=$(date +%s)
+  python quantum/sla/provider_sla_test_harness.py > "$OUTDIR/report_$ts.json" 2>&1 || true
+  sleep 30
+done
+echo "Wrote reports to $OUTDIR"
+
*** End Patch
*** Begin Patch
*** Add File: quantum/hardening/adapter_config_updater.py
+#!/usr/bin/env python3
+"""
+Consume /tmp/adapter_recommendations.json and apply to k8s ConfigMap or DB used by adapters.
+This script writes to a k8s ConfigMap named aegis-adapter-config in namespace aegis (requires kubectl).
+"""
+import json, subprocess, os
+CFG="/tmp/adapter_recommendations.json"
+if not os.path.exists(CFG):
+    print("No recommendations file", CFG); raise SystemExit(2)
+rec=json.load(open(CFG))
+cm=json.dumps({"apiVersion":"v1","kind":"ConfigMap","metadata":{"name":"aegis-adapter-config","namespace":"aegis"},"data":{"config":json.dumps(rec)}})
+# apply via kubectl
+proc=subprocess.run(["kubectl","apply","-f","-"], input=cm.encode(), check=False)
+print("Applied configmap; returncode", proc.returncode)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/mitigation/readout_mitigation_extended.py
+"""
+Extended readout mitigation using Qiskit Ignis style measurement calibration.
+Produces a mitigation fitter and applies it to result counts to correct readout errors.
+"""
+from qiskit.ignis.mitigation.measurement import complete_meas_cal, CompleteMeasFitter
+from qiskit import execute
+import logging
+
+LOG=logging.getLogger("aegis.mitigation")
+
+def build_meas_fitter(backend, qubit_list, shots=2048):
+    meas_calibs, state_labels = complete_meas_cal(qubit_list=qubit_list, circlabel='mcal')
+    job = execute(meas_calibs, backend=backend, shots=shots)
+    res = job.result()
+    meas_fitter = CompleteMeasFitter(res, state_labels)
+    return meas_fitter
+
+def apply_mitigation(counts, meas_fitter):
+    try:
+        mitigated = meas_fitter.filter.apply(counts)
+        return mitigated
+    except Exception as e:
+        LOG.exception("Mitigation apply failed: %s", e)
+        return counts
+
*** End Patch
*** Begin Patch
*** Add File: quantum/mitigation/zne_qiskit.py
+"""
+Zero Noise Extrapolation (ZNE) basic implementation using repetition method.
+Note: real pulse stretching required for hardware; here we use gate folding as a proxy.
+"""
+from qiskit import transpile, QuantumCircuit
+import numpy as np
+import logging
+LOG=logging.getLogger("aegis.zne")
+
+def fold_gates(circuit, factor):
+    # naive gate folding: repeat each non-measurement instruction factor times (proxy)
+    qc = circuit.copy()
+    folded = QuantumCircuit(qc.num_qubits)
+    for instr, qargs, cargs in qc.data:
+        for _ in range(max(1,int(factor))):
+            folded.append(instr, qargs, cargs)
+    return folded
+
+def zne_run(adapter, circuit, scales=[1,3,5], shots=1024):
+    results=[]
+    for s in scales:
+        c = fold_gates(c, s)
+        res = adapter.submit(c, shots=shots)
+        results.append({"scale":s,"res":res})
+    # placeholder extrapolation
+    xs=np.array([r["scale"] for r in results],dtype=float)
+    ys=np.ones_like(xs)
+    coeffs=np.polyfit(xs,ys,1)
+    est=np.polyval(coeffs,0.0)
+    return {"zero_noise_estimate":float(est),"raw":results}
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile_mapper.py
+#!/usr/bin/env python3
+"""
+Map provider Cost Explorer tag 'aegis:tenant' to qpu_charges.owner and mark reconciled.
+Sends alert via SNS for unmatched items.
+"""
+import boto3, os, json, logging
+from datetime import date, timedelta
+from sqlalchemy import create_engine, text
+
+LOG=logging.getLogger("aegis.billing")
+logging.basicConfig(level=logging.INFO)
+DB_URL=os.environ.get("DATABASE_URL","postgresql://aegis:aegis@postgres:5432/aegis")
+engine=create_engine(DB_URL)
+SNS_TOPIC=os.environ.get("BILLING_SNS_TOPIC","")
+
+def fetch_tagged_costs(start,end):
+    client=boto3.client("ce",region_name="us-east-1")
+    resp=client.get_cost_and_usage(TimePeriod={"Start":start,"End":end},Granularity="DAILY",Metrics=["UnblendedCost"],GroupBy=[{"Type":"TAG","Key":"aegis:tenant"}])
+    return resp
+
+def reconcile(start,end):
+    resp=fetch_tagged_costs(start,end)
+    items=resp.get("ResultsByTime",[])
+    unmatched=[]
+    with engine.connect() as conn:
+        for day in items:
+            for g in day.get("Groups",[]):
+                key=g.get("Keys",[None])[0]
+                if not key or "aegis:tenant$" not in key:
+                    continue
+                owner=key.split("$",1)[1]
+                amount=float(g.get("Metrics",{}).get("UnblendedCost",{}).get("Amount",0.0))
+                q=conn.execute(text("SELECT id, amount_usd FROM qpu_charges WHERE owner=:owner AND DATE_TRUNC('day', TO_TIMESTAMP(created_at)) = DATE_TRUNC('day', NOW())"),{"owner":owner})
+                rows=list(q)
+                if not rows:
+                    unmatched.append({"owner":owner,"amount":amount})
+                else:
+                    conn.execute(text("UPDATE qpu_charges SET reconciled=true, provider_invoice_id=:inv WHERE owner=:owner"),{"inv":"aws-"+day.get("TimePeriod")["Start"],"owner":owner})
+    if unmatched:
+        LOG.warning("Unmatched billing: %s", unmatched)
+        if SNS_TOPIC:
+            sns=boto3.client("sns")
+            sns.publish(TopicArn=SNS_TOPIC,Message=json.dumps(unmatched),Subject="Unmatched QPU billing items")
+
+if __name__=="__main__":
+    today=date.today(); start=(today - timedelta(days=7)).isoformat(); end=today.isoformat()
+    reconcile(start,end)
+
*** End Patch
*** Begin Patch
*** Add File: edge/loadtest/scale_launcher_k8s.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Launch many locust workers as Kubernetes Jobs for large-scale load testing
+N=${1:-200}
+MASTER=locust-master.aegis.svc.cluster.local
+for i in $(seq 1 $N); do
+  kubectl run locust-worker-$i --image=locustio/locust --restart=OnFailure -- /bin/sh -c "locust -f /mnt/locust/locustfile.py --worker --master-host $MASTER" &
+done
+echo "Launched $N locust workers"
+
*** End Patch
*** Begin Patch
*** Add File: db/partitioning/create_partitioned_telemetry.sql
+-- Create a partitioned telemetry table by day to scale ingestion
+CREATE TABLE IF NOT EXISTS device_telemetry (
+  id BIGSERIAL PRIMARY KEY,
+  device_id TEXT NOT NULL,
+  ts TIMESTAMP NOT NULL DEFAULT now(),
+  payload JSONB
+) PARTITION BY RANGE (ts);
+
+-- helper to create a partition for a given date
+CREATE OR REPLACE FUNCTION create_telemetry_partition(p_date DATE) RETURNS VOID AS $$
+BEGIN
+  EXECUTE format('CREATE TABLE IF NOT EXISTS device_telemetry_%s PARTITION OF device_telemetry FOR VALUES FROM (''%s'') TO (''%s'')', to_char(p_date,'YYYYMMDD'), p_date::text, (p_date + INTERVAL '1 day')::date::text);
+END;
+$$ LANGUAGE plpgsql;
+
*** End Patch
*** Begin Patch
*** Add File: edge/db/apply_postgres_tuning.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Advisory script to compute and optionally apply Postgres GUCs based on device scale.
+EXPECTED_DEVICES=${1:-100000}
+EXPECTED_QPS=$(awk "BEGIN {print ${EXPECTED_DEVICES}*0.01}")
+WORKERS=$(awk "BEGIN {print int(${EXPECTED_QPS}/10)}")
+MAX_CONN=$(awk "BEGIN {print ${WORKERS}*50 + 100}")
+echo "Expected QPS: $EXPECTED_QS"
+echo "Suggested workers: $WORKERS"
+echo "Suggested max_connections: $MAX_CONN"
+echo "To apply: edit your postgres.conf or RDS parameter group to set max_connections and tune work_mem/pool sizes."
+
*** End Patch
*** End Patch
