*** Begin Patch
*** Add File: provider/legal/baa_workflow.md
+BAA & Provider Contract Workflow
+
+Purpose:
+- Track, collect, and validate Business Associate Agreements (BAA) and provider SLAs before sending regulated data (PHI/PII) to external QPU providers.
+
+Responsibilities:
+- Legal: review & sign BAAs and SLAs.
+- SRE/CloudOps: ensure technical controls (encryption, KMS/HSM, CRL/PKI, audit logs).
+- Product: flag workloads that will be sending regulated data; require BAA validation step in CI.
+
+Process:
+1) Initiate BAA request: fill provider/legal/baa_request_template.md and submit to provider legal contact.
+2) Track BAA status in provider/legal/baas_tracker.csv.
+3) When BAA signed: store signed PDF in secure object storage (S3/GCS) under path /compliance/BAA/{provider}/{date}/ and update baas_tracker.csv to true with link.
+4) CI will call provider/legal/check_baa.py during promotion — blocking promotion if BAA missing when regulated flag is set.
+5) Keep an audit: evidence bundle (signed by cosign key) containing BAA, manifest, and attestation stored in compliance evidence store.
+
+Automation hooks:
+- provider/legal/upload_baa.py: upload signed BAAs to storage and update tracker.
+- provider/legal/check_baa.py used by promotion flows to verify BAA presence.
+
+Records retention:
+- Keep BAAs for minimum 7 years (or as required by jurisdiction).
+
*** End Patch
*** Begin Patch
*** Add File: provider/legal/upload_baa.py
+#!/usr/bin/env python3
+"""
+Upload signed BAA PDF to secure object store and update baas_tracker.csv
+
+Usage:
+  python upload_baa.py --provider braket --file signed_baa_braket.pdf --uploader alice@example.com --bucket my-compliance-bucket
+"""
+import argparse
+import csv
+import os
+import datetime
+import boto3
+from pathlib import Path
+
+BAA_CSV = Path("provider/legal/baas_tracker.csv")
+
+def upload_to_s3(bucket, src_path, dest_key):
+    s3 = boto3.client("s3")
+    s3.upload_file(src_path, bucket, dest_key, ExtraArgs={"ServerSideEncryption": "AES256"})
+    return f"s3://{bucket}/{dest_key}"
+
+def update_tracker(provider, url):
+    # read existing CSV, update provider row or append
+    rows=[]
+    if BAA_CSV.exists():
+        with BAA_CSV.open() as fh:
+            rows=list(csv.DictReader(fh))
+    found=False
+    for r in rows:
+        if r.get("provider")==provider:
+            r["baa_signed"]="true"
+            r["baa_url"]=url
+            r["notes"]=f"signed:{datetime.datetime.utcnow().isoformat()}"
+            found=True
+    if not found:
+        rows.append({"provider":provider,"contact":"","baa_signed":"true","baa_url":url,"notes":"uploaded"})
+    with BAA_CSV.open("w", newline="") as fh:
+        writer=csv.DictWriter(fh, fieldnames=["provider","contact","baa_signed","baa_url","notes"])
+        writer.writeheader()
+        writer.writerows(rows)
+
+def main():
+    p=argparse.ArgumentParser()
+    p.add_argument("--provider", required=True)
+    p.add_argument("--file", required=True)
+    p.add_argument("--bucket", required=True)
+    p.add_argument("--uploader", required=True)
+    args=p.parse_args()
+    src=Path(args.file)
+    if not src.exists():
+        raise SystemExit("file not found")
+    dest_key=f"compliance/BAA/{args.provider}/{src.name}"
+    url=upload_to_s3(args.bucket, str(src), dest_key)
+    update_tracker(args.provider, url)
+    print("Uploaded and updated tracker:", url)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: infra/hsm/cloudhsm_full.tf
+/*
+AWS CloudHSM full provisioning scaffold.
+Note: CloudHSM cluster initialization and HSM user creation involve manual steps.
+This Terraform creates the cluster and subnet assignment; full HSM provisioning is documented in playbooks.
+*/
+terraform {
+  required_providers {
+    aws = {
+      source  = "hashicorp/aws"
+      version = "~> 4.0"
+    }
+  }
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_cloudhsm_v2_cluster" "aegis_cloudhsm" {
+  hsm_type  = "hsm1.medium"
+  subnet_ids = var.subnet_ids
+  vpc_id     = var.vpc_id
+  tags = { Name = "aegis-cloudhsm" }
+}
+
+output "cloudhsm_cluster_id" {
+  value = aws_cloudhsm_v2_cluster.aegis_cloudhsm.cluster_id
+}
+
+variable "aws_region" { type = string }
+variable "subnet_ids" { type = list(string) }
+variable "vpc_id" { type = string }
+
*** End Patch
*** Begin Patch
*** Add File: hsm/pkcs11/cosign_pkcs11_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Test cosign signing via PKCS11 provider (CloudHSM). Requires PKCS11 module configured on host.
+MANIFEST="${1:-manifest.json}"
+PKCS11_MODULE="${PKCS11_MODULE:-/usr/lib/libcloudhsm_pkcs11.so}"
+TOKEN_LABEL="${TOKEN_LABEL:-cosign-key}"
+if [ ! -f "${MANIFEST}" ]; then
+  echo "manifest not found: ${MANIFEST}" >&2
+  exit 2
+fi
+echo "Signing ${MANIFEST} via PKCS11"
+cosign sign --key "pkcs11:token=${TOKEN_LABEL}?module-path=${PKCS11_MODULE}" "${MANIFEST}"
+echo "Verify signature"
+cosign verify --key "pkcs11:token=${TOKEN_LABEL}?module-path=${PKCS11_MODULE}" "${MANIFEST}" || true
+echo "PKCS11 cosign test completed"
+
*** End Patch
*** Begin Patch
*** Add File: hsm/cloudtrail/alert_on_kms_usage.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Query CloudTrail for recent KMS/CloudHSM cosign sign activity and send alert if unexpected principals used.
+THRESHOLD_MINUTES=${1:-60}
+EXPECTED_PRINCIPAL="${EXPECTED_PRINCIPAL:-arn:aws:iam::123456789012:role/aegis-ci}"
+TMP=/tmp/aegis_kms_events.json
+aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=Sign --max-results 50 > "$TMP" || true
+jq '.Events[] | {EventId:.EventId, EventName:.EventName, Username:.Username, EventTime:.EventTime}' "$TMP" || true
+echo "Please review CloudTrail output for unexpected KMS usage. (Automate integration with PagerDuty/SNS for production)"
+
*** End Patch
*** Begin Patch
*** Add File: ansible/provision/provision_bastion.yml
+- name: Provision Bastion host for runner access
+  hosts: bastion
+  become: true
+  vars:
+    bastion_user: ubuntu
+  tasks:
+    - name: Ensure necessary packages
+      apt:
+        name:
+          - fail2ban
+          - ufw
+          - tmux
+        state: present
+        update_cache: yes
+
+    - name: Configure UFW to allow SSH only from admin CIDRs
+      ufw:
+        rule: allow
+        proto: tcp
+        port: 22
+        from_ip: "{{ admin_allowed_cidrs | default(['10.0.0.0/8']) }}"
+
+    - name: Disable password auth for SSH (force key-based)
+      lineinfile:
+        path: /etc/ssh/sshd_config
+        regexp: '^#?PasswordAuthentication'
+        line: 'PasswordAuthentication no'
+      notify: restart ssh
+
+    - name: Install SSH bastion jump script
+      copy:
+        dest: /usr/local/bin/jump-to-runner
+        content: |
+          #!/usr/bin/env bash
+          # Usage: jump-to-runner <runner-host>
+          ssh -A {{ bastion_user }}@$1
+      mode: 0755
+
+  handlers:
+    - name: restart ssh
+      service:
+        name: ssh
+        state: restarted
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/runner/node_exporter_install.yml
+- name: Install node_exporter on runners
+  hosts: runners
+  become: true
+  tasks:
+    - name: Download node_exporter
+      get_url:
+        url: https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-arm64.tar.gz
+        dest: /tmp/node_exporter.tar.gz
+    - name: Extract
+      unarchive:
+        src: /tmp/node_exporter.tar.gz
+        dest: /opt/
+        remote_src: yes
+    - name: Create systemd unit
+      copy:
+        dest: /etc/systemd/system/node_exporter.service
+        content: |
+          [Unit]
+          Description=Prometheus Node Exporter
+          After=network.target
+
+          [Service]
+          ExecStart=/opt/node_exporter-1.5.0.linux-arm64/node_exporter
+          Restart=always
+          User=root
+
+          [Install]
+          WantedBy=multi-user.target
+    - name: Start node_exporter
+      systemd:
+        name: node_exporter
+        enabled: yes
+        state: started
+
*** End Patch
*** Begin Patch
*** Add File: runners/health/auto_reprovision_systemd.service
+[Unit]
+Description=Aegis Runner Health Watcher
+After=network.target
+
+[Service]
+Type=simple
+ExecStart=/usr/local/bin/runner_health_watch.sh
+Restart=always
+RestartSec=30
+
+[Install]
+WantedBy=multi-user.target
+
*** End Patch
*** Begin Patch
*** Add File: runners/health/runner_health_watch.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Periodically verify runner service and trigger Ansible reprovision if unhealthy.
+RUNNER_HOST=$(hostname -f)
+ANSIBLE_PLAYBOOK="/opt/ansible/reprovision_runners.yml"
+SLEEP=300
+while true; do
+  if ! systemctl is-active --quiet github-runner; then
+    echo "$(date): runner service not active — invoking reprovision"
+    # Run local playbook (ansible installed & inventory prepared); in practice use central orchestrator
+    /usr/bin/ansible-playbook $ANSIBLE_PLAYBOOK --limit "$RUNNER_HOST" || true
+  fi
+  sleep $SLEEP
+done
+
*** End Patch
*** Begin Patch
*** Add File: edge/loadtest/locust_master_deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: locust-master
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: locust-master
+  template:
+    metadata:
+      labels:
+        app: locust-master
+    spec:
+      containers:
+        - name: locust
+          image: locustio/locust
+          args: ["-f", "/mnt/locust/locustfile.py", "--master"]
+          volumeMounts:
+            - name: locust-scripts
+              mountPath: /mnt/locust
+      volumes:
+        - name: locust-scripts
+          configMap:
+            name: locust-scripts
+
*** End Patch
*** Begin Patch
*** Add File: edge/loadtest/locust-scripts-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: locust-scripts
+  namespace: aegis
+data:
+  locustfile.py: |
+    from locust import HttpUser, task, between
+    import random, json
+
+    class EdgeDeviceUser(HttpUser):
+        wait_time = between(0.1, 1)
+
+        @task(5)
+        def poll_bundle(self):
+            device_id = f"device-{random.randint(1,200000)}"
+            self.client.get(f"/api/v1/devices/{device_id}/bundle", name="/device/bundle")
+
+        @task(2)
+        def post_telemetry(self):
+            device_id = f"device-{random.randint(1,200000)}"
+            payload = {"temp": random.random()*80, "status": "ok"}
+            self.client.post(f"/api/v1/devices/{device_id}/telemetry", json=payload, name="/device/telemetry")
+
*** End Patch
*** Begin Patch
*** Add File: edge/db/shard_recommend.py
+#!/usr/bin/env python3
+"""
+Analyze locust result metrics and recommend shard count and DB pool sizes.
+Input: device count simulated and observed qps metrics (sample).
+"""
+import math,sys,json
+
+def recommend(device_count, qps_per_device=0.01):
+    expected_qps = device_count * qps_per_device
+    workers = max(8, int(expected_qps / 20))
+    max_connections = workers * 25 + 100
+    return {"expected_qps": expected_qps, "workers": workers, "max_connections": max_connections}
+
+if __name__=="__main__":
+    devices = int(sys.argv[1]) if len(sys.argv)>1 else 100000
+    print(json.dumps(recommend(devices), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File: quantum/sla/tune_adapters.py
+#!/usr/bin/env python3
+"""
+Dynamic adapter tuning:
+ - Run SLA harness to measure provider error rates and latency.
+ - Produce recommended retry/backoff/throttle parameters and optionally write to adapter config store.
+"""
+import time, json, logging
+from quantum.sla.provider_sla_ci import run_test_braket, run_test_qiskit
+LOG = logging.getLogger("aegis.tuner")
+logging.basicConfig(level=logging.INFO)
+
+def analyze_reports(reports):
+    # naive: if error_rate > 0.1 -> increase retries, increase backoff
+    recommendations=[]
+    for r in reports:
+        if r.get("elapsed",0)>60 or r.get("error",False):
+            recommendations.append({"provider": r.get("provider","?"), "retries": 8, "backoff": 2.0, "throttle_rate": 0.5})
+        else:
+            recommendations.append({"provider": r.get("provider","?"), "retries": 4, "backoff": 1.0, "throttle_rate": 2.0})
+    return recommendations
+
+def main():
+    # This script expects provider SLA harness to be run and produce JSON reports
+    # For demo we assume reports are available in /tmp/provider_reports.json
+    try:
+        reports=json.load(open("/tmp/provider_reports.json"))
+    except Exception:
+        reports=[]
+    recs=analyze_reports(reports)
+    print(json.dumps(recs, indent=2))
+    # Optionally write to config backing store (e.g., k8s configmap or DB) for adapters to reload
+    open("/tmp/adapter_recommendations.json","w").write(json.dumps(recs))
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile_mapping.sql
+-- SQL migration to add invoice and reconciled columns to qpu_charges
+ALTER TABLE qpu_charges
+  ADD COLUMN provider_invoice_id TEXT,
+  ADD COLUMN reconciled BOOLEAN DEFAULT false;
+
+-- Example index to accelerate reconciliation queries
+CREATE INDEX IF NOT EXISTS idx_qpu_charges_owner_reconciled ON qpu_charges (owner, reconciled);
+
*** End Patch
*** Begin Patch
*** Add File: billing/reconcile_automation.py
+#!/usr/bin/env python3
+"""
+Automated reconciliation:
+ - Fetch provider billing lines (AWS Cost Explorer with resource tags)
+ - Map cost lines tagged with 'aegis:tenant' to qpu_charges.owner and reconcile
+ - Create alert when unmatched charges exceed threshold
+"""
+import boto3, json, logging
+from datetime import date, timedelta
+from sqlalchemy import create_engine, text
+import os
+
+LOG = logging.getLogger("aegis.reconcile")
+logging.basicConfig(level=logging.INFO)
+DB_URL = os.environ.get("DATABASE_URL", "postgresql://aegis:aegis@postgres:5432/aegis")
+engine = create_engine(DB_URL)
+
+def fetch_cost_items(start, end):
+    client = boto3.client("ce", region_name="us-east-1")
+    resp = client.get_cost_and_usage(TimePeriod={"Start":start,"End":end}, Granularity="DAILY", Metrics=["UnblendedCost"], GroupBy=[{"Type":"TAG","Key":"aegis:tenant"}])
+    return resp
+
+def reconcile(start, end):
+    resp = fetch_cost_items(start,end)
+    items = resp.get("ResultsByTime", [])
+    unmatched=[]
+    with engine.connect() as conn:
+        for day in items:
+            for g in day.get("Groups", []):
+                key = g.get("Keys", [None])[0]
+                # Key is like "aegis:tenant$tenant-123"
+                if not key or "aegis:tenant$" not in key:
+                    continue
+                owner = key.split("$",1)[1]
+                amount = float(g.get("Metrics", {}).get("UnblendedCost", {}).get("Amount", 0.0))
+                # naive matching: find qpu_charges same owner same day and approximate amount
+                q = conn.execute(text("SELECT id, amount_usd FROM qpu_charges WHERE owner=:owner AND DATE_TRUNC('day', TO_TIMESTAMP(created_at)) = DATE_TRUNC('day', NOW())"), {"owner": owner})
+                rows = list(q)
+                if not rows:
+                    unmatched.append({"owner":owner,"day":day.get("TimePeriod"),"amount":amount})
+                else:
+                    # mark reconciled for demonstration
+                    conn.execute(text("UPDATE qpu_charges SET reconciled=true, provider_invoice_id=:inv WHERE owner=:owner"), {"inv":"aws-cost-"+day.get("TimePeriod")["Start"], "owner":owner})
+    if unmatched:
+        LOG.warning("Unmatched billing items: %s", json.dumps(unmatched))
+
+if __name__=="__main__":
+    today=date.today()
+    start=(today - timedelta(days=7)).isoformat()
+    end=today.isoformat()
+    reconcile(start,end)
+
*** End Patch
*** Begin Patch
*** Add File: quantum/mitigation/pipeline.py
+#!/usr/bin/env python3
+"""
+Per-device mitigation pipeline:
+ - Collect calibration snapshots from MLflow
+ - For each device, run readout mitigation construct & Zero Noise Extrapolation (ZNE) experiments on small test circuits
+ - Store mitigation models/artifacts back to MLflow and export suggested mitigation parameters (fidelity thresholds, apply_mitigation flags)
+"""
+import os, time, json, logging
+import mlflow
+from quantum.adapters.qiskit_adapter import QiskitAdapter
+from quantum.transpile.zero_noise_extrapolation import zne_execute
+
+MLFLOW_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
+mlflow.set_tracking_uri(MLFLOW_URI)
+LOG = logging.getLogger("aegis.mitigation")
+logging.basicConfig(level=logging.INFO)
+
+def fetch_device_calibration(device_name):
+    # placeholder: load calibration from MLflow artifact store for device
+    # path convention: runs/<runid>/artifacts/calibration/<device>.json
+    # For demo return empty dict
+    return {}
+
+def run_mitigation_for_device(device_name, adapter, test_circuits):
+    results = []
+    for circ in test_circuits:
+        res = zne_execute(adapter, circ, shots=1024, scales=[1.0,2.0])
+        results.append(res)
+    # analyze results and produce mitigation suggestion
+    suggestion = {"device":device_name,"apply_readout_mitigation": True, "zne_enabled": True}
+    # log to MLflow
+    run_name=f"mitigation-{device_name}-{int(time.time())}"
+    with mlflow.start_run(run_name=run_name):
+        mlflow.log_param("device", device_name)
+        mlflow.log_dict(suggestion, "suggestion.json")
+        mlflow.log_dict({"results": results}, "results.json")
+    return suggestion
+
+def main():
+    devices = ["ibm_santiago","ibm_perth"]
+    test_circuits = ["OPENQASM 2.0; // placeholder"]  # in real use, provide Qiskit circuit objects
+    adapter = QiskitAdapter(use_simulator=True)
+    for d in devices:
+        LOG.info("Running mitigation for %s", d)
+        s = run_mitigation_for_device(d, adapter, test_circuits)
+        print("suggestion:", s)
+
+if __name__=="__main__":
+    main()
+
*** End Patch
*** End Patch
