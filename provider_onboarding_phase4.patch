*** Begin Patch
*** Add File: scripts/setup_self_hosted_runner.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Automate installation & registration of a GitHub self-hosted runner on a secure VM.
+# This script is a template: pass the following environment variables or CLI args:
+#   GITHUB_URL  - e.g. https://github.com/yourorg/yourrepo or https://github.com/yourorg
+#   RUNNER_TOKEN - registration token (create via GitHub UI / API). Keep secret.
+#   RUNNER_NAME  - name for the runner (defaults to hostname)
+#   RUNNER_LABELS - comma-separated labels (e.g. "self-hosted,qpu-adjacent")
+#   RUNNER_WORK_DIR - optional work dir
+#
+# Usage (example):
+#   export GITHUB_URL="https://github.com/yourorg/yourrepo"
+#   export RUNNER_TOKEN="ghr_xxx"
+#   sudo ./scripts/setup_self_hosted_runner.sh
+
+GITHUB_URL="${GITHUB_URL:-}"
+RUNNER_TOKEN="${RUNNER_TOKEN:-}"
+RUNNER_NAME="${RUNNER_NAME:-$(hostname)}"
+RUNNER_LABELS="${RUNNER_LABELS:-self-hosted,qpu-adjacent}"
+WORK_DIR="${RUNNER_WORK_DIR:-/opt/actions-runner}"
+ARCH="$(uname -m)"
+
+if [ -z "$GITHUB_URL" ] || [ -z "$RUNNER_TOKEN" ]; then
+  echo "GITHUB_URL and RUNNER_TOKEN are required (export or pass env)." >&2
+  exit 2
+fi
+
+mkdir -p "$WORK_DIR"
+cd "$WORK_DIR"
+
+echo "Installing dependencies..."
+if ! command -v jq >/dev/null 2>&1; then
+  sudo apt-get update && sudo apt-get install -y jq curl tar
+fi
+
+echo "Detecting latest runner release..."
+LATEST_JSON=$(curl -sS https://api.github.com/repos/actions/runner/releases/latest)
+TAR_URL=$(echo "$LATEST_JSON" | jq -r '.assets[] | select(.name | test("linux-x64.*tar.gz")) | .browser_download_url')
+if [ -z "$TAR_URL" ]; then
+  TAR_URL=$(echo "$LATEST_JSON" | jq -r '.assets[0].browser_download_url')
+fi
+
+echo "Downloading runner from $TAR_URL"
+curl -sS -L "$TAR_URL" -o actions-runner.tar.gz
+tar xzf actions-runner.tar.gz --strip-components=0
+
+echo "Configuring runner..."
+./config.sh --url "$GITHUB_URL" --token "$RUNNER_TOKEN" --name "$RUNNER_NAME" --labels "$RUNNER_LABELS" --work "${WORK_DIR}/_work" --unattended
+
+echo "Installing runner as systemd service..."
+sudo ./svc.sh install
+sudo ./svc.sh start
+
+echo "Runner installed and started. Use 'sudo ./svc.sh status' to check."
+echo "Security notes: keep RUNNER_TOKEN secret; restrict VM network & access; store signing keys under a protected path (600)."
+
*** End Patch
*** Begin Patch
*** Add File: terraform/vault/create_k8s_secret_from_tf_outputs.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create Kubernetes secret `vault-credentials` in namespace `aegis` from Terraform outputs JSON
+# Usage:
+#   terraform output -json > tf_out.json
+#   ./terraform/vault/create_k8s_secret_from_tf_outputs.sh tf_out.json
+
+TF_JSON="${1:-}"
+NS="${2:-aegis}"
+SECRET_NAME="${3:-vault-credentials}"
+
+if [ -z "$TF_JSON" ] || [ ! -f "$TF_JSON" ]; then
+  echo "Usage: $0 <terraform-output-json> [namespace] [secret-name]" >&2
+  exit 2
+fi
+
+ROLE_ID=$(jq -r '."approle_role_id".value' "$TF_JSON")
+SECRET_ID=$(jq -r '."approle_secret_id".value' "$TF_JSON")
+VAULT_ADDR="${VAULT_ADDR:-}"
+
+if [ -z "$ROLE_ID" ] || [ -z "$SECRET_ID" ]; then
+  echo "Missing role_id or secret_id in terraform outputs" >&2
+  exit 3
+fi
+
+kubectl create namespace "$NS" --dry-run=client -o yaml | kubectl apply -f -
+
+kubectl delete secret "$SECRET_NAME" -n "$NS" --ignore-not-found
+
+kubectl create secret generic "$SECRET_NAME" -n "$NS" \
+  --from-literal=role_id="$ROLE_ID" \
+  --from-literal=secret_id="$SECRET_ID" \
+  --from-literal=addr="${VAULT_ADDR}" \
+  || { echo "Failed to create secret"; exit 4; }
+
+echo "Created secret $SECRET_NAME in namespace $NS (contains approle role_id & secret_id)."
+echo "Operator should run vault_token_rotator to exchange approle for short-lived tokens and write a runtime secret token for the controller."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/trigger_provider_job.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Trigger the provider end-to-end Kubernetes Job and copy results locally.
+# Usage:
+#   ./scripts/trigger_provider_job.sh --provider ibm --namespace aegis --wait-seconds 600 --out-dir ./results
+
+PROVIDER="${1:-ibm}"
+NAMESPACE="${2:-aegis}"
+WAIT_SECONDS="${3:-600}"
+OUT_DIR="${4:-./provider_job_results}"
+
+# Simple CLI arg handling (positional)
+if [ "$#" -ge 1 ]; then PROVIDER="$1"; fi
+if [ "$#" -ge 2 ]; then NAMESPACE="$2"; fi
+
+mkdir -p "$OUT_DIR"
+
+echo "Patching provider in job manifest and applying..."
+kubectl -n "$NAMESPACE" delete job provider-endtoend-verify --ignore-not-found || true
+kubectl -n "$NAMESPACE" apply -f k8s/jobs/provider-endtoend-job.yaml
+
+echo "Waiting for job to start..."
+START=$(date +%s)
+END=$((START + WAIT_SECONDS))
+JOB_NAME="provider-endtoend-verify"
+
+while [ "$(date +%s)" -lt "$END" ]; do
+  status=$(kubectl -n "$NAMESPACE" get job "$JOB_NAME" -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "")
+  if [ "$status" = "1" ]; then
+    echo "Job succeeded"
+    break
+  fi
+  failed=$(kubectl -n "$NAMESPACE" get job "$JOB_NAME" -o jsonpath='{.status.failed}' 2>/dev/null || echo "")
+  if [ -n "$failed" ] && [ "$failed" != "0" ]; then
+    echo "Job failed (failed count=$failed)"
+    kubectl -n "$NAMESPACE" logs job/"$JOB_NAME" --all-containers || true
+    exit 3
+  fi
+  echo -n "."
+  sleep 5
+done
+
+if [ "$(kubectl -n "$NAMESPACE" get job "$JOB_NAME" -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "")" != "1" ]; then
+  echo "Job did not complete within ${WAIT_SECONDS}s" >&2
+  kubectl -n "$NAMESPACE" get pods -l job-name="$JOB_NAME" -o wide
+  kubectl -n "$NAMESPACE" logs job/"$JOB_NAME" --all-containers || true
+  exit 4
+fi
+
+POD=$(kubectl -n "$NAMESPACE" get pods -l job-name="$JOB_NAME" -o jsonpath='{.items[0].metadata.name}')
+echo "Copying /results from pod $POD to $OUT_DIR"
+kubectl -n "$NAMESPACE" cp "$POD":/results "$OUT_DIR" || echo "kubectl cp failed; inspecting logs instead"
+
+echo "Job output copied to $OUT_DIR"
+
*** End Patch
*** Begin Patch
*** Add File: docs/runbooks/staging_deploy_and_test.md
+# Staging Deploy & Test Runbook (provider E2E + infra smoke)
+
+This runbook walks an operator through the steps to:
+- register a secure self-hosted runner (optional),
+- deploy Argo apps to staging via ArgoCD (redis/prometheus/grafana/oauth2/fluentd),
+- create Vault AppRole secrets in Kubernetes from Terraform outputs,
+- run the provider end-to-end job and collect results,
+- run Redis failover and provider stress tests.
+
+Prereqs:
+- Access to cluster with kubectl configured
+- ArgoCD deployed & accessible
+- Vault admin outputs from terraform/vault (approle_role_id, approle_secret_id)
+- Self-hosted runner VM (if using self-hosted CI)
+
+Steps:
+1) (Optional) Setup secure self-hosted runner
+   - On secure VM, place runner registration token in env: RUNNER_TOKEN
+   - Run: sudo ./scripts/setup_self_hosted_runner.sh
+   - Confirm runner shows in GitHub with label 'qpu-adjacent'
+
+2) Provision Vault AppRole (admin)
+   - Vault admin runs terraform in terraform/vault/ as described in terraform/vault/README_admin.md
+   - Admin provides approle role_id & secret_id (out-of-band) to operator
+
+3) Create Kubernetes secret from Terraform outputs
+   - Save TF outputs JSON:
+       terraform output -json > tf_out.json
+   - Run:
+       ./terraform/vault/create_k8s_secret_from_tf_outputs.sh tf_out.json aegis vault-credentials
+   - Verify secret:
+       kubectl get secret vault-credentials -n aegis -o yaml
+
+4) Deploy Argo apps to staging
+   - Ensure ArgoCD has access to repo and that apps are present under argo/apps/
+   - In ArgoCD UI or CLI, sync the apps (aegis-redis, aegis-prometheus, aegis-grafana, aegis-oauth2-proxy, aegis-fluentd)
+   - Wait for sync & health status to be Green
+
+5) Start vault-token-rotator CronJob (operator)
+   - Ensure secret `vault-admin-credentials` exists with admin token to allow rotation
+   - kubectl apply -f k8s/cronjobs/vault-token-rotator-cronjob.yaml
+   - Wait for CronJob to run and check that secret `vault-credentials` is updated with a runtime token
+
+6) Run infra tests & provider stress tests
+   - Run:
+       ./scripts/argo_sync_and_test.sh
+   - Or run:
+       ./scripts/trigger_provider_job.sh ibm aegis 600 ./provider_results
+   - Check logs and provider_results for signed artifacts and Rekor entries
+
+7) Run Redis failover test
+   - Run:
+       python3 scripts/redis_failover_test.py
+   - Verify job consumed after Redis pod restart
+
+8) Review results
+   - provider_results/summary.json should contain result metadata and signature path
+   - Upload artifacts to secure storage and rotate any test credentials used
+
+Security notes:
+- Never check secrets into git. Use Vault and Kubernetes Secrets.
+- Use provider sandboxes to avoid billing surprises.
+- Remove any admin tokens from runtimes after tests; prefer the rotator workflow.
+
*** End Patch
*** End Patch
