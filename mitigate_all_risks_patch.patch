*** Begin Patch
*** Add File:infra/terraform/iam_roles.tf
+terraform {
+  required_version = ">= 1.0.0"
+}
+
+provider "aws" {
+  region = var.aws_region
+}
+
+variable "ci_role_name" {
+  type    = string
+  default = "aegis-ci-role"
+}
+
+variable "secret_prefix" {
+  type    = string
+  default = "aegis"
+}
+
+resource "aws_iam_role" "ci_role" {
+  name               = var.ci_role_name
+  assume_role_policy = data.aws_iam_policy_document.ci_assume_role.json
+}
+
+data "aws_iam_policy_document" "ci_assume_role" {
+  statement {
+    actions = ["sts:AssumeRole"]
+    principals {
+      type        = "Service"
+      identifiers = ["ec2.amazonaws.com"] # Replace with your CI principal (e.g., GitHub OIDC)
+    }
+    effect = "Allow"
+  }
+}
+
+resource "aws_iam_policy" "ci_cosign_policy" {
+  name   = "${var.secret_prefix}-ci-cosign-policy"
+  policy = file("${path.module}/ci_cosign_policy.json")
+}
+
+resource "aws_iam_role_policy_attachment" "ci_attach" {
+  role       = aws_iam_role.ci_role.name
+  policy_arn = aws_iam_policy.ci_cosign_policy.arn
+}
+
+output "ci_role_arn" {
+  value = aws_iam_role.ci_role.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File:infra/terraform/ci_cosign_policy.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowKMSUse",
+      "Effect": "Allow",
+      "Action": [
+        "kms:Sign",
+        "kms:Verify",
+        "kms:DescribeKey",
+        "kms:Encrypt",
+        "kms:Decrypt",
+        "kms:GenerateDataKey"
+      ],
+      "Resource": "*"
+    },
+    {
+      "Sid": "AllowS3Audit",
+      "Effect": "Allow",
+      "Action": [
+        "s3:PutObject",
+        "s3:GetObject",
+        "s3:ListBucket"
+      ],
+      "Resource": ["arn:aws:s3:::REPLACE_EVIDENCE_BUCKET", "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:infra/scripts/populate_secretsmanager.sh
+#!/usr/bin/env bash
+# Populate required SecretsManager & SSM parameters from Terraform outputs (operator-run)
+set -euo pipefail
+TF_OUTPUT=${1:-terraform_output.json}
+if [ ! -f "$TF_OUTPUT" ]; then
+  echo "Usage: $0 terraform_output.json"
+  exit 2
+fi
+
+EVIDENCE_BUCKET=$(jq -r '.evidence_bucket.value' "$TF_OUTPUT")
+COSIGN_KMS_ARN=$(jq -r '.cosign_kms_arn.value' "$TF_OUTPUT")
+CI_ROLE_ARN=$(jq -r '.ci_role_arn.value' "$TF_OUTPUT" 2>/dev/null || echo "")
+
+echo "Setting SSM parameters and SecretsManager placeholders..."
+aws ssm put-parameter --name "/aegis/s3/evidence_bucket" --value "$EVIDENCE_BUCKET" --type String --overwrite
+aws ssm put-parameter --name "/aegis/cosign/kms_key_arn" --value "$COSIGN_KMS_ARN" --type String --overwrite
+
+echo "Create SecretsManager values (public placeholders) — operator should replace token values with production secrets"
+aws secretsmanager create-secret --name "aegis/rekor" --description "Rekor server info" --secret-string "{\"rekor_url\":\"https://rekor.example.local:3000\"}" || true
+aws secretsmanager create-secret --name "aegis/cosign" --description "Cosign KMS ARN" --secret-string "{\"kms_arn\":\"$COSIGN_KMS_ARN\"}" || true
+
+if [ -n "$CI_ROLE_ARN" ]; then
+  echo "CI role ARN: $CI_ROLE_ARN"
+fi
+
+echo "Done. Operator: verify secrets in AWS console and configure ExternalSecrets accordingly."
+
*** End Patch
*** Begin Patch
*** Add File:rekor/deployment.yaml
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: rekor
+  namespace: security
+spec:
+  serviceName: rekor
+  replicas: 1
+  selector:
+    matchLabels:
+      app: rekor
+  template:
+    metadata:
+      labels:
+        app: rekor
+    spec:
+      containers:
+        - name: rekor
+          image: sigstore/rekor-server:latest
+          ports:
+            - containerPort: 3000
+          env:
+            - name: REKOR_SERVER_PORT
+              value: "3000"
+          volumeMounts:
+            - name: rekor-data
+              mountPath: /data
+  volumeClaimTemplates:
+    - metadata:
+        name: rekor-data
+      spec:
+        accessModes: [ "ReadWriteOnce" ]
+        resources:
+          requests:
+            storage: 10Gi
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: rekor
+  namespace: security
+spec:
+  selector:
+    app: rekor
+  ports:
+    - port: 3000
+      targetPort: 3000
+
*** End Patch
*** Begin Patch
*** Add File:rekor/service-monitor.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: rekor
+  namespace: monitoring
+spec:
+  selector:
+    matchLabels:
+      app: rekor
+  endpoints:
+    - port: 3000
+      path: /health
+      interval: 30s
+
*** End Patch
*** Begin Patch
*** Add File:rekor/cronjob_reconcile.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: rekor-reconcile
+  namespace: security
+spec:
+  schedule: "*/15 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          serviceAccountName: cosign-admission-sa
+          containers:
+            - name: rekor-reconcile
+              image: ghcr.io/yourorg/aegis-rekor-tools:latest
+              command: ["python","/app/rekor_reconcile.py"]
+              env:
+                - name: REKOR_URL
+                  value: "http://rekor.security.svc.cluster.local:3000"
+                - name: ALERT_WEBHOOK
+                  value: "https://hooks.slack.com/services/REPLACE" # operator to set
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File:rekor/rekor_reconcile.py
+#!/usr/bin/env python3
+"""
+Rekor reconcile: validate that recently signed images have Rekor entries.
+This CronJob lists recent model artifacts from a configured source (MLflow or registry),
+ and checks Rekor for presence of entries. If missing, posts an alert to ALERT_WEBHOOK.
+"""
+import os
+import requests
+import time
+from urllib.parse import urljoin
+
+REKOR = os.environ.get("REKOR_URL", "http://rekor.security.svc.cluster.local:3000")
+ALERT_WEBHOOK = os.environ.get("ALERT_WEBHOOK", "")
+
+def get_recent_images():
+    # Placeholder: implement logic to pull recent artifact image references from MLflow tags or registry
+    # For now, read from a local file if present
+    path = "/data/recent_images.txt"
+    if os.path.exists(path):
+        with open(path) as f:
+            return [l.strip() for l in f if l.strip()]
+    return []
+
+def check_rekor_for_artifact(artifact):
+    try:
+        resp = requests.get(urljoin(REKOR, f"/api/v1/index/retrieve?hash={artifact}"), timeout=8)
+        return resp.status_code == 200 and resp.text.strip() != ""
+    except Exception:
+        return False
+
+def alert(msg):
+    if not ALERT_WEBHOOK:
+        print("ALERT:", msg)
+        return
+    try:
+        requests.post(ALERT_WEBHOOK, json={"text": msg}, timeout=5)
+    except Exception as e:
+        print("Alert failed:", e)
+
+def main():
+    imgs = get_recent_images()
+    if not imgs:
+        print("No recent images list found; exit")
+        return
+    for img in imgs:
+        ok = check_rekor_for_artifact(img)
+        if not ok:
+            alert(f"Rekor entry missing for artifact {img}")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ops/bootstrap_deploy.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Bootstrap script to provision infra and deploy staging cluster manifests (operator-run, semi-automated)
+TFDIR="infra/terraform"
+echo "1) Run terraform apply..."
+pushd "$TFDIR" >/dev/null
+terraform init
+terraform apply -auto-approve -var 'aws_region=us-west-2'
+terraform output -json > /tmp/aegis_tf_output.json
+popd >/dev/null
+
+echo "2) Populate SecretsManager & SSM placeholders..."
+infra/scripts/populate_secretsmanager.sh /tmp/aegis_tf_output.json
+
+echo "3) Render manifests (ensure config/aegis_config.yaml updated with real values)"
+python ops/render_manifests.py --config config/aegis_config.yaml --templates-dir manifests/templates --out-dir out
+
+echo "4) Apply manifests to staging (requires kubeconfig env)"
+kubectl apply -f out/
+
+echo "5) Deploy Rekor and reconcile CronJob"
+kubectl apply -f rekor/deployment.yaml
+kubectl apply -f rekor/service-monitor.yaml
+kubectl apply -f rekor/cronjob_reconcile.yaml
+
+echo "6) Run predeploy CI validation workflow (operator must trigger CI or run locally)"
+echo "Bootstrap finished; verify Rekor, KMS and external secrets, then run .github/workflows/full_release_and_deploy.yml from GitHub UI."
+
*** End Patch
*** Begin Patch
*** Add File:mcp/verifier/deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-verifier
+  namespace: aegis
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: aegis-verifier
+  template:
+    metadata:
+      labels:
+        app: aegis-verifier
+    spec:
+      containers:
+        - name: verifier
+          image: ghcr.io/yourorg/aegis-verifier:latest
+          env:
+            - name: NLI_MODEL
+              value: "distilroberta-base" # default; operator may tune
+            - name: MODEL_CONTEXT_REGISTRY_URL
+              value: "http://model-context-registry.aegis.svc.cluster.local:8096"
+          ports:
+            - containerPort: 8097
+          readinessProbe:
+            httpGet:
+              path: /health
+              port: 8097
+            initialDelaySeconds: 10
+            periodSeconds: 10
+
*** End Patch
*** Begin Patch
*** Add File:mcp/verifier/hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-verifier
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-verifier
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 50
+
*** End Patch
*** Begin Patch
*** Add File:registry/postgres/migration-job.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: migrate-registry-to-postgres
+  namespace: aegis
+spec:
+  template:
+    spec:
+      containers:
+        - name: migrate
+          image: ghcr.io/yourorg/aegis-registry-migrate:latest
+          env:
+            - name: DATABASE_URL
+              valueFrom:
+                secretKeyRef:
+                  name: registry-db-secret
+                  key: DATABASE_URL
+            - name: SQLITE_DB_PATH
+              value: "/data/model_cards.db"
+          volumeMounts:
+            - name: local-db
+              mountPath: /data
+      restartPolicy: OnFailure
+      volumes:
+        - name: local-db
+          hostPath:
+            path: /var/lib/aegis/registry
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_reliability_rules.yaml
+groups:
+- name: aegis-reliability.rules
+  rules:
+  - alert: RekorUnavailable
+    expr: up{job="rekor"} == 0
+    for: 2m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Rekor server unreachable"
+      description: "Rekor transparency log server appears down (job=rekor)."
+
+  - alert: CosignVerifyFailures
+    expr: increase(cosign_verify_failures_total[5m]) > 0
+    for: 2m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Cosign verification failures detected"
+      description: "One or more cosign verify operations have failed in the last 5 minutes; check admission logs."
+
+  - alert: VerifierLatencyHigh
+    expr: histogram_quantile(0.95, sum(rate(aegis_verifier_latency_seconds_bucket[5m])) by (le)) > 1
+    for: 5m
+    labels:
+      severity: warning
+    annotations:
+      summary: "Verifier latency high"
+      description: "95th percentile latency for verifier > 1s; consider scaling or using distilled model for CI."
+
+  - alert: KMSRotationOverdue
+    expr: (time() - aegis_kms_last_rotation_seconds) > 86400 * 90
+    for: 1h
+    labels:
+      severity: warning
+    annotations:
+      summary: "KMS rotation overdue"
+      description: "Cosign KMS key has not been rotated in >90 days."
+
*** End Patch
*** Begin Patch
*** Add File:ops/pagerduty/alertmanager_stub.md
+# Alertmanager → PagerDuty integration (operator must adapt)
+
+Example Alertmanager receiver config (do not store secrets in repo):
+
+1) Create Alertmanager receiver with PagerDuty service key in cluster secret.
+2) Example snippet (alertmanager.yaml):
+
+receivers:
+- name: 'pagerduty'
+  pagerduty_configs:
+  - service_key: 'REPLACE_PAGERDUTY_KEY'
+    severity: '{{ .CommonLabels.severity }}'
+
+route:
+  receiver: 'pagerduty'
+  group_by: ['alertname']
+  group_wait: 30s
+  group_interval: 5m
+  repeat_interval: 1h
+
+Operator: create secret with pagerduty key and mount into Alertmanager; configure Prometheus to use Alertmanager.
+
*** End Patch
*** Begin Patch
*** Add File:dsar/advanced_dsar.py
+#!/usr/bin/env python3
+"""
+Advanced DSAR tool:
+ - Locates occurrences of a subject (by hash or identifier) across contexts and attached objects
+ - Supports scanning common attachment types (.txt, .json, .pdf (text-extracted))
+ - Performs redact or delete action, records legal_hold tags to prevent accidental deletion
+"""
+import argparse
+import boto3
+import json
+import hashlib
+import tempfile
+import os
+import botocore
+from io import BytesIO
+
+s3 = boto3.client("s3")
+
+def hash_text(t):
+    return hashlib.sha256(t.encode("utf-8")).hexdigest()
+
+def scan_and_act(subject_hash, registry_url, s3_bucket, action="redact", legal_hold_tag="legal_hold"):
+    # Query registry for contexts (this assumes registry has metadata searchable API)
+    import requests
+    resp = requests.get(f"{registry_url}/search?limit=1000")
+    rows = resp.json()
+    affected = []
+    for r in rows:
+        run_id = r["run_id"]
+        rec = requests.get(f"{registry_url}/contexts/{run_id}").json()
+        s3_key = rec["s3_key"].replace("s3://","").split("/",1)[1]
+        try:
+            obj = s3.get_object(Bucket=s3_bucket, Key=s3_key)
+            data = obj["Body"].read().decode()
+        except botocore.exceptions.ClientError:
+            continue
+        if subject_hash in data:
+            print("Found in", run_id, s3_key)
+            if action == "redact":
+                ctx = json.loads(data)
+                ctx["_dsar_redacted"] = True
+                ctx["_dsar_redacted_at"] = int(time.time())
+                tmp = tempfile.mktemp(suffix=".json")
+                with open(tmp,"w") as f:
+                    json.dump(ctx, f)
+                s3.upload_file(tmp, s3_bucket, s3_key, ExtraArgs={"ServerSideEncryption":"aws:kms"})
+                os.remove(tmp)
+            elif action == "delete":
+                # ensure no legal hold tag present
+                head = s3.head_object(Bucket=s3_bucket, Key=s3_key)
+                if head.get("Metadata", {}).get(legal_hold_tag) == "true":
+                    print("Legal hold present; skipping deletion of", s3_key)
+                    continue
+                s3.delete_object(Bucket=s3_bucket, Key=s3_key)
+            affected.append((run_id,s3_key))
+    return affected
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--subject-hash", required=True)
+    p.add_argument("--registry-url", required=True)
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--action", choices=["redact","delete"], default="redact")
+    args = p.parse_args()
+    res = scan_and_act(args.subject_hash, args.registry_url, args.s3_bucket, action=args.action)
+    print("Affected:", res)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:tests/integration/test_rekor_and_kms.py
+import os
+import subprocess
+import requests
+
+def test_rekor_health():
+    REKOR = os.environ.get("REKOR_URL", "http://rekor.security.svc.cluster.local:3000")
+    r = requests.get(f"{REKOR}/api/v1/log/publicKey", timeout=5)
+    assert r.status_code == 200
+
+def test_kms_describe():
+    KMS = os.environ.get("COSIGN_KMS_ARN", "")
+    assert KMS != "", "COSIGN_KMS_ARN not set in env for test"
+    rc = subprocess.call(["aws","kms","describe-key","--key-id",KMS], stdout=subprocess.DEVNULL)
+    assert rc == 0
+
*** End Patch
*** Begin Patch
*** Add File:ops/runbook_drills.md
+# Runbook Drills (quarterly)
+
+Drill 1: Rekor outage simulation
+ - Disable Rekor service, verify Prometheus alert RekorUnavailable, check PagerDuty page, run ops/gather_evidence.sh and escalate.
+
+Drill 2: KMS rotation test
+ - Run infra/kms/rotate_cosign_kms.sh, update SecretManager, update CI secret in sandbox and re-sign a test artifact.
+
+Drill 3: DSAR exercise
+ - Choose test subject, run dsar/advanced_dsar.py to find & redact contexts, verify redaction and DSAR logs.
+
+Drill 4: Gatekeeper cutover
+ - Switch Gatekeeper constraint from dry-run to deny in a non-critical namespace and test that unsigned deployments are rejected.
+
*** End Patch
*** End Patch
