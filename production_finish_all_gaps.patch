*** Begin Patch
*** Add File: .github/workflows/ci_cosign_kms_rekor_final.yml
+name: CI: Build, SBOM, SCA, Sign with KMS (AWS/GCP) & Rekor verify
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+permissions:
+  contents: read
+  id-token: write
+  packages: write
+
+jobs:
+  build-sbom-scan-sign:
+    runs-on: ubuntu-latest
+    env:
+      IMAGE: ${{ secrets.REGISTRY_HOST }}/aegis-model:${{ github.sha }}
+      REKOR_URL: ${{ secrets.REKOR_URL }}
+      COSIGN_EXPERIMENTAL: "1"
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+
+      - name: Setup Docker Buildx
+        uses: docker/setup-buildx-action@v2
+
+      - name: Build image
+        run: docker build -t "$IMAGE" -f services/serving/Dockerfile .
+
+      - name: Generate SBOM (syft)
+        uses: anchore/syft-action@v1
+        with:
+          image: ${{ env.IMAGE }}
+          output: sbom.json
+
+      - name: Scan with Trivy
+        uses: aquasecurity/trivy-action@v1
+        with:
+          image-ref: ${{ env.IMAGE }}
+          format: json
+          output: trivy.json
+
+      - name: Fail on critical CVEs
+        run: |
+          if jq -e '.[] | .Vulnerabilities?[]? | select(.Severity=="CRITICAL")' trivy.json >/dev/null 2>&1; then
+            echo "Critical vulnerability found"; jq . trivy.json; exit 1
+          else
+            echo "No critical vulnerabilities"
+          fi
+
+      - name: Login to registry
+        uses: docker/login-action@v2
+        with:
+          registry: ${{ secrets.REGISTRY_HOST }}
+          username: ${{ secrets.REGISTRY_USER }}
+          password: ${{ secrets.REGISTRY_TOKEN }}
+
+      - name: Push image
+        run: docker push "$IMAGE"
+
+      - name: Get image digest
+        id: digest
+        run: |
+          DIGEST=$(docker inspect --format='{{index .RepoDigests 0}}' "$IMAGE" | awk -F@ '{print $2}')
+          echo "digest=$DIGEST" >> $GITHUB_OUTPUT
+
+      - name: Sign image with KMS-backed cosign (AWS or GCP)
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          if [ "${{ secrets.CLOUD_PROVIDER }}" = "aws" ]; then
+            # AWS KMS ARN e.g., arn:aws:kms:region:acct:key/key-id
+            cosign sign --key "awskms://${{ secrets.AWS_KMS_KEY_ARN }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+          elif [ "${{ secrets.CLOUD_PROVIDER }}" = "gcp" ]; then
+            # GCP KMS key ref e.g., gcpkms://projects/PROJECT/locations/global/keyRings/kr/cryptoKeys/key
+            cosign sign --key "gcpkms://${{ secrets.GCP_KMS_KEY_REF }}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+          else
+            echo "Unsupported CLOUD_PROVIDER; configure secrets.CLOUD_PROVIDER=aws|gcp"; exit 2
+          fi
+
+      - name: Verify cosign signature & Rekor entry
+        run: |
+          cosign verify --rekor-url "${REKOR_URL}" "${IMAGE}@${{ steps.digest.outputs.digest }}"
+
+      - name: Upload artifacts to compliance store (optional)
+        if: ${{ secrets.COMPLIANCE_BUCKET != '' }}
+        run: |
+          aws s3 cp sbom.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/sbom.json" || true
+          aws s3 cp trivy.json "${{ secrets.COMPLIANCE_BUCKET }}/ci/${GITHUB_RUN_ID}/trivy.json" || true
+
*** End Patch
*** Begin Patch
*** Add File: terraform/aws/kms_and_oidc.tf
+/*
+Terraform snippet: create AWS KMS key + GitHub OIDC IAM Role to allow GitHub Actions to sign via KMS.
+Customize variables for your account.
+*/
+provider "aws" {
+  region = var.aws_region
+}
+
+resource "aws_kms_key" "cosign" {
+  description             = "KMS key for cosign signing by GitHub Actions"
+  deletion_window_in_days = 30
+}
+
+resource "aws_iam_role" "github_oidc" {
+  name = "aegis-github-oidc-role"
+  assume_role_policy = data.aws_iam_policy_document.oidc_assume.json
+}
+
+data "aws_iam_policy_document" "oidc_assume" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "Federated"
+      identifiers = [var.oidc_provider_arn]
+    }
+    actions = ["sts:AssumeRoleWithWebIdentity"]
+    condition {
+      test     = "StringEquals"
+      variable = "${replace(var.oidc_provider_url, "https://", "")}:sub"
+      values   = ["repo:${var.github_org}/${var.github_repo}:ref:refs/heads/main"]
+    }
+  }
+}
+
+resource "aws_kms_key_policy" "cosign_policy" {
+  key_id = aws_kms_key.cosign.key_id
+  policy = data.aws_iam_policy_document.kms_policy.json
+}
+
+data "aws_iam_policy_document" "kms_policy" {
+  statement {
+    effect = "Allow"
+    principals {
+      type        = "AWS"
+      identifiers = [aws_iam_role.github_oidc.arn]
+    }
+    actions = [
+      "kms:Sign",
+      "kms:DescribeKey",
+      "kms:GetPublicKey"
+    ]
+    resources = ["*"]
+  }
+}
+
+output "kms_key_arn" {
+  value = aws_kms_key.cosign.arn
+}
+
*** End Patch
*** Begin Patch
*** Add File: helm/vault/values-production.yaml
+# Vault Helm values for production-like deployment (raft storage)
+server:
+  ha:
+    enabled: true
+  dataStorage:
+    enabled: true
+    storageClass: "standard"
+  extraEnvironmentVars:
+    VAULT_LOCAL_CONFIG: |
+      ui = true
+      telemetry {
+        prometheus_retention_time = "24h"
+      }
+      listener "tcp" {
+        address = "0.0.0.0:8200"
+        tls_disable = 0
+      }
+  service:
+    type: ClusterIP
+injector:
+  enabled: true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/vault_k8s_auth_setup.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Configure Vault Kubernetes auth for agent/controller to read secrets
+#
+VAULT_ADDR=${VAULT_ADDR:-http://vault.ops.svc.cluster.local:8200}
+VAULT_TOKEN=${VAULT_TOKEN:-}
+K8S_SA=${1:-agent-controller-sa}
+K8S_NS=${2:-staging}
+
+if [ -z "$VAULT_TOKEN" ]; then
+  echo "Set VAULT_TOKEN env var (bootstrap admin token)"
+  exit 2
+fi
+
+export VAULT_ADDR
+vault login "$VAULT_TOKEN"
+vault auth enable kubernetes || true
+
+# configure token reviewer using service account token
+SA_SECRET=$(kubectl get sa "$K8S_SA" -n "$K8S_NS" -o jsonpath='{.secrets[0].name}')
+TOKEN=$(kubectl get secret "$SA_SECRET" -n "$K8S_NS" -o go-template='{{ index .data "token" }}' | base64 --decode)
+CA_CERT=$(kubectl get secret "$SA_SECRET" -n "$K8S_NS" -o go-template='{{ index .data "ca.crt" }}' | base64 --decode)
+KUBERNETES_HOST=$(kubectl config view --raw --minify -o jsonpath='{.clusters[0].cluster.server}')
+
+vault write auth/kubernetes/config token_reviewer_jwt="$TOKEN" kubernetes_host="$KUBERNETES_HOST" kubernetes_ca_cert="$CA_CERT"
+
+vault policy write agent-read - <<EOF
+path "secret/data/agent/*" {
+  capabilities = ["read","list"]
+}
+EOF
+
+vault write auth/kubernetes/role/agent-role bound_service_account_names="$K8S_SA" bound_service_account_namespaces="$K8S_NS" policies=agent-read ttl=1h
+
+echo "Vault Kubernetes auth configured for role agent-role"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/postgres-primary-replica.yaml
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: ops
+---
+# Example using Bitnami Postgres primary-replica deployment (recommended: use managed DB in prod)
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: postgres
+  namespace: ops
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: postgres
+  template:
+    metadata:
+      labels:
+        app: postgres
+    spec:
+      containers:
+      - name: postgres
+        image: postgres:15
+        env:
+        - name: POSTGRES_USER
+          valueFrom:
+            secretKeyRef:
+              name: aegis-db
+              key: user
+        - name: POSTGRES_PASSWORD
+          valueFrom:
+            secretKeyRef:
+              name: aegis-db
+              key: password
+        ports:
+        - containerPort: 5432
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/redis-cluster-helm-values.yaml
+# Redis cluster values (example)
+cluster:
+  enabled: true
+  slaveCount: 2
+resources:
+  requests:
+    cpu: 200m
+    memory: 512Mi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/milvus-values-ha.yaml
+# Milvus HA values (example)
+replicaCount: 3
+minIO:
+  enabled: true
+persistentVolume:
+  enabled: true
+  size: 200Gi
+
*** End Patch
*** Begin Patch
*** Add File: k8s/ha/pg_backup_cronjob.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: postgres-backup
+  namespace: ops
+spec:
+  schedule: "0 3 * * *"
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: pgdump
+              image: bitnami/postgresql:15
+              env:
+                - name: PGPASSWORD
+                  valueFrom:
+                    secretKeyRef:
+                      name: aegis-db
+                      key: password
+              command:
+                - /bin/sh
+                - -c
+                - |
+                  pg_dump -h postgres -U $POSTGRES_USER -d aegis > /tmp/backup.sql
+                  if [ -n "${COMPLIANCE_BUCKET:-}" ]; then
+                    aws s3 cp /tmp/backup.sql ${COMPLIANCE_BUCKET}/backups/backup-$(date +%s).sql
+                  fi
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: scripts/backup_restore_instructions.md
+# Backup & Restore Instructions
+
+1) Postgres backups are uploaded to ${COMPLIANCE_BUCKET}/backups by CronJob.
+2) To restore:
+   - Download backup: aws s3 cp s3://.../backup.sql /tmp/backup.sql
+   - kubectl exec -i deploy/postgres -n ops -- psql -U $POSTGRES_USER -d aegis < /tmp/backup.sql
+
+3) Milvus: ensure snapshot or MinIO backup configured per Milvus operator docs.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/attestation/attestation-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: attestation
+  namespace: ops
+  labels:
+    app: attestation
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: attestation
+  template:
+    metadata:
+      labels:
+        app: attestation
+    spec:
+      containers:
+      - name: attestation
+        image: ghcr.io/yourorg/attestation-service:latest
+        ports:
+        - containerPort: 8443
+        env:
+          - name: COSIGN_BIN
+            value: "cosign"
+          - name: REKOR_URL
+            value: "https://rekor.sigstore.dev"
+        volumeMounts:
+          - name: tls
+            mountPath: /tls
+            readOnly: true
+      volumes:
+        - name: tls
+          secret:
+            secretName: attestation-tls
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: attestation
+  namespace: ops
+spec:
+  selector:
+    app: attestation
+  ports:
+    - port: 8443
+      targetPort: 8443
+
*** End Patch
*** Begin Patch
*** Add File: k8s/webhook/validatingwebhook.yaml
+apiVersion: admissionregistration.k8s.io/v1
+kind: ValidatingWebhookConfiguration
+metadata:
+  name: aegis-attestation-webhook
+webhooks:
+  - name: attestation.aegis.local
+    sideEffects: None
+    admissionReviewVersions: ["v1"]
+    clientConfig:
+      service:
+        name: attestation
+        namespace: ops
+        path: /verify
+      caBundle: "" # replace with base64 CA bundle produced by create_webhook_certs.sh
+    rules:
+      - operations: ["CREATE", "UPDATE"]
+        apiGroups: ["apps",""]
+        apiVersions: ["v1"]
+        resources: ["deployments","pods"]
+    failurePolicy: Fail
+    timeoutSeconds: 10
+    matchPolicy: Equivalent
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/require_attestation_advanced.rego
+package k8srequireattested
+
+default allow = false
+
+violation[{"msg": msg}] {
+  input.review.object.kind == "Deployment"
+  ann := input.review.object.metadata.annotations
+  not ann_ok(ann)
+  msg := "Missing required attestation; deployment must have annotation aegis.attested=true"
+}
+
+ann_ok(ann) {
+  ann["aegis.attested"] == "true"
+}
+
*** End Patch
*** Begin Patch
*** Add File: governance/gatekeeper/constraint_require_attested.yaml
+apiVersion: constraints.gatekeeper.sh/v1beta1
+kind: K8sRequireAttested
+metadata:
+  name: require-attested-annotation
+spec:
+  match:
+    kinds:
+      - apiGroups: ["apps"]
+        kinds: ["Deployment"]
+    namespaces: ["staging","ops"]
+  parameters: {}
+
*** End Patch
*** Begin Patch
*** Add File: services/agent_controller/app_prod_patch.py
+"""
+Agent controller production patch:
+ - Uses Postgres (SQLAlchemy) for persistent state and Redis for short-term memory
+ - Integrates leader election so only leader runs planner loops
+ - Uses TokenRequest projected tokens for sandboxed tool execution (via k8s_executor_projected)
+"""
+import os, threading, json, time, uuid
+from flask import Flask, request, jsonify
+from sqlalchemy import create_engine, Column, String, Text, Integer
+from sqlalchemy.ext.declarative import declarative_base
+from sqlalchemy.orm import sessionmaker
+import redis
+from services.agent_controller import k8s_executor_projected
+from services.agent_controller.leader_election import run_election
+
+DATABASE_URL = os.environ.get("AGENT_DB_URL", "postgresql://aegis:aegis@postgres.ops.svc.cluster.local:5432/agentdb")
+REDIS_URL = os.environ.get("REDIS_URL", "redis://redis.ops.svc.cluster.local:6379/0")
+AGENT_NS = os.environ.get("AGENT_EXEC_NS", "agent-exec")
+TOOL_IMAGE = os.environ.get("TOOL_JOB_IMAGE", "ghcr.io/yourorg/agent-tool-runner:latest")
+
+Base = declarative_base()
+
+class Agent(Base):
+    __tablename__ = "agents"
+    id = Column(String, primary_key=True)
+    spec = Column(Text)
+    state = Column(Text)
+    created_ts = Column(Integer)
+    updated_ts = Column(Integer)
+
+engine = create_engine(DATABASE_URL, pool_size=20, max_overflow=10)
+SessionLocal = sessionmaker(bind=engine)
+redis_client = redis.from_url(REDIS_URL)
+
+app = Flask("agent-controller-prod")
+
+def init_db():
+    Base.metadata.create_all(bind=engine)
+
+def planner_worker(agent_id):
+    session = SessionLocal()
+    try:
+        agent = session.query(Agent).filter(Agent.id==agent_id).first()
+        # simple loop that may produce tool calls; placeholder logic
+        for i in range(3):
+            # simulate planning and tool call
+            action = {"tool":"echo-tool","args":{"msg":f"iter-{i}"}}
+            res = execute_tool(action, agent_id)
+            # persist result in Redis for short term
+            redis_client.rpush(f"agent:{agent_id}:results", json.dumps(res))
+            time.sleep(2)
+    finally:
+        session.close()
+
+def execute_tool(action, agent_id):
+    # OPA preflight can be added here
+    jobname = f"tool-{agent_id}-{uuid.uuid4().hex[:6]}"
+    ok = k8s_executor_projected.create_projected_job(jobname, namespace=AGENT_NS, image=TOOL_IMAGE, tool_name=action["tool"], args_json=action.get("args",{}), sa_name="tool-runner-sa")
+    return {"ok": ok, "job": jobname}
+
+def on_become_leader():
+    print("Node became leader: scheduling background planner tasks")
+    # Start any leader-only background threads (example: resume unfinished agents)
+    t = threading.Thread(target=leader_background_loop, daemon=True)
+    t.start()
+
+def on_standby():
+    print("Node is standby (not leader)")
+
+def leader_background_loop():
+    session = SessionLocal()
+    while True:
+        agents = session.query(Agent).all()
+        for a in agents:
+            threading.Thread(target=planner_worker, args=(a.id,), daemon=True).start()
+        time.sleep(60)
+
+@app.post("/agents")
+def create_agent():
+    body = request.get_json() or {}
+    agent_id = f"agent-{uuid.uuid4().hex[:8]}"
+    session = SessionLocal()
+    a = Agent(id=agent_id, spec=json.dumps(body), state=json.dumps({"created":True}), created_ts=int(time.time()), updated_ts=int(time.time()))
+    session.add(a); session.commit()
+    return jsonify({"agent_id": agent_id})
+
+if __name__ == "__main__":
+    init_db()
+    # Start leader election loop in background
+    threading.Thread(target=lambda: run_election(on_become_leader, on_standby), daemon=True).start()
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8200)))
+
*** End Patch
*** Begin Patch
*** Add File: k8s/manifests/agent-controller-deployment-with-lease.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: agent-controller
+  namespace: staging
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: agent-controller
+  template:
+    metadata:
+      labels:
+        app: agent-controller
+    spec:
+      serviceAccountName: agent-controller-sa
+      containers:
+      - name: controller
+        image: ghcr.io/yourorg/agent-controller:prod
+        env:
+          - name: AGENT_DB_URL
+            valueFrom:
+              secretKeyRef:
+                name: aegis-db
+                key: agent_db_url
+          - name: REDIS_URL
+            value: redis://redis.ops.svc.cluster.local:6379/0
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: leader-election-role
+  namespace: staging
+rules:
+- apiGroups: ["coordination.k8s.io"]
+  resources: ["leases"]
+  verbs: ["get","create","update","patch","delete"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: leader-election-binding
+  namespace: staging
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: leader-election-role
+subjects:
+- kind: ServiceAccount
+  name: agent-controller-sa
+  namespace: staging
+
*** End Patch
*** Begin Patch
*** Add File: tests/agent_sandbox_pen_test.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Pen test: attempt to run a job that tries to exfiltrate token or contact external host.
+#
+NAMESPACE=${NAMESPACE:-agent-exec}
+JOB_NAME="sandbox-escape-test"
+
+cat > /tmp/job.yaml <<EOF
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: ${JOB_NAME}
+  namespace: ${NAMESPACE}
+spec:
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: tool-runner-sa
+      containers:
+      - name: exploit
+        image: busybox
+        command: ["sh","-c","curl -sS http://169.254.169.254 || echo no-metadata; sleep 2"]
+      restartPolicy: Never
+EOF
+
+kubectl apply -f /tmp/job.yaml
+echo "Waiting job completion..."
+kubectl wait --for=condition=complete job/${JOB_NAME} -n ${NAMESPACE} --timeout=30s || true
+kubectl logs job/${JOB_NAME} -n ${NAMESPACE} || true
+kubectl delete -f /tmp/job.yaml --ignore-not-found
+
+echo "Check NetworkPolicy and egress proxy logs to validate the request was blocked."
+
*** End Patch
*** Begin Patch
*** Add File: argo/analysis/loadtest-analysis-template.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: AnalysisTemplate
+metadata:
+  name: loadtest-analysis
+  namespace: staging
+spec:
+  metrics:
+  - name: p95-latency
+    interval: 30s
+    count: 3
+    successCondition: result[0] < 1.0
+    failureCondition: result[0] > 2.0
+    provider:
+      prometheus:
+        address: http://prometheus.monitoring.svc.cluster.local
+        query: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le))
+  - name: load-job
+    interval: 1m
+    count: 1
+    provider:
+      job:
+        spec:
+          template:
+            spec:
+              containers:
+              - name: locust
+                image: python:3.10
+                command: ["/bin/sh","-c"]
+                args: ["pip install locust && locust -f services/loadtest/locustfile.py --headless -u 200 -r 20 --run-time 1m --host http://model-serving.staging.svc.cluster.local"]
+              restartPolicy: Never
+
*** End Patch
*** Begin Patch
*** Add File: services/loadtest/locustfile.py
+from locust import HttpUser, task, between
+
+class InferenceUser(HttpUser):
+    wait_time = between(0.5, 1)
+
+    @task(5)
+    def predict(self):
+        payload = {"inputs": ["hello world"]*4}
+        self.client.post("/predict", json=payload, timeout=30)
+
+    @task(1)
+    def health(self):
+        self.client.get("/health")
+
*** End Patch
*** Begin Patch
*** Add File: services/edge_fleet/ota_canary_test.py
+"""
+Edge OTA canary test harness (simulated devices).
+This script simulates a fleet of devices that poll the enrollment service and accept updates.
+Used to test canary rollout and rollback logic in staging using the OTA controller.
+"""
+import time, random, requests
+
+ENROLL_URL = "http://edge-enroll.ops.svc.cluster.local:8086"
+OTA_CTRL_URL = "http://edge-ota.ops.svc.cluster.local:8087"
+
+def simulate_device(device_id):
+    # simulate device health reporting and applying updates
+    return {"device_id": device_id, "status": "healthy", "version": "v1"}
+
+def run_canary(artifact_uri):
+    # request OTA controller to run a rollout
+    r = requests.post(f"{OTA_CTRL_URL}/start", json={"artifact": artifact_uri, "canary_count": 2}, timeout=30)
+    return r.json()
+
+if __name__ == "__main__":
+    print("Simulate canary")
+    resp = run_canary("s3://aegis-edge/artifact-v2.tflite")
+    print(resp)
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus/production_slos_and_alerts.yaml
+groups:
+- name: aegis.production.slo
+  rules:
+  - alert: ServingP95Exceeded
+    expr: histogram_quantile(0.95, sum(rate(aegis_inference_latency_seconds_bucket[5m])) by (le)) > 1.0
+    for: 5m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Model serving p95 > 1s"
+
+  - alert: CheckpointUploadMissing
+    expr: increase(aegis_training_steps_total[1h]) > 0 and absent(aegis_checkpoint_uploaded_total)
+    for: 30m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Checkpoints not being uploaded during training runs"
+
+  - alert: BudgetExceeded
+    expr: (sum by (instance) (increase(cloud_billing_cost_total[1h])) / sum by (instance) (increase(cloud_billing_budget_total[1h])) ) * 100 > 90
+    for: 10m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Budget usage > 90%"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/production_validation_runbook.md
+# Production Validation & Acceptance Runbook
+
+Goal: verify the platform enforces attestation, runs HA, passes large-scale tests, agent sandboxing, edge OTA canary and SLO-based rollbacks.
+
+Prereqs:
+- KMS (AWS/GCP) created and KMS key ARN available
+- GitHub OIDC role configured and allowed to use KMS key
+- Vault deployed and configured via scripts/vault_k8s_auth_setup.sh
+- Postgres/Redis/Milvus installed in HA (helm), attestation deployed and ValidatingWebhook patched with CA bundle
+
+Steps (high level):
+1) CI attestation:
+   - Push a model build to main -> check GitHub Action signs using KMS and Rekor entry created.
+   - Confirm sbom & trivy artifacts uploaded to COMPLIANCE_BUCKET.
+
+2) Webhook + Gatekeeper:
+   - Apply unsigned deployment (tests/unsigned_deploy.yaml) -> expect rejection
+   - Build & sign image using CI (or local cosign with KMS) -> annotate manifest; apply signed deployment and expect success
+
+3) HA & backups:
+   - Stop primary Postgres pod and verify replica promotion or restore from backup (run backup restore instructions)
+
+4) Large-scale tests:
+   - Run argo/workflows/distributed-tpu-real (use real dataset URI)
+   - Run Milvus scale script scripts/milvus_tune.py and scripts/scale_test_milvus.py
+   - Run loadtest via Argo analysis/loadtest-analysis-template.yaml or GitHub Action (.github/workflows/load_test.yml)
+
+5) Agent sandbox & leader election:
+   - Deploy two controller replicas; verify only leader runs planner (check logs)
+   - Run tests/agent_sandbox_pen_test.sh to ensure attempted egress is blocked
+
+6) Edge OTA canary:
+   - Run services/edge_fleet/ota_canary_test.py to exercise OTA controller canary flow
+   - Validate rollback by simulating failure in canary devices
+
+7) SLO & auto-rollback:
+   - Deploy a canary via Argo Rollout with analysis template loadtest-analysis and intentionally degrade canary (e.g., replace with high-latency image) -> verify Argo auto-rollbacks
+
+8) Pen tests:
+   - Execute tests in tests/ (agent & deployment) and record remediation tickets for any failures
+
+Acceptance:
+- All unsigned artifacts rejected; signed artifacts accepted.
+- Postgres/Redis have HA and backup/restore flows validated.
+- TPU & inference scale tests pass SLOs or actionable remediation recorded.
+- Agent sandbox prevents exfiltration and leader election ensures single planner runs.
+- OTA canary validated with rollback on failures.
+
*** End Patch
*** End Patch
