*** Begin Patch
*** Add File:spark/compaction/compact_parquet.py
+#!/usr/bin/env python3
+"""
+Spark compaction job: read small Parquet files under an input prefix,
+coalesce and write out compacted Parquet files with target file size.
+
+Usage:
+  spark-submit --master <...> compact_parquet.py \
+    --input s3://bucket/path/raw/ --output s3://bucket/path/compacted/ --target_mb 256
+
+Notes:
+ - Requires PySpark and boto3 available in the driver image.
+ - This script uses simple repartitioning heuristics; for large scale replace with dynamic sizing logic.
+"""
+import argparse
+import math
+from pyspark.sql import SparkSession
+
+def parse_args():
+    p = argparse.ArgumentParser()
+    p.add_argument("--input", required=True)
+    p.add_argument("--output", required=True)
+    p.add_argument("--target-mb", type=int, default=256, help="Target file size in MB")
+    p.add_argument("--partition-cols", default="", help="Comma separated partition columns (optional)")
+    return p.parse_args()
+
+def main():
+    args = parse_args()
+    spark = SparkSession.builder.appName("compact_parquet").getOrCreate()
+    df = spark.read.option("mergeSchema", "true").parquet(args.input)
+    # Estimate total size by sampling file sizes via Hadoop FileSystem (approx)
+    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
+    path = spark._jvm.org.apache.hadoop.fs.Path(args.input)
+    total_bytes = 0
+    it = fs.listFiles(path, True)
+    while it.hasNext():
+        total_bytes += it.next().getLen()
+    target_bytes = args.target_mb * 1024 * 1024
+    partitions = max(1, int(total_bytes / target_bytes))
+    # Bound partitions
+    partitions = min(partitions, 10000)
+    print(f"Total bytes ~{total_bytes}, target bytes {target_bytes}, partitions {partitions}")
+
+    # If partition cols specified, write partitioned
+    if args.partition_cols:
+        cols = [c.strip() for c in args.partition_cols.split(",") if c.strip()]
+        # Repartition by partition cols + target partitions
+        df.repartition(partitions, *cols).write \
+            .mode("overwrite") \
+            .option("compression", "snappy") \
+            .option("parquet.block.size", 134217728) \
+            .partitionBy(*cols) \
+            .parquet(args.output)
+    else:
+        df.repartition(partitions).write \
+            .mode("overwrite") \
+            .option("compression", "snappy") \
+            .option("parquet.block.size", 134217728) \
+            .parquet(args.output)
+
+    print("Compaction finished.")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:argo/compact_parquet_cron.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: CronWorkflow
+metadata:
+  name: compact-parquet-cron
+  namespace: aegis
+spec:
+  schedule: "0 3 * * *" # daily at 03:00 UTC
+  concurrencyPolicy: "Forbid"
+  successfulJobsHistoryLimit: 7
+  failedJobsHistoryLimit: 3
+  timezone: "UTC"
+  workflowSpec:
+    entrypoint: compact
+    arguments:
+      parameters:
+        - name: input
+          value: "s3://REPLACE_EVIDENCE_BUCKET/raw/events/"
+        - name: output
+          value: "s3://REPLACE_EVIDENCE_BUCKET/curated/events/"
+        - name: target_mb
+          value: "256"
+        - name: partition_cols
+          value: "dt"
+        - name: image
+          value: "ghcr.io/yourorg/aegis-spark:latest"
+    templates:
+      - name: compact
+        inputs:
+          parameters:
+            - name: input
+            - name: output
+            - name: target_mb
+            - name: partition_cols
+            - name: image
+        container:
+          image: "{{inputs.parameters.image}}"
+          command: ["/bin/bash","-lc"]
+          args:
+            - |
+              set -euo pipefail
+              echo "Running parquet compaction"
+              spark-submit /opt/spark/jobs/compaction/compact_parquet.py \
+                --input "{{inputs.parameters.input}}" \
+                --output "{{inputs.parameters.output}}" \
+                --target-mb "{{inputs.parameters.target_mb}}" \
+                --partition-cols "{{inputs.parameters.partition_cols}}"
+          resources:
+            requests:
+              cpu: "2000m"
+              memory: "8Gi"
+            limits:
+              cpu: "4000m"
+              memory: "16Gi"
+
*** End Patch
*** Begin Patch
*** Add File:trino/resource_groups.properties
+# Trino resource groups example (production-ready starting point)
+# Place under /etc/trino/resource_groups.properties and mount as ConfigMap.
+
+root.groups=analysts,etl,adhoc
+
+root.analysts.maxRunning=8
+root.analysts.softConcurrencyLimit=10
+root.analysts.maxQueued=50
+root.analysts.softMemoryLimit=8GB
+root.analysts.hardMemoryLimit=12GB
+root.analysts.priority=5
+
+root.etl.maxRunning=4
+root.etl.softConcurrencyLimit=6
+root.etl.maxQueued=20
+root.etl.softMemoryLimit=20GB
+root.etl.hardMemoryLimit=24GB
+root.etl.priority=2
+
+root.adhoc.maxRunning=4
+root.adhoc.softConcurrencyLimit=6
+root.adhoc.maxQueued=40
+root.adhoc.softMemoryLimit=6GB
+root.adhoc.hardMemoryLimit=8GB
+root.adhoc.priority=10
+
+# Default timeout (ms)
+query_max_run_time=3600000
+
*** End Patch
*** Begin Patch
*** Add File:k8s/trino/configmap-resource-groups.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: trino-resource-groups
+  namespace: aegis
+data:
+  resource_groups.properties: |
+    root.groups=analysts,etl,adhoc
+
+    root.analysts.maxRunning=8
+    root.analysts.softConcurrencyLimit=10
+    root.analysts.maxQueued=50
+    root.analysts.softMemoryLimit=8GB
+    root.analysts.hardMemoryLimit=12GB
+    root.analysts.priority=5
+
+    root.etl.maxRunning=4
+    root.etl.softConcurrencyLimit=6
+    root.etl.maxQueued=20
+    root.etl.softMemoryLimit=20GB
+    root.etl.hardMemoryLimit=24GB
+    root.etl.priority=2
+
+    root.adhoc.maxRunning=4
+    root.adhoc.softConcurrencyLimit=6
+    root.adhoc.maxQueued=40
+    root.adhoc.softMemoryLimit=6GB
+    root.adhoc.hardMemoryLimit=8GB
+    root.adhoc.priority=10
+
+    query_max_run_time=3600000
+
*** End Patch
*** Begin Patch
*** Add File:alluxio/values.yaml
+# Alluxio Helm values (basic production suggestions)
+master:
+  replicas: 2
+  resources:
+    requests:
+      cpu: 500m
+      memory: 1Gi
+    limits:
+      cpu: 1000m
+      memory: 2Gi
+worker:
+  replicas: 3
+  resources:
+    requests:
+      cpu: 2000m
+      memory: 4Gi
+    limits:
+      cpu: 4000m
+      memory: 8Gi
+underFS:
+  # configure to use S3 (lakeFS) or cloud storage
+  s3:
+    endpoint: "https://s3.amazonaws.com"
+    bucket: "REPLACE_EVIDENCE_BUCKET"
+    accessKey: ""
+    secretKey: ""
+persistence:
+  enabled: true
+  storageClass: gp3
+  size: 200Gi
+service:
+  type: ClusterIP
+ingress:
+  enabled: false
+
*** End Patch
*** Begin Patch
*** Add File:dbt/profiles.yml.template
+# Template for dbt profiles.yml
+# Copy to ~/.dbt/profiles.yml and fill in credentials
+default:
+  outputs:
+    dev:
+      type: trino
+      host: trino-coordinator.aegis.svc.cluster.local
+      port: 8080
+      user: dev_user
+      schema: analytics
+      catalog: hive
+      http_scheme: http
+  target: dev
+
*** End Patch
*** Begin Patch
*** Add File:dbt/project.yml
+name: aegis_analytics
+version: '0.1'
+config-version: 2
+profile: default
+source-paths: ["models"]
+target-path: "target"
+clean-targets:
+  - "target"
+  - "dbt_modules"
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/curated_events.sql
+-- dbt model: curated_events
+-- This model demonstrates incremental partitioned writes for analytics
+{{ config(materialized='incremental', unique_key='event_id') }}
+
+with raw as (
+    select * from {{ source('raw','events') }}
+)
+
, transformed as (
+    select
+      event_id,
+      user_id,
+      event,
+      event_time,
+      date(event_time) as dt,
+      properties
+    from raw
+)
+
+select * from transformed
+{% if is_incremental() %}
+where event_time > (select max(event_time) from {{ this }})
+{% endif %}
+
*** End Patch
*** Begin Patch
*** Add File:superset/dashboards/additional_analytics_dashboard.json
+{
+  "dashboard_title": "Analytics Summary",
+  "description": "Key analytics metrics and heavy-query safe charts",
+  "charts": [
+    {
+      "slice_name": "Top Events Last 7 Days",
+      "viz_type": "table",
+      "query": {
+        "sql": "SELECT event, count(*) as cnt FROM analytics.events WHERE dt >= current_date - interval '7' day GROUP BY event ORDER BY cnt DESC LIMIT 50"
+      }
+    },
+    {
+      "slice_name": "Daily Active Users (7d)",
+      "viz_type": "line",
+      "query": {
+        "sql": "SELECT dt, count(distinct user_id) as dau FROM analytics.events WHERE dt >= current_date - interval '30' day GROUP BY dt ORDER BY dt"
+      }
+    }
+  ],
+  "position_json": "{}"
+}
+
*** End Patch
*** Begin Patch
*** Add File:feast/feature_repo.yaml
+# Minimal Feast feature repo scaffold (operator must adapt to infra)
+project: aegis-feast
+provider: aws
+registry: s3://REPLACE_EVIDENCE_BUCKET/feast/registry.db
+provider_config:
+  aws_region: REPLACE_REGION
+  s3_bucket: REPLACE_EVIDENCE_BUCKET
+online_store:
+  type: dynamodb
+  dynamodb:
+    table_name: feast-online-store
+    region: REPLACE_REGION
+offline_store:
+  type: s3
+  s3:
+    bucket: REPLACE_EVIDENCE_BUCKET
+    prefix: feast/offline
+entities:
+  - name: user
+    join_key: user_id
+features:
+  - name: user_7d_avg_spend
+    dtype: float
+    entity: user
+    transform: "rolling_mean(spend, 7)"
+
*** End Patch
*** Begin Patch
*** Add File:prometheus/trino_s3_alerts.yaml
+groups:
+- name: aegis-trino.rules
+  rules:
+  - alert: TrinoHighQueryCount
+    expr: increase(trino_queries_total[5m]) > 200
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "High number of Trino queries in last 5m"
+
+  - alert: TrinoHighS3Egress
+    expr: increase(aws_s3_bytes_downloaded_total[1h]) > 10000000000
+    for: 30m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Large S3 egress from Trino in last hour"
+
*** End Patch
*** Begin Patch
*** Add File:grafana/dashboards/trino_s3_dashboard.json
+{
+  "title": "Trino & S3 Analytics Overview",
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Queries per Minute",
+      "targets": [{"expr": "rate(trino_queries_total[5m])"}]
+    },
+    {
+      "type": "graph",
+      "title": "S3 Bytes Downloaded (1h)",
+      "targets": [{"expr": "increase(aws_s3_bytes_downloaded_total[1h])"}]
+    },
+    {
+      "type": "table",
+      "title": "Top Long Queries",
+      "targets": [{"expr": "topk(10, trino_query_duration_seconds)"}]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:README_ANALYTICS_OPTIMIZATION.md
+# Analytics Optimization Artifacts
+
+This patch provides production-ready starting artifacts to optimize data analytics on Aegis:
+
+- Storage compaction:
+  - spark/compaction/compact_parquet.py — Spark compaction script
+  - argo/compact_parquet_cron.yaml — Argo CronWorkflow to run compaction daily
+
+- Trino resource governance:
+  - trino/resource_groups.properties — resource groups config
+  - k8s/trino/configmap-resource-groups.yaml — ConfigMap to mount into Trino
+
+- Caching:
+  - alluxio/values.yaml — Alluxio helm values to accelerate hot datasets (fill placeholders)
+
+- Transformation & CI:
+  - dbt/ (project scaffold + profiles template) — dbt scaffold for modular transformations
+
+- BI dashboards:
+  - superset/dashboards/* — sample dashboards (sales funnel, analytics summary, top co-occurrences)
+
+- Feature store:
+  - feast/feature_repo.yaml — minimal Feast scaffold (offline+online stores)
+
+- Observability & cost:
+  - prometheus/trino_s3_alerts.yaml — Prometheus alerting rules for Trino/S3 egress
+  - grafana/dashboards/trino_s3_dashboard.json — Grafana dashboard template
+
+How to use
+1. Edit REPLACE_* placeholders (buckets, regions, image registry).
+2. Build/push images: use your CI to build aegis-spark and mount compaction script under /opt/spark/jobs/compaction/.
+3. Apply Trino resource group ConfigMap and mount into Trino pods; restart coordinator to pick up.
+4. Deploy Alluxio via Helm using alluxio/values.yaml and configure underFS to point at lakeFS S3 if desired.
+5. Use dbt to manage transformations; copy dbt/profiles.yml.template to analysts' ~/.dbt/profiles.yml.
+6. Import Superset dashboard JSON files via Superset UI or API.
+7. Deploy Prometheus rules and Grafana dashboard.
+8. Schedule Argo CronWorkflow for compaction; monitor RTO/RPO and S3 cost metrics.
+
+If you want, I can:
+- generate Helm chart values for Trino/Alluxio/Superset tailored to your cluster size,
+- produce Dockerfiles & CI workflow to build/push the aegis-spark and aegis-flink images with compaction & jobs included,
+- produce a dbt CI pipeline (github action) to run dbt models and tests on merge,
+- or implement automated compaction reporting (report file counts & avg file size) as an Argo step.
+
*** End Patch
*** End Patch
