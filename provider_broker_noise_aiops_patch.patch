*** Begin Patch
*** Add File: provider/broker.py
+#!/usr/bin/env python3
+"""
+Provider Broker prototype for cost/latency/carbon-aware selection.
+
+Usage:
+  python provider/broker.py --candidates braket ibm quantinuum \
+    --job-type generative --budget 2.0 --latency-ms 200
+Outputs JSON with selected provider and scoring breakdown.
+
+Notes:
+ - Reads configuration from /etc/aegis/provider_broker_config.json (mount a ConfigMap)
+ - This is a prototype scoring function; replace heuristics with ML or RL as needed.
+"""
+import argparse
+import json
+import os
+from datetime import datetime
+
+DEFAULT_CONFIG_PATH = "/etc/aegis/provider_broker_config.json"
+
+
+def load_config(path=DEFAULT_CONFIG_PATH):
+    if os.path.exists(path):
+        return json.load(open(path))
+    # fallback config
+    return {
+        "providers": {
+            "braket": {"cost_per_job": 1.0, "latency_ms": 120, "carbon_g_per_kwh": 400, "score_bias": 0},
+            "ibm": {"cost_per_job": 1.2, "latency_ms": 80, "carbon_g_per_kwh": 200, "score_bias": 0},
+            "quantinuum": {"cost_per_job": 1.5, "latency_ms": 150, "carbon_g_per_kwh": 250, "score_bias": 0}
+        },
+        "weights": {"cost": 0.4, "latency": 0.3, "carbon": 0.2, "historical": 0.1}
+    }
+
+
+def score_provider(meta, weights, job_budget=None, job_latency_target=None, hist_score=1.0):
+    # Normalize metrics to a comparable scale (simple heuristics)
+    cost = meta.get("cost_per_job", 1.0)
+    latency = meta.get("latency_ms", 100)
+    carbon = meta.get("carbon_g_per_kwh", 300)
+    # lower is better; compute a composite
+    score = weights["cost"] * cost + weights["latency"] * (latency / 100.0) + weights["carbon"] * (carbon / 100.0)
+    # historical success factor (lower score if provider historically good)
+    score = score * (1.0 / max(0.1, hist_score))  # hist_score >1 increases preference
+    return score
+
+
+def pick_provider(candidates, cfg, budget=None, latency_target=None, hist_scores=None):
+    hist_scores = hist_scores or {}
+    weights = cfg.get("weights", {})
+    best = None
+    best_score = float("inf")
+    details = {}
+    for p in candidates:
+        meta = cfg["providers"].get(p, {})
+        hist = hist_scores.get(p, 1.0)
+        s = score_provider(meta, weights, job_budget=budget, job_latency_target=latency_target, hist_score=hist)
+        details[p] = {"score": s, "meta": meta, "hist_score": hist}
+        if s < best_score:
+            best_score = s
+            best = p
+    return best, best_score, details
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--candidates", nargs="+", required=True)
+    p.add_argument("--config", default=DEFAULT_CONFIG_PATH)
+    p.add_argument("--budget", type=float, default=None)
+    p.add_argument("--latency-ms", type=int, default=None)
+    p.add_argument("--job-type", default="generic")
+    args = p.parse_args()
+
+    cfg = load_config(args.config)
+    # Historical scores can be pulled from monitoring/DB; here we stub
+    hist_scores = {c: 1.0 for c in args.candidates}
+
+    selected, score, details = pick_provider(args.candidates, cfg, budget=args.budget, latency_target=args.latency_ms, hist_scores=hist_scores)
+    out = {"timestamp": datetime.utcnow().isoformat(), "selected": selected, "score": score, "details": details}
+    print(json.dumps(out, indent=2))
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/noise_aware_scheduler.py
+#!/usr/bin/env python3
+"""
+Noise-aware scheduler for quantum jobs.
+
+Prototype:
+ - reads per-device calibration metrics (from MLflow or local JSON)
+ - computes a noise score for candidate devices
+ - combines noise score with cost to pick device/qubit(s)
+ - emits decision JSON consumed by the scheduler/adapters
+
+Usage:
+  python quantum/noise_aware_scheduler.py --candidates device1 device2 --calib-dir ./calib
+"""
+import argparse
+import json
+import os
+from datetime import datetime
+from math import inf
+
+CALIB_DIR = "/etc/aegis/qpu_calibrations"  # mount of calibration artifacts or leave local folder
+
+
+def load_calibration(device, calib_dir=CALIB_DIR):
+    path = os.path.join(calib_dir, f"{device}.json")
+    if os.path.exists(path):
+        return json.load(open(path))
+    # default high-noise stub
+    return {"device": device, "t1_ms": 50.0, "t2_ms": 20.0, "readout_error": 0.1, "gate_error": 0.02, "avg_fidelity": 0.6, "cost": 1.0}
+
+
+def noise_score(calib):
+    # heuristic: combine readout/gate/fidelity into a single higher-is-worse score
+    readout = calib.get("readout_error", 0.1)
+    gate = calib.get("gate_error", 0.02)
+    fidelity = calib.get("avg_fidelity", 0.6)
+    # higher score is worse
+    score = (readout * 0.5 + gate * 0.4 + (1.0 - fidelity) * 0.1) * 100.0
+    return score
+
+
+def pick_device(candidates, calib_dir=CALIB_DIR, cost_weight=0.5, noise_weight=0.5):
+    best = None
+    best_score = inf
+    details = {}
+    for d in candidates:
+        c = load_calibration(d, calib_dir)
+        n = noise_score(c)
+        cost = c.get("cost", 1.0)
+        composite = noise_weight * n + cost_weight * cost
+        details[d] = {"noise": n, "cost": cost, "composite": composite, "calibration": c}
+        if composite < best_score:
+            best_score = composite
+            best = d
+    return best, best_score, details
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--candidates", nargs="+", required=True)
+    p.add_argument("--calib-dir", default=CALIB_DIR)
+    p.add_argument("--noise-weight", type=float, default=0.7)
+    p.add_argument("--cost-weight", type=float, default=0.3)
+    args = p.parse_args()
+
+    selected, score, details = pick_device(args.candidates, calib_dir=args.calib_dir, cost_weight=args.cost_weight, noise_weight=args.noise_weight)
+    out = {"timestamp": datetime.utcnow().isoformat(), "selected_device": selected, "score": score, "details": details}
+    print(json.dumps(out, indent=2))
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: quantum/mitigation_autopilot.py
+#!/usr/bin/env python3
+"""
+Adaptive mitigation autopilot prototype.
+
+- Reads mitigation suggestions produced by experiments (e.g. mitigation_suggestions.json)
+- Applies adaptive decision rules and writes a ConfigMap (aegis-mitigation-config) via kubectl
+- Rules:
+  - If expected fidelity improvement > threshold and recent regression risk low -> enable mitigation
+  - Otherwise keep mitigation off or in recommended mode
+"""
+import json
+import os
+import subprocess
+from datetime import datetime
+
+SUG_FILE = "/tmp/mitigation_suggestions.json"
+THRESHOLD_IMPROVEMENT = float(os.environ.get("MITIGATION_MIN_IMPROVEMENT", "0.02"))  # 2% default
+
+
+def load_suggestions(path=SUG_FILE):
+    if not os.path.exists(path):
+        return []
+    return json.load(open(path))
+
+
+def decide_apply(suggestion):
+    # suggestion example: {"device": "device-a", "suggestion": {"apply_readout": True}, "expected_gain": 0.03}
+    gain = suggestion.get("expected_gain", 0.0)
+    if gain >= THRESHOLD_IMPROVEMENT and suggestion.get("risk", 0.001) < 0.05:
+        return True
+    return False
+
+
+def build_configmap(mapping):
+    cm = {
+        "apiVersion": "v1",
+        "kind": "ConfigMap",
+        "metadata": {"name": "aegis-mitigation-config", "namespace": "aegis"},
+        "data": {"mitigation": json.dumps(mapping)}
+    }
+    return cm
+
+
+def apply_configmap(cm):
+    p = subprocess.run(["kubectl", "apply", "-f", "-"], input=json.dumps(cm).encode())
+    return p.returncode == 0
+
+
+def main():
+    suggestions = load_suggestions()
+    mapping = {}
+    for s in suggestions:
+        dev = s.get("device")
+        if decide_apply(s):
+            mapping[dev] = s.get("suggestion", {})
+        else:
+            mapping[dev] = {"apply": False}
+    cm = build_configmap(mapping)
+    ok = apply_configmap(cm)
+    print(f"{datetime.utcnow().isoformat()} applied={ok} mapping_count={len(mapping)}")
+    if not ok:
+        print("kubectl apply failed; ensure KUBECONFIG and permissions are set.")
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: aiops/tracing_correlation.py
+#!/usr/bin/env python3
+"""
+AIOps tracing correlation prototype.
+
+- Fetches traces from a Jaeger/OTEL collector HTTP API for a time window
+- Correlates traces by job_id tag (expected in spans)
+- Produces a short report linking planner -> adapter -> provider spans and latencies
+
+Usage:
+  python aiops/tracing_correlation.py --jaeger http://jaeger-query:16686 --job-id <job-id>
+"""
+import argparse
+import json
+import os
+import requests
+from datetime import datetime, timedelta
+
+def fetch_traces(jaeger_url, service=None, lookback_minutes=10):
+    # Query Jaeger search API for traces in window
+    end = datetime.utcnow()
+    start = end - timedelta(minutes=lookback_minutes)
+    params = {"start": int(start.timestamp() * 1000), "end": int(end.timestamp() * 1000), "limit": 50}
+    if service:
+        params["service"] = service
+    url = jaeger_url.rstrip("/") + "/api/traces"
+    r = requests.get(url, params=params, timeout=30)
+    r.raise_for_status()
+    return r.json().get("data", [])
+
+def correlate_by_job(traces, job_id):
+    correlated = []
+    for t in traces:
+        for span in t.get("spans", []):
+            tags = {x["key"]: x["value"] for x in span.get("tags", [])}
+            if tags.get("job_id") == job_id:
+                correlated.append({"traceID": t.get("traceID"), "span": span, "tags": tags})
+    return correlated
+
+def summarize(correlated):
+    # produce simple ordering by startTime
+    items = sorted(correlated, key=lambda x: x["span"].get("startTime", 0))
+    out = []
+    for it in items:
+        sp = it["span"]
+        tags = it["tags"]
+        out.append({
+            "traceID": it["traceID"],
+            "operation": sp.get("operationName"),
+            "service": tags.get("service.name", tags.get("service")),
+            "start": sp.get("startTime"),
+            "duration_ms": sp.get("duration") / 1000.0 if sp.get("duration") else None,
+            "tags": tags
+        })
+    return out
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--jaeger", required=True)
+    p.add_argument("--job-id", required=True)
+    p.add_argument("--lookback-minutes", type=int, default=15)
+    args = p.parse_args()
+
+    traces = fetch_traces(args.jaeger, lookback_minutes=args.lookback_minutes)
+    correlated = correlate_by_job(traces, args.job_id)
+    summary = summarize(correlated)
+    print(json.dumps({"job_id": args.job_id, "count": len(summary), "summary": summary}, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: autoscale/slo_autoscaler.py
+#!/usr/bin/env python3
+"""
+SLO-driven autoscaler prototype.
+
+- Queries Prometheus for an SLO metric (e.g., request p95) and scales a k8s deployment up/down
+- Intended to be triggered periodically (CronJob) or run as a sidecar operator.
+"""
+import argparse
+import os
+import subprocess
+import requests
+import math
+from datetime import datetime
+
+PROM_URL = os.environ.get("PROM_URL", "http://prometheus-operated.monitoring.svc:9090")
+
+
+def query_prometheus(query):
+    resp = requests.get(f"{PROM_URL}/api/v1/query", params={"query": query}, timeout=10)
+    resp.raise_for_status()
+    data = resp.json()
+    if data["status"] != "success":
+        raise RuntimeError("Prometheus query failed")
+    return data["data"]["result"]
+
+
+def current_replicas(namespace, deployment):
+    out = subprocess.check_output(["kubectl", "get", "deployment", deployment, "-n", namespace, "-o", "json"])
+    j = json.loads(out)
+    return j["spec"]["replicas"]
+
+
+def scale_deployment(namespace, deployment, replicas):
+    replicas = int(max(1, replicas))
+    subprocess.run(["kubectl", "scale", "deployment", deployment, f"--replicas={replicas}", "-n", namespace], check=False)
+    print(f"{datetime.utcnow().isoformat()} scaled {deployment} to {replicas} replicas in {namespace}")
+
+
+def decide_scale(p95_value_ms, slo_target_ms, current_replicas):
+    # simple proportional controller: if p95 > target, increase; if well below, decrease
+    if p95_value_ms is None:
+        return current_replicas
+    ratio = p95_value_ms / max(1, slo_target_ms)
+    if ratio > 1.2:
+        return math.ceil(current_replicas * 1.5)
+    if ratio > 1.0:
+        return math.ceil(current_replicas * 1.2)
+    if ratio < 0.6:
+        return max(1, math.floor(current_replicas * 0.75))
+    return current_replicas
+
+
+def main():
+    import json, sys
+    p = argparse.ArgumentParser()
+    p.add_argument("--namespace", default="aegis")
+    p.add_argument("--deployment", required=True)
+    p.add_argument("--slo-p95-ms", type=int, required=True)
+    p.add_argument("--prom-query", default='histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))')
+    args = p.parse_args()
+
+    # query prometheus
+    try:
+        res = query_prometheus(args.prom_query)
+        # pick first result value
+        if res and len(res) > 0:
+            p95_value = float(res[0]["value"][1])
+        else:
+            p95_value = None
+    except Exception as e:
+        print("Prometheus query failed:", e)
+        p95_value = None
+
+    # current replicas
+    cur = 1
+    try:
+        out = subprocess.check_output(["kubectl", "get", "deployment", args.deployment, "-n", args.namespace, "-o", "json"], stderr=subprocess.DEVNULL)
+        cur = int(json.loads(out)["spec"]["replicas"])
+    except Exception:
+        cur = 1
+
+    new = decide_scale(p95_value, args.slo_p95_ms, cur)
+    if new != cur:
+        scale_deployment(args.namespace, args.deployment, new)
+    else:
+        print(f"No scaling action required (current {cur}, p95={p95_value})")
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: provider/failover_manager.py
+#!/usr/bin/env python3
+"""
+Active-active provider failover manager prototype.
+
+- Monitors provider health (via HTTP healthcheck or metrics)
+- Maintains a simple penalty score and blacklist
+- Writes routing preferences to a ConfigMap for scheduler/adapters to consume
+"""
+import argparse
+import json
+import os
+import subprocess
+import requests
+from datetime import datetime
+
+CONFIGMAP_NAME = os.environ.get("PROVIDER_PREF_CM", "aegis-provider-preferences")
+NAMESPACE = os.environ.get("NAMESPACE", "aegis")
+
+def health_check(url, timeout=5):
+    try:
+        r = requests.get(url, timeout=timeout)
+        return r.status_code == 200
+    except Exception:
+        return False
+
+def write_configmap(mapping):
+    cm = {
+        "apiVersion": "v1",
+        "kind": "ConfigMap",
+        "metadata": {"name": CONFIGMAP_NAME, "namespace": NAMESPACE},
+        "data": {"preferences": json.dumps(mapping)}
+    }
+    p = subprocess.run(["kubectl", "apply", "-f", "-"], input=json.dumps(cm).encode())
+    return p.returncode == 0
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--providers-file", default="/etc/aegis/providers_health.json")
+    args = p.parse_args()
+
+    providers = {}
+    if os.path.exists(args.providers_file):
+        providers = json.load(open(args.providers_file))
+    else:
+        providers = {
+            "braket": {"health_url": "https://braket.example/health"},
+            "ibm": {"health_url": "https://ibm.example/health"},
+            "quantinuum": {"health_url": "https://quantinuum.example/health"}
+        }
+
+    mapping = {}
+    for name, meta in providers.items():
+        ok = health_check(meta["health_url"])
+        mapping[name] = {"healthy": ok, "last_checked": datetime.utcnow().isoformat()}
+        if not ok:
+            mapping[name]["penalty"] = meta.get("penalty", 10) + 1
+        else:
+            mapping[name]["penalty"] = max(0, meta.get("penalty", 0) - 1)
+
+    success = write_configmap(mapping)
+    print("Wrote provider preferences ConfigMap:", success, "details:", mapping)
+
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: sbom/sbom_generator.py
+#!/usr/bin/env python3
+"""
+SBOM generator prototype.
+
+- Attempts to use `syft` (if installed) to generate an SBOM for a container image or local path.
+- Falls back to pip freeze + file listing for Python projects.
+- Writes sbom.json to /tmp/sbom_<name>.json
+"""
+import argparse
+import json
+import os
+import shutil
+import subprocess
+from datetime import datetime
+
+OUT_DIR = "/tmp"
+
+def run_syft(target, outpath):
+    try:
+        subprocess.run(["syft", target, "-o", "json", "-q", "-f", outpath], check=True)
+        return True
+    except Exception:
+        return False
+
+def fallback_sbom(project_path, outpath):
+    sbom = {"generated": datetime.utcnow().isoformat(), "type": "fallback", "components": []}
+    # pip freeze if Python
+    try:
+        pip = subprocess.check_output(["pip", "freeze"]).decode().splitlines()
+        for p in pip:
+            sbom["components"].append({"pypi": p})
+    except Exception:
+        pass
+    # list files
+    files = []
+    for root, dirs, fnames in os.walk(project_path):
+        for f in fnames:
+            files.append(os.path.relpath(os.path.join(root, f), project_path))
+    sbom["files"] = files[:1000]
+    with open(outpath, "w") as fh:
+        json.dump(sbom, fh, indent=2)
+    return True
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--target", required=True, help="image:tag or local path")
+    p.add_argument("--out", default=None)
+    args = p.parse_args()
+    out = args.out or os.path.join(OUT_DIR, f"sbom_{os.path.basename(args.target).replace('/', '_')}.json")
+    if shutil.which("syft"):
+        ok = run_syft(args.target, out)
+        if ok:
+            print("SBOM written to", out)
+            return
+    print("syft not available or failed; using fallback")
+    fallback_sbom(args.target if os.path.isdir(args.target) else ".", out)
+    print("Fallback SBOM written to", out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: sbom/notebook_provenance.py
+#!/usr/bin/env python3
+"""
+Notebook provenance capture.
+
+- Extracts notebook metadata (kernelspec, packages), cell counts, and lightweight execution footprints
+- Writes a provenance artifact (JSON) suitable for archiving or SBOM linkage
+"""
+import argparse
+import json
+import os
+import nbformat
+import subprocess
+from datetime import datetime
+
+def extract_notebook(path):
+    nb = nbformat.read(path, as_version=4)
+    meta = {
+        "path": path,
+        "kernelspec": nb.metadata.get("kernelspec"),
+        "language_info": nb.metadata.get("language_info"),
+        "cell_count": len(nb.cells),
+        "code_cells": sum(1 for c in nb.cells if c.cell_type == "code"),
+        "markdown_cells": sum(1 for c in nb.cells if c.cell_type == "markdown"),
+        "executed_cells": sum(1 for c in nb.cells if c.get("execution_count") is not None)
+    }
+    return meta
+
+def env_packages():
+    try:
+        out = subprocess.check_output(["pip", "freeze"]).decode().splitlines()
+        return out
+    except Exception:
+        return []
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--notebook", required=True)
+    p.add_argument("--out", default="/tmp/notebook_provenance.json")
+    args = p.parse_args()
+
+    meta = extract_notebook(args.notebook)
+    pkgs = env_packages()
+    prov = {"generated": datetime.utcnow().isoformat(), "notebook": meta, "packages": pkgs}
+    with open(args.out, "w") as fh:
+        json.dump(prov, fh, indent=2)
+    print("Wrote notebook provenance to", args.out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/supply_chain_sbom.yml
+name: SBOM & Supply-Chain Checks
+
+on:
+  push:
+    paths:
+      - 'models/**'
+      - 'notebooks/**'
+      - 'sbom/**'
+  workflow_dispatch:
+
+jobs:
+  generate-sbom:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.9'
+      - name: Install deps
+        run: |
+          pip install nbformat mlflow
+      - name: Generate SBOM for model artifacts (placeholder)
+        run: |
+          python sbom/sbom_generator.py --target models/ --out /tmp/sbom_models.json || true
+      - name: Notebook provenance
+        run: |
+          for nb in notebooks/*.ipynb; do
+            python sbom/notebook_provenance.py --notebook "$nb" --out "/tmp/prov_$(basename $nb .ipynb).json" || true
+          done
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: sbom-artifacts
+          path: /tmp/*.json
+
+  supply-chain-check:
+    needs: generate-sbom
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Fetch artifacts
+        uses: actions/download-artifact@v4
+        with:
+          name: sbom-artifacts
+      - name: Basic supply-chain checks
+        run: |
+          echo "Running basic checks: SBOM present and no blacklisted packages (prototype)"
+          python - <<'PY'
+import glob,json
+blacklist=['evil-package']
+for f in glob.glob('/tmp/*.json'):
+    j=json.load(open(f))
+    s=str(j)
+    for b in blacklist:
+        if b in s:
+            print("Found blacklisted package",b,"in",f); exit(2)
+print("Supply-chain checks passed (prototype)")
+PY
+      - name: Sign SBOM artifacts (optional cosign)
+        if: ${{ secrets.COSIGN_KEY_ID != '' }}
+        env:
+          COSIGN_KEY: ${{ secrets.COSIGN_KEY_ID }}
+        run: |
+          echo "Cosign signing not implemented in this prototype - add cosign call here"
+
*** End Patch
*** Begin Patch
*** Add File: docs/provider_broker.md
+```markdown
+# Provider Broker (prototype)
+
+Files:
+- provider/broker.py
+
+Description:
+- A small decision service that scores candidate providers using cost, latency and carbon factors.
+- Intended to be invoked by scheduler to choose provider for jobs (generative or quantum).
+
+Integration:
+- Mount config as ConfigMap to /etc/aegis/provider_broker_config.json
+- Call broker with a candidate list; parse JSON output and route job accordingly.
+
+Notes:
+- Replace heuristic scoring with a learned model (use historical metrics from Prometheus/MLflow).
+```
+
*** End Patch
*** Begin Patch
*** Add File: docs/mitigation_autopilot.md
+```markdown
+# Mitigation Autopilot
+
+Files:
+- quantum/mitigation_autopilot.py
+
+Description:
+- Reads mitigation suggestions produced by experiment pipeline.
+- Applies decision rules (expected_gain threshold + risk) and writes a ConfigMap that adapters consume to enable/disable per-device mitigation.
+
+Operational notes:
+- Ensure mitigation suggestions are written to /tmp/mitigation_suggestions.json by your experiment runner.
+- This prototype uses kubectl; service account must have permissions to apply ConfigMaps.
+```
+
*** End Patch
*** Begin Patch
*** Add File: docs/aiops_tracing.md
+```markdown
+# AIOps Tracing Correlation
+
+Files:
+- aiops/tracing_correlation.py
+
+Description:
+- Fetch traces from Jaeger/OTEL HTTP API and correlate spans by job_id.
+- Produces a JSON summary that helps triage planner->adapter->provider latency/failed spans.
+
+Usage:
+- Provide Jaeger query endpoint and job_id:
+  python aiops/tracing_correlation.py --jaeger http://jaeger-query:16686 --job-id <job_id>
+
+Notes:
+- Extend to pull logs from Loki and metrics from Prometheus to build richer incident reports.
+```
+
*** End Patch
*** Begin Patch
*** Add File: docs/sbom_pipeline.md
+```markdown
+# SBOM & Notebook Provenance Pipeline
+
+Files:
+- sbom/sbom_generator.py
+- sbom/notebook_provenance.py
+- .github/workflows/supply_chain_sbom.yml
+
+Description:
+- Generates SBOMs (uses syft if available) and collects notebook provenance metadata.
+- CI workflow collects artifacts and runs basic supply-chain checks.
+
+Operational notes:
+- Replace prototype blacklist logic with vulnerability scanning and policy gates.
+- Integrate cosign signing of SBOM artifacts and Rekor attestation for auditable supply-chain evidence.
+```
+
*** End Patch
*** End Patch
