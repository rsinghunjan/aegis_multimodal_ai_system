*** Begin Patch
*** Add File:optimize/README_COMPLETE_OPTIMIZE.md
+This patch contains a broad, integrated set of artifacts to move Aegis from scaffolding to a production-ready data science & analytics platform while preserving Aegis core guarantees: cross-cloud model lifecycle, cost discipline, auditability (signed evidence), and governance.
+
+High-level contents:
+- Telemetry collection (DaemonSet example) + healthcheck & summary scripts
+- Iterative calibration + training jobs + CI gating workflows and validators
+- Enforcement: scheduler extender, Argo admission enforcer, test harness, RBAC
+- ETL: s3_parquet_pipeline_v3, manifest generator, idempotent bulk-loader wrappers, Airflow DAGs (idempotent, alerts)
+- dbt: project, staging/mart models, evidence propagation macros and tests
+- MLflow helpers to log experiments with evidence & cosign signing helpers
+- DataHub ingestion recipe and handbook for lineage integration
+- Superset dashboards (finance & carbon) and SSO/RLS snippets
+- Security: cert-manager corporate ClusterIssuer, cosign rotation tool, KMS TF skeleton, RBAC audit scripts
+- Monitoring: Prometheus rules, pushgateway/metrics reporters
+- Runbooks & incident playbooks
+- Terraform IAM for Redshift COPY access (example)
+- GitHub Actions/CI workflows for calibration, gating and promotion
+
+Usage notes:
+- Replace placeholders (bucket names, ARNs, endpoints, secrets) with your environment values.
+- Deploy to staging first and validate metrics (median rel error, enforcement coverage, ETL lag).
+- Promotions to production are manual and require evidence review & cosign signing.
+
*** End Patch
*** Begin Patch
*** Add File:telemetry/daemonset/node_power_exporter_ds.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: node-power-exporter
+  namespace: observability
+spec:
+  selector:
+    matchLabels:
+      app: node-power-exporter
+  template:
+    metadata:
+      labels:
+        app: node-power-exporter
+    spec:
+      serviceAccountName: node-power-sa
+      containers:
+        - name: node-power-exporter
+          image: ghcr.io/yourorg/node-power-exporter:latest
+          ports:
+            - containerPort: 9100
+          volumeMounts:
+            - name: host-sys
+              mountPath: /host/sys
+      volumes:
+        - name: host-sys
+          hostPath:
+            path: /sys
+
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: node-power-sa
+  namespace: observability
+
*** End Patch
*** Begin Patch
*** Add File:telemetry/telemetry_healthcheck.py
+#!/usr/bin/env python3
+"""
+Telemetry healthcheck and reporter
+ - Scans S3 telemetry prefix for recent files and writes a health JSON summary
+ - Uploads to s3://<EVIDENCE_BUCKET>/telemetry/health/
+"""
+import os, sys, json, argparse, boto3, datetime
+from collections import defaultdict
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+def list_recent_keys(s3, bucket, prefix, start_ts):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            if obj["LastModified"] >= start_ts:
+                yield obj["Key"]
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--prefix", default="telemetry/")
+    p.add_argument("--days", type=int, default=7)
+    args = p.parse_args()
+    if not EVIDENCE_BUCKET:
+        print("EVIDENCE_BUCKET required", file=sys.stderr)
+        sys.exit(2)
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    start_ts = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=args.days)
+    counts = defaultdict(int)
+    total = 0
+    for k in list_recent_keys(s3, EVIDENCE_BUCKET, args.prefix, start_ts):
+        total += 1
+        parts = k.split("/")
+        node_type = parts[1] if len(parts) > 1 else "unknown"
+        counts[node_type] += 1
+    report = {"window_days": args.days, "total_files": total, "counts_by_node": counts, "generated_at": datetime.datetime.utcnow().isoformat()+"Z"}
+    out_key = f"{args.prefix}health/telemetry_health_{int(datetime.datetime.utcnow().timestamp())}.json"
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=out_key, Body=json.dumps(report).encode("utf-8"))
+    print("Wrote", out_key)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:calibration/aegis_iterative_calibration.py
+#!/usr/bin/env python3
+"""
+Orchestrates iterative calibration:
+ - Computes median relative error from reconciliations in S3.
+ - If exceeds threshold, triggers calibration job and training job in the cluster.
+ - Uploads validation reports and draft profile suggestions to S3.
+ - Does not auto-apply; operator must approve promotion.
+"""
+import os, time, json, statistics, subprocess, boto3
+from datetime import datetime
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+THRESH = float(os.environ.get("MEDIAN_THRESHOLD", "0.10"))
+S3_PREFIX = os.environ.get("RECON_PREFIX", "reconciliations/")
+
+def list_recons(s3):
+    paginator = s3.get_paginator("list_objects_v2")
+    for p in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=S3_PREFIX):
+        for o in p.get("Contents", []):
+            if o["Key"].endswith(".json"):
+                yield o["Key"]
+
+def median_error(s3):
+    vals = []
+    for k in list_recons(s3):
+        try:
+            j = s3.get_object(Bucket=EVIDENCE_BUCKET, Key=k)["Body"].read().decode()
+            obj = json.loads(j)
+            est = obj.get("estimate", {}).get("estimated_emissions_kg")
+            meas = obj.get("measured", {}).get("measured_emissions_kg")
+            if not est or not meas:
+                continue
+            vals.append(abs(meas - est) / float(meas))
+        except Exception:
+            continue
+    return statistics.median(vals) if vals else None, len(vals)
+
+def upload_report(s3, report, key_prefix="calibration/validation_reports/"):
+    key = f"{key_prefix}validation_{int(time.time())}.json"
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(report).encode())
+    return key
+
+def trigger_k8s_job(name):
+    # assumes kubectl context points to staging cluster where CronJob exists
+    subprocess.run(["kubectl","-n","aegis","create","job","--from=cronjob/aegis-iterative-calibration", f"{name}"], check=False)
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET required")
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    med, count = median_error(s3)
+    report = {"median_relative_error": med, "sample_count": count, "ts": datetime.utcnow().isoformat()+"Z"}
+    if med is None:
+        report["status"] = "no_data"
+    elif med > THRESH:
+        report["status"] = "above_threshold"
+        trigger_k8s_job("auto-calib-"+str(int(time.time())))
+        report["calibration_triggered"] = True
+    else:
+        report["status"] = "within_threshold"
+        report["calibration_triggered"] = False
+    key = upload_report(s3, report)
+    print("Uploaded report:", key)
+    if med and med > THRESH:
+        raise SystemExit(3)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:ci/telemetry_calibration_validate.yml
+name: Telemetry → Iterative Calibration → Validate
+
+on:
+  workflow_dispatch:
+  schedule:
+    - cron: '0 6 * * 1' # weekly
+
+env:
+  EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+  AWS_REGION: ${{ secrets.AWS_REGION }}
+  KUBECONFIG: ${{ secrets.KUBECONFIG }}
+  MEDIAN_THRESHOLD: "0.10"
+
+jobs:
+  telemetry-calib-validate:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: pip install boto3 kubernetes requests
+      - name: Configure kubeconfig
+        run: |
+          echo "${{ secrets.KUBECONFIG }}" > /tmp/kubeconfig
+          mkdir -p ~/.kube
+          cp /tmp/kubeconfig ~/.kube/config
+      - name: Run telemetry healthcheck
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+        run: python3 telemetry/telemetry_healthcheck.py --prefix telemetry/ --days 7
+      - name: Run iterative calibration & validation
+        env:
+          EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+          AWS_REGION: ${{ secrets.AWS_REGION }}
+          MEDIAN_THRESHOLD: ${{ env.MEDIAN_THRESHOLD }}
+        run: python3 calibration/aegis_iterative_calibration.py
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/extender/aegis_scheduler_extender.py
+#!/usr/bin/env python3
+"""
+Kube-scheduler extender scaffold with basic filtering/prioritization and metrics endpoint.
+"""
+from flask import Flask, request, jsonify, Response
+import os, json
+
+app = Flask("aegis-scheduler-extender")
+PREFERRED_LABEL = os.environ.get("PREFERRED_LABEL", "carbon_preference")
+PREFERRED_VALUE = os.environ.get("PREFERRED_VALUE", "low")
+
+@app.route("/filter", methods=["POST"])
+def filter_nodes():
+    body = request.get_json(force=True)
+    pod = body.get("pod", {})
+    nodes = body.get("nodes", {}).get("items", [])
+    ann = pod.get("metadata", {}).get("annotations", {}) or {}
+    hint = ann.get("aegis.carbon.hint")
+    if not hint:
+        return jsonify({"nodes": {"items": nodes}})
+    try:
+        hint_obj = json.loads(hint)
+    except Exception:
+        hint_obj = {}
+    selector = {}
+    if hint_obj.get("prefer_spot"):
+        selector["lifecycle"] = "spot"
+    filtered = []
+    for n in nodes:
+        labels = n.get("labels", {})
+        ok = True
+        for k,v in selector.items():
+            if labels.get(k) != v:
+                ok = False
+                break
+        if ok:
+            filtered.append(n)
+    return jsonify({"nodes": {"items": filtered}})
+
+@app.route("/prioritize", methods=["POST"])
+def prioritize():
+    body = request.get_json(force=True)
+    nodes = body.get("nodes", {}).get("items", [])
+    hostPriorityList = []
+    for n in nodes:
+        score = 0
+        labels = n.get("labels", {})
+        if labels.get(PREFERRED_LABEL) == PREFERRED_VALUE:
+            score += 100
+        if labels.get("lifecycle") == "spot":
+            score += 50
+        hostPriorityList.append({"host": n.get("name"), "score": score})
+    return jsonify({"hostPriorityList": hostPriorityList})
+
+@app.route("/healthz")
+def healthz():
+    return "ok"
+
+@app.route("/metrics")
+def metrics():
+    # minimal static metrics; integrate prometheus_client in production
+    lines = [
+        "# HELP aegis_extender_up 1=up",
+        "# TYPE aegis_extender_up gauge",
+        "aegis_extender_up 1"
+    ]
+    return Response("\n".join(lines), mimetype="text/plain")
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8000")))
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/argo_admission/enforcer_admission.py
+#!/usr/bin/env python3
+"""
+Admission webhook for Argo Workflows: enforces or mutates workflows at creation time based on Aegis policies.
+Uses TLS certs mounted into /tls (cert-manager manages them).
+"""
+from flask import Flask, request, jsonify
+import os, json, base64, traceback
+
+app = Flask("aegis-argo-enforcer")
+
+def build_patch_annotations(obj, new_ann):
+    patch = []
+    meta = obj.get("metadata", {})
+    curr_ann = meta.get("annotations") or {}
+    # merge
+    for k,v in new_ann.items():
+        if k in curr_ann:
+            # replace
+            path = "/metadata/annotations/" + k.replace("/", "~1")
+            patch.append({"op":"replace","path":path,"value":v})
+        else:
+            if not curr_ann:
+                patch.append({"op":"add","path":"/metadata/annotations","value":{k:v}})
+            else:
+                path = "/metadata/annotations/" + k.replace("/", "~1")
+                patch.append({"op":"add","path":path,"value":v})
+    return patch
+
+@app.route("/mutate", methods=["POST"])
+def mutate():
+    req = request.get_json(force=True)
+    try:
+        obj = req["request"]["object"]
+        annotations = obj.get("metadata", {}).get("annotations", {}) or {}
+        team = annotations.get("aegis.team", "unknown")
+        est = float(annotations.get("aegis.carbon.estimated_kg", "0"))
+        hint = json.loads(annotations.get("aegis.carbon.hint","{}"))
+        # Policy check: simple token-budget call or redis lookup would be here; for scaffold allow best-effort
+        mode = os.environ.get("POLICY_MODE", "best-effort")
+        allowed = True
+        if mode == "low-carbon-only" and est > float(os.environ.get("CARBON_LIMIT_KG","100")):
+            allowed = False
+        # mutate: inject nodeSelector for prefer_spot
+        patch = None
+        if hint.get("prefer_spot"):
+            patch = build_patch_annotations(obj, {"aegis.nodeSelector": json.dumps({"lifecycle":"spot"})})
+        resp = {"uid": req["request"]["uid"], "allowed": allowed}
+        if patch:
+            resp.update({"patch": base64.b64encode(json.dumps(patch).encode()).decode(), "patchType":"JSONPatch"})
+        return jsonify({"response": resp})
+    except Exception:
+        traceback.print_exc()
+        return jsonify({"response":{"uid": req.get("request",{}).get("uid",""), "allowed": False, "status":{"message":"internal error"}}})
+
+@app.route("/healthz")
+def healthz():
+    return "ok"
+
+if __name__ == "__main__":
+    import ssl
+    crt = os.environ.get("TLS_CRT","/tls/tls.crt")
+    key = os.environ.get("TLS_KEY","/tls/tls.key")
+    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
+    context.load_cert_chain(crt, key)
+    app.run(host="0.0.0.0", port=9443, ssl_context=context)
+
*** End Patch
*** Begin Patch
*** Add File:enforcement/test_harness/enforcement_coverage_harness.py
+#!/usr/bin/env python3
+"""
+Submits synthetic workloads and reports enforcement coverage.
+Uploads coverage report to S3 for audit.
+"""
+import os, time, json, boto3, requests
+from kubernetes import client, config
+
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+PUSHGATEWAY = os.environ.get("PUSHGATEWAY")
+NAMESPACE = os.environ.get("TEST_NAMESPACE", "aegis-test")
+COUNT = int(os.environ.get("TEST_COUNT", "50"))
+
+def submit(count=10):
+    config.load_kube_config()
+    crd = client.CustomObjectsApi()
+    wf = {
+        "apiVersion": "argoproj.io/v1alpha1",
+        "kind": "Workflow",
+        "metadata": {"generateName": "enf-test-"},
+        "spec": {
+            "entrypoint": "whalesay",
+            "templates": [{"name":"whalesay","container":{"image":"docker/whalesay","command":["cowsay","hello"]}}]
+        }
+    }
+    for i in range(count):
+        w = wf.copy()
+        ann = {"aegis.carbon.hint": json.dumps({"action":"run","prefer_spot": i%2==0}), "aegis.team":"test-team"}
+        w["metadata"]["annotations"] = ann
+        try:
+            crd.create_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=NAMESPACE, plural="workflows", body=w)
+        except Exception:
+            pass
+        time.sleep(0.2)
+
+def fetch_pushgateway():
+    try:
+        r = requests.get(f"http://{PUSHGATEWAY}/metrics", timeout=5)
+        txt = r.text
+        def get(name):
+            for line in txt.splitlines():
+                if line.startswith(name+" "):
+                    return float(line.split(" ",1)[1])
+            return 0.0
+        return {"triggered": get("aegis_policy_triggered_total"), "enforced": get("aegis_enforcer_enforced_total"), "failures": get("aegis_enforcer_failures_total")}
+    except Exception:
+        return {}
+
+def upload_report(s3, report):
+    key = f"enforcement/coverage_reports/coverage_{int(time.time())}.json"
+    s3.put_object(Bucket=EVIDENCE_BUCKET, Key=key, Body=json.dumps(report).encode())
+    return key
+
+def main():
+    if not EVIDENCE_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET required")
+    submit(COUNT)
+    time.sleep(30)
+    metrics = fetch_pushgateway()
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    report = {"metrics": metrics, "ts": time.time()}
+    key = upload_report(s3, report)
+    print("Uploaded", key)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:etl/s3_parquet_pipeline_v3.py
+#!/usr/bin/env python3
+"""
+Parquet pipeline v3 (idempotent)
+ - Converts JSON evidence to partitioned Parquet per start_dt
+ - Avoids re-uploading already uploaded files
+ - Writes an atomic checkpoint manifest to S3
+"""
+import os, json, tempfile, boto3, pyarrow as pa, pyarrow.parquet as pq
+import pandas as pd
+from datetime import datetime, timezone
+
+S3_BUCKET = os.environ.get("EVIDENCE_BUCKET", "aegis-evidence")
+PREFIX = os.environ.get("EVIDENCE_PREFIX", "evidence/")
+OUT_PREFIX = os.environ.get("PARQUET_OUT_PREFIX", "parquet/evidence")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+CHECKPOINT_KEY = os.environ.get("PARQUET_CHECKPOINT_KEY", f"{PREFIX}.parquet_checkpoint.json")
+BATCH_SIZE = int(os.environ.get("PARQUET_BATCH_SIZE", "2000"))
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def read_checkpoint():
+    try:
+        obj = s3.get_object(Bucket=S3_BUCKET, Key=CHECKPOINT_KEY)
+        return json.loads(obj["Body"].read().decode())
+    except Exception:
+        return {"last_processed": None, "uploaded": []}
+
+def atomic_write_checkpoint(tmp_key, final_key, payload):
+    s3.put_object(Bucket=S3_BUCKET, Key=tmp_key, Body=payload)
+    s3.copy_object(Bucket=S3_BUCKET, CopySource={'Bucket': S3_BUCKET, 'Key': tmp_key}, Key=final_key)
+    s3.delete_object(Bucket=S3_BUCKET, Key=tmp_key)
+
+def list_new_json_keys(marker=None):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=PREFIX):
+        for obj in page.get("Contents", []):
+            key = obj["Key"]
+            if marker and key <= marker:
+                continue
+            if key.endswith(".json"):
+                yield key
+
+def convert_batch(keys):
+    rows = []
+    for k in keys:
+        try:
+            body = s3.get_object(Bucket=S3_BUCKET, Key=k)["Body"].read().decode()
+            j = json.loads(body)
+            rows.append({
+                "s3_key": k,
+                "job_id": j.get("job_id"),
+                "team": j.get("team"),
+                "start_ts": j.get("start_ts"),
+                "end_ts": j.get("end_ts"),
+                "estimated_kg": j.get("estimated_kg"),
+                "model_name": j.get("model_name")
+            })
+        except Exception:
+            continue
+    if not rows:
+        return []
+    df = pd.DataFrame(rows)
+    df["start_dt"] = pd.to_datetime(df["start_ts"]).dt.date.astype(str)
+    tmpdir = tempfile.mkdtemp()
+    uploaded = []
+    for part, sub in df.groupby("start_dt"):
+        out_file = f"{tmpdir}/batch_{part}_{int(datetime.now(timezone.utc).timestamp())}.parquet"
+        table = pa.Table.from_pandas(sub.drop(columns=["start_dt"]))
+        pq.write_table(table, out_file, compression="snappy")
+        s3_key = f"{OUT_PREFIX}/start_dt={part}/{os.path.basename(out_file)}"
+        # skip if exists
+        try:
+            s3.head_object(Bucket=S3_BUCKET, Key=s3_key)
+            uploaded.append(s3_key)
+            continue
+        except Exception:
+            pass
+        s3.upload_file(out_file, S3_BUCKET, s3_key)
+        uploaded.append(s3_key)
+    return uploaded
+
+def main():
+    ck = read_checkpoint()
+    last = ck.get("last_processed")
+    keys = list(list_new_json_keys(marker=last))
+    batch = []
+    manifest = []
+    for k in keys:
+        batch.append(k)
+        if len(batch) >= BATCH_SIZE:
+            uploaded = convert_batch(batch)
+            manifest.extend(uploaded)
+            last = batch[-1]
+            batch = []
+    if batch:
+        uploaded = convert_batch(batch)
+        manifest.extend(uploaded)
+        last = batch[-1] if batch else last
+    if manifest:
+        payload = json.dumps({"last_processed": last, "uploaded": manifest, "ts": datetime.now(timezone.utc).isoformat()}).encode()
+        atomic_write_checkpoint(f"{CHECKPOINT_KEY}.tmp", CHECKPOINT_KEY, payload)
+        manifest_key = f"{OUT_PREFIX}/manifest_{int(datetime.now().timestamp())}.json"
+        s3.put_object(Bucket=S3_BUCKET, Key=manifest_key, Body=json.dumps({"entries": manifest}).encode())
+        print("Wrote manifest", manifest_key)
+    else:
+        print("No new uploads")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:etl/copy_manifest_generator.py
+#!/usr/bin/env python3
+"""
+Generate a COPY manifest for Redshift from parquet output prefix.
+"""
+import os, boto3, json
+
+S3_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+OUT_PREFIX = os.environ.get("PARQUET_OUT_PREFIX", "parquet/evidence")
+AWS_REGION = os.environ.get("AWS_REGION", "us-west-2")
+
+s3 = boto3.client("s3", region_name=AWS_REGION)
+
+def list_parquet(bucket, prefix):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            if obj["Key"].endswith(".parquet"):
+                yield f"s3://{bucket}/{obj['Key']}"
+
+def main():
+    if not S3_BUCKET:
+        raise SystemExit("EVIDENCE_BUCKET required")
+    entries = [{"url": u, "mandatory": True} for u in list_parquet(S3_BUCKET, OUT_PREFIX)]
+    manifest = {"entries": entries}
+    out = "/tmp/copy_manifest.json"
+    with open(out, "w") as f:
+        json.dump(manifest, f, indent=2)
+    print("Wrote manifest to", out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:etl/redshift_loader_wrapper.sh
+#!/usr/bin/env bash
+set -euo pipefail
+MANIFEST=${MANIFEST_S3_URI:-s3://aegis-evidence/parquet/evidence/manifest.json}
+REDSHIFT_DSN=${REDSHIFT_DSN:-postgresql://user:pass@redshift-host:5439/dev}
+REDSHIFT_IAM_ROLE=${REDSHIFT_IAM_ROLE:-arn:aws:iam::123456789012:role/aegis-redshift-copy}
+MAX_RETRIES=${MAX_RETRIES:-3}
+
+SQL="COPY staging.raw_job_runs_stg FROM '${MANIFEST}' IAM_ROLE '${REDSHIFT_IAM_ROLE}' FORMAT AS PARQUET;"
+
+attempt=1
+while [ $attempt -le $MAX_RETRIES ]; do
+  echo "Attempt $attempt: COPY"
+  if psql "${REDSHIFT_DSN}" -c "${SQL}"; then
+    echo "COPY successful - running MERGE"
+    psql "${REDSHIFT_DSN}" -f /opt/etl/redshift_merge_from_staging.sql
+    exit 0
+  else
+    echo "COPY failed - retrying after sleep"
+    sleep $((attempt * 30))
+    attempt=$((attempt+1))
+  fi
+done
+echo "COPY failed after ${MAX_RETRIES} attempts" >&2
+exit 2
+
*** End Patch
*** Begin Patch
*** Add File:etl/redshift_merge_from_staging.sql
+-- Example idempotent merge/upsert pattern for Redshift
+BEGIN;
+CREATE TABLE IF NOT EXISTS raw.raw_job_runs (
+  job_id text primary key,
+  workflow_name text,
+  team text,
+  start_ts timestamptz,
+  end_ts timestamptz,
+  cpu real,
+  gpu int,
+  mem_mb int,
+  estimated_kg real,
+  evidence_s3_key text,
+  status text,
+  model_name text
+);
+
+-- Use staging table loaded by COPY into staging.raw_job_runs_stg
+DELETE FROM raw.raw_job_runs USING staging.raw_job_runs_stg WHERE raw.raw_job_runs.job_id = staging.raw_job_runs_stg.job_id;
+INSERT INTO raw.raw_job_runs SELECT * FROM staging.raw_job_runs_stg;
+END;
+
*** End Patch
*** Begin Patch
*** Add File:airflow/dags/aegis_idempotent_etl_dag.py
+from datetime import datetime, timedelta
+from airflow import DAG
+from airflow.operators.python import PythonOperator
+from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
+import subprocess, os, requests
+from operational.airflow_idempotent_helper import already_done_marker, mark_done
+
+DEFAULT_ARGS = {
+    'owner': 'aegis',
+    'depends_on_past': False,
+    'email_on_failure': False,
+    'retries': 2,
+    'retry_delay': timedelta(minutes=10),
+}
+
+WAREHOUSE_DSN = os.environ.get("WAREHOUSE_DSN", "postgresql://user:pass@host:5432/aegis_dw")
+SLACK_WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL")
+
+def notify_slack(msg):
+    if SLACK_WEBHOOK:
+        try:
+            requests.post(SLACK_WEBHOOK, json={"text": msg}, timeout=5)
+        except Exception:
+            pass
+
+def run_parquet():
+    job_key = "parquet_pipeline_" + datetime.utcnow().strftime("%Y%m%d")
+    if already_done_marker(WAREHOUSE_DSN, job_key):
+        print("Parquet already processed; skipping.")
+        return
+    rc = subprocess.call("python3 /opt/ingest/s3_parquet_pipeline_v3.py", shell=True)
+    if rc != 0:
+        notify_slack("Parquet pipeline failed")
+        raise RuntimeError("Parquet pipeline failed")
+    mark_done(WAREHOUSE_DSN, job_key)
+
+def gen_manifest():
+    subprocess.check_call("python3 /opt/ingest/copy_manifest_generator.py", shell=True)
+
+def bulk_load():
+    subprocess.check_call("/opt/etl/redshift_loader_wrapper.sh", shell=True)
+
+def ge_validate():
+    rc = subprocess.call("python3 /opt/data_quality/run_expectations.py", shell=True)
+    if rc != 0:
+        notify_slack("GE validations failed")
+        raise RuntimeError("GE failed")
+
+with DAG(
+    dag_id="aegis_idempotent_etl",
+    default_args=DEFAULT_ARGS,
+    schedule_interval="@daily",
+    start_date=datetime(2025,1,1),
+    catchup=False,
+    max_active_runs=1
+) as dag:
+
+    t_parquet = PythonOperator(task_id="parquet_pipeline", python_callable=run_parquet)
+    t_manifest = PythonOperator(task_id="generate_manifest", python_callable=gen_manifest)
+    t_bulk = KubernetesPodOperator(task_id="bulk_load", name="redshift-copy", namespace="aegis", image="ghcr.io/yourorg/aegis-redshift-loader:latest", cmds=["/bin/sh","-c"], arguments=["/opt/etl/redshift_loader_wrapper.sh"], get_logs=True, is_delete_operator_pod=True, in_cluster=True)
+    t_ge = PythonOperator(task_id="great_expectations", python_callable=ge_validate)
+
+    t_parquet >> t_manifest >> t_bulk >> t_ge
+
*** End Patch
*** Begin Patch
*** Add File:dbt/dbt_project.yml
+name: aegis_analytics
+version: '1.0'
+config-version: 2
+profile: aegis_dw_profile
+source-paths: ["models"]
+target-path: "target"
+clean-targets:
+  - "target"
+  - "dbt_modules"
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/schema.yml
+version: 2
+models:
+  - name: core_job_runs
+    description: "Staging/core job run events"
+    columns:
+      - name: job_id
+        tests:
+          - not_null
+          - unique
+      - name: evidence_s3_key
+        tests:
+          - not_null
+
+  - name: daily_team_rollup
+    description: "Daily per-team rollups"
+    columns:
+      - name: team
+        tests:
+          - not_null
+      - name: day
+        tests:
+          - not_null
+
+  - name: cost_allocation
+    description: "Team cost allocation"
+    columns:
+      - name: team
+        tests:
+          - not_null
+
+  - name: model_performance
+    description: "Model performance metrics"
+    columns:
+      - name: model_name
+        tests:
+          - not_null
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/staging/stg_core_job_runs.sql
+-- staging model for core_job_runs, ensures evidence_s3_key present
+select
+  job_id,
+  workflow_name,
+  team,
+  start_ts,
+  end_ts,
+  cpu,
+  gpu,
+  mem_mb,
+  estimated_kg,
+  measured_emissions_kg,
+  evidence_s3_key,
+  status,
+  model_name,
+  created_at
+from {{ source('raw','raw_job_runs') }}
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/daily_team_rollup.sql
+with core as (
+  select
+    job_id, coalesce(team,'unknown') as team, start_ts, estimated_kg, measured_emissions_kg, coalesce(allocated_cost_usd,0) as allocated_cost_usd
+  from {{ ref('core_job_runs') }}
+)
+select team, date_trunc('day', start_ts)::date as day,
+       count(*) as jobs,
+       sum(coalesce(estimated_kg,0)) as total_estimated_kg,
+       sum(coalesce(measured_emissions_kg,0)) as total_measured_kg,
+       sum(coalesce(allocated_cost_usd,0)) as allocated_cost_usd
+from core
+group by 1,2
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/cost_allocation.sql
+with costs as (
+  select resource_id, cost_usd, tags->>'team' as team from {{ source('raw','raw_costs') }}
+),
+usage as (
+  select team, sum(coalesce(cpu,0)+coalesce(gpu,0)*4) as compute_score from {{ ref('core_job_runs') }} group by team
+),
+alloc as (
+  select coalesce(team,'unallocated') as team, sum(cost_usd) as total_cost from costs group by 1
+)
+select a.team, a.total_cost as allocated_cost_usd, coalesce(u.compute_score,0) as compute_score from alloc a left join usage u on a.team = u.team
+
*** End Patch
*** Begin Patch
*** Add File:dbt/models/marts/model_performance.sql
+select model_name, date_trunc('day', start_ts)::date as day, count(*) as requests, avg(duration_s) as avg_duration_s, percentile_cont(0.95) within group (order by duration_s) as p95_duration_s, sum(case when status != 'success' then 1 else 0 end) as errors from {{ ref('core_job_runs') }} group by 1,2
+
*** End Patch
*** Begin Patch
*** Add File:dbt/macros/evidence_macro.sql
+{% macro ensure_evidence(col='evidence_s3_key') -%}
+  -- enforce evidence column exists or raise in CI (simple check)
+  coalesce({{ col }}, '') as {{ col }}
+{%- endmacro %}
+
*** End Patch
*** Begin Patch
*** Add File:mlflow/mlflow_evidence_logger.py
+#!/usr/bin/env python3
+import os, subprocess, mlflow, boto3
+
+class MLflowEvidenceLogger:
+    def __init__(self, tracking_uri=None, evidence_bucket=None, cosign_kms_arn=None):
+        if tracking_uri:
+            mlflow.set_tracking_uri(tracking_uri)
+        self.s3 = boto3.client("s3", region_name=os.environ.get("AWS_REGION","us-west-2"))
+        self.evidence_bucket = evidence_bucket or os.environ.get("EVIDENCE_BUCKET")
+        self.cosign_kms = cosign_kms_arn or os.environ.get("COSIGN_KMS_KEY_ARN")
+
+    def start_run(self, experiment_name=None, run_name=None):
+        if experiment_name:
+            mlflow.set_experiment(experiment_name)
+        return mlflow.start_run(run_name=run_name)
+
+    def log_params(self, params):
+        for k,v in params.items():
+            mlflow.log_param(k,str(v))
+
+    def log_and_sign_artifact(self, local_path, s3_path):
+        if s3_path.startswith("s3://"):
+            bucket = s3_path.split("s3://",1)[1].split("/",1)[0]
+            key = s3_path.split("s3://",1)[1].split("/",1)[1]
+            self.s3.upload_file(local_path, bucket, key)
+            mlflow.log_artifact(local_path)
+            if self.cosign_kms:
+                try:
+                    subprocess.run(["cosign","sign","--key",f"awskms://{self.cosign_kms}", local_path], check=True)
+                except Exception:
+                    print("cosign sign failed; record and continue")
+            return s3_path
+        raise ValueError("s3_path must start with s3://")
+
*** End Patch
*** Begin Patch
*** Add File:datahub/datahub_ingest_enhanced.yml
+source:
+  type: dbt
+  config:
+    manifest_path: /opt/bi/target/manifest.json
+    catalog_path: /opt/bi/target/catalog.json
+    project_name: aegis_analytics
+transformations:
+  - type: mlflow
+    config:
+      tracking_uri: ${MLFLOW_TRACKING_URI}
+sink:
+  type: datahub-rest
+  config:
+    server: ${DATAHUB_SERVER}
+    api_key: ${DATAHUB_API_KEY}
+
*** End Patch
*** Begin Patch
*** Add File:superset/finance_and_carbon_dashboards.json
+{
+  "dashboards": [
+    {
+      "title": "Finance / Carbon Overview",
+      "slices": [
+        {"name": "Daily Team Rollup", "datasource": "marts.daily_team_rollup"},
+        {"name": "Top Models by Cost", "datasource": "marts.model_performance"}
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:security/certmanager_clusterissuer_corporate.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: corporate-ca-issuer
+spec:
+  ca:
+    secretName: corporate-ca-keypair
+
+# Create secret corporate-ca-keypair in cert-manager namespace with tls.crt/tls.key (your org CA)
+
*** End Patch
*** Begin Patch
*** Add File:security/cosign_rotate.sh
+#!/usr/bin/env bash
+set -euo pipefail
+if [ -z "${1:-}" ]; then
+  echo "Usage: $0 <kms-alias>"
+  exit 2
+fi
+ALIAS=$1
+KEY_ID=$(aws kms create-key --description "Aegis cosign rotation" --query KeyMetadata.KeyId --output text)
+aws kms create-alias --alias-name "alias/${ALIAS}" --target-key-id "${KEY_ID}"
+ARN=$(aws kms describe-key --key-id "${KEY_ID}" --query KeyMetadata.Arn --output text)
+echo "Created KMS key: ${ARN}"
+echo "Update CI secret COSIGN_KMS_KEY_ARN to awskms://${ARN}"
+echo "Rotate carefully: do not delete old key until all signatures validated."
+
*** End Patch
*** Begin Patch
*** Add File:security/kms_kms_policy.tf
+variable "allowed_principals" { type = list(string) }
+
+resource "aws_kms_key" "cosign" {
+  description = "KMS key for cosign signing (Aegis)"
+  policy = jsonencode({
+    Version = "2012-10-17",
+    Statement = [
+      {
+        Sid = "AllowUse",
+        Effect = "Allow",
+        Principal = { AWS = var.allowed_principals },
+        Action = ["kms:Sign","kms:GetPublicKey","kms:DescribeKey"],
+        Resource = "*"
+      },
+      {
+        Sid = "AllowAdmin",
+        Effect = "Allow",
+        Principal = { AWS = var.allowed_principals },
+        Action = ["kms:*"],
+        Resource = "*"
+      }
+    ]
+  })
+}
+
+output "kms_key_arn" { value = aws_kms_key.cosign.arn }
+
*** End Patch
*** Begin Patch
*** Add File:security/rbac_audit.sh
+#!/usr/bin/env bash
+set -euo pipefail
+NS=${1:-aegis}
+echo "ServiceAccounts in ${NS} with ClusterRoleBindings:"
+kubectl get clusterrolebinding -o json | jq -r --arg NS "${NS}" '.items[] | select(.subjects != null) | . as $cr | $cr.subjects[] | select(.namespace==$NS) | "\($cr.metadata.name) \(.name) \(.kind) -> \($cr.roleRef.name)"'
+echo "Review for broad cluster roles (cluster-admin)."
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/median_prometheus_reporter.py
+#!/usr/bin/env python3
+"""
+Compute median relative error from reconciliation artifacts and push to Pushgateway.
+"""
+import os, json, statistics, boto3, requests
+EVIDENCE_BUCKET = os.environ.get("EVIDENCE_BUCKET")
+AWS_REGION = os.environ.get("AWS_REGION","us-west-2")
+PUSHGATEWAY = os.environ.get("PUSHGATEWAY","pushgateway.monitoring.svc:9091")
+PREFIX = os.environ.get("RECON_PREFIX","reconciliations/")
+
+def list_keys(s3):
+    paginator = s3.get_paginator("list_objects_v2")
+    for p in paginator.paginate(Bucket=EVIDENCE_BUCKET, Prefix=PREFIX):
+        for o in p.get("Contents", []):
+            if o["Key"].endswith(".json"):
+                yield o["Key"]
+
+def main():
+    s3 = boto3.client("s3", region_name=AWS_REGION)
+    vals = []
+    for k in list_keys(s3):
+        try:
+            j = json.loads(s3.get_object(Bucket=EVIDENCE_BUCKET, Key=k)["Body"].read().decode())
+            est = j.get("estimate",{}).get("estimated_emissions_kg")
+            meas = j.get("measured",{}).get("measured_emissions_kg")
+            if est and meas:
+                vals.append(abs(meas - est) / float(meas))
+        except Exception:
+            continue
+    med = statistics.median(vals) if vals else 0
+    try:
+        requests.post(f"http://{PUSHGATEWAY}/metrics/job/aegis_recon", data=f"aegis_reconciliation_median_rel_error {med}\naegis_reconciliation_sample_count {len(vals)}\n", timeout=5)
+    except Exception:
+        pass
+    print("median", med, "count", len(vals))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus_slo_rules.yaml
+groups:
+- name: aegis-prod-slos
+  rules:
+  - alert: MedianEstimatorErrorHigh
+    expr: aegis_reconciliation_median_rel_error > 0.10
+    for: 60m
+    labels:
+      severity: page
+    annotations:
+      summary: "Median reconciliation relative error above 10% (audit risk)"
+
+  - alert: EnforcementCoverageLow
+    expr: aegis_enforcement_coverage < 0.95
+    for: 30m
+    labels:
+      severity: page
+    annotations:
+      summary: "Enforcement coverage below 95%"
+
+  - alert: ETLIngestionLagHigh
+    expr: aegis_etl_ingestion_lag_seconds > 86400
+    for: 30m
+    labels:
+      severity: page
+    annotations:
+      summary: "ETL ingestion lag > 24h"
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/ops_dr_runbook.md
+# DR & Resume runbook (Aegis)
+
+Purpose: Validate suspend/resume controller & iterative resume flow for Argo workflows.
+
+Steps:
+1. In staging, run:
+   python3 operational/dr_resume_test.py
+2. Confirm job created and workflow resumed within X minutes.
+3. If resume fails:
+   - Fetch controller logs: kubectl -n aegis logs -l app=aegis-enforcer
+   - Check resume job: kubectl -n aegis get jobs
+   - Re-run resume logic manually: kubectl patch workflow <name> -n <ns> --type merge -p '{"spec":{"suspend":false}}'
+4. Document result under s3://<EVIDENCE_BUCKET>/dr_drills/
+
*** End Patch
*** Begin Patch
*** Add File:runbooks/ops_daily_checklist.md
+# Daily Ops Checklist (Aegis)
+- Check telemetry health: run telemetry/telemetry_healthcheck.py and inspect S3 report.
+- Check Prometheus SLO dashboards (median error, enforcement coverage, ETL lag).
+- Check Airflow DAGs: aegs_idempotent_etl, aegis_etl_ml_optimized last run statuses.
+- Inspect GE validation artifacts in S3 and fix failing expectations.
+- If DR/resume drill scheduled, run DR script and record outcome.
+
*** End Patch
*** Begin Patch
*** Add File:issues/PRIORITIZED_BACKLOG.md
+High priority items to finish optimization (create as GitHub issues):
+
+- Telemetry collection & confirmation (7–14 days): ensure representative coverage and Prometheus targets.
+- Iterative calibration: run weekly, validate median_rel_error <= 0.10; add human approval gate.
+- Enforcement extender/webhook: stage + run enforcement_coverage_harness until coverage >= 95%.
+- Bulk load pipeline: run parquet → manifest → COPY into dev DW; validate MERGE/upsert idempotency.
+- Security: provision corporate ClusterIssuer, KMS cosign key with restrictive policy, rotate keys.
+- Operationalize: add alerts, run DR/drills, enact incident playbooks.
+
*** End Patch
*** Begin Patch
*** Add File:operational/dr_resume_test.py
+#!/usr/bin/env python3
+from kubernetes import client, config
+import time
+
+NAMESPACE = "aegis-test"
+
+def main():
+    config.load_kube_config()
+    crd = client.CustomObjectsApi()
+    wf = {
+      "apiVersion": "argoproj.io/v1alpha1",
+      "kind": "Workflow",
+      "metadata": {"generateName": "dr-test-"},
+      "spec": {"entrypoint":"whalesay","templates":[{"name":"whalesay","container":{"image":"docker/whalesay","command":["cowsay","hello"]}}]}
+    }
+    obj = crd.create_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=NAMESPACE, plural="workflows", body=wf)
+    name = obj["metadata"]["name"]
+    print("Created workflow", name)
+    # Suspend
+    crd.patch_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=NAMESPACE, plural="workflows", name=name, body={"spec":{"suspend": True}})
+    print("Suspended, creating resume job")
+    batch = client.BatchV1Api()
+    job = {
+      "apiVersion":"batch/v1","kind":"Job","metadata":{"name":"dr-resumer-"+name,"namespace":NAMESPACE},
+      "spec":{"template":{"spec":{"containers":[{"name":"resumer","image":"bitnami/kubectl:latest","command":["/bin/sh","-c"],"args":[f"sleep 5; kubectl patch workflow {name} -n {NAMESPACE} --type merge -p '{{\"spec\":{{\"suspend\":false}}}}'"]}],"restartPolicy":"OnFailure"}}}
+    }
+    batch.create_namespaced_job(namespace=NAMESPACE, body=job)
+    time.sleep(15)
+    wf2 = crd.get_namespaced_custom_object(group="argoproj.io", version="v1alpha1", namespace=NAMESPACE, plural="workflows", name=name)
+    print("Workflow suspend status now:", wf2.get("spec",{}).get("suspend"))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File:security/README_SECURITY_HARDENING.md
+# Security hardening checklist
+
+1) PKI & certs
+ - Create k8s secret `corporate-ca-keypair` in cert-manager namespace (tls.crt/tls.key).
+ - Apply security/certmanager_clusterissuer_corporate.yaml.
+ - Use cert-manager to provision webhook TLS secrets for enforcer and extender.
+
+2) KMS & cosign
+ - Use Terraform to provision KMS key with restricted principals (security/kms_kms_policy.tf).
+ - Rotate cosign keys periodically using security/cosign_rotate.sh and update CI secrets.
+
+3) RBAC
+ - Run security/rbac_audit.sh and tighten service accounts with ClusterRoleBindings.
+ - Use least-privilege roles for webhook/enforcer, node-power-exporter and ETL service accounts.
+
+4) SSO & RLS for BI
+ - Configure Superset OIDC (see superset snippet).
+ - Use DB pre-query to set `app.team` and enable row-level security policies.
+
*** End Patch
*** Begin Patch
*** Add File:acceptance/ACCEPTANCE_CRITERIA.md
+# Acceptance Criteria (summary)
+
+- Data accuracy: median(|measured - estimated| / measured) ≤ 0.10 across representative workloads for 14 consecutive days.
+- Enforcement: enforcement_coverage ≥ 0.95 rolling 7 days.
+- ETL: daily ETL success rate ≥ 99%; ingestion lag < 24 hours.
+- Security: webhook certs via corporate PKI; cosign KMS key with restrictive policy and rotation plan; no broad RBAC.
+- Traceability: every production metric links to evidence_s3_key and appears in DataHub lineage.
+- Ops: monthly DR/resume drills executed and artifacts archived.
+
*** End Patch
*** End Patch
