*** Begin Patch
*** Add File: scripts/provider_stress_test.py
+#!/usr/bin/env python3
+"""
+Provider stress & rate-limit tester
+
+This tool repeatedly submits small test jobs to a provider adapter (Qiskit/Braket)
+to exercise rate limits, error modes and to measure retry/backoff behavior.
+It is intended to be run from a self-hosted runner (qpu-adjacent) with provider
+credentials configured (Vault or env).
+
+Usage:
+  python scripts/provider_stress_test.py --provider ibm --count 50 --concurrency 5
+
+The script prints a summary of successes, failures and observed HTTP/error codes.
+"""
+from __future__ import annotations
+import argparse
+import concurrent.futures
+import json
+import os
+import sys
+import time
+from typing import Dict, Any
+
+def submit_job_ibm(i):
+    from providers.qiskit_runtime_adapter_real import QiskitRuntimeAdapter
+    try:
+        adapter = QiskitRuntimeAdapter(provider_name="ibm")
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+    try:
+        # build minimal circuit
+        from qiskit import QuantumCircuit
+        qc = QuantumCircuit(1,1)
+        qc.h(0)
+        qc.measure(0,0)
+        adapter.preflight_transpile(qc, backend_name="aer_simulator")
+        desc = adapter.submit(program=qc, options={"backend": "aer_simulator", "shots": 64})
+        return {"ok": True, "result": desc}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def submit_job_braket(i):
+    from providers.braket_adapter import BraketAdapter, BraketAdapterError
+    try:
+        adapter = BraketAdapter()
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+    try:
+        payload = {"qasm": "H 0\nMEASURE 0\n"}
+        res = adapter.submit(payload, s3_bucket=os.environ.get("BRK_S3_BUCKET", "bucket"), s3_prefix="aegis/test", shots=100)
+        return {"ok": True, "result": res}
+    except Exception as e:
+        return {"ok": False, "error": str(e)}
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--provider", choices=["ibm", "braket"], required=True)
+    p.add_argument("--count", type=int, default=20)
+    p.add_argument("--concurrency", type=int, default=4)
+    args = p.parse_args()
+
+    submit = submit_job_ibm if args.provider == "ibm" else submit_job_braket
+    results = []
+    start = time.time()
+    with concurrent.futures.ThreadPoolExecutor(max_workers=args.concurrency) as ex:
+        futures = [ex.submit(submit, i) for i in range(args.count)]
+        for f in concurrent.futures.as_completed(futures):
+            results.append(f.result())
+    dur = time.time() - start
+    succ = sum(1 for r in results if r.get("ok"))
+    fail = len(results) - succ
+    print(f"Provider stress test for {args.provider} count={args.count} concurrency={args.concurrency}")
+    print(f"Duration: {dur:.2f}s successes={succ} failures={fail}")
+    # print failures detail sample
+    for r in results:
+        if not r.get("ok"):
+            print("FAIL:", r.get("error"))
+    # summary JSON
+    summary = {"provider": args.provider, "count": args.count, "success": succ, "fail": fail, "duration": dur}
+    Path = __import__("pathlib").Path
+    Path("provider_stress_summary.json").write_text(json.dumps(summary, indent=2))
+    print("Wrote provider_stress_summary.json")
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: scripts/vault/create_provider_policy_and_secret.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Create provider-read policy and write provider secret to Vault (PoC)
+# Requires VAULT_ADDR and VAULT_TOKEN to be set (admin).
+#
+# Usage:
+#   VAULT_ADDR=... VAULT_TOKEN=... ./scripts/vault/create_provider_policy_and_secret.sh ibm provider_ibm.json
+
+PROVIDER="${1:-}"
+JSON="${2:-}"
+if [ -z "$PROVIDER" ] || [ -z "$JSON" ]; then
+  echo "Usage: $0 <provider> <provider-json>"
+  exit 2
+fi
+
+POL_NAME="quantum-provider-read"
+POL_FILE="$(mktemp)"
+cat > "${POL_FILE}" <<'HCL'
+path "secret/data/quantum/providers/*" {
+  capabilities = ["read"]
+}
+HCL
+
+echo "Writing policy ${POL_NAME}"
+vault policy write "${POL_NAME}" "${POL_FILE}"
+rm -f "${POL_FILE}"
+
+echo "Writing provider secret at secret/data/quantum/providers/${PROVIDER}"
+curl -sS --header "X-Vault-Token: $VAULT_TOKEN" --request POST --data "{\"data\": $(cat "$JSON")}" "${VAULT_ADDR%/}/v1/secret/data/quantum/providers/${PROVIDER}" | jq .
+echo "Provider secret written. Create an AppRole for controllers to read provider secrets (see runbooks)."
+
*** End Patch
*** Begin Patch
*** Add File: k8s/monitoring/prometheus-deployment.yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: prometheus-config
+  namespace: aegis
+data:
+  prometheus.yml: |
+    global:
+      scrape_interval: 15s
+    scrape_configs:
+      - job_name: 'aegis-quantum-controller'
+        static_configs:
+          - targets: ['quantum-controller:8001']
+      - job_name: 'aegis-quantum-worker'
+        static_configs:
+          - targets: ['quantum-worker:8001']
+
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: prometheus
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: prometheus
+  template:
+    metadata:
+      labels:
+        app: prometheus
+    spec:
+      containers:
+      - name: prometheus
+        image: prom/prometheus:v2.37.0
+        args:
+          - "--config.file=/etc/prometheus/prometheus.yml"
+        ports:
+          - containerPort: 9090
+        volumeMounts:
+          - name: prometheus-config
+            mountPath: /etc/prometheus
+      volumes:
+        - name: prometheus-config
+          configMap:
+            name: prometheus-config
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: prometheus
+  namespace: aegis
+spec:
+  selector:
+    app: prometheus
+  ports:
+    - port: 9090
+      targetPort: 9090
+
*** End Patch
*** Begin Patch
*** Add File: k8s/monitoring/grafana-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: grafana
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: grafana
+  template:
+    metadata:
+      labels:
+        app: grafana
+    spec:
+      containers:
+      - name: grafana
+        image: grafana/grafana:9.4.7
+        env:
+          - name: GF_SECURITY_ADMIN_USER
+            valueFrom:
+              secretKeyRef:
+                name: grafana-admin
+                key: user
+          - name: GF_SECURITY_ADMIN_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: grafana-admin
+                key: password
+        ports:
+        - containerPort: 3000
+        volumeMounts:
+          - name: grafana-dashboards
+            mountPath: /var/lib/grafana/dashboards
+      volumes:
+        - name: grafana-dashboards
+          configMap:
+            name: grafana-dashboards
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: grafana
+  namespace: aegis
+spec:
+  selector:
+    app: grafana
+  ports:
+    - port: 3000
+      targetPort: 3000
+
*** End Patch
*** Begin Patch
*** Add File: k8s/auth/oauth2-proxy.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: oauth2-proxy
+  namespace: aegis
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: oauth2-proxy
+  template:
+    metadata:
+      labels:
+        app: oauth2-proxy
+    spec:
+      containers:
+      - name: oauth2-proxy
+        image: quay.io/oauth2-proxy/oauth2-proxy:v7.4.0
+        args:
+          - --provider=oidc
+          - --http-addr=0.0.0.0:4180
+          - --upstream=http://admin-ui:8080/
+          - --redirect-url=https://admin.example.com/oauth2/callback
+          - --oidc-issuer-url=${OIDC_ISSUER}
+          - --cookie-secret=$(OAUTH_COOKIE_SECRET)
+        env:
+          - name: OIDC_ISSUER
+            value: "https://accounts.example.com"
+          - name: OAUTH_COOKIE_SECRET
+            valueFrom:
+              secretKeyRef:
+                name: oauth2-cookie
+                key: secret
+        ports:
+          - containerPort: 4180
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: oauth2-proxy
+  namespace: aegis
+spec:
+  selector:
+    app: oauth2-proxy
+  ports:
+    - port: 4180
+      targetPort: 4180
+
*** End Patch
*** Begin Patch
*** Add File: billing/billing_reporter.py
+#!/usr/bin/env python3
+"""
+Billing & chargeback reporter for quantum jobs (PoC).
+
+ - Reads per-team costs from utils/cost_meter store (default: /var/lib/aegis/quantum_costs.json)
+ - Produces CSV or posts JSON to a billing webhook (env BILLING_WEBHOOK)
+ - Intended to be run as a CronJob to aggregate daily costs
+"""
+from __future__ import annotations
+import csv
+import json
+import os
+from pathlib import Path
+from typing import Dict
+
+STORE = Path(os.environ.get("COST_STORE", "/var/lib/aegis/quantum_costs.json"))
+OUT_CSV = Path(os.environ.get("BILLING_OUT", "/tmp/quantum_billing_report.csv"))
+WEBHOOK = os.environ.get("BILLING_WEBHOOK", "")
+
+def load_costs() -> Dict[str, float]:
+    if STORE.exists():
+        return json.loads(STORE.read_text())
+    return {}
+
+def write_csv(data: Dict[str, float]):
+    with OUT_CSV.open("w", newline="") as fh:
+        w = csv.writer(fh)
+        w.writerow(["team", "accumulated_cost"])
+        for team, amt in data.items():
+            w.writerow([team, f"{amt:.6f}"])
+    print("Wrote", OUT_CSV)
+
+def post_webhook(data: Dict[str, float]):
+    if not WEBHOOK:
+        return
+    import requests
+    payload = {"billing": data}
+    r = requests.post(WEBHOOK, json=payload, timeout=10)
+    print("Webhook status:", r.status_code)
+
+def main():
+    data = load_costs()
+    write_csv(data)
+    post_webhook(data)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: auditing/k8s-audit-policy.yaml
+apiVersion: audit.k8s.io/v1
+kind: Policy
+rules:
+  - level: RequestResponse
+    verbs: ["create", "update", "patch", "delete"]
+    resources:
+      - group: "aegis.ai"
+        resources: ["quantumjobs"]
+  - level: Metadata
+    resources:
+      - group: ""
+        resources: ["pods", "configmaps", "secrets"]
+  - level: None
+    users: ["system:kube-proxy"]
+
*** End Patch
*** Begin Patch
*** Add File: auditing/fluentd-audit-daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: fluentd-audit
+  namespace: kube-system
+spec:
+  selector:
+    matchLabels:
+      name: fluentd-audit
+  template:
+    metadata:
+      labels:
+        name: fluentd-audit
+    spec:
+      containers:
+      - name: fluentd
+        image: fluent/fluentd:latest
+        env:
+          - name: FLUENTD_ARGS
+            value: "-c /fluentd/etc/fluent.conf"
+        volumeMounts:
+          - name: varlog
+            mountPath: /var/log
+          - name: config
+            mountPath: /fluentd/etc
+      volumes:
+        - name: varlog
+          hostPath:
+            path: /var/log
+        - name: config
+          configMap:
+            name: fluentd-audit-config
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fluentd-audit-config
+  namespace: kube-system
+data:
+  fluent.conf: |
+    <source>
+      @type tail
+      path /var/log/kubernetes/audit.log
+      pos_file /var/log/fluentd-audit.pos
+      tag kubernetes.audit
+      format json
+    </source>
+    <match kubernetes.audit>
+      @type http
+      endpoint_url https://logging.example.com/audit
+      serializer json
+    </match>
+
*** End Patch
*** Begin Patch
*** Add File: docs/runbooks/deploy_prod_infra.md
+# Deploying Production Quantum Infra (Redis HA, Prometheus, Grafana, OIDC, Fluentd)
+
+Prereqs:
+- Kubernetes cluster with storage (PVC) support.
+- Vault admin credentials to create policies and provider secrets.
+- Domain and OIDC provider for admin UI.
+
+Steps (high-level):
+1. Deploy Redis HA:
+   kubectl apply -f k8s/redis/redis-statefulset.yaml
+
+2. Deploy controller & worker:
+   kubectl apply -f k8s/deployments/quantum-controller-deploy.yaml
+   kubectl apply -f k8s/deployments/quantum-worker-deploy.yaml
+
+3. Deploy Prometheus & Grafana:
+   kubectl apply -f k8s/monitoring/prometheus-deployment.yaml
+   kubectl apply -f k8s/monitoring/grafana-deployment.yaml
+   Create Secret grafana-admin with user/password and import dashboards (grafana/dashboards/*)
+
+4. Deploy OAuth2 Proxy (or integrate OIDC into ingress):
+   kubectl apply -f k8s/auth/oauth2-proxy.yaml
+
+5. Deploy Fluentd audit DaemonSet (ensure audit.log path configured on kube-apiserver):
+   kubectl apply -f auditing/fluentd-audit-daemonset.yaml
+   Configure kube-apiserver to write audit logs to /var/log/kubernetes/audit.log using k8s audit policy auditing/k8s-audit-policy.yaml
+
+6. Set up Vault policies & provider secrets:
+   VAULT_ADDR=... VAULT_TOKEN=... ./scripts/vault/create_provider_policy_and_secret.sh ibm provider_ibm.json
+   Create AppRole for controllers and rotate scoped token into K8s secret vault-credentials
+
+7. Run provider onboarding tests from self-hosted runner:
+   python scripts/provider_stress_test.py --provider ibm --count 20 --concurrency 4
+
+8. Configure Prometheus scrape and Grafana dashboards; import grafana/dashboards/quantum_dashboard.json
+
+9. Configure billing webhook and schedule billing_reporter.py as a CronJob to run daily.
+
*** End Patch
*** Begin Patch
*** Update File: README.edge.md
@@
 Notes & next steps:
 - You need Vault admin to create policies/roles and write provider secrets. See scripts/vault/roles_and_policies for examples.
 - Deploy Redis in HA and provide REDIS_URL to controller/worker.
 - Configure Prometheus to scrape controller/worker metrics and create Grafana dashboards.
 - Replace PoC basic-auth with OIDC or OAuth2 for production admin UI.
 - Final provider integration testing requires QPU/sandbox credentials â€” follow runbook to provision and test.
+
+New in this patch (production infra, provider validation, billing & auditing)
+- Provider stress & rate-limit tester (scripts/provider_stress_test.py) to exercise adapters against real provider sandboxes and measure failures.
+- Vault helper to create provider policy and write provider secrets (scripts/vault/create_provider_policy_and_secret.sh).
+- Prometheus & Grafana deployment manifests and ConfigMaps (k8s/monitoring/*) for observability.
+- OAuth2 Proxy deployment manifest example to protect admin UI (k8s/auth/oauth2-proxy.yaml).
+- Billing reporter to aggregate per-team quantum costs and post to billing webhook (billing/billing_reporter.py).
+- Kubernetes audit policy and Fluentd DaemonSet to forward audit logs to centralized logging (auditing/*).
+- Runbook for deploying production infra and provider onboarding (docs/runbooks/deploy_prod_infra.md).
+
+Caveats:
+- Some manifests are PoC and need adaptation to your cluster: storage class names, domain names, secrets (Grafana admin, OIDC config), and logging endpoints.
+- Provider testing requires credentials and may consume quota/cost. Use provider sandboxes where possible.
+- Fluentd/auditing and Prometheus/Grafana should be integrated with your monitoring/logging platforms for production readiness.
+
*** End Patch
