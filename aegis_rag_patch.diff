diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,8 @@
+flask==2.3.2
+boto3==1.28.87
+sentence-transformers==2.2.2
+faiss-cpu==1.7.4
+psycopg2-binary==2.9.7
+openai==0.27.8
+python-dotenv==1.0.0
+requests==2.31.0
diff --git a/rag/ingest.py b/rag/ingest.py
new file mode 100644
index 0000000..2222222
--- /dev/null
+++ b/rag/ingest.py
@@ -0,0 +1,252 @@
+#!/usr/bin/env python3
+"""
+Ingest documents from an S3-compatible LakeFS path, chunk them, embed and index into a FAISS index.
+Saves:
+ - index file: ./data/faiss.index
+ - metadata file: ./data/metadata.json
+
+Environment variables:
+ - AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY (or other S3 creds)
+ - S3_ENDPOINT (optional): custom endpoint (e.g., LakeFS)
+ - S3_REGION (optional)
+ - OPENAI_API_KEY (optional): when present, use OpenAI embeddings; otherwise use sentence-transformers
+
+Usage:
+  python rag/ingest.py --s3-bucket my-bucket --s3-prefix my/corpus --out-dir ./data
+"""
+import os
+import argparse
+import json
+import tempfile
+import boto3
+from botocore.client import Config
+from sentence_transformers import SentenceTransformer
+import numpy as np
+import faiss
+import math
+from typing import List
+
+# Simple chunker
+def chunk_text(text: str, chunk_size: int = 800, overlap: int = 128) -> List[str]:
+    tokens = text.split()
+    chunks = []
+    i = 0
+    while i < len(tokens):
+        chunk = tokens[i : i + chunk_size]
+        chunks.append(" ".join(chunk))
+        i += chunk_size - overlap
+    return chunks
+
+def list_s3_objects(s3, bucket: str, prefix: str):
+    paginator = s3.get_paginator("list_objects_v2")
+    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
+        for obj in page.get("Contents", []):
+            yield obj["Key"]
+
+def download_s3_object(s3, bucket: str, key: str) -> bytes:
+    obj = s3.get_object(Bucket=bucket, Key=key)
+    return obj["Body"].read()
+
+def get_embedding_openai(texts: List[str], model: str = "text-embedding-3-small"):
+    import openai
+    openai.api_key = os.environ.get("OPENAI_API_KEY")
+    resp = openai.Embedding.create(input=texts, model=model)
+    return [r["embedding"] for r in resp["data"]]
+
+def get_embedding_local(texts: List[str], model_name: str = "all-MiniLM-L6-v2"):
+    model = SentenceTransformer(model_name)
+    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--s3-bucket", required=True)
+    p.add_argument("--s3-prefix", required=True)
+    p.add_argument("--out-dir", default="./data")
+    p.add_argument("--chunk-size", type=int, default=800)
+    p.add_argument("--chunk-overlap", type=int, default=128)
+    p.add_argument("--top-k", type=int, default=8)
+    args = p.parse_args()
+
+    os.makedirs(args.out_dir, exist_ok=True)
+    # configure s3 client (LakeFS-compatible)
+    s3_endpoint = os.environ.get("S3_ENDPOINT")
+    s3_region = os.environ.get("S3_REGION", "us-east-1")
+    s3_cfg = Config(signature_version="s3v4")
+    if s3_endpoint:
+        s3 = boto3.client("s3", endpoint_url=s3_endpoint, region_name=s3_region, config=s3_cfg)
+    else:
+        s3 = boto3.client("s3", region_name=s3_region, config=s3_cfg)
+
+    # gather text chunks and metadata
+    texts = []
+    metadatas = []
+
+    print("Listing objects...")
+    for key in list_s3_objects(s3, args.s3_bucket, args.s3_prefix):
+        print("Found:", key)
+        try:
+            data = download_s3_object(s3, args.s3_bucket, key)
+            # assume text/plain or utf-8
+            txt = data.decode("utf-8", errors="ignore")
+        except Exception as e:
+            print("Skipping", key, ":", e)
+            continue
+        chunks = chunk_text(txt, chunk_size=args.chunk_size, overlap=args.chunk_overlap)
+        for i, c in enumerate(chunks):
+            texts.append(c)
+            metadatas.append({"source": f"s3://{args.s3_bucket}/{key}", "chunk_index": i})
+    if not texts:
+        print("No texts found for ingestion. Exiting.")
+        return
+
+    # get embeddings
+    if os.environ.get("OPENAI_API_KEY"):
+        print("Using OpenAI embeddings...")
+        embeddings = get_embedding_openai(texts)
+        embeddings = np.array(embeddings).astype("float32")
+    else:
+        print("Using local sentence-transformers embeddings...")
+        embeddings = get_embedding_local(texts)
+        embeddings = np.array(embeddings).astype("float32")
+
+    dim = embeddings.shape[1]
+    print(f"Embedding dimension: {dim}, number of vectors: {len(embeddings)}")
+
+    index = faiss.IndexFlatL2(dim)
+    index.add(embeddings)
+
+    idx_path = os.path.join(args.out_dir, "faiss.index")
+    faiss.write_index(index, idx_path)
+    print("Wrote FAISS index to", idx_path)
+
+    # store metadata
+    meta_path = os.path.join(args.out_dir, "metadata.json")
+    with open(meta_path, "w") as fh:
+        json.dump(metadatas, fh)
+    print("Wrote metadata to", meta_path)
+
+if __name__ == "__main__":
+    main()
diff --git a/tools/decisionlog_client.py b/tools/decisionlog_client.py
new file mode 100644
index 0000000..3333333
--- /dev/null
+++ b/tools/decisionlog_client.py
@@ -0,0 +1,120 @@
+#!/usr/bin/env python3
+"""
+Minimal helper to write audit entries to decision_log in Postgres.
+
+Environment:
+ - POSTGRES_URL e.g. postgresql://user:pass@host:5432/db
+
+Assumes decision_log table has columns:
+  id (serial), created_at (default now()), agent text, action text, payload jsonb, evidence jsonb
+
+This is a lightweight helper for the RAG service to log retrieval evidence.
+"""
+import os
+import json
+import psycopg2
+import psycopg2.extras
+from typing import Any, Dict, Optional
+
+POSTGRES_URL = os.environ.get("POSTGRES_URL")
+
+def insert_decision(agent: str, action: str, payload: Dict[str, Any], evidence: Dict[str, Any]):
+    if not POSTGRES_URL:
+        print("POSTGRES_URL not set; skipping decision_log write")
+        return
+    conn = psycopg2.connect(POSTGRES_URL)
+    cur = conn.cursor()
+    cur.execute(
+        """
+        INSERT INTO decision_log (agent, action, payload, evidence)
+        VALUES (%s, %s, %s, %s)
+        RETURNING id;
+        """,
+        (agent, action, json.dumps(payload), json.dumps(evidence)),
+    )
+    id_ = cur.fetchone()[0]
+    conn.commit()
+    cur.close()
+    conn.close()
+    return id_
diff --git a/rag/retriever_service.py b/rag/retriever_service.py
new file mode 100644
index 0000000..4444444
--- /dev/null
+++ b/rag/retriever_service.py
@@ -0,0 +1,240 @@
+#!/usr/bin/env python3
+"""
+Simple retrieval service that loads a FAISS index + metadata and exposes:
+ - POST /query { "query": "...", "top_k": 5, "call_llm": false }
+
+If call_llm is true and OPENAI_API_KEY is present, the service will call OpenAI chat completions
+to generate an answer using retrieved context.
+
+It logs retrieval events to decision_log via tools.decisionlog_client.insert_decision.
+"""
+import os
+import json
+import faiss
+import numpy as np
+from flask import Flask, request, jsonify
+from sentence_transformers import SentenceTransformer
+from tools.decisionlog_client import insert_decision
+
+app = Flask(__name__)
+
+DATA_DIR = os.environ.get("RAG_DATA_DIR", "/data")
+INDEX_PATH = os.path.join(DATA_DIR, "faiss.index")
+META_PATH = os.path.join(DATA_DIR, "metadata.json")
+
+# load index & metadata
+if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):
+    print("FAISS index or metadata not found; start service after ingestion")
+    INDEX = None
+    METADATA = []
+else:
+    INDEX = faiss.read_index(INDEX_PATH)
+    with open(META_PATH, "r") as fh:
+        METADATA = json.load(fh)
+
+# embedding model (cache)
+if os.environ.get("OPENAI_API_KEY"):
+    import openai
+    OPENAI_AVAILABLE = True
+    openai.api_key = os.environ.get("OPENAI_API_KEY")
+else:
+    OPENAI_AVAILABLE = False
+    EMB_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
+
+def embed_query(q: str):
+    if OPENAI_AVAILABLE:
+        resp = openai.Embedding.create(input=[q], model="text-embedding-3-small")
+        vec = np.array(resp["data"][0]["embedding"]).astype("float32")
+        return vec
+    else:
+        vec = EMB_MODEL.encode([q], convert_to_numpy=True)[0].astype("float32")
+        return vec
+
+@app.route("/query", methods=["POST"])
+def query():
+    payload = request.get_json()
+    query = payload.get("query", "")
+    top_k = int(payload.get("top_k", 5))
+    call_llm = bool(payload.get("call_llm", False))
+    if not INDEX:
+        return jsonify({"error": "index not loaded"}), 500
+
+    qvec = embed_query(query).reshape(1, -1)
+    D, I = INDEX.search(qvec, top_k)
+    hits = []
+    for dist, idx in zip(D[0], I[0]):
+        try:
+            meta = METADATA[idx]
+        except Exception:
+            meta = {}
+        hits.append({"score": float(dist), "metadata": meta})
+
+    # assemble context
+    context_pieces = []
+    for h in hits:
+        # load the source chunk content for more context; best-effort - we may need S3 access to retrieve content
+        context_pieces.append(f"source:{h['metadata'].get('source')} chunk:{h['metadata'].get('chunk_index')}")
+
+    evidence = {"retrieval": {"query": query, "top_k": top_k, "hits": hits}}
+    # log retrieval event
+    decision_id = insert_decision(agent="aegis-rag-retriever", action="retrieve", payload={"query": query}, evidence=evidence)
+
+    result = {"query": query, "hits": hits, "decision_log_id": decision_id}
+
+    if call_llm:
+        if OPENAI_AVAILABLE:
+            # build prompt with context
+            prompt = "Use the following context to answer the question.\n\n"
+            prompt += "\n".join([f"Context {i+1}: {h['metadata'].get('source')}" for i, h in enumerate(hits)])
+            prompt += f"\n\nQuestion: {query}\n\nAnswer:"
+            chat_resp = openai.ChatCompletion.create(
+                model=os.environ.get("OPENAI_CHAT_MODEL", "gpt-4o-mini"),
+                messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":prompt}],
+                max_tokens=800,
+            )
+            answer = chat_resp["choices"][0]["message"]["content"]
+            # log answer as evidence
+            insert_decision(agent="aegis-rag-retriever", action="generate", payload={"query": query}, evidence={"answer": answer, "source_hits": hits})
+            result["answer"] = answer
+        else:
+            result["note"] = "call_llm requested but OPENAI_API_KEY missing; returning retrieval hits only"
+
+    return jsonify(result)
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", "8080")))
diff --git a/scripts/run_ingest.sh b/scripts/run_ingest.sh
new file mode 100644
index 0000000..5555555
--- /dev/null
+++ b/scripts/run_ingest.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+set -euo pipefail
+DATA_DIR="${1:-./data}"
+S3_BUCKET="${2:-my-lakefs-bucket}"
+S3_PREFIX="${3:-corpus/}"
+
+mkdir -p "$DATA_DIR"
+python3 rag/ingest.py --s3-bucket "$S3_BUCKET" --s3-prefix "$S3_PREFIX" --out-dir "$DATA_DIR"
+echo "Ingestion complete. Index and metadata are at $DATA_DIR"
diff --git a/k8s/manifests/rag-retriever-deployment.yaml b/k8s/manifests/rag-retriever-deployment.yaml
new file mode 100644
index 0000000..6666666
--- /dev/null
+++ b/k8s/manifests/rag-retriever-deployment.yaml
@@ -0,0 +1,72 @@
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: aegis-rag-retriever
+  namespace: aegis-ml
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: aegis-rag-retriever
+  template:
+    metadata:
+      labels:
+        app: aegis-rag-retriever
+    spec:
+      serviceAccountName: aegis-agent-sa
+      containers:
+        - name: retriever
+          image: <REGISTRY>/aegis-rag-retriever:latest
+          ports:
+            - containerPort: 8080
+          env:
+            - name: RAG_DATA_DIR
+              value: "/data"
+            - name: POSTGRES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-postgres-secret
+                  key: POSTGRES_URL
+            - name: OPENAI_API_KEY
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-openai-secret
+                  key: OPENAI_API_KEY
+          volumeMounts:
+            - name: rag-data
+              mountPath: /data
+      volumes:
+        - name: rag-data
+          persistentVolumeClaim:
+            claimName: aegis-rag-pvc
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: aegis-rag-retriever
+  namespace: aegis-ml
+spec:
+  selector:
+    app: aegis-rag-retriever
+  ports:
+    - port: 8080
+      targetPort: 8080
diff --git a/README_rag.md b/README_rag.md
new file mode 100644
index 0000000..7777777
--- /dev/null
+++ b/README_rag.md
@@ -0,0 +1,162 @@
+# Aegis RAG Prototype
+
+This patch adds a simple Retrieval-Augmented Generation (RAG) prototype:
+- `rag/ingest.py` — ingest documents from an S3-compatible LakeFS path, chunk, embed and index into FAISS.
+- `rag/retriever_service.py` — simple Flask service that performs retrieval, optionally calls OpenAI to generate an answer, and logs retrieval/generation events to `decision_log`.
+- `tools/decisionlog_client.py` — helper to write audit entries to `decision_log`.
+- Kubernetes manifest to deploy the retriever service.
+- `scripts/run_ingest.sh` helper.
+
+Quick start (local)
+1. Install requirements:
+   ```bash
+   pip install -r requirements.txt
+   ```
+
+2. Ingest documents from LakeFS/S3:
+   - Ensure AWS/LakeFS credentials are set via environment variables (AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY / S3_ENDPOINT if needed).
+   - Run:
+     ```bash
+     ./scripts/run_ingest.sh ./data my-bucket corpus/
+     ```
+   - This writes `./data/faiss.index` and `./data/metadata.json`.
+
+3. Run the retriever service:
+   ```bash
+   export POSTGRES_URL="postgresql://user:pass@host:5432/aegis"
+   export RAG_DATA_DIR=./data
+   flask --app rag.retriever_service run --host=0.0.0.0 --port=8080
+   ```
+   Or:
+   ```bash
+   python3 rag/retriever_service.py
+   ```
+
+4. Query the service:
+   ```bash
+   curl -X POST http://localhost:8080/query -H "Content-Type: application/json" -d '{"query":"What is the policy for model promotion?","top_k":5}'
+   ```
+
+Notes and next steps
+- Index persistence: prototype stores FAISS index and metadata locally in `RAG_DATA_DIR`. For production, upload index files to durable object storage and/or use a managed vector DB (Milvus / Weaviate / Pinecone).
+- Embeddings: if `OPENAI_API_KEY` is present, the service will use OpenAI embeddings and ChatCompletions (configurable). Otherwise it uses `sentence-transformers` locally.
+- Security: secrets in k8s manifest are placeholders. Replace with Vault Injector/CSI annotations to mount secrets from Vault (we have existing Vault enforcement patch).
+- Logging: retrieval and generation events are written to `decision_log` via `tools/decisionlog_client.insert_decision`. Adjust schema if needed.
+- Retrieval quality: use smarter chunking (token-aware), deduplication, and metadata enrichment. Consider vector DB that supports metadata filters.
+- Integration: wire the retriever service to Orchestrator and to downstream LLM inference services (vLLM/Triton) for production traffic.
+
+If you want, I can:
+- Extend this prototype to use Milvus or Chroma as the vector store and provide Kubernetes manifests.
+- Add ingestion DAG (Argo) to schedule periodic indexing jobs.
+- Wire the retriever to your existing model-serving gateway and add prompt templates and RAG prompt orchestration.
+- Add unit tests, and CI workflow to validate the RAG pipeline.
+
+
