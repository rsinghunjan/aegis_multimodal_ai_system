*** Begin Patch
*** Add File:ops/populate_and_verify_secrets_irsa.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Populate GitHub secrets and AWS secrets (SSM/SecretsManager) from Terraform output,
+# then verify ExternalSecrets sync and IRSA annotations in the cluster.
+#
+# Usage:
+#   export GITHUB_TOKEN=...
+#   ./ops/populate_and_verify_secrets_irsa.sh /tmp/aegis_tf_output.json [kubecontext]
+#
+TF_OUT=${1:-/tmp/aegis_tf_output.json}
+KUBECTX=${2:-}
+REPO=${GITHUB_REPOSITORY:-owner/repo}
+
+if [ ! -f "$TF_OUT" ]; then
+  echo "Terraform output JSON required at $TF_OUT" >&2
+  exit 2
+fi
+
+: "${GITHUB_TOKEN:?GITHUB_TOKEN must be set (repo:write & secrets perms)}"
+
+COSIGN_KMS_ARN=$(jq -r '.cosign_kms_arn.value // .cosign_kms_arn' "$TF_OUT")
+EVIDENCE_BUCKET=$(jq -r '.evidence_bucket.value // .evidence_bucket' "$TF_OUT")
+REKOR_URL=$(jq -r '.rekor_url.value // .rekor_url' "$TF_OUT")
+DB_SECRET_ARN=$(jq -r '.db_secret_arn.value // .db_secret_arn' "$TF_OUT")
+
+echo "Setting GitHub secrets (COSIGN_KMS_ARN, REKOR_URL, EVIDENCE_BUCKET)..."
+if command -v gh >/dev/null 2>&1; then
+  echo -n "$COSIGN_KMS_ARN" | gh secret set COSIGN_KMS_ARN --repo "$REPO" --body -
+  echo -n "$REKOR_URL" | gh secret set REKOR_URL --repo "$REPO" --body -
+  echo -n "$EVIDENCE_BUCKET" | gh secret set EVIDENCE_BUCKET --repo "$REPO" --body -
+else
+  echo "gh CLI not found. Please set GitHub secrets manually." >&2
+fi
+
+echo "Ensure AWS SecretsManager contains entries used by ExternalSecrets..."
+if [ -n "$DB_SECRET_ARN" ] && aws secretsmanager get-secret-value --secret-id "$DB_SECRET_ARN" >/dev/null 2>&1; then
+  echo "Found DB secret in SecretsManager: $DB_SECRET_ARN"
+else
+  echo "DB secret missing in SecretsManager; ensure you created secret and ExternalSecret mapping" >&2
+fi
+
+KUBECTL="kubectl"
+if [ -n "$KUBECTX" ]; then
+  KUBECTL="kubectl --context $KUBECTX"
+fi
+
+echo "Waiting for ExternalSecrets controller and secrets to sync..."
+for ns in security aegis; do
+  for s in aegis-cosign aegis-rekor aegis-db-creds; do
+    echo -n "Checking $ns/$s ... "
+    if ! $KUBECTL -n "$ns" get secret "$s" >/dev/null 2>&1; then
+      echo "MISSING"
+      echo "ExternalSecret has not synced $s in namespace $ns - check external-secrets controller logs" >&2
+      exit 3
+    fi
+    echo "OK"
+  done
+done
+
+echo "Verify IRSA annotations for critical serviceaccounts..."
+declare -A SA_ROLE_MAP
+SA_ROLE_MAP[cosign-signer]=arn:aws:iam::REPLACE_ACCOUNT:role/aegis-cosign-irsa-role
+for sa in "${!SA_ROLE_MAP[@]}"; do
+  ns=security
+  ann=$($KUBECTL -n "$ns" get sa "$sa" -o jsonpath='{.metadata.annotations}' 2>/dev/null || echo "")
+  if echo "$ann" | grep -q 'eks.amazonaws.com/role-arn'; then
+    echo "ServiceAccount $ns/$sa IRSA annotation present"
+  else
+    echo "MISSING IRSA annotation on $ns/$sa. Annotate using k8s/irsa/annotate_serviceaccounts.sh" >&2
+    exit 4
+  fi
+done
+
+echo "All secrets and IRSA checks passed."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/build_sign_push.yml
+name: Build, Sign & Push Images
+
+on:
+  workflow_dispatch:
+
+permissions:
+  contents: write
+  id-token: write
+
+env:
+  IMAGE_TAG: "latest"
+
+jobs:
+  build-and-push:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v2
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      - name: Login to GHCR
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.actor }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - name: Build aegis-tools
+        uses: docker/build-push-action@v4
+        with:
+          push: true
+          tags: ${{ secrets.GHCR_ORG }}/aegis-tools:${{ env.IMAGE_TAG }}
+          file: tools/Dockerfile
+      - name: Build aegis-train
+        uses: docker/build-push-action@v4
+        with:
+          push: true
+          tags: ${{ secrets.GHCR_ORG }}/aegis-train:${{ env.IMAGE_TAG }}
+          file: train/Dockerfile
+      - name: Build aegis-transformer
+        uses: docker/build-push-action@v4
+        with:
+          push: true
+          tags: ${{ secrets.GHCR_ORG }}/aegis-transformer:${{ env.IMAGE_TAG }}
+          file: transformer/Dockerfile
+      - name: Cosign sign images using KMS
+        uses: sigstore/cosign-installer@v2
+      - name: Sign images with cosign (KMS)
+        env:
+          COSIGN_KMS_ARN: ${{ secrets.COSIGN_KMS_ARN }}
+        run: |
+          cosign sign -key $COSIGN_KMS_ARN ${{ secrets.GHCR_ORG }}/aegis-tools:${{ env.IMAGE_TAG }}
+          cosign sign -key $COSIGN_KMS_ARN ${{ secrets.GHCR_ORG }}/aegis-train:${{ env.IMAGE_TAG }}
+          cosign sign -key $COSIGN_KMS_ARN ${{ secrets.GHCR_ORG }}/aegis-transformer:${{ env.IMAGE_TAG }}
+      - name: Verify cosign signatures and Rekor entries
+        env:
+          REKOR_URL: ${{ secrets.REKOR_URL }}
+        run: |
+          cosign verify --rekor-server $REKOR_URL ${{ secrets.GHCR_ORG }}/aegis-tools:${{ env.IMAGE_TAG }}
+          cosign verify --rekor-server $REKOR_URL ${{ secrets.GHCR_ORG }}/aegis-train:${{ env.IMAGE_TAG }}
+          cosign verify --rekor-server $REKOR_URL ${{ secrets.GHCR_ORG }}/aegis-transformer:${{ env.IMAGE_TAG }}
+
*** End Patch
*** Begin Patch
*** Add File:k8s/ha/ha_values_examples.yaml
+#
+# Example HA & sizing values for stateful components (edit to match your workload).
+# Apply via Helm for each chart.
+#
+postgres:
+  storageClass: gp3
+  size: 400Gi
+  replicas: 2
+  resources:
+    requests:
+      cpu: 1000m
+      memory: 2Gi
+    limits:
+      cpu: 2000m
+      memory: 6Gi
+
+milvus:
+  storageClass: gp3
+  size: 500Gi
+  replicas: 2
+  resources:
+    requests:
+      cpu: 1000m
+      memory: 4Gi
+    limits:
+      cpu: 4000m
+      memory: 16Gi
+
+rekor:
+  pvc_size: 100Gi
+  replicas: 2
+
*** End Patch
*** Begin Patch
*** Add File:scripts/dr/extended_restore_and_rto.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Restore a Postgres dump into a test DB and measure RTO. Meant for operator-run DR drills.
+#
+EVIDENCE_BUCKET=${1:-aegis-evidence-12345}
+DUMP_KEY=${2:-backups/postgres/latest.sql.gz}
+RESTORE_NS=${3:-aegis-dr-test}
+TIME_START=$(date +%s)
+
+echo "Creating test namespace $RESTORE_NS"
+kubectl create ns "$RESTORE_NS" || true
+
+echo "Deploying temporary Postgres restore"
+kubectl -n "$RESTORE_NS" apply -f - <<EOF
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: postgres-restore
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: postgres-restore
+  template:
+    metadata:
+      labels:
+        app: postgres-restore
+    spec:
+      containers:
+        - name: postgres
+          image: bitnami/postgresql:13
+          env:
+            - name: POSTGRESQL_USERNAME
+              value: "aegis"
+            - name: POSTGRESQL_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-db-creds
+                  key: password
+            - name: POSTGRESQL_DATABASE
+              value: "aegis_registry"
+          ports:
+            - containerPort: 5432
+EOF
+
+kubectl -n "$RESTORE_NS" wait --for=condition=ready pod -l app=postgres-restore --timeout=180s
+RESTORE_POD=$(kubectl -n "$RESTORE_NS" get pods -l app=postgres-restore -o jsonpath='{.items[0].metadata.name}')
+
+echo "Downloading dump from s3://$EVIDENCE_BUCKET/$DUMP_KEY to restore pod"
+kubectl -n "$RESTORE_NS" exec "$RESTORE_POD" -- bash -lc "apt-get update -y && apt-get install -y awscli gzip"
+kubectl -n "$RESTORE_NS" exec "$RESTORE_POD" -- bash -lc "aws s3 cp s3://$EVIDENCE_BUCKET/$DUMP_KEY /tmp/dump.sql.gz"
+kubectl -n "$RESTORE_NS" exec "$RESTORE_POD" -- bash -lc "gunzip -c /tmp/dump.sql.gz | psql -U aegis -d aegis_registry"
+
+echo "Running verification query"
+kubectl -n "$RESTORE_NS" exec "$RESTORE_POD" -- bash -lc "psql -U aegis -d aegis_registry -c 'SELECT count(*) FROM information_schema.tables;'"
+
+TIME_END=$(date +%s)
+RTO=$((TIME_END - TIME_START))
+echo "DR restore RTO (seconds): $RTO"
+echo "Record RTO and compare with RTO target."
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kserve/cert-manager-clusterissuer.yaml
+apiVersion: cert-manager.io/v1
+kind: ClusterIssuer
+metadata:
+  name: letsencrypt-staging
+spec:
+  acme:
+    server: https://acme-staging-v02.api.letsencrypt.org/directory
+    email: ops@example.com
+    privateKeySecretRef:
+      name: letsencrypt-staging
+    solvers:
+      - http01:
+          ingress:
+            class: nginx
+
*** End Patch
*** Begin Patch
*** Add File:k8s/kserve/ingress_tls.yaml
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: kserve-ingress
+  namespace: aegis
+  annotations:
+    kubernetes.io/ingress.class: "nginx"
+    cert-manager.io/cluster-issuer: "letsencrypt-staging"
+    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
+spec:
+  tls:
+    - hosts:
+        - kserve.aegis.example.com
+      secretName: kserve-tls-secret
+  rules:
+    - host: kserve.aegis.example.com
+      http:
+        paths:
+          - path: /
+            pathType: Prefix
+            backend:
+              service:
+                name: ambassador
+                port:
+                  number: 80
+
*** End Patch
*** Begin Patch
*** Add File:k8s/mcpx/mcpx-hardened-auth-sidecar.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: mcpx-logger
+  namespace: aegis
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: mcpx-logger
+  template:
+    metadata:
+      labels:
+        app: mcpx-logger
+    spec:
+      serviceAccountName: mcpx-sa
+      containers:
+        - name: mcpx-logger
+          image: ghcr.io/yourorg/mcpx-logger:latest
+          env:
+            - name: EVIDENCE_BUCKET
+              valueFrom:
+                secretKeyRef:
+                  name: aegis-secrets
+                  key: evidence_bucket
+            - name: AWS_REGION
+              value: "us-west-2"
+          ports:
+            - containerPort: 8080
+        - name: auth-sidecar
+          image: nginx:stable
+          ports:
+            - containerPort: 8081
+          volumeMounts:
+            - mountPath: /etc/nginx/nginx.conf
+              name: nginx-config
+              subPath: nginx.conf
+      volumes:
+        - name: nginx-config
+          configMap:
+            name: mcpx-nginx-config
+
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: mcpx-nginx-config
+  namespace: aegis
+data:
+  nginx.conf: |
+    worker_processes  1;
+    events { worker_connections 1024; }
+    http {
+      server {
+        listen 8081;
+        location / {
+          if ($http_authorization = "") { return 401; }
+          proxy_pass http://127.0.0.1:8080;
+          proxy_set_header Authorization $http_authorization;
+        }
+      }
+    }
+
*** End Patch
*** Begin Patch
*** Add File:iam/least_privilege_full.json
+{
+  "Version": "2012-10-17",
+  "Statement": [
+    {
+      "Sid": "AllowUseOfCosignKey",
+      "Effect": "Allow",
+      "Action": [
+        "kms:Sign",
+        "kms:GetPublicKey",
+        "kms:DescribeKey"
+      ],
+      "Resource": "REPLACE_KMS_KEY_ARN"
+    },
+    {
+      "Sid": "AllowSecretsManagerRead",
+      "Effect": "Allow",
+      "Action": [
+        "secretsmanager:GetSecretValue",
+        "secretsmanager:DescribeSecret"
+      ],
+      "Resource": [
+        "arn:aws:secretsmanager:REPLACE_REGION:REPLACE_ACCOUNT:secret:aegis/*"
+      ]
+    },
+    {
+      "Sid": "AllowS3EvidenceBucket",
+      "Effect": "Allow",
+      "Action": [
+        "s3:PutObject",
+        "s3:GetObject",
+        "s3:ListBucket"
+      ],
+      "Resource": [
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET",
+        "arn:aws:s3:::REPLACE_EVIDENCE_BUCKET/*"
+      ]
+    },
+    {
+      "Sid": "AllowCloudTrailRead",
+      "Effect": "Allow",
+      "Action": [
+        "cloudtrail:LookupEvents",
+        "cloudtrail:GetTrail"
+      ],
+      "Resource": "*"
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File:ops/enable_cloudtrail_and_configure.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Enable CloudTrail and configure logging to S3 (evidence bucket). Use AWS CLI with operator creds.
+#
+EVIDENCE_BUCKET=${1:-${EVIDENCE_BUCKET:-aegis-evidence-12345}}
+TRAIL_NAME=${2:-aegis-trail}
+REGION=${3:-us-west-2}
+
+echo "Creating CloudTrail $TRAIL_NAME writing to s3://$EVIDENCE_BUCKET/cloudtrail/"
+aws s3api create-bucket --bucket "$EVIDENCE_BUCKET" --region "$REGION" --create-bucket-configuration LocationConstraint="$REGION" 2>/dev/null || true
+aws cloudtrail create-trail --name "$TRAIL_NAME" --s3-bucket-name "$EVIDENCE_BUCKET" --is-multi-region-trail --include-global-service-events
+aws cloudtrail start-logging --name "$TRAIL_NAME"
+echo "CloudTrail created and logging started."
+
+echo "Enable CloudWatch metric filter or S3 notifications separately if required."
+
*** End Patch
*** Begin Patch
*** Add File:ops/generate_baseline_and_tune_prometheus.py
+#!/usr/bin/env python3
+"""
+Generate baseline distributions from a sample dataset and create a tuned Prometheus rule file.
+Usage: python3 ops/generate_baseline_and_tune_prometheus.py --sample s3://bucket/path/features.parquet --out tuned_rules.yaml
+"""
+import argparse, json, tempfile, boto3, pandas as pd
+
+parser = argparse.ArgumentParser()
+parser.add_argument("--sample", required=True)
+parser.add_argument("--out", default="monitoring/tuned_prometheus_rules.yaml")
+args = parser.parse_args()
+
+def download_s3(s3uri):
+    parts = s3uri[5:].split("/",1)
+    s3 = boto3.client("s3")
+    tmp = tempfile.mktemp(suffix=".parquet")
+    s3.download_file(parts[0], parts[1], tmp)
+    return tmp
+
+tmp = download_s3(args.sample)
+df = pd.read_parquet(tmp)
+rules = {"groups":[{"name":"aegis-drift-tuned","rules": []}]}
+for c in df.select_dtypes(include=["number"]).columns:
+    # heuristic threshold for drift alert; tune as needed
+    rules["groups"][0]["rules"].append({
+        "alert": "FeatureDriftHigh_"+c,
+        "expr": f'aegis_feature_drift_ks{{feature="{c}"}} > 0.25',
+        "for": "10m",
+        "labels": {"severity":"warning"},
+        "annotations": {"summary": f"High KS drift for {c}"}
+    })
+
+import yaml
+with open(args.out, "w") as f:
+    yaml.dump(rules, f)
+print("Wrote tuned rules to", args.out)
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/tuned_prometheus_rules.yaml
+groups:
+- name: aegis-critical.rules
+  rules:
+  - alert: PostgresDiskUsageHigh
+    expr: kubelet_volume_stats_available_bytes{namespace="aegis",persistentvolumeclaim=~".*postgres.*"} / kubelet_volume_stats_capacity_bytes{namespace="aegis",persistentvolumeclaim=~".*postgres.*"} < 0.20
+    for: 15m
+    labels:
+      severity: critical
+    annotations:
+      summary: "Postgres PVC nearly full"
+
+  - alert: MilvusReplicaDown
+    expr: kube_statefulset_replicas_unavailable{statefulset="milvus",namespace="aegis"} > 0
+    for: 10m
+    labels:
+      severity: critical
+
+  - alert: FeatureDriftHigh
+    expr: aegis_feature_drift_ks > 0.25
+    for: 10m
+    labels:
+      severity: warning
+    annotations:
+      summary: "High KS drift detected for feature {{ $labels.feature }}"
+
*** End Patch
*** Begin Patch
*** Add File:pipelines/enforce_prod_reads.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# CI helper to ensure prod scripts are used in pipelines (no synthetic fallbacks).
+# Fails if any training script still uses demo/synthetic fallback patterns.
+#
+bad=$(grep -RIn --exclude-dir=.git --exclude-dir=venv -E "synthetic|np.random|random_state=.*make|fallback|demo_data|GENERATE_DEMO" || true)
+if [ -n "$bad" ]; then
+  echo "Found potential synthetic/demo fallback usage:"
+  echo "$bad"
+  exit 2
+fi
+echo "No synthetic fallbacks found."
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/enforce_no_synthetic.yml
+name: Enforce no synthetic fallbacks
+
+on:
+  pull_request:
+    types: [opened, synchronize, reopened]
+
+jobs:
+  check-no-synthetic:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Check for synthetic fallback usage
+        run: |
+          chmod +x pipelines/enforce_prod_reads.sh
+          ./pipelines/enforce_prod_reads.sh
+
*** End Patch
*** Begin Patch
*** Add File:ops/ensure_github_branch_protection.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Ensure branch protection to require PR reviews for main branch and enable status checks.
+# Requires gh CLI and repo admin privileges.
+#
+REPO=${1:-${GITHUB_REPOSITORY:-owner/repo}}
+BRANCH=${2:-main}
+
+if ! command -v gh >/dev/null 2>&1; then
+  echo "gh CLI required"; exit 2
+fi
+
+echo "Enabling branch protection on $REPO:$BRANCH (require reviews, require status checks)"
+gh api repos/${REPO}/branches/${BRANCH}/protection -X PUT -f - <<'JSON'
+{
+  "required_status_checks": {
+    "strict": true,
+    "contexts": ["build-and-push", "dr-test", "verifier-eval", "explainability"]
+  },
+  "required_pull_request_reviews": {
+    "dismiss_stale_reviews": true,
+    "require_code_owner_reviews": true,
+    "required_approving_review_count": 1
+  },
+  "enforce_admins": true,
+  "restrictions": null
+}
+JSON
+
+echo "Branch protection applied. Confirm in GitHub repo settings."
+
*** End Patch
*** Begin Patch
*** Add File:CODEOWNERS
+# Require ML platform owners to review changes to verifier thresholds and critical configs
+/.github/** @aegis/platform-team
+config/verifier_thresholds.yaml @aegis/platform-team
+k8s/** @aegis/platform-team
+ops/** @aegis/platform-team
+
*** End Patch
*** Begin Patch
*** Add File:k8s/verifier/hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: verifier-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: verifier
+  minReplicas: 2
+  maxReplicas: 8
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 55
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/enforce_verifier_hpa_on_merge.yml
+name: Apply Verifier HPA on Threshold Merge
+
+on:
+  pull_request:
+    types: [closed]
+
+jobs:
+  apply-hpa:
+    if: github.event.pull_request.merged == true
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Detect verifier thresholds change
+        id: detect
+        run: |
+          git fetch origin main:main
+          CHANGED=$(git diff --name-only HEAD origin/main | grep -E '^config/verifier_thresholds.yaml' || true)
+          echo "changed=$CHANGED" >> $GITHUB_OUTPUT
+      - name: Apply HPA if thresholds changed
+        if: steps.detect.outputs.changed != ''
+        env:
+          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}
+        run: |
+          kubectl --kubeconfig $KUBECONFIG -n aegis apply -f k8s/verifier/hpa.yaml
+          echo "Applied verifier HPA"
+
*** End Patch
*** Begin Patch
*** Add File:federated/secure_aggregation_design.md
+# Secure Aggregation Design (Federated)
+
+Overview
+- Use Flower (flwr) for orchestration and a secure aggregation library for model parameter aggregation (e.g., PySyft/Opacus + MPC).
+Key components
+- Client attestations: mTLS client certs issued by a CA to enroll clients.
+- Secure aggregation: use threshold-based secure aggregation so server cannot see individual updates.
+- Differential privacy: optional DP-SGD on client side to protect contributions.
+- Audit & consent: per-client consent records, retention & revocation flows.
+
+Next steps & references
+- Prototype with Flower + a secure aggregation library (research examples exist) and validate latency & throughput.
+- Implement client certificate issuance (Small CA + S3 distribution) and per-client quota.
+
*** End Patch
*** Begin Patch
*** Add File:rl/rl_orchestration_design.md
+# RL Orchestration Design (Distributed RL)
+
+Overview
+- For production RL: use Ray RLlib or a Ray + DeepSpeed hybrid for large policies.
+Key requirements
+- Checkpointing & reproducibility: sign checkpoints and record Rekor.
+- Safe rollout: shadow evaluation, off-policy evaluation and progressive canary deployment.
+- Autoscaling of actor/learner fleets and gang scheduling for synchronous updates.
+
+Next steps
+- Prototype RayCluster (Ray Operator) and RLlib experiment pipelines; integrate with Argo for workflow automation and MLflow for run logging.
+
*** End Patch
*** End Patch
