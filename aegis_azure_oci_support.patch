*** Begin Patch
*** Update File: scripts/storage_cp.sh
@@
 case "$CLOUD_PROVIDER" in
   "aws")
     case "$cmd" in
       cp) aws s3 cp "$src" "$dst" ;;
       sync) aws s3 sync "$src" "$dst" ;;
       *) echo "Unsupported cmd $cmd for aws" >&2; exit 3 ;;
     esac
     ;;
   "gcp")
     case "$cmd" in
       cp) gsutil cp -r "$src" "$dst" ;;
       sync) gsutil -m rsync -r "$src" "$dst" ;;
       *) echo "Unsupported cmd $cmd for gcp" >&2; exit 3 ;;
     esac
     ;;
+  "azure")
+    # Azure: uses az storage blob or azcopy for performance
+    case "$cmd" in
+      cp)
+        # dst example: https://<account>.blob.core.windows.net/<container>/<path> or wasabi-style gs:// replacement
+        if [[ "$dst" == gs://* || "$dst" == s3://* ]]; then
+          echo "Azure destination should be an https://...blob.core.windows.net/... URL or use az storage blob commands" >&2
+        fi
+        az storage blob upload --account-name "${AZURE_STORAGE_ACCOUNT:-}" --container-name "${AZURE_STORAGE_CONTAINER:-}" --name "${dst##*/}" --file "$src" --auth-mode login
+        ;;
+      sync)
+        az storage blob sync --account-name "${AZURE_STORAGE_ACCOUNT:-}" --container "${AZURE_STORAGE_CONTAINER:-}" --source "$src"
+        ;;
+      *) echo "Unsupported cmd $cmd for azure" >&2; exit 3 ;;
+    esac
+    ;;
+  "oci")
+    # OCI Object Storage: use oci CLI (oci os object put)
+    case "$cmd" in
+      cp)
+        # dst example: oci://<bucket>@<namespace>/<path>
+        if [[ "$dst" =~ ^oci:// ]]; then
+          # parse oci://bucket@namespace/path
+          tgt="${dst#oci://}"
+          bucket="${tgt%%@*}"
+          rest="${tgt#*@}"
+          namespace="${rest%%/*}"
+          objpath="${rest#*/}"
+          oci os object put --bucket-name "${bucket}" --namespace "${namespace}" --name "${objpath}" --file "$src" --force
+        else
+          oci os object put --bucket-name "${OCI_BUCKET_NAME:-}" --name "${dst##*/}" --file "$src" --namespace "${OCI_NAMESPACE:-}" --force
+        fi
+        ;;
+      sync)
+        echo "OCI sync not implemented; use oci os object put in scripts" >&2
+        exit 3
+        ;;
+      *) echo "Unsupported cmd $cmd for oci" >&2; exit 3 ;;
+    esac
+    ;;
   "ghcr")
     case "$cmd" in
       cp) gsutil cp -r "$src" "$dst" ;;
       sync) gsutil -m rsync -r "$src" "$dst" ;;
       *) echo "Unsupported cmd $cmd for gcp" >&2; exit 3 ;;
     esac
     ;;
   *)
     echo "Unsupported CLOUD_PROVIDER: $CLOUD_PROVIDER" >&2
     exit 4
     ;;
 esac
*** End Patch
*** Begin Patch
*** Update File: scripts/push_image_crosscloud.sh
@@
 case "$CLOUD_PROVIDER" in
   "aws")
@@
     ;;
   "gcp")
@@
     ;;
   "ghcr")
@@
     ;;
+  "azure")
+    # Azure ACR
+    if [ -z "$PROJECT" ]; then
+      echo "For Azure set PROJECT to subscription id or registry resource group" >&2
+      exit 2
+    fi
+    ACR_NAME="${ACR_NAME:-${REPO}}"
+    ACR_LOGIN_SERVER="$(az acr show -n ${ACR_NAME} -g ${PROJECT} --query loginServer -o tsv 2>/dev/null || true)"
+    if [ -z "$ACR_LOGIN_SERVER" ]; then
+      echo "ACR ${ACR_NAME} not found in resource group ${PROJECT}; create one or set ACR_NAME/PROJECT correctly" >&2
+      exit 3
+    fi
+    FULL="${ACR_LOGIN_SERVER}/${IMAGE}"
+    echo "Logging in to ACR: ${ACR_LOGIN_SERVER}"
+    az acr login --name "${ACR_NAME}"
+    docker build -t "${FULL}" -f "${DOCKERFILE}" .
+    docker push "${FULL}"
+    echo "${FULL}"
+    ;;
+  "oci")
+    # OCI Registry (OCIR). Requires oci CLI configured with tenancy/user/apikey or instance principal
+    if [ -z "$PROJECT" ]; then
+      echo "For OCI set PROJECT to your tenancy namespace (OCIR namespace) or set OCI_NAMESPACE env" >&2
+      exit 2
+    fi
+    OCIR_HOST="${OCI_REGISTRY_HOST:-iad.ocir.io}"
+    TENANCY_NS="${OCI_NAMESPACE:-$PROJECT}"
+    # Full name: iad.ocir.io/<tenancy_namespace>/<repo>/<image>
+    FULL="${OCIR_HOST}/${TENANCY_NS}/${REPO}/${IMAGE}"
+    echo "Logging in to OCIR (OCI CLI must be configured)"
+    # docker login will require an auth token (use user/password from OCI Console) or use oci auth helper
+    if command -v oci >/dev/null 2>&1; then
+      echo "OCI CLI present — ensure you have created auth token and logged in: docker login ${OCIR_HOST}"
+    fi
+    docker build -t "${FULL}" -f "${DOCKERFILE}" .
+    docker push "${FULL}"
+    echo "${FULL}"
+    ;;
   *)
     echo "Unsupported CLOUD_PROVIDER: ${CLOUD_PROVIDER}" >&2
     exit 3
     ;;
 esac
*** End Patch
*** Begin Patch
*** Add File: scripts/azure_enable_setup.sh
+#!/usr/bin/env bash
+#
+# Bootstrap script to enable Azure resources for Aegis:
+#  - create resource group
+#  - create storage account + blob container
+#  - create Azure Container Registry (ACR)
+#  - create service principal and assign roles for storage and ACR
+#
+# Usage:
+#   AZ_RG=my-rg AZ_REGION=eastus AZ_ACCOUNT=myacct AZ_ACR=myacr ./scripts/azure_enable_setup.sh
+set -euo pipefail
+
+AZ_RG="${AZ_RG:-aegis-rg}"
+AZ_REGION="${AZ_REGION:-eastus}"
+AZ_STORAGE_ACCOUNT="${AZ_STORAGE_ACCOUNT:-aegisstorage$((RANDOM%10000))}"
+AZ_STORAGE_CONTAINER="${AZ_STORAGE_CONTAINER:-model-artifacts}"
+AZ_ACR="${AZ_ACR:-aegisacr}"
+
+echo "Creating resource group: $AZ_RG (region: $AZ_REGION)"
+az group create -n "$AZ_RG" -l "$AZ_REGION"
+
+echo "Creating storage account: $AZ_STORAGE_ACCOUNT"
+az storage account create -n "$AZ_STORAGE_ACCOUNT" -g "$AZ_RG" -l "$AZ_REGION" --sku Standard_LRS
+
+echo "Creating blob container: $AZ_STORAGE_CONTAINER"
+AZ_KEY=$(az storage account keys list -n "$AZ_STORAGE_ACCOUNT" -g "$AZ_RG" --query '[0].value' -o tsv)
+az storage container create --account-name "$AZ_STORAGE_ACCOUNT" --name "$AZ_STORAGE_CONTAINER" --account-key "$AZ_KEY"
+
+echo "Creating Azure Container Registry: $AZ_ACR"
+az acr create -n "$AZ_ACR" -g "$AZ_RG" --sku Standard --location "$AZ_REGION"
+
+echo "Creating Service Principal for Aegis workloads"
+SP_NAME="${SP_NAME:-aegis-sp-$(date +%s)}"
+SP=$(az ad sp create-for-rbac --name "$SP_NAME" --role "Contributor" --scopes "/subscriptions/$(az account show --query id -o tsv)" )
+echo "Service principal created: $SP"
+echo "You should store the output JSON securely (clientId, clientSecret, tenant)"
+
+echo "Grant Storage Blob Data Contributor role to the service principal for the storage account"
+SP_APPID=$(echo "$SP" | jq -r .appId)
+az role assignment create --assignee-object-id "$(az ad sp show --id "$SP_APPID" --query objectId -o tsv)" --role "Storage Blob Data Contributor" --scope "$(az storage account show -n $AZ_STORAGE_ACCOUNT -g $AZ_RG --query id -o tsv)"
+
+cat <<EOF
+Next steps:
+ - Store SP credentials in Kubernetes secret: kubectl create secret generic azure-sp -n aegis --from-literal=clientId=... --from-literal=clientSecret=... --from-literal=tenant=...
+ - Use az acr login to push images: az acr login --name $AZ_ACR
+ - Use environment variables AZ_STORAGE_ACCOUNT and AZ_STORAGE_CONTAINER for scripts/storage_cp.sh to upload blobs
+EOF
+
*** End Patch
*** Begin Patch
*** Add File: infra/azure/terraform_aks_gpu.tf
+/*
+Terraform skeleton to create an AKS cluster with a GPU-enabled node pool (labels/taints included).
+Adaptable starting point — ensure proper subscription, networking and RBAC values per your org.
+*/
+terraform {
+  required_providers {
+    azurerm = {
+      source  = "hashicorp/azurerm"
+      version = ">= 3.0"
+    }
+  }
+}
+
+provider "azurerm" {
+  features {}
+}
+
+variable "prefix" { default = "aegis" }
+variable "location" { default = "eastus" }
+variable "resource_group_name" { default = "${var.prefix}-rg" }
+
+resource "azurerm_resource_group" "rg" {
+  name     = var.resource_group_name
+  location = var.location
+}
+
+resource "azurerm_kubernetes_cluster" "aks" {
+  name                = "${var.prefix}-aks"
+  location            = azurerm_resource_group.rg.location
+  resource_group_name = azurerm_resource_group.rg.name
+  dns_prefix          = "${var.prefix}-dns"
+
+  default_node_pool {
+    name       = "default"
+    node_count = 1
+    vm_size    = "Standard_DS2_v2"
+  }
+
+  identity {
+    type = "SystemAssigned"
+  }
+
+  addon_profile {
+    oms_agent {
+      enabled = false
+    }
+  }
+}
+
+# GPU node pool (virtual node pool requires specialized VM size)
+resource "azurerm_kubernetes_cluster_node_pool" "gpu_pool" {
+  name                = "gpu"
+  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks.id
+  vm_size             = "Standard_NC6" # adjust for region / quota
+  node_count          = 0
+  min_count           = 0
+  max_count           = 2
+  enable_auto_scaling = true
+  node_labels = {
+    "aegis.cloud/gpu" = "true"
+  }
+  node_taints = ["aegis.cloud/gpu=true:NoSchedule"]
+}
+
*** End Patch
*** Begin Patch
*** Add File: scripts/azure_wi_bind.sh
+#!/usr/bin/env bash
+#
+# Bind a Kubernetes ServiceAccount to an Azure AD workload identity for AKS (Workload Identity on AKS).
+# This script assumes the AKS cluster was created with OIDC issuer enabled and managed identity.
+#
+# Usage:
+#  AKS_RG=my-rg AKS_NAME=my-aks NAMESPACE=aegis K8S_SA=aegis-trainer AZ_SP_CLIENT_ID=<client-id> ./scripts/azure_wi_bind.sh
+set -euo pipefail
+
+AKS_RG="${AKS_RG:-}"
+AKS_NAME="${AKS_NAME:-}"
+NAMESPACE="${NAMESPACE:-aegis}"
+K8S_SA="${K8S_SA:-aegis-trainer}"
+AZ_SP_CLIENT_ID="${AZ_SP_CLIENT_ID:-}" # Application (client) ID of the Azure AD app (service principal)
+
+if [ -z "$AKS_RG" ] || [ -z "$AKS_NAME" ] || [ -z "$AZ_SP_CLIENT_ID" ]; then
+  echo "Set AKS_RG, AKS_NAME, and AZ_SP_CLIENT_ID (appId of service principal)" >&2
+  exit 2
+fi
+
+echo "Getting AKS OIDC issuer URL"
+OIDC_ISSUER=$(az aks show -g "$AKS_RG" -n "$AKS_NAME" --query addonProfiles.aciConnectorLinux -o json 2>/dev/null || true)
+# Note: The method to retrieve OIDC issuer may vary; AKS supports workload identity mapping via Azure AD
+echo "Assuming AKS is configured for Workload Identity; annotate k8s SA with federated identity binding"
+
+kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
+kubectl create serviceaccount -n "$NAMESPACE" "$K8S_SA" --dry-run=client -o yaml | kubectl apply -f -
+
+echo "Create federated identity credential in Azure AD App (requires az cli and proper permissions)"
+AZ_APP_OBJECT_ID=$(az ad app list --filter "appId eq '$AZ_SP_CLIENT_ID'" --query [].id -o tsv)
+if [ -z "$AZ_APP_OBJECT_ID" ]; then
+  echo "Azure AD app not found for client id $AZ_SP_CLIENT_ID" >&2
+  exit 3
+fi
+
+echo "Note: Creating federated credentials via az cli requires preview features; consult Azure docs for Workload Identity on AKS."
+echo "Please follow Azure AKS Workload Identity guide: https://learn.microsoft.com/azure/aks/workload-identity-overview"
+
+kubectl annotate serviceaccount -n "$NAMESPACE" "$K8S_SA" "azure.workload.identity/client-id=$AZ_SP_CLIENT_ID" --overwrite || true
+echo "Annotated k8s SA $K8S_SA in ns $NAMESPACE. Follow Azure docs to create federated identity credential mapping in AAD."
+
*** End Patch
*** Begin Patch
*** Add File: scripts/oci_enable_setup.sh
+#!/usr/bin/env bash
+#
+# Bootstrap script to create OCI Object Storage bucket and provide guidance for OCIR setup.
+# Requires OCI CLI configured with user credentials or instance principal.
+#
+# Usage:
+#  OCI_NAMESPACE=mytenancy OCI_BUCKET=mybucket ./scripts/oci_enable_setup.sh
+set -euo pipefail
+
+OCI_NAMESPACE="${OCI_NAMESPACE:-}"
+OCI_BUCKET="${OCI_BUCKET:-aegis-model-bucket}"
+OCI_COMPARTMENT_ID="${OCI_COMPARTMENT_ID:-}"
+
+if [ -z "$OCI_NAMESPACE" ] || [ -z "$OCI_COMPARTMENT_ID" ]; then
+  echo "Set OCI_NAMESPACE and OCI_COMPARTMENT_ID environment variables (see OCI tenancy and compartment)" >&2
+  exit 2
+fi
+
+echo "Creating bucket ${OCI_BUCKET} in namespace ${OCI_NAMESPACE} (compartment ${OCI_COMPARTMENT_ID})"
+oci os bucket create --name "${OCI_BUCKET}" --compartment-id "${OCI_COMPARTMENT_ID}" --public-access-type None --storage-tier Standard --namespace-name "${OCI_NAMESPACE}"
+
+echo "OCI bucket created. To upload objects use:"
+echo "  oci os object put --bucket-name ${OCI_BUCKET} --name artifact.tar.gz --file artifact.tar.gz --namespace ${OCI_NAMESPACE}"
+
+echo "OCIR (Oracle Cloud Infrastructure Registry) guidance:"
+echo " - Create a repository/namespace in OCIR and generate an auth token for Docker login (User -> Auth Tokens)."
+echo " - Login: docker login iad.ocir.io -u '<tenancy_namespace>/<username>' -p '<auth_token>'"
+
*** End Patch
*** Begin Patch
*** Add File: infra/oci/terraform_oke_gpu.tf
+/*
+Terraform skeleton for OCI Container Engine for Kubernetes (OKE) with GPU node pool.
+Adapt to your tenancy and compartment. This is a starting point for OKE + BM.GPU shapes.
+*/
+terraform {
+  required_providers {
+    oci = {
+      source  = "hashicorp/oci"
+      version = ">= 4.0"
+    }
+  }
+}
+
+provider "oci" {
+  tenancy_ocid     = var.tenancy_ocid
+  user_ocid        = var.user_ocid
+  fingerprint      = var.fingerprint
+  private_key_path = var.private_key_path
+  region           = var.region
+}
+
+variable "compartment_ocid" {}
+variable "vcn_id" {}
+variable "subnet_ids" { type = list(string) }
+
+# OKE cluster and GPU node pool omitted details; replace with org-specific networking and shape info.
+resource "oci_containerengine_cluster" "aegis" {
+  name           = var.cluster_name
+  compartment_id = var.compartment_ocid
+  vcn_id         = var.vcn_id
+  kubernetes_version = "v1.26.3"
+}
+
+resource "oci_containerengine_node_pool" "gpu_pool" {
+  compartment_id = var.compartment_ocid
+  cluster_id     = oci_containerengine_cluster.aegis.id
+  name           = "${var.cluster_name}-gpu"
+  kubernetes_version = oci_containerengine_cluster.aegis.kubernetes_version
+  node_shape     = var.gpu_shape # e.g., "BM.GPU2.2"
+  node_config_details {
+    size = 1
+    subnet_id = var.subnet_ids[0]
+  }
+  # Add labels/taints via node pool options or DaemonSet bootstrap
+}
+
*** End Patch
*** Begin Patch
*** Add File: docs/AZURE_OCI_README.md
+````markdown
+name=docs/AZURE_OCI_README.md
+
+# Azure & OCI Support for Aegis
+
+This patch adds scripts, Terraform skeletons and CI helpers to enable Aegis to run on Azure (AKS + ACR + Blob Storage) and OCI (OKE + OCIR + Object Storage).
+
+Included artifacts
+- scripts/storage_cp.sh: extended to support azure (az storage blob) and oci (oci os object put) destinations.
+- scripts/push_image_crosscloud.sh: extended to support pushing to ACR and OCIR.
+- scripts/azure_enable_setup.sh: create resource group, storage account, container and ACR; create service principal and assign roles.
+- infra/azure/terraform_aks_gpu.tf: Terraform skeleton to provision AKS with a GPU node pool (labels/taints included).
+- scripts/azure_wi_bind.sh: helper and guidance to bind Kubernetes ServiceAccount to Azure AD identity (Workload Identity on AKS).
+- scripts/oci_enable_setup.sh: create OCI object storage bucket and guidance for OCIR usage.
+- infra/oci/terraform_oke_gpu.tf: Terraform skeleton for OKE + GPU node pool.
+
+Quickstart (Azure)
+1. Create resource group, storage and ACR:
+   AZ_RG=my-rg AZ_REGION=eastus AZ_STORAGE_ACCOUNT=myacct AZ_ACR=myacr ./scripts/azure_enable_setup.sh
+2. Create Kubernetes cluster with Terraform (infra/azure/terraform_aks_gpu.tf) or via portal/az cli.
+3. Use scripts/push_image_crosscloud.sh with CLOUD_PROVIDER=azure to push images to ACR (set AZ_RG/AZ_ACR/PROJECT accordingly).
+4. Upload artifacts via scripts/storage_cp.sh (set CLOUD_PROVIDER=azure and AZURE_STORAGE_ACCOUNT/AZURE_STORAGE_CONTAINER).
+5. Map k8s SA to Azure identity (see scripts/azure_wi_bind.sh and Azure docs).
+
+Quickstart (OCI)
+1. Create bucket and configure OCIR:
+   OCI_NAMESPACE=mytenancy OCI_COMPARTMENT_ID=ocid1.compartment... ./scripts/oci_enable_setup.sh
+2. Build and push images to OCIR using scripts/push_image_crosscloud.sh with CLOUD_PROVIDER=oci (ensure docker login with OCIR auth token).
+3. Upload artifacts via scripts/storage_cp.sh with CLOUD_PROVIDER=oci or use oci os object put directly.
+4. Provision OKE cluster and GPU node pool (infra/oci/terraform_oke_gpu.tf).
+
+Notes & Caveats
+- Workload identity mapping differs per cloud. Prefer platform-native identity (IRSA/Workload Identity/Azure AD Workload Identity) instead of mounting long-lived keys.
+- GPU shapes and availability vary by region; adjust Terraform and VM sizes accordingly.
+- TPUs remain GCP-specific — for Azure & OCI use GPUs.
+- Validate prometheus scraping and DCGM exporter per cluster; prometheus-adapter may need tuning for each cloud.
+
+````

*** End Patch
*** End Patchae
