*** Begin Patch
*** Add File:productization/case_studies/SAMPLE_CASE_STUDY_TEMPLATE.md
+Title: <Customer Name> — <Solution Title>
+========================================
+
+Executive summary
+- Problem: (one sentence)
+- Solution: (one sentence)
+- Outcome: (top-line metric: e.g., reduced inference cost by 40%, improved accuracy by X points)
+
+Context & challenge
+- Industry & compliance constraints
+- Existing infra & scale
+- Stakeholders and success metrics
+
+Our approach
+1. Discovery & data readiness
+2. Pilot design & sandboxing
+3. Model development (algorithms, datasets)
+4. Governance: signing, admission, audit
+5. Productionization: managed backends, autoscale, DR
+
+Technical architecture (diagram placeholder)
+- Components used: lakeFS, Feast, MLflow, Vault, KServe/Triton, Agent Manager, Rekor
+
+Implementation details
+- Data pipeline: sources, privacy/consent controls, retention
+- Model training: infra (multi-GPU/HPC / spot handling), checkpointing
+- Serving: KServe/Triton config, batching, SLOs
+- Governance: MLflow promote → Vault sign → Rekor entry timestamps
+
+Results & metrics
+- Pre/post metrics: latency, accuracy/F1, cost per 1M requests, SLO attainment
+- Time-to-first-inference (staging) and time-to-production
+
+Lessons learned & recommendations
+- Key risks encountered, mitigations, follow-ups
+
+Appendix
+- Rekor entries (ids)
+- Terraform/helm diff snippets
+- Runbook references
+
+How to use: copy this template and fill sections with customer data. Produce a short one‑page executive summary derived from this for sales collateral.
+
*** End Patch
*** Begin Patch
*** Add File:productization/case_studies/CASE_STUDY_EXAMPLES.md
+Two short example case studies (redacted)
+
+1) Retail Recommendation Pilot
+- Problem: Cold-start personalization causing <1% click-through.
+- Solution: Onboarded dataset to lakeFS, built hybrid RAG + collaborative filter model, deployed with KServe and Feast for online features. Promoted models via MLflow + cosign. Implemented KEDA autoscale and spot checkpointing.
+- Outcome: CTR +1.8% in 30 days; inference p95 latency 210ms < 300ms SLO; cost per inference down 25% using spot training.
+
+2) Financial Document Understanding (Regulated)
+- Problem: Need auditable model lifecycle for document triage under compliance.
+- Solution: Hosted model on dedicated VPC, strict RBAC, Vault transit signing, Rekor evidence log for each promotion, OPA rules for tool usage, offline Rekor bundles for auditors.
+- Outcome: Audit readiness proven (internal audit), 0 production drift incidents in first 90 days, RTO validated at 1 hour in DR drill dry-run.
+
*** End Patch
*** Begin Patch
*** Add File:legal/MSA_FINAL_REVIEW_CHECKLIST.md
+MSA Final Review Checklist
+- Replace placeholders (company names, addresses) and have legal counsel review:
+  - Data ownership and IP clauses
+  - Liability cap — align with company policy
+  - Data handling and retention obligations
+  - Security requirements (annual pentest, incident notification SLAs)
+  - Subprocessor list and cloud provider responsibilities
+  - Support & escalation, SLA credits
+  - Termination & data return/destruction
+
+Action: include final MSA version in repo/legal and add signed date stamp on handoff.
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/README.md
+Hosted learner sandboxes — design notes & runbooks
+
+Goal:
+- Provide ephemeral, cost-bounded sandboxes for training attendees that are reproducible and isolated.
+
+Architectural options:
+- Local (k3d/kind) — cheap for small cohorts
+- Cloud ephemeral (AWS/GCP/Azure) — scale for large cohorts; use Terraform templates with budget alarms and automatic destroy after TTL.
+
+Provisioning flow (cloud):
+1. Student requests sandbox via UI or CLI (creates ticket with TTL, e.g., 24h)
+2. Provision VPC + EKS/GKE/AKS minimal cluster using terraform/managed/sandbox module (tfvars per student)
+3. Install minimal components: cert-manager, ingress, MLflow (demo), JupyterHub (optional)
+4. Run preflight tests and return kubeconfig with expiry metadata
+5. When TTL expires, auto-destroy via scheduled job and snapshot logs/artifacts to central storage
+
+Security:
+- Limit network egress via NetworkPolicies; block cloud console access; use per-sandbox service accounts.
+
*** End Patch
*** Begin Patch
*** Add File:training/hosted_sandbox/terraform_sandbox_module/variables.tf
+variable "student_id" { type = string }
+variable "ttl_hours" { type = number, default = 24 }
+variable "region" { type = string, default = "us-east-1" }
+variable "cluster_name" { type = string, default = "aegis-sandbox-${var.student_id}" }
+variable "instance_type" { type = string, default = "t3.medium" }
+
*** End Patch
*** Begin Patch
*** Add File:training/auto_grader/grader.py
+#!/usr/bin/env python3
+"""
+Notebook grader for training labs.
+- Runs Jupyter notebooks in a sandbox, extracts expected output cells, and scores based on asserts/metrics.
+Requires: nbformat, nbclient
+Usage:
+  python grader.py <notebook.ipynb> --expect <expected.json>
+Expected.json format:
+  {"cells": [{"id": <cell_index>, "contains": "expected string"}, ...], "metrics": {"accuracy": 0.85}}
+"""
+import json, sys, argparse
+import nbformat
+from nbclient import NotebookClient, CellExecutionError
+
+def grade_notebook(nb_path, expect_path):
+    nb = nbformat.read(nb_path, as_version=4)
+    client = NotebookClient(nb, timeout=600, kernel_name='python3')
+    try:
+        client.execute()
+    except CellExecutionError as e:
+        print("Notebook execution error:", e)
+        return {"ok": False, "error": str(e)}
+    outputs = []
+    for i,cell in enumerate(nb.cells):
+        if cell.cell_type == 'code':
+            outputs.append(cell.get('outputs', []))
+    expected = json.load(open(expect_path))
+    score = 0
+    total = 0
+    for ex in expected.get("cells", []):
+        total += 1
+        idx = ex.get("id")
+        contains = ex.get("contains")
+        outtexts = []
+        if idx < len(outputs):
+            for o in outputs[idx]:
+                if 'text' in o:
+                    outtexts.append(o['text'])
+        joined = "\n".join(outtexts)
+        if contains in joined:
+            score += 1
+    metric_score = score / max(1,total)
+    # metric checks (optional)
+    metrics_ok = True
+    for k,v in expected.get("metrics", {}).items():
+        # naive: look for print("metric: <k> = <val>") in outputs
+        metric_found = False
+        for o in outputs:
+            for item in o:
+                if 'text' in item and f"{k}" in item['text']:
+                    metric_found = True
+        # not implementing numeric checks here; trainers may extend
+    return {"ok": True, "score": metric_score}
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("notebook")
+    parser.add_argument("--expect", required=True)
+    args = parser.parse_args()
+    print(json.dumps(grade_notebook(args.notebook, args.expect), indent=2))
+
*** End Patch
*** Begin Patch
*** Add File:training/slide_decks/datasci_slides.md
+# Data Scientist Workshop (slides)
+
+## Slide: Why reproducibility matters
+- Bullet: reproducible experiments, auditability, model drift
+
+## Slide: lakeFS for datasets
+- How snapshotting enables exact training reproducibility
+
+## Slide: MLflow + Model Cards + Signing
+- How to track experiments and sign promoted artifacts
+
+## Labs
+- Train sample model, log to MLflow, promote & verify Rekor
+
+Notes: Export to reveal.js for presentation or convert to pptx using pandoc if needed.
+
*** End Patch
*** Begin Patch
*** Add File:infra/preflight/terraform_preflight.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Validate terraform modules and run a safe plan (no apply). Fails if sensitive placeholders remain.
+
+DIR=${1:-terraform/managed}
+PLACEHOLDER_PATTERN="REPLACE|<REPLACE|YOUR_"
+echo "Running terraform fmt & validate in ${DIR}"
+find "$DIR" -name '*.tf' -print0 | xargs -0 -n1 dirname | sort -u | while read d; do
+  (cd "$d" && terraform fmt -check) || true
+  (cd "$d" && terraform init -backend=false -input=false) || true
+  (cd "$d" && terraform validate) || true
+done
+
+echo "Scanning for placeholder values (pattern: $PLACEHOLDER_PATTERN)..."
+if grep -R --exclude-dir=.terraform -E "$PLACEHOLDER_PATTERN" $DIR | grep -v -E "README|example"; then
+  echo "Found placeholder values. Replace before applying."
+  exit 2
+fi
+
+echo "Performing terraform plan (non-destructive):"
+for d in $(find "$DIR" -maxdepth 2 -type d); do
+  if [ -f "$d/main.tf" ] || [ -f "$d/elasticache.tf" ]; then
+    pushd "$d" >/dev/null
+    terraform plan -input=false -no-color -out=plan.out || true
+    popd >/dev/null
+  fi
+done
+
+echo "Preflight checks completed. Review plans before apply."
+
*** End Patch
*** Begin Patch
*** Add File:infra/validation/failover_validate.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Validate managed backend failover behavior post-migration.
+# Requires: REDIS_HOST, LAKEFS_BUCKET env vars
+
+REDIS_HOST=${REDIS_HOST:-}
+LAKEFS_BUCKET=${LAKEFS_BUCKET:-}
+
+if [ -z "$REDIS_HOST" ] || [ -z "$LAKEFS_BUCKET" ]; then
+  echo "Set REDIS_HOST and LAKEFS_BUCKET before running."
+  exit 1
+fi
+
+echo "Writing verification key to Redis / Feast online store"
+TMP_KEY="aegis-validate-$(date +%s)"
+redis-cli -h ${REDIS_HOST%:*} -p ${REDIS_HOST##*:} set $TMP_KEY "ok" || true
+
+echo "Checking lakeFS bucket listing"
+aws s3 ls "s3://${LAKEFS_BUCKET}" --recursive | head -n 10
+
+echo "Prompt operator to initiate failover now (cloud console or api). Press Enter when completed."
+read -p "Proceed when failover initiated..."
+
+echo "Re-checking Redis key"
+val=$(redis-cli -h ${REDIS_HOST%:*} -p ${REDIS_HOST##*:} get $TMP_KEY || true)
+echo "Value: $val"
+if [ "$val" = "ok" ]; then
+  echo "Failover validation passed"
+else
+  echo "Failover validation failed — investigate network/endpoint/role permissions"
+  exit 2
+fi
+
*** End Patch
*** Begin Patch
*** Add File:rl/rlhf/argo_rlhf_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: aegis-rlhf-
+spec:
+  entrypoint: rl-pipeline
+  templates:
+  - name: rl-pipeline
+    steps:
+    - - name: collect-feedback
+        template: collect-feedback
+    - - name: train-reward-model
+        template: train-reward
+    - - name: sft-train
+        template: sft-train
+    - - name: ppo-train
+        template: ppo-train
+    - - name: validate-and-promote
+        template: validate-promote
+
+  - name: collect-feedback
+    container:
+      image: aegis/rl-tools:latest
+      command: ["/bin/sh","-c"]
+      args: ["python /opt/rl/collect_feedback.py --out /tmp/feedback.json"]
+
+  - name: train-reward
+    container:
+      image: aegis/rl-tools:latest
+      args: ["python","/opt/rl/train_reward.py","--input","/tmp/feedback.json","--out","/tmp/reward_model.pt"]
+
+  - name: sft-train
+    container:
+      image: aegis/rl-tools:latest
+      args: ["python","/opt/rl/sft_train.py","--reward","/tmp/reward_model.pt","--out","/tmp/sft_model.pt"]
+
+  - name: ppo-train
+    container:
+      image: aegis/rl-tools:latest
+      args: ["python","/opt/rl/ppo_train.py","--sft","/tmp/sft_model.pt","--out","/tmp/ppo_model.pt"]
+
+  - name: validate-promote
+    container:
+      image: aegis/release-tools:latest
+      args: ["sh","-c","python /opt/release/validate_model.py --model /tmp/ppo_model.pt && python /opt/release/promote_and_sign.py --model /tmp/ppo_model.pt"]
+
+  ttlStrategy:
+    secondsAfterCompletion: 86400
+
*** End Patch
*** Begin Patch
*** Add File:rl/rlhf/collect_feedback.py
+#!/usr/bin/env python3
+"""
+Collect human preference data scaffold.
+- For demo: reads CSV of (prompt, response_a, response_b, preferred) and writes JSON for reward training.
+Operator must replace with real collection endpoints (App UI, annotation tool).
+"""
+import csv, json, sys
+
+def convert(csv_in, json_out):
+    rows=[]
+    with open(csv_in) as fh:
+        rdr=csv.DictReader(fh)
+        for r in rdr:
+            rows.append({"prompt": r["prompt"], "a": r["a"], "b": r["b"], "pref": int(r.get("pref",0))})
+    with open(json_out,"w") as fh:
+        json.dump(rows, fh)
+
+if __name__=="__main__":
+    if len(sys.argv)!=3:
+        print("Usage: collect_feedback.py <in.csv> <out.json>")
+        sys.exit(1)
+    convert(sys.argv[1], sys.argv[2])
+
*** End Patch
*** Begin Patch
*** Add File:verticals/customer_support/template/README.md
+Customer Support Solution Template
+- Components:
+  - RAG retrieval pipeline (ingest docs into lakeFS, embed to Qdrant)
+  - Model training / fine-tune or HF import
+  - Serving via KServe -> tool-runner for external connectors (CRM)
+  - Feature store for user signals (Feast)
+  - Governance: MLflow -> sign -> Rekor
+
+Privacy checklist:
+ - PII classification for docs, redaction steps, consent flags
+ - Access logs enabled for dataset stores (S3 + lakeFS)
+ - Data retention policy enforced via lifecycle rules
+
*** End Patch
*** Begin Patch
*** Add File:verticals/cv/template/README.md
+Computer Vision Template
+- Components:
+  - Data pipeline: image ingestion -> lakeFS + versioned manifests (labels)
+  - Training: multi-GPU DeepSpeed examples and checkpointing to S3
+  - Serving: Triton model repository + KServe wrapper
+  - Metrics: confusion matrix logging and drift detection
+
+Compliance:
+ - Face-identification: add opt-out & legal checkboxes; maintain audit logs for inferences
+
*** End Patch
*** Begin Patch
*** Add File:verticals/recommender/template/README.md
+Recommender Template
+- Components:
+  - Feature pipelines in Feast (user/item features)
+  - Online store on ElastiCache; offline store in Bigtable
+  - Training: batched CTR models with incremental retrain
+  - A/B framework for rollout and SLO gating
+
+Performance tips:
+ - Use Volcano for gang scheduling; checkpointing to S3 for spot recovery
+
*** End Patch
*** Begin Patch
*** Add File:compliance/soc2/EVIDENCE_MAPPING.md
+SOC2 Evidence Mapping (sample)
+
+Control area -> Evidence artifact (stored in compliance/evidence/)
+- Access controls -> RBAC manifests, OIDC trust, GH org access logs
+- Change management -> Git history, PR approvals, CI run logs
+- Logging & monitoring -> Prometheus metrics, Grafana dashboards, SIEM ingestion logs
+- Backup & DR -> Vault snapshots, lakeFS backup prefixes, drill reports
+- Incident response -> Incident report templates & postmortems
+- Vendor & pentest -> pentest reports, remediation tracker
+
+Action: collect artifacts into a versioned store (lakeFS or dedicated bucket) and produce SOC2 pack.
+
*** End Patch
*** Begin Patch
*** Add File:managed/ops/ops_runbook_staffing.md
+Managed Operations — Runbook & Staffing
+
+Core roles:
+- Platform SRE (1 FTE): platform health, infra, DR, patching
+- Security Engineer (0.5 FTE): pentest triage, SIEM alerts
+- DevOps/Build (0.5 FTE): CI pipelines, image promotions
+- Customer Success (0.5 FTE): onboarding, SLAs
+
+On-call rotation:
+- 24/7 P1 on-call shared between Platform SRE and Security Engineer (follow escalation)
+
+Operational tasks:
+- Weekly patch window, monthly DR drill, quarterly pentest engagement
+- Monthly cost review & optimization
+
*** End Patch
*** Begin Patch
*** Add File:pilots/pilot_metrics/recording_rules.yaml
+groups:
+  - name: aegis-pilot-metrics
+    rules:
+      - record: pilot:model_p95_latency
+        expr: histogram_quantile(0.95, sum(rate(inference_request_duration_seconds_bucket[5m])) by (le,model))
+      - record: pilot:model_request_count
+        expr: sum(rate(inference_requests_total[5m])) by (model)
+      - alert: PilotModelSloBreach
+        expr: pilot:model_p95_latency > 0.5
+        for: 5m
+        labels:
+          severity: critical
+
*** End Patch
*** Begin Patch
*** Add File:pilots/pilot_metrics/README.md
+Pilot metrics & KPI dashboard
+
+Suggested KPIs:
+- Time-to-first-inference (staging)
+- P95 latency per model
+- Production model promotion count (per week)
+- Audit trail completeness (percentage of promotions with Rekor entries)
+- DR drill RTO and RPO
+
+Use the provided recording rules and connect to Grafana for a dashboard. Create an executive view showing the KPIs above for customer signoff.
+
*** End Patch
*** Begin Patch
*** Add File:tests/regulatory/dataset_pipeline_check.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Lightweight checks to ensure dataset pipeline includes required controls for regulated data.
+DATA_BUCKET=${1:-aegis-lakefs-staging}
+
+echo "Checking S3 server-side encryption and access logging for bucket: $DATA_BUCKET"
+aws s3api get-bucket-encryption --bucket "$DATA_BUCKET" || { echo "No SSE configured"; exit 2; }
+aws s3api get-bucket-logging --bucket "$DATA_BUCKET" || echo "No access logging configured (recommend enabling)"
+
+echo "Checking lakeFS metadata for consent flags (sample check)"
+# operator must implement consent metadata schema; here we check for sample file existence
+aws s3 ls "s3://${DATA_BUCKET}/metadata/consent/" || echo "No consent metadata found (manual review required)"
+
+echo "Dataset pipeline compliance checks completed (manual review recommended for sensitive data)."
+
*** End Patch
*** Begin Patch
*** Add File:docs/marketing/case_study_one_pager_template.md
+Case Study One-Pager (customer-facing)
+
+Header: Customer name, short descriptor, logo
+Problem:
+- Short paragraph
+Solution:
+- Bullet list of major features deployed
+Impact:
+- Top-line metrics with visual (e.g., bar or percent)
+Quote:
+- <Customer Exec Quote Placeholder>
+CTA:
+- "Schedule a pilot" + contact
+
*** End Patch
*** Begin Patch
*** Add File:automation/cleanup/auto_destroy_sandbox.sh
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# Auto-destroy sandboxes older than TTL (example using AWS tag 'aegis:sandbox-ttl')
+DAYS=${1:-1}
+echo "Finding sandbox stacks older than $DAYS days (placeholder - implement per provider)"
+echo "This script is a template: implement provider-specific listing and deletion (CloudFormation/terraform state)."
+
*** End Patch
*** Begin Patch
*** Add File:docs/guides/execute_full_hardening_checklist.md
+# Execute Full Hardening Checklist
+
+Step 1: Preflight terraform checks
+  - ./infra/preflight/terraform_preflight.sh
+Step 2: Tenant wiring (run aws_tenant_commands.sh locally)
+Step 3: Build & push images via .github/workflows/build_scan_push.yml (dispatch)
+Step 4: Apply core infra in staging (terraform apply) and deploy helm charts
+Step 5: Run failover_validate.sh to validate managed backends
+Step 6: Run pentest bundle generation and handoff (prod/security/pentest/prepare_pentest_bundle.sh)
+Step 7: Run DR drill (dry) using prod/reliability/drill/drill_runner.sh
+Step 8: Execute RLHF Argo workflow (submit argo/rlhf/argo_rlhf_workflow.yaml)
+Step 9: Run tests/regulatory/dataset_pipeline_check.sh against data bucket
+Step 10: Verify acceptance criteria in runbooks/hardening_acceptance_criteria.md
+
+Notes: follow change control and maintenance window policies; do not run live restores without SRE approval.
+
*** End Patch
*** End Patch
