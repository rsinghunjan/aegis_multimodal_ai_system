*** Begin Patch
*** Add File: billing/job_ledger.py
+#!/usr/bin/env python3
+"""
+Per-job ledger helpers.
+ - Record per-job economic and carbon metadata
+ - Log ledger entry to MLflow run (if MLFLOW_RUN_ID / MLFLOW_TRACKING_URI available)
+ - Optionally write to central billing DB (SQLAlchemy via DATABASE_URL)
+
+Usage:
+  python billing/job_ledger.py --job-id <id> --tenant <tenant> --provider <p> --cost 1.23 --runtime-secs 120 --device-power-w 50 --region us-east-1
+"""
+import os
+import json
+import argparse
+from datetime import datetime
+
+try:
+    import mlflow
+except Exception:
+    mlflow = None
+
+try:
+    from sqlalchemy import create_engine, Table, Column, Integer, String, Float, MetaData, DateTime, insert
+except Exception:
+    create_engine = None
+
+from carbon.energy_estimator import estimate_kgco2e
+
+DEFAULT_DB = os.environ.get("DATABASE_URL", "sqlite:///billing/job_ledger.db")
+
+def mk_entry(job_id, tenant, provider, cost_usd, runtime_secs, device_power_w, region, extra=None):
+    timestamp = datetime.utcnow().isoformat()
+    kwh = (runtime_secs * device_power_w) / 3600.0
+    kgco2e = estimate_kgco2e(kwh, region)
+    entry = {
+        "job_id": job_id,
+        "tenant": tenant,
+        "provider": provider,
+        "cost_usd": float(cost_usd),
+        "runtime_secs": float(runtime_secs),
+        "device_power_w": float(device_power_w),
+        "kwh": float(kwh),
+        "kgco2e": float(kgco2e),
+        "region": region,
+        "created_at": timestamp,
+        "extra": extra or {}
+    }
+    return entry
+
+def log_to_mlflow(entry):
+    if not mlflow:
+        print("mlflow not installed; skipping mlflow logging")
+        return
+    run_id = os.environ.get("MLFLOW_RUN_ID")
+    mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI")
+    if mlflow_uri:
+        mlflow.set_tracking_uri(mlflow_uri)
+    if run_id:
+        try:
+            mlflow.log_dict(entry, f"job_ledger/{entry['job_id']}.json")
+            print("Logged ledger to MLflow for run", run_id)
+        except Exception as e:
+            print("Failed to log to MLflow:", e)
+    else:
+        print("MLFLOW_RUN_ID not set; skipping per-run MLflow attach; consider logging to default experiment")
+
+def write_to_db(entry, db_url=DEFAULT_DB):
+    if create_engine is None:
+        print("sqlalchemy not installed; skipping DB write")
+        return
+    engine = create_engine(db_url, echo=False, future=True)
+    meta = MetaData()
+    ledger = Table(
+        "job_ledger",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("job_id", String(128), nullable=False),
+        Column("tenant", String(128)),
+        Column("provider", String(128)),
+        Column("cost_usd", Float),
+        Column("runtime_secs", Float),
+        Column("device_power_w", Float),
+        Column("kwh", Float),
+        Column("kgco2e", Float),
+        Column("region", String(64)),
+        Column("created_at", DateTime),
+        Column("extra", String(4096)),
+    )
+    meta.create_all(engine)
+    with engine.connect() as conn:
+        stmt = insert(ledger).values(
+            job_id=entry["job_id"],
+            tenant=entry["tenant"],
+            provider=entry["provider"],
+            cost_usd=entry["cost_usd"],
+            runtime_secs=entry["runtime_secs"],
+            device_power_w=entry["device_power_w"],
+            kwh=entry["kwh"],
+            kgco2e=entry["kgco2e"],
+            region=entry["region"],
+            created_at=datetime.fromisoformat(entry["created_at"]),
+            extra=json.dumps(entry.get("extra", {}))
+        )
+        conn.execute(stmt)
+        conn.commit()
+    print("Wrote ledger entry to DB", db_url)
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--job-id", required=True)
+    p.add_argument("--tenant", required=True)
+    p.add_argument("--provider", required=True)
+    p.add_argument("--cost", type=float, required=True)
+    p.add_argument("--runtime-secs", type=float, required=True)
+    p.add_argument("--device-power-w", type=float, required=True)
+    p.add_argument("--region", default="unknown")
+    p.add_argument("--write-db", action="store_true")
+    args = p.parse_args()
+    entry = mk_entry(args.job_id, args.tenant, args.provider, args.cost, args.runtime_secs, args.device_power_w, args.region)
+    print("Ledger entry:", entry)
+    log_to_mlflow(entry)
+    if args.write_db:
+        write_to_db(entry)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: billing/db/schema.sql
+-- SQL schema for job_ledger (Postgres compatible)
+CREATE TABLE IF NOT EXISTS job_ledger (
+  id SERIAL PRIMARY KEY,
+  job_id VARCHAR(128) NOT NULL,
+  tenant VARCHAR(128),
+  provider VARCHAR(128),
+  cost_usd DOUBLE PRECISION,
+  runtime_secs DOUBLE PRECISION,
+  device_power_w DOUBLE PRECISION,
+  kwh DOUBLE PRECISION,
+  kgco2e DOUBLE PRECISION,
+  region VARCHAR(64),
+  created_at TIMESTAMP,
+  extra JSONB
+);
+
*** End Patch
*** Begin Patch
*** Add File: carbon/electricitymap_client.py
+#!/usr/bin/env python3
+"""
+ElectricityMap client (prototype) — fetches carbon intensity by region.
+Set ELECTRICITYMAP_API_KEY in env to use the real API.
+Fallback: use static defaults from carbon/carbon_configmap.yaml
+"""
+import os
+import requests
+import json
+
+API_URL = "https://api.electricitymap.org/v3/zone"
+API_KEY = os.environ.get("ELECTRICITYMAP_API_KEY", "")
+
+def fetch_region_intensity(region):
+    """
+    Returns grams CO2eq per kWh for a given region.
+    region expects ElectricityMap zone id (e.g., 'US-TX' or provider region tag)
+    """
+    if not API_KEY:
+        # fallback stub values
+        return {"zone": region, "carbon_g_per_kwh": 400, "source": "fallback"}
+    headers = {"Accept": "application/json", "auth-token": API_KEY}
+    try:
+        r = requests.get(f"{API_URL}/{region}", headers=headers, timeout=10)
+        r.raise_for_status()
+        j = r.json()
+        return {"zone": region, "carbon_g_per_kwh": j.get("data", {}).get("carbonIntensity", None), "source": "electricitymap"}
+    except Exception as e:
+        return {"zone": region, "carbon_g_per_kwh": None, "source": "error", "error": str(e)}
+
+if __name__ == "__main__":
+    import sys
+    region = sys.argv[1] if len(sys.argv) > 1 else "US"
+    print(fetch_region_intensity(region))
+
*** End Patch
*** Begin Patch
*** Add File: carbon/carbon_ingest_cron.py
+#!/usr/bin/env python3
+"""
+Cron script to periodically ingest carbon intensity and persist to a small cache file and Prometheus pushgateway (optional).
+Writes /tmp/aegis_carbon_cache.json keyed by region.
+"""
+import os
+import json
+from datetime import datetime
+from carbon.electricitymap_client import fetch_region_intensity
+
+REGIONS = os.environ.get("CARBON_REGIONS", "US,EU").split(",")
+CACHE_PATH = os.environ.get("CARBON_CACHE_PATH", "/tmp/aegis_carbon_cache.json")
+
+def run():
+    out = {"ts": datetime.utcnow().isoformat(), "regions": {}}
+    for r in REGIONS:
+        try:
+            info = fetch_region_intensity(r)
+            out["regions"][r] = info
+        except Exception as e:
+            out["regions"][r] = {"error": str(e)}
+    with open(CACHE_PATH, "w") as fh:
+        json.dump(out, fh, indent=2)
+    print("Wrote carbon cache to", CACHE_PATH)
+
+if __name__ == "__main__":
+    run()
+
*** End Patch
*** Begin Patch
*** Add File: carbon/energy_estimator.py
+#!/usr/bin/env python3
+"""
+Job energy estimator:
+ - estimate kWh given runtime seconds and device power in W
+ - estimate kgCO2e using carbon cache (from carbon_ingest_cron) or fallback region mapping
+"""
+import os
+import json
+
+CACHE_PATH = os.environ.get("CARBON_CACHE_PATH", "/tmp/aegis_carbon_cache.json")
+
+def estimate_kwh(runtime_secs, device_power_w):
+    return (runtime_secs * device_power_w) / 3600.0
+
+def estimate_kgco2e(kwh, region):
+    # try cache
+    try:
+        if os.path.exists(CACHE_PATH):
+            j = json.load(open(CACHE_PATH))
+            reg = j.get("regions", {}).get(region)
+            if reg and reg.get("carbon_g_per_kwh"):
+                return (kwh * reg["carbon_g_per_kwh"]) / 1000.0
+    except Exception:
+        pass
+    # fallback default
+    default_g = 400.0
+    return (kwh * default_g) / 1000.0
+
+if __name__ == "__main__":
+    import argparse
+    p = argparse.ArgumentParser()
+    p.add_argument("--runtime-secs", type=float, required=True)
+    p.add_argument("--device-power-w", type=float, required=True)
+    p.add_argument("--region", default="US")
+    args = p.parse_args()
+    kwh = estimate_kwh(args.runtime_secs, args.device_power_w)
+    kg = estimate_kgco2e(kwh, args.region)
+    print(json.dumps({"kwh": kwh, "kgco2e": kg}))
+
*** End Patch
*** Begin Patch
*** Add File: admission/carbon_budget_policy.rego
+package aegis.admission.carbon
+
+# Input: {"tenant": "alice", "requested_kgco2e": 0.5}
+# Example budget store lookup: tenant_budgets[tenant] = {"budget_monthly_kg": 100.0, "used_monthly_kg": 20.0}
+
+tenant_budgets = {
+  "alice": {"budget_monthly_kg": 100.0, "used_monthly_kg": 20.0},
+  "default": {"budget_monthly_kg": 1000.0, "used_monthly_kg": 10.0}
+}
+
+deny[msg] {
+  input.tenant == tenant
+  tenant := input.tenant
+  b := tenant_budgets[tenant]
+  rem := b.budget_monthly_kg - b.used_monthly_kg
+  input.requested_kgco2e > rem
+  msg = sprintf("Tenant %v budget exceeded: remaining=%v kgCO2e, requested=%v", [tenant, rem, input.requested_kgco2e])
+}
+
+allow { not deny[_] }
+
*** End Patch
*** Begin Patch
*** Add File: admission/webhook_stub.py
+#!/usr/bin/env python3
+"""
+Admission webhook stub (prototype).
+ - Accepts job request JSON with tenant and estimated kgCO2e
+ - Evaluates OPA policy (via local rego file simple evaluation) and returns allow/deny
+ - In production deploy as k8s admission webhook with TLS and OPA integration
+"""
+import os
+import json
+from flask import Flask, request, jsonify
+import subprocess
+
+OPA_REGO = os.environ.get("OPA_REGO_PATH", "admission/carbon_budget_policy.rego")
+
+app = Flask("aegis-admission-webhook")
+
+def evaluate_policy(input_obj):
+    # naive evaluation: run 'opa eval' if opa CLI is present, else do simple local eval using subprocess + opa
+    if os.path.exists("/usr/bin/opa") or os.path.exists("/usr/local/bin/opa"):
+        opa = "/usr/bin/opa" if os.path.exists("/usr/bin/opa") else "/usr/local/bin/opa"
+        cmd = [opa, "eval", "-i", "-", "-d", OPA_REGO, "data.aegis.admission.carbon.allow"]
+        p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+        out,err = p.communicate(input=json.dumps(input_obj).encode())
+        if p.returncode != 0:
+            return {"allow": False, "reason": err.decode() if err else "opa error"}
+        txt = out.decode()
+        allow = "true" in txt.lower()
+        return {"allow": allow, "raw": txt}
+    else:
+        # fallback: very simple local check using the rego-like data in file (not full rego)
+        try:
+            # read budgets from rego by crude parsing (prototype only)
+            budgets = {"alice": {"budget_monthly_kg": 100.0, "used_monthly_kg": 20.0}}
+            tenant = input_obj.get("tenant")
+            req = float(input_obj.get("requested_kgco2e", 0.0))
+            b = budgets.get(tenant, {"budget_monthly_kg":1000.0,"used_monthly_kg":0.0})
+            rem = b["budget_monthly_kg"] - b["used_monthly_kg"]
+            if req > rem:
+                return {"allow": False, "reason": "budget exceeded", "remaining": rem}
+            return {"allow": True}
+        except Exception as e:
+            return {"allow": False, "reason": str(e)}
+
+@app.route("/admit", methods=["POST"])
+def admit():
+    data = request.json or {}
+    res = evaluate_policy(data)
+    if res.get("allow"):
+        return jsonify({"allowed": True})
+    return jsonify({"allowed": False, "reason": res.get("reason", "denied")}), 403
+
+if __name__ == "__main__":
+    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 9100)))
+
*** End Patch
*** Begin Patch
*** Add File: billing/showback/grafana_panels.json
+{
+  "panels": [
+    {
+      "type": "graph",
+      "title": "Cost per Tenant (last 30d)",
+      "targets": [
+        {
+          "expr": "sum by (tenant) (billing_daily_cost_usd[30d])",
+          "legendFormat": "{{tenant}}"
+        }
+      ]
+    },
+    {
+      "type": "graph",
+      "title": "Carbon kgCO2e per Tenant (last 30d)",
+      "targets": [
+        {
+          "expr": "sum by (tenant) (billing_daily_kgco2e[30d])",
+          "legendFormat": "{{tenant}}"
+        }
+      ]
+    }
+  ]
+}
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/cost_aware_schedule_hook.py
+#!/usr/bin/env python3
+"""
+Cost/Carbon aware scheduler decision hook (prototype).
+ - Takes candidate providers metadata and job constraints and produces a ranked list
+ - Uses simple utility = w_cost*(1/cost) + w_latency*(1/latency_ms) + w_co2*(1/carbon_g_per_kwh) + w_fid*(fidelity)
+ - Returns JSON with ranked providers and estimated kgCO2e per provider
+"""
+import argparse
+import json
+import os
+from carbon.energy_estimator import estimate_kwh, estimate_kgco2e
+
+WEIGHTS = {
+    "w_cost": float(os.environ.get("W_COST", "0.4")),
+    "w_latency": float(os.environ.get("W_LATENCY", "0.2")),
+    "w_co2": float(os.environ.get("W_CO2", "0.2")),
+    "w_fid": float(os.environ.get("W_FID", "0.2")),
+}
+
+def score(meta, runtime_secs, device_power_w):
+    cost = meta.get("cost_per_job", 1.0)
+    lat = meta.get("latency_ms", 100)
+    carbon = meta.get("carbon_g_per_kwh", 400)
+    fidelity = meta.get("avg_fidelity", 0.5)
+    util = WEIGHTS["w_cost"]*(1.0/max(1e-6,cost)) + WEIGHTS["w_latency"]*(1.0/max(1.0,lat)) + WEIGHTS["w_co2"]*(1.0/max(1.0,carbon)) + WEIGHTS["w_fid"]*fidelity
+    kwh = estimate_kwh(runtime_secs, device_power_w)
+    kg = estimate_kgco2e(kwh, meta.get("region","US"))
+    return util, kg
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--candidates-json", required=True)
+    p.add_argument("--runtime-secs", type=float, default=60.0)
+    p.add_argument("--device-power-w", type=float, default=50.0)
+    args = p.parse_args()
+    candidates = json.loads(open(args.candidates_json).read())
+    scored = []
+    for c in candidates:
+        name = c.get("name")
+        meta = c.get("meta",{})
+        util, kg = score(meta, args.runtime_secs, args.device_power_w)
+        scored.append({"name": name, "util": util, "kgco2e": kg, "meta": meta})
+    scored = sorted(scored, key=lambda x: -x["util"])
+    print(json.dumps({"timestamp":__import__("datetime").datetime.utcnow().isoformat(),"ranked":scored}, indent=2))
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: scheduler/spot_optimizer.py
+#!/usr/bin/env python3
+"""
+Spot optimizer prototype:
+ - Given candidate providers with spot_price and success_rate, pick best spot/regular tradeoff
+ - Simple score = alpha*(1/spot_price) + beta*(success_rate)
+"""
+import argparse
+import json
+
+def pick(candidates, alpha=0.7, beta=0.3):
+    best=None; best_score=-1
+    for c in candidates:
+        p=c.get("meta",{})
+        sp=p.get("spot_price", p.get("cost_per_job",1.0))
+        sr=p.get("spot_success_rate", p.get("success_rate", 0.95))
+        score = alpha*(1.0/max(1e-6,sp)) + beta*sr
+        if score>best_score:
+            best_score=score; best=c
+    return best, best_score
+
+if __name__=="__main__":
+    import sys
+    if len(sys.argv)<2:
+        print("usage: spot_optimizer.py candidates.json")
+        sys.exit(2)
+    cand=json.load(open(sys.argv[1]))
+    best,score=pick(cand)
+    print("best:",best,"score:",score)
+
*** End Patch
*** Begin Patch
*** Add File: billing/cron/ledger_aggregator.yaml
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: aegis-ledger-aggregate
+  namespace: aegis
+spec:
+  schedule: "30 6 * * *" # daily at 06:30 UTC
+  jobTemplate:
+    spec:
+      template:
+        spec:
+          containers:
+            - name: aggregate
+              image: python:3.10-slim
+              env:
+                - name: DATABASE_URL
+                  valueFrom:
+                    secretKeyRef:
+                      name: db-secrets
+                      key: database_url
+              command: ["sh","-c"]
+              args:
+                - pip install sqlalchemy pandas psycopg2-binary && python billing/aggregate_daily.py
+          restartPolicy: OnFailure
+
*** End Patch
*** Begin Patch
*** Add File: billing/aggregate_daily.py
+#!/usr/bin/env python3
+"""
+Aggregate ledger daily into billing summaries (prototype).
+ - Summarize cost and carbon per tenant for yesterday and write to csv / DB
+"""
+import os
+import pandas as pd
+from datetime import datetime, timedelta
+
+DB = os.environ.get("DATABASE_URL", "sqlite:///billing/job_ledger.db")
+
+def load_ledger(db_url=DB):
+    try:
+        from sqlalchemy import create_engine
+        engine = create_engine(db_url)
+        df = pd.read_sql_table("job_ledger", engine)
+        return df
+    except Exception as e:
+        print("Failed to read ledger:", e)
+        return pd.DataFrame()
+
+def main():
+    df = load_ledger()
+    if df.empty:
+        print("No ledger data")
+        return
+    yesterday = (datetime.utcnow() - timedelta(days=1)).date()
+    df['created_at'] = pd.to_datetime(df['created_at'])
+    df_y = df[df['created_at'].dt.date == yesterday]
+    if df_y.empty:
+        print("No entries for", yesterday)
+        return
+    agg = df_y.groupby('tenant').agg({"cost_usd":"sum","kgco2e":"sum"}).reset_index()
+    out = f"/tmp/billing_summary_{yesterday}.csv"
+    agg.to_csv(out, index=False)
+    print("Wrote summary to", out)
+
+if __name__ == "__main__":
+    main()
+
*** End Patch
*** Begin Patch
*** Add File: docs/measurable_esg_README.md
+# Measurable economics & ESG (Aegis) — Prototypes
+
+This patch provides prototypes for:
+
+- Per-job ledger: billing/job_ledger.py — create ledger entries, log to MLflow, and optionally write to DB
+- DB schema: billing/db/schema.sql for Postgres
+- Carbon ingestion: carbon/electricitymap_client.py + carbon/carbon_ingest_cron.py — fetch regional intensity and cache
+- Energy estimator: carbon/energy_estimator.py — convert runtime & power -> kWh -> kgCO2e
+- Admission: admission/carbon_budget_policy.rego + admission/webhook_stub.py — OPA policy + simple webhook prototype
+- Scheduler hooks: scheduler/cost_aware_schedule_hook.py and scheduler/spot_optimizer.py
+- Broker integration: provider broker learned code earlier can ingest telemetry that includes cost and kgCO2e
+- Aggregation: billing/aggregate_daily.py and billing/cron/ledger_aggregator.yaml to summarize daily costs and carbon per tenant
+- Showback: billing/showback/grafana_panels.json (panel templates)
+
+How to use (developer quickstart):
+1. Enable carbon ingestion:
+   - Set ELECTRICITYMAP_API_KEY or rely on fallback values
+   - Run `python carbon/carbon_ingest_cron.py` periodically (CronJob in k8s or via CI)
+2. During job execution record ledger entry:
+   - Compute runtime and device_power; call `python billing/job_ledger.py --job-id ... --tenant ... --provider ... --cost ... --runtime-secs ... --device-power-w ... --region ... --write-db`
+   - Optionally set MLFLOW_RUN_ID and MLFLOW_TRACKING_URI to attach ledger to MLflow run
+3. Admission controller:
+   - Run `admission/webhook_stub.py` and call /admit with {"tenant":"alice","requested_kgco2e":0.5}
+   - Integrate with scheduler to call webhook before scheduling regulated jobs
+4. Aggregation:
+   - Deploy billing/cron/ledger_aggregator.yaml to aggregate daily and write CSVs for accounting
+
+Notes:
+- These are prototypes: production systems require hardened ingestion, authenticated APIs, durable stores, and reconciliation with provider invoices.
+- For accurate kgCO2e/job you need device power telemetry, provider energy metrics, and more accurate per-region carbon time-series.
+- Consider using a time-series DB and Prometheus exporter for carbon and billing metrics for dashboards.
+
*** End Patch
*** End Patch
