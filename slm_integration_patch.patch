*** Begin Patch
*** Add File:slm/serving/requirements.txt
+fastapi
+uvicorn[standard]
+onnxruntime
+numpy
+prometheus-client
+requests
+transformers
+sentence-transformers
+
*** End Patch
*** Begin Patch
*** Add File:slm/serving/Dockerfile
+FROM python:3.11-slim
+WORKDIR /app
+COPY requirements.txt .
+RUN apt-get update && apt-get install -y git build-essential ffmpeg && rm -rf /var/lib/apt/lists/*
+RUN pip install --no-cache-dir -r requirements.txt
+COPY slm_service.py /app/slm_service.py
+EXPOSE 8088
+CMD ["uvicorn","slm_service:app","--host","0.0.0.0","--port","8088","--workers","1"]
+
*** End Patch
*** Begin Patch
*** Add File:slm/serving/slm_service.py
+#!/usr/bin/env python3
+"""
+SLM Inference microservice (FastAPI)
+ - Loads ONNX model via onnxruntime if available, otherwise falls back to HF pipeline
+ - Exposes /predict with a simple JSON {prompt: "..."} returning {output, confidence}
+ - Exposes /health and Prometheus metrics
+
+Environment:
+ - SLM_MODEL_PATH: path to onnx model file (local path in container)
+ - SLM_HF_MODEL: HF model id to fallback to (e.g. 'sshleifer/tiny-gpt2')
+ - USE_ONNX: '1' to attempt onnxruntime
+"""
+import os
+import time
+import json
+import traceback
+from fastapi import FastAPI, HTTPException, Request
+from pydantic import BaseModel
+from prometheus_client import Counter, Histogram, start_http_server, generate_latest, CONTENT_TYPE_LATEST
+import threading
+
+SLM_MODEL_PATH = os.environ.get("SLM_MODEL_PATH", "/models/slm.onnx")
+SLM_HF_MODEL = os.environ.get("SLM_HF_MODEL", "sshleifer/tiny-gpt2")
+USE_ONNX = os.environ.get("USE_ONNX", "1") == "1"
+
+app = FastAPI(title="Aegis SLM Service")
+
+# Prometheus metrics
+SLM_REQUESTS = Counter("aegis_slm_requests_total", "Total SLM requests")
+SLM_LATENCY = Histogram("aegis_slm_latency_seconds", "SLM request latency seconds")
+SLM_FALLBACKS = Counter("aegis_slm_fallbacks_total", "SLM->LLM fallback occurrences")
+start_http_server(9108)  # separate metrics port for Prometheus scraping
+
+# Try to load ONNX runtime
+onnx_session = None
+if USE_ONNX:
+    try:
+        import onnxruntime as ort
+        if os.path.exists(SLM_MODEL_PATH):
+            onnx_session = ort.InferenceSession(SLM_MODEL_PATH, providers=["CPUExecutionProvider"])
+            print("Loaded ONNX model:", SLM_MODEL_PATH)
+        else:
+            print("ONNX model not found at", SLM_MODEL_PATH)
+    except Exception as e:
+        print("onnxruntime not available or failed to load model:", e)
+        onnx_session = None
+
+# HF fallback
+hf_pipe = None
+if onnx_session is None:
+    try:
+        from transformers import pipeline
+        hf_pipe = pipeline("text-generation", model=SLM_HF_MODEL, device=-1)
+        print("Loaded HF fallback model:", SLM_HF_MODEL)
+    except Exception as e:
+        print("HF pipeline fallback failed:", e)
+        hf_pipe = None
+
+class PredictReq(BaseModel):
+    prompt: str
+    max_new_tokens: int = 64
+
+def onnx_infer(prompt: str, max_new_tokens: int = 64):
+    """
+    A minimal onnx wrapper. Real models need proper tokenization and input/output names.
+    This function demonstrates the integration: you must tailor tokenization to the exported ONNX graph.
+    """
+    try:
+        # NOTE: production: use proper tokenizer and onnx graph input naming
+        input_name = onnx_session.get_inputs()[0].name
+        # dummy numeric input for illustrative purposes
+        import numpy as np
+        dummy = np.zeros((1, max_new_tokens), dtype=np.int64)
+        out = onnx_session.run(None, {input_name: dummy})
+        # mock output
+        text = "onnx-output (placeholder)"
+        confidence = 0.7
+        return {"output": text, "confidence": confidence}
+    except Exception:
+        traceback.print_exc()
+        raise
+
+def hf_infer(prompt: str, max_new_tokens: int = 64):
+    if not hf_pipe:
+        raise RuntimeError("No inference backend available")
+    out = hf_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)
+    text = out[0]["generated_text"]
+    # crude confidence proxy: longer generated => lower confidence (placeholder)
+    confidence = min(0.99, 0.5 + len(text) / 1000.0)
+    return {"output": text, "confidence": confidence}
+
+@app.post("/predict")
+@SLM_LATENCY.time()
+def predict(req: PredictReq):
+    SLM_REQUESTS.inc()
+    try:
+        if onnx_session:
+            res = onnx_infer(req.prompt, req.max_new_tokens)
+        else:
+            res = hf_infer(req.prompt, req.max_new_tokens)
+        return {"output": res["output"], "confidence": float(res["confidence"])}
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=str(e))
+
+@app.get("/health")
+def health():
+    return {"status": "ok", "onnx_loaded": onnx_session is not None, "hf_loaded": hf_pipe is not None}
+
+@app.get("/metrics")
+def metrics():
+    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}
+
+if __name__ == "__main__":
+    import uvicorn
+    uvicorn.run("slm_service:app", host="0.0.0.0", port=8088, workers=1)
+
*** End Patch
*** Begin Patch
*** Add File:slm/convert/convert_to_onnx.py
+#!/usr/bin/env python3
+"""
+Convert a Hugging Face causal LM to ONNX and optionally quantize with ONNXRuntime.
+This script is a pragmatic starting point and must be tuned for your model/tokenizer specifics.
+
+Usage:
+  python convert_to_onnx.py --hf_model sshleifer/tiny-gpt2 --output /out/slm.onnx --quantize
+
+Notes:
+ - For production you should use the transformers.onnx tooling or optimum export for optimized graphs.
+ - Quantization uses onnxruntime.transformers or onnxruntime.quantization tools when available.
+"""
+import os
+import argparse
+import torch
+from transformers import AutoTokenizer, AutoModelForCausalLM
+
+def export_to_onnx(hf_model, output_path, max_length=64):
+    tok = AutoTokenizer.from_pretrained(hf_model)
+    model = AutoModelForCausalLM.from_pretrained(hf_model).eval().to("cpu")
+    # Create dummy input
+    sample = tok("Hello world", return_tensors="pt")
+    input_ids = sample["input_ids"]
+    attention_mask = sample.get("attention_mask", None)
+    # ONNX export - this is example and might require adapting input/output names
+    torch.onnx.export(
+        model,
+        (input_ids,),
+        output_path,
+        do_constant_folding=True,
+        input_names=["input_ids"],
+        output_names=["logits"],
+        dynamic_axes={"input_ids": {0: "batch_size", 1: "sequence"}}
+    )
+    print("Exported ONNX model to", output_path)
+
+def quantize_onnx(model_path, out_path):
+    try:
+        from onnxruntime.quantization import quantize_dynamic, QuantType
+        quantize_dynamic(model_input=model_path, model_output=out_path, weight_type=QuantType.QInt8)
+        print("Quantized ONNX model to", out_path)
+    except Exception as e:
+        print("onnx quantization not available:", e)
+
+if __name__ == "__main__":
+    p = argparse.ArgumentParser()
+    p.add_argument("--hf_model", required=True)
+    p.add_argument("--output", required=True)
+    p.add_argument("--quantize", action="store_true")
+    args = p.parse_args()
+    export_to_onnx(args.hf_model, args.output)
+    if args.quantize:
+        quantize_onnx(args.output, args.output.replace(".onnx", ".quant.onnx"))
+
*** End Patch
*** Begin Patch
*** Add File:.github/workflows/slm_build_sign.yml
+name: Build, Quantize and Sign SLM artifact
+
+on:
+  workflow_dispatch:
+  push:
+    paths:
+      - 'slm/**'
+
+jobs:
+  build-slm:
+    runs-on: ubuntu-latest
+    env:
+      COSIGN_KMS_ARN: ${{ secrets.COSIGN_KMS_ARN }}
+      EVIDENCE_BUCKET: ${{ secrets.EVIDENCE_BUCKET }}
+    steps:
+      - uses: actions/checkout@v4
+      - name: Setup Python
+        uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - name: Install deps
+        run: |
+          pip install --upgrade pip
+          pip install -r slm/serving/requirements.txt
+          pip install onnx onnxruntime onnxruntime-tools
+      - name: Convert small HF model to ONNX (mock)
+        run: |
+          mkdir -p artifacts
+          python slm/convert/convert_to_onnx.py --hf_model sshleifer/tiny-gpt2 --output artifacts/slm.onnx --quantize
+      - name: Archive artifact
+        uses: actions/upload-artifact@v4
+        with:
+          name: slm-artifact
+          path: artifacts
+      - name: Install cosign
+        run: |
+          COSIGN_VERSION=2.1.1
+          curl -fsSL -o /usr/local/bin/cosign https://github.com/sigstore/cosign/releases/download/v${COSIGN_VERSION}/cosign-linux-amd64
+          chmod +x /usr/local/bin/cosign
+      - name: Sign artifact with cosign (KMS)
+        env:
+          COSIGN_KMS_ARN: ${{ secrets.COSIGN_KMS_ARN }}
+        run: |
+          if [ -n "$COSIGN_KMS_ARN" ]; then
+            cosign sign --key "awskms://$COSIGN_KMS_ARN" artifacts/slm.onnx || echo "cosign failed (best-effort)"
+          else
+            echo "No COSIGN_KMS_ARN provided; skipping signing"
+          fi
+
*** End Patch
*** Begin Patch
*** Add File:agent/slm_client.py
+#!/usr/bin/env python3
+"""
+SLM client for Agent Manager integration.
+ - Calls the SLM inference microservice and returns (text, confidence)
+ - Provides a simple threshold-based decision helper
+"""
+import os, requests
+
+SLM_ENDPOINT = os.environ.get("SLM_ENDPOINT", "http://slm-service.aegis.svc.cluster.local:8088/predict")
+
+def run_slm(prompt, max_new_tokens=64, timeout=5):
+    try:
+        res = requests.post(SLM_ENDPOINT, json={"prompt": prompt, "max_new_tokens": max_new_tokens}, timeout=timeout)
+        res.raise_for_status()
+        jd = res.json()
+        return jd.get("output",""), float(jd.get("confidence",0.0))
+    except Exception as e:
+        return "", 0.0
+
*** End Patch
*** Begin Patch
*** Add File:agent/manager_slm.py
+#!/usr/bin/env python3
+"""
+Agent Manager (SLM-first) — lightweight wrapper to demonstrate SLM-first planner with fallback to LLM.
+ - Provides /agent/slm_start which runs the agent loop where SLM is used for planning.
+ - If SLM confidence < threshold, escalate to LLM (OpenAI/HF) and record fallback metric.
+ - Records actions to SessionStore and AuditLogger (best-effort).
+"""
+import os, uuid, time, asyncio
+from fastapi import FastAPI, BackgroundTasks
+from prometheus_client import start_http_server, Counter, Histogram
+
+from agent.session_store import SessionStore
+from agent.tool_registry import ToolRegistryClient
+from agent.tool_runner import ToolRunner
+from agent.audit.audit_logger import AuditLogger
+from agent.cost.accounting import CostAccounting
+from agent.slm_client import run_slm
+from llm.connectors.openai_wrapper import chat_completion
+
+APP_PORT = int(os.environ.get("AGENT_SLM_PORT", "8095"))
+start_http_server(int(os.environ.get("AGENT_SLM_METRICS_PORT", "9110")))
+
+SLM_FALLBACK_THRESHOLD = float(os.environ.get("SLM_FALLBACK_THRESHOLD", "0.7"))
+
+SLM_FALLBACKS = Counter("aegis_slm_fallbacks_total", "SLM->LLM fallback occurrences")
+SLM_REQUESTS = Counter("aegis_slm_requests_total", "SLM planner requests")
+SLM_LATENCY = Histogram("aegis_slm_latency_seconds", "SLM latency seconds")
+
+app = FastAPI(title="Aegis Agent Manager SLM-first")
+SESSION_STORE = SessionStore()
+TOOL_REG = ToolRegistryClient()
+TOOL_RUNNER = ToolRunner()
+AUDIT = AuditLogger()
+COST = CostAccounting()
+
+def call_llm_fallback(prompt):
+    # Simple wrapper to call OpenAI (or HF) - uses existing connector
+    try:
+        text, tokens, cost = chat_completion([{"role":"user","content":prompt}])
+        return text
+    except Exception:
+        # local HF fallback (best-effort)
+        from transformers import pipeline
+        p = pipeline("text-generation", model=os.environ.get("HF_MODEL","gpt2"))
+        out = p(prompt, max_length=256)[0]["generated_text"]
+        return out
+
+async def slm_agent_loop(run_id, prompt, max_steps=6):
+    SESSION_STORE.create_session(run_id, {"prompt": prompt})
+    for step in range(max_steps):
+        start = time.time()
+        SLM_REQUESTS.inc()
+        text, conf = run_slm(prompt)
+        SLM_LATENCY.observe(time.time() - start)
+        if conf < SLM_FALLBACK_THRESHOLD:
+            SLM_FALLBACKS.inc()
+            # escalate to LLM
+            plan = call_llm_fallback(prompt)
+            AUDIT.log_action(run_id, "slm_fallback", prompt, {"reason": "low_confidence", "slm_conf": conf, "plan": plan})
+            # Tool selection: naive parse for tool name (demo)
+            tools = TOOL_REG.find_tools_for_prompt(plan)
+        else:
+            plan = text
+            tools = TOOL_REG.find_tools_for_prompt(plan)
+            AUDIT.log_action(run_id, "slm_plan", prompt, {"plan": plan, "conf": conf})
+
+        if not tools:
+            SESSION_STORE.append_action(run_id, {"step": step, "plan": plan, "tools": []})
+            return
+        tool = tools[0]
+        # run tool (sync)
+        result = await TOOL_RUNNER.run_tool(tool["name"], plan, run_id)
+        SESSION_STORE.append_action(run_id, {"step": step, "plan": plan, "tool": tool["name"], "result": result})
+        # prepare next prompt (simple)
+        prompt = plan + "\n\nResult: " + (result.get("output") or str(result.get("error","")))
+
+@app.post("/agent/slm_start")
+async def slm_start(payload: dict, background: BackgroundTasks):
+    prompt = payload.get("prompt","")
+    run_id = str(uuid.uuid4())
+    background.add_task(slm_agent_loop, run_id, prompt, payload.get("max_steps",6))
+    return {"run_id": run_id}
+
+if __name__ == "__main__":
+    import uvicorn
+    uvicorn.run(app, host="0.0.0.0", port=APP_PORT)
+
*** End Patch
*** Begin Patch
*** Add File:slm/k8s/slm-deployment.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: slm-service
+  namespace: aegis
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: slm-service
+  template:
+    metadata:
+      labels:
+        app: slm-service
+    spec:
+      containers:
+        - name: slm
+          image: ghcr.io/yourorg/aegis-slm:latest
+          env:
+            - name: SLM_MODEL_PATH
+              value: "/models/slm.onnx"
+            - name: USE_ONNX
+              value: "1"
+          resources:
+            requests:
+              cpu: "200m"
+              memory: "256Mi"
+            limits:
+              cpu: "1"
+              memory: "1Gi"
+          ports:
+            - containerPort: 8088
+
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: slm-service
+  namespace: aegis
+spec:
+  selector:
+    app: slm-service
+  ports:
+    - protocol: TCP
+      port: 8088
+      targetPort: 8088
+  type: ClusterIP
+
*** End Patch
*** Begin Patch
*** Add File:slm/k8s/slm-hpa.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: slm-service-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: slm-service
+  minReplicas: 2
+  maxReplicas: 10
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 60
+
*** End Patch
*** Begin Patch
*** Add File:slm/k8s/slm-daemonset-edge.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: slm-edge
+  namespace: aegis-edge
+spec:
+  selector:
+    matchLabels:
+      app: slm-edge
+  template:
+    metadata:
+      labels:
+        app: slm-edge
+    spec:
+      containers:
+        - name: slm-edge
+          image: ghcr.io/yourorg/aegis-slm-edge:latest
+          env:
+            - name: SLM_HF_MODEL
+              value: "sshleifer/tiny-gpt2"
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+            limits:
+              cpu: "500m"
+              memory: "512Mi"
+
+---
+# This DaemonSet template is for edge nodes; ensure edge nodes are labeled and tainted appropriately.
+
*** End Patch
*** Begin Patch
*** Add File:slm/k8s/nodepool_guidance.md
+# SLM NodePool Guidance
+
+- SLMs are latency-sensitive but light: prefer CPU-heavy nodepools for many small instances or small GPU nodes for quantized models requiring GPU.
+- Example: create a nodepool labeled "aegis-slm" with taint aegis/slm=true:NoSchedule and annotate deployments to use nodeSelector: aegis.slm=true
+- Use HPA with CPU utilization target (50-70%) and consider pod-disruption budgets for rolling updates.
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/prometheus/slm_rules.yaml
+apiVersion: monitoring.coreos.com/v1
+kind: PrometheusRule
+metadata:
+  name: aegis-slm-rules
+  namespace: monitoring
+spec:
+  groups:
+    - name: slm.rules
+      rules:
+        - alert: SLMHighFallbackRate
+          expr: increase(aegis_slm_fallbacks_total[5m]) > 5
+          for: 10m
+          labels:
+            severity: warning
+          annotations:
+            summary: "High SLM fallback rate — many SLM plans are escalating to LLM"
+        - alert: SLMLatencyHigh
+          expr: histogram_quantile(0.95, sum(rate(aegis_slm_latency_seconds_bucket[5m])) by (le)) > 1
+          for: 10m
+          labels:
+            severity: warning
+          annotations:
+            summary: "SLM p95 latency > 1s"
+
*** End Patch
*** Begin Patch
*** Add File:monitoring/grafana/slm_dashboard.json
+{
+  "dashboard": {
+    "title": "Aegis SLM Ops",
+    "panels": [
+      { "title": "SLM Requests / minute", "type": "graph", "targets": [{"expr":"rate(aegis_slm_requests_total[1m])"}]},
+      { "title": "SLM Fallbacks / minute", "type": "graph", "targets": [{"expr":"rate(aegis_slm_fallbacks_total[1m])"}]},
+      { "title": "SLM p95 Latency (s)", "type": "graph", "targets": [{"expr":"histogram_quantile(0.95, sum(rate(aegis_slm_latency_seconds_bucket[5m])) by (le))"}]}
+    ]
+  }
+}
+
*** End Patch
*** Begin Patch
*** Add File:slm/registry/register_slm_mlflow.py
+#!/usr/bin/env python3
+"""
+Register SLM artifact in MLflow Model Registry and upload model files to artifact store.
+Requires MLFLOW_TRACKING_URI and proper S3 artifact store config.
+"""
+import os
+import mlflow
+
+MODEL_PATH = os.environ.get("SLM_ARTIFACT_PATH", "artifacts/slm.quant.onnx")
+MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "")
+MODEL_NAME = os.environ.get("SLM_MODEL_NAME", "aegis-slm")
+
+def register():
+    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
+    with mlflow.start_run():
+        mlflow.log_artifact(MODEL_PATH, artifact_path="slm")
+        mlflow.set_tag("model_type", "slm")
+        mlflow.set_tag("signed", os.environ.get("SIGNED","false"))
+        print("Logged artifact to MLflow:", MODEL_PATH)
+        # register model - optional
+        model_uri = f"runs:/{mlflow.active_run().info.run_id}/slm/{os.path.basename(MODEL_PATH)}"
+        mv = mlflow.register_model(model_uri, MODEL_NAME)
+        print("Registered model", mv.name, mv.version)
+
+if __name__ == "__main__":
+    register()
+
*** End Patch
*** Begin Patch
*** Add File:agent/ci/slm_conftest.yml
+# CI lint placeholder: ensure SLM deployments include signature annotation and resource limits
+---
+# Example conftest test would assert:
+# - metadata.annotations['aegis.cosign_signed'] == 'true'
+# - containers have requests/limits
+
*** End Patch
*** End Patch
