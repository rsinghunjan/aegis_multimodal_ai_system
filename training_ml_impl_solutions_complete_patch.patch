*** Begin Patch
*** Add File: argo/deepspeed_fsdp_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: deepspeed-fsdp-train
+  namespace: staging
+spec:
+  entrypoint: fsdp-train
+  templates:
+  - name: fsdp-train
+    dag:
+      tasks:
+      - name: launch-fsdp
+        template: deepspeed-fsdp-job
+
+  - name: deepspeed-fsdp-job
+    container:
+      image: ${DEEPSPEED_FSDP_IMAGE:-ghcr.io/yourorg/deepspeed-fsdp:latest}
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          set -euxo pipefail
+          export NCCL_DEBUG=INFO
+          export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:-eth0}
+          export MASTER_PORT=${MASTER_PORT:-29500}
+          # Launch FSDP training with torchrun for multi-node
+          torchrun --nproc_per_node=${NUM_GPUS_PER_NODE:-8} --nnodes=${NUM_NODES:-2} --node_rank=${NODE_RANK:-0} --rdzv_backend=c10d --rdzv_endpoint=${RDZV_ENDPOINT:-$MASTER_ADDR:$MASTER_PORT} train_fsdp.py --ckpt_dir /mnt/checkpoints --epochs ${EPOCHS:-3}
+      resources:
+        requests:
+          cpu: "32"
+          memory: "256Gi"
+        limits:
+          nvidia.com/gpu: "{{workflow.parameters.gpus}}"
+    nodeSelector:
+      aegis/worker-type: dgx
+    tolerations:
+    - key: nvidia.com/gpu
+      operator: Exists
+    volumeMounts:
+    - name: checkpoints
+      mountPath: /mnt/checkpoints
+    volumes:
+    - name: checkpoints
+      persistentVolumeClaim:
+        claimName: deepspeed-data-pvc
+
*** End Patch
*** Begin Patch
*** Add File: docker/deepspeed_fsdp.Dockerfile
+FROM nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04
+ENV DEBIAN_FRONTEND=noninteractive
+RUN apt-get update && apt-get install -y python3 python3-pip build-essential git wget && rm -rf /var/lib/apt/lists/*
+RUN python3 -m pip install --upgrade pip
+# PyTorch + FSDP + DeepSpeed compatible stack
+RUN pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu121
+RUN pip install deepspeed==2.9.1 fairscale accelerate transformers datasets
+RUN pip install mlflow aiohttp
+WORKDIR /workspace
+COPY train_fsdp.py /workspace/train_fsdp.py
+COPY requirements.txt /workspace/requirements.txt
+RUN pip install -r /workspace/requirements.txt || true
+ENTRYPOINT ["bash","-lc"]
+CMD ["python train_fsdp.py"]
+
*** End Patch
*** Begin Patch
*** Add File: argo/horovod_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: horovod-train
+  namespace: staging
+spec:
+  entrypoint: horovod-train
+  templates:
+  - name: horovod-train
+    container:
+      image: ghcr.io/yourorg/horovod-training:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          mpirun -np ${TOTAL_RANKS:-16} -H ${HOSTS:-localhost:16} -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH python horovod_train.py --epochs 3 --ckpt /data/ckpt
+      resources:
+        requests:
+          cpu: "32"
+          memory: "200Gi"
+        limits:
+          nvidia.com/gpu: 4
+
*** End Patch
*** Begin Patch
*** Add File: argo/katib_hpo_workflow.yaml
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  name: katib-hpo
+  namespace: staging
+spec:
+  entrypoint: hpo
+  templates:
+  - name: hpo
+    container:
+      image: ghcr.io/yourorg/katib-client:latest
+      command: ["/bin/bash","-lc"]
+      args:
+        - |
+          # This script acts as a light Katib experiment submitter.
+          python submit_katib_experiment.py --name "aegis-hpo" --trials 20
+      resources:
+        requests:
+          cpu: "2"
+          memory: "4Gi"
+
*** End Patch
*** Begin Patch
*** Add File: argo/ray_cluster_operator.yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: ray-operator
+  namespace: kube-system
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: ray-operator
+  template:
+    metadata:
+      labels:
+        app: ray-operator
+    spec:
+      containers:
+      - name: ray-operator
+        image: ghcr.io/ray-project/ray-operator:latest
+        resources:
+          requests:
+            cpu: "200m"
+            memory: "256Mi"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/spot/preemption-handler-daemonset.yaml
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: preemption-handler
+  namespace: kube-system
+spec:
+  selector:
+    matchLabels:
+      name: preemption-handler
+  template:
+    metadata:
+      labels:
+        name: preemption-handler
+    spec:
+      serviceAccountName: preemption-handler-sa
+      containers:
+      - name: preemption-handler
+        image: ghcr.io/yourorg/preemption-handler:latest
+        securityContext:
+          runAsUser: 0
+        volumeMounts:
+        - name: dockersock
+          mountPath: /var/run/docker.sock
+      volumes:
+      - name: dockersock
+        hostPath:
+          path: /var/run/docker.sock
+
*** End Patch
*** Begin Patch
*** Add File: scripts/preemption_handler.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# This is a simple preemption handler that listens for cloud provider shutdown signals (metadata) and annotates pods to checkpoint.
+# In production use provider-specific event stream (GCP preemptible, AWS Spot interruptions, Azure evictions).
+
+LOG=/var/log/preemption-handler.log
+echo "$(date) starting preemption handler" >> "$LOG"
+while true; do
+  # Example: GCP preemptible metadata endpoint
+  if curl -sS -m 2 "http://169.254.169.254/computeMetadata/v1/instance/maintenance-event" -H "Metadata-Flavor: Google" | grep -q STARTING_MAINTENANCE; then
+    echo "$(date) preemption signal detected" >> "$LOG"
+    # annotate local node's training pods for checkpoint
+    for pod in $(kubectl get pods -n staging -o jsonpath='{range .items[?(@.spec.nodeName=="'$(hostname)'")]}{.metadata.name}{"\n"}{end}'); do
+      kubectl annotate pod -n staging "$pod" aegis/preempting=true --overwrite || true
+      echo "$(date) annotated $pod" >> "$LOG"
+    done
+    # sleep until next check
+    sleep 30
+  fi
+  sleep 5
+done
+
*** End Patch
*** Begin Patch
*** Add File: scripts/checkpoint_on_signal.sh
+#!/usr/bin/env bash
+#
+# This script is intended as a preStop hook or sidecar entrypoint for training pods.
+# When the pod receives SIGTERM (preStop), it triggers an immediate checkpoint and uploads it to persistent storage.
+set -euo pipefail
+trap 'echo "Received SIGTERM: saving checkpoint"; python save_checkpoint.py --dest /mnt/checkpoints; exit 0' TERM
+# Keep process alive as sidecar; main process will run training
+while true; do sleep 60; done
+
*** End Patch
*** Begin Patch
*** Add File: k8s/job_pod_with_checkpoint_hook.yaml
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: deepspeed-job-with-checkpoint-hook
+  namespace: staging
+spec:
+  template:
+    spec:
+      serviceAccountName: training-sa
+      containers:
+      - name: trainer
+        image: ghcr.io/yourorg/deepspeed-poc:latest
+        lifecycle:
+          preStop:
+            exec:
+              command: ["/bin/bash","-lc","python save_checkpoint.py --dest /mnt/checkpoints"]
+        volumeMounts:
+        - name: checkpoints
+          mountPath: /mnt/checkpoints
+      - name: checkpoint-hook
+        image: ghcr.io/yourorg/checkpoint-hook:latest
+        lifecycle:
+          preStop:
+            exec:
+              command: ["/bin/bash","-lc","/hook/checkpoint_on_signal.sh"]
+      restartPolicy: OnFailure
+      volumes:
+      - name: checkpoints
+        persistentVolumeClaim:
+          claimName: deepspeed-data-pvc
+
*** End Patch
*** Begin Patch
*** Add File: scripts/cost_enforcer.py
+#!/usr/bin/env python3
+"""
+Simple budget watcher that enforces cost limits by pausing new training jobs and annotating running jobs.
+Integrate with cloud billing APIs in production.
+"""
+import os, time, requests, subprocess, json
+
+THRESHOLD_PCT = float(os.environ.get("COST_THRESHOLD_PCT", "90"))
+CHECK_INTERVAL = int(os.environ.get("COST_CHECK_INTERVAL", "300"))
+NAMESPACE = os.environ.get("TRAINING_NAMESPACE", "staging")
+
+def current_usage_pct():
+    # Placeholder: implement per-cloud billing API call.
+    # For demo, read env var CURRENT_USAGE_PCT or return 50
+    return float(os.environ.get("CURRENT_USAGE_PCT", "50"))
+
+def list_training_jobs():
+    out = subprocess.check_output(["kubectl","get","jobs","-n",NAMESPACE,"-o","json"])
+    return json.loads(out)
+
+def annotate_job(name, note):
+    subprocess.run(["kubectl","annotate","job",name,"-n",NAMESPACE,f"aegis/cost-enforced={note}","--overwrite"], check=False)
+
+if __name__ == "__main__":
+    while True:
+        usage = current_usage_pct()
+        if usage > THRESHOLD_PCT:
+            print("Budget exceeded: annotating running jobs and blocking new submits")
+            jobs = list_training_jobs()
+            for j in jobs.get("items",[]):
+                annotate_job(j["metadata"]["name"], "paused_due_to_budget")
+            # Optionally: scale down node groups via cloud API or alert
+        time.sleep(CHECK_INTERVAL)
+
*** End Patch
*** Begin Patch
*** Add File: .github/workflows/nightly_training_validation.yml
+name: Nightly: Training validation & profiling
+on:
+  schedule:
+    - cron: '0 2 * * *' # 02:00 UTC
+  workflow_dispatch:
+
+jobs:
+  validation:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+      - name: Run deepspeed smoke validation (submit to cluster via kubectl)
+        env:
+          KUBECONFIG: ${{ secrets.KUBE_CONFIG_DATA }}
+        run: |
+          kubectl apply -f argo/deepspeed_zeo_workflow_poc.yaml
+          echo "Submitted deepspeed poc. Monitor cluster for completion."
+      - name: Run profiling collection (best-effort)
+        run: |
+          ./scripts/collect_gpu_profiles.sh || true
+      - name: Upload artifacts
+        uses: actions/upload-artifact@v4
+        with:
+          name: training-validation-artifacts
+          path: artifacts || true
+
*** End Patch
*** Begin Patch
*** Add File: scripts/mlflow_enforce_tags.py
+#!/usr/bin/env python3
+"""
+Small utility to enforce MLflow experiment naming and tags.
+Call this in training entrypoint early to ensure consistent experiment metadata.
+"""
+import mlflow, os, sys
+
+def ensure_experiment(name):
+    prefix = os.environ.get("MLFLOW_EXPERIMENT_PREFIX","aegis-")
+    if not name.startswith(prefix):
+        name = prefix + name
+    mlflow.set_experiment(name)
+    return name
+
+if __name__ == "__main__":
+    exp = sys.argv[1] if len(sys.argv)>1 else "default"
+    exp_final = ensure_experiment(exp)
+    print("Experiment set to", exp_final)
+
*** End Patch
*** Begin Patch
*** Add File: scripts/collect_gpu_profiles.sh
+#!/usr/bin/env bash
+set -euo pipefail
+mkdir -p artifacts/profiling
+# This script will attempt to collect GPU metrics and PyTorch traces from training pods (best-effort)
+for pod in $(kubectl get pods -n staging -l app=deepspeed-poc -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
+  echo "Collecting nvidia-smi for $pod"
+  kubectl exec -n staging "$pod" -- nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv -q > artifacts/profiling/${pod}_nvidia-smi.csv || true
+  kubectl cp staging/$pod:/tmp/trace.json artifacts/profiling/${pod}_trace.json || true
+done
+echo "Profiles collected into artifacts/profiling/"
+
*** End Patch
*** Begin Patch
*** Add File: k8s/serviceaccount_vault_job.yaml
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: training-vault-sa
+  namespace: staging
+automountServiceAccountToken: false
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: training-vault-role
+  namespace: staging
+rules:
+- apiGroups: [""]
+  resources: ["secrets"]
+  verbs: ["get","watch","list"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: training-vault-rolebinding
+  namespace: staging
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: training-vault-role
+subjects:
+- kind: ServiceAccount
+  name: training-vault-sa
+  namespace: staging
+
*** End Patch
*** Begin Patch
*** Add File: scripts/experiment_ci_smoke.sh
+#!/usr/bin/env bash
+set -euo pipefail
+# Example script to run a reproducible smoke experiment in CI to validate train-as-code
+IMAGE=${1:-ghcr.io/yourorg/deepspeed-poc:latest}
+EXPERIMENT=${2:-test-smoke}
+docker run --rm -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow.staging.svc.cluster.local:5000} "$IMAGE" python train_fsdp.py --epochs 1 --experiment "$EXPERIMENT"
+python scripts/mlflow_enforce_tags.py "$EXPERIMENT"
+echo "Smoke experiment completed and logged to mlflow"
+
*** End Patch
*** Begin Patch
*** Add File: runbooks/training_5star_runbook.md
+# Aegis 5/5 Training & Productionization Runbook
+
+Purpose
+- Execution checklist and acceptance criteria to bring Aegis training, ML implementation and AI solutions to production-grade (5/5).
+
+Key acceptance criteria (all required)
+- Multi-node training validation:
+  - DeepSpeed/FSDP and Horovod workflows successfully run on target infra (DGX, GKE-TPU, or GKE-GPU).
+  - Checkpoint/resume tested via deepspeed validation workflow; resume after simulated preemption.
+
+- Autoscaling & cost control:
+  - Spot/preemptible pools used with preemption handlers and preStop checkpointing enabled.
+  - cost_enforcer running in staging and annotating jobs when budget exceeds threshold.
+
+- Framework breadth:
+  - Ray operator deployed and Ray HPO/Ray Tune integration validated.
+  - Katib experiment run and results logged to MLflow.
+
+- Developer UX:
+  - JupyterHub deployed and integrated with MLflow and Vault for secrets in dev workspaces.
+  - Train-as-code CI smoke run (scripts/experiment_ci_smoke.sh) executed and reproducible.
+
+- Observability & profiling:
+  - GPU traces and PyTorch profiler artifacts uploaded to compliance store for at least one training run.
+  - Prometheus metrics for training jobs (GPU utilization, job progress) show in dashboards.
+
+- Robustness & chaos:
+  - Chaos drill (argo/chaos/chaos_train_drill.yaml) executed and system resumed properly.
+
+- Security:
+  - Per-job least-privilege SA used (k8s/serviceaccount_vault_job.yaml).
+  - Datasets on encrypted PVCs; audit logs for dataset access recorded.
+
+Run sequence (high-level)
+1) Provision infra (KMS, Vault, storage) and set GitHub secrets.
+2) Deploy core platform: Vault, Argo, Ray operator, Katib, MLflow, JupyterHub.
+3) Run DeepSpeed FSDP & validation workflows; collect checkpoint/resume evidence.
+4) Run HPO via Ray/Katib and confirm MLflow artifacts.
+5) Execute automated chaos drill and run pen test harness for training sandbox.
+6) Run budget watcher and validate cost enforcement triggers.
+7) Collect all artifacts via scripts/collect_evidence.sh and complete sign-off checklist.
+
+Sign-off: Platform, SRE, Security and ML leads must check all acceptance criteria and sign.
+
*** End Patch
*** End Patch
