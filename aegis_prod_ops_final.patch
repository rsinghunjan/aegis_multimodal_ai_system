*** Begin Patch
*** Add File: k8s/serving/kserve_prod_batched.yaml
+apiVersion: serving.kserve.io/v1beta1
+kind: InferenceService
+metadata:
+  name: aegis-model-prod
+  namespace: aegis
+  annotations:
+    # Knative/KServe concurrency target and request/response tuning
+    autoscaling.knative.dev/target: "100"
+    serving.kserve.io/enable-batcher: "true"
+    serving.kserve.io/request-batch-size: "16"
+    serving.kserve.io/request-batch-max-delay: "50ms"
+spec:
+  predictor:
+    serviceAccountName: aegis-model-sa
+    timeout: 60s
+    container:
+      image: ghcr.io/yourorg/aegis-model-server:latest
+      name: model-server
+      args:
+        - "--model-store"
+        - "s3://MODEL_BUCKET/model-archives/aegis-model/"
+        - "--max-concurrency=16"
+        - "--batching=true"
+      resources:
+        requests:
+          cpu: "500m"
+          memory: "1Gi"
+        limits:
+          cpu: "2000m"
+          memory: "8Gi"
+          nvidia.com/gpu: "1"
+      env:
+        - name: S3_ENDPOINT
+          value: "https://s3.example"
+        - name: S3_BUCKET
+          value: "MODEL_BUCKET"
+
+# Notes:
+# - Adjust request-batch-size/ max-delay to your model latency characteristics.
+# - Ensure the model-server image supports server-side batching and concurrency tuning.
+
*** End Patch
*** Begin Patch
*** Add File: k8s/autoscale/gpu_hpa_custom_metric.yaml
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: aegis-model-gpu-hpa
+  namespace: aegis
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: aegis-model-server
+  minReplicas: 1
+  maxReplicas: 20
+  metrics:
+  - type: Resource
+    resource:
+      name: cpu
+      target:
+        type: Utilization
+        averageUtilization: 70
+  - type: Pods
+    pods:
+      metric:
+        name: nvidia_gpu_utilization
+      target:
+        type: AverageValue
+        averageValue: "75"
+
+# Requirements:
+# - prometheus-adapter must expose a custom metric "nvidia_gpu_utilization" backed by DCGM exporter.
+# - Deployment name should match your model server deployment.
+
*** End Patch
*** Begin Patch
*** Add File: monitoring/prometheus-adapter-configmap.yaml
+apiVersion: v1
+kind: ConfigMap
+
î€€
